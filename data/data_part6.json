[
  {
        "key": "LUCENE-2675",
        "summary": "Add support for 3.0 indexes in 2.9 branch",
        "description": "There was a lot of user requests to be able to read Lucene 3.0 indexes also with 2.9. This would make the migration easier. There is no problem in doing that, as the new stored fields version in Lucene 3.0 is only used to mark a segment's stored fields file as no longer containing compressed fields. But index format did not really change. This patch simply allows FieldsReader to pass a Lucene 3.0 version number, but still writes segments in 2.9 format (as you could suddenly turn on compression for added documents).\n\nI added ZIP files for 3.0 indexes for TestBackwards. Without the patch it does not pass, as FieldsReader complains about incorrect version number (although it could read the file easily). If we would release maybe a 2.9.4 release of Lucene we should include that patch.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-763",
        "summary": "LuceneDictionary skips first word in enumeration",
        "description": "The current code for LuceneDictionary will always skip the first word of the TermEnum. The reason is that it doesn't initially retrieve TermEnum.term - its first call is to TermEnum.next, which moves it past the first term (line 76).\nTo see this problem cause a failure, add this test to TestSpellChecker:\nsimilar = spellChecker.suggestSimilar(\"eihgt\",2);\n      assertEquals(1, similar.length);\n      assertEquals(similar[0], \"eight\");\n\nBecause \"eight\" is the first word in the index, it will fail.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1508",
        "summary": "CartesianTierPlotter fieldPrefix should be configurable",
        "description": "CartesianTierPlotter field prefix is currrently hardcoded to \"_localTier\" -- this should be configurable",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1094",
        "summary": "Exception in DocumentsWriter.addDocument can corrupt stored fields file (fdt)",
        "description": "DocumentsWriter writes the number of stored fields, up front, into the\nfdtLocal buffer.  Then, as each field is processed, it writes each\nstored field into this buffer.  When the document is done, in a\nfinally clause, it flushes the buffer to the real fdt file in the\nDirectory.\n\nThe problem is, if an exception is hit, that number of stored fields\ncan be too high, which corrupts the fdt file.\n\nThe solution is to not write it up front, and instead write only the\nnumber of fields we actually saw.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3662",
        "summary": "extend LevenshteinAutomata to support transpositions as primitive edits",
        "description": "This would be a nice improvement for spell correction: currently a transposition counts as 2 edits,\nwhich means users of DirectSpellChecker must use larger values of n (e.g. 2 instead of 1) and \nlarger priority queue sizes, plus some sort of re-ranking with another distance measure for good results.\n\nInstead if we can integrate \"chapter 7\" of http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652 \nthen you can just build an alternative DFA where a transposition is only a single edit \n(http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)\n\nAccording to the benchmarks in the original paper, the performance for LevT looks to be very similar to Lev.\n\nSupport for this is now in moman (https://bitbucket.org/jpbarrette/moman/) thanks to Jean-Philippe \nBarrette-LaPierre.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2123",
        "summary": "Move FuzzyQuery rewrite as separate RewriteMode into MTQ, was: Highlighter fails to highlight FuzzyQuery",
        "description": "As FuzzyQuery does not allow to change the rewrite mode, highlighter fails with UOE in flex since LUCENE-2110, because it changes the rewrite mode to Boolean query. The fix is: Allow MTQ to change rewrite method and make FUZZY_REWRITE public for that.\n\nThe rewrite mode will live in MTQ as TOP_TERMS_SCORING_BOOLEAN_REWRITE. Also the code will be refactored to make heavy reuse of term enumeration code and only plug in the PQ for filtering the top terms.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1528",
        "summary": "Add support for Ideographic Space to the queryparser - also know as fullwith space and wide-space",
        "description": "The Ideographic Space is a space character that is as wide as a normal CJK character cell.\nIt is also known as wide-space or fullwith space.This type of space is used in CJK languages.\n\nThis patch adds support for the wide space, making the queryparser component more friendly\nto queries that contain CJK text.\n\nReference:\n'http://en.wikipedia.org/wiki/Space_(punctuation)' - see Table of spaces, char U+3000.\n\nI also added a new testcase that fails before the patch.\nAfter the patch is applied all junits pass.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1257",
        "summary": "Port to Java5",
        "description": "For my needs I've updated Lucene so that it uses Java 5 constructs. I know Java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. This patch against the trunk includes :\n\n- most obvious generics usage (there are tons of usages of sets, ... Those which are commonly used have been generified)\n- PriorityQueue generification\n- replacement of indexed for loops with for each constructs\n- removal of unnececessary unboxing\n\nThe code is to my opinion much more readable with those features (you actually *know* what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms.\n\nNote that this patch also includes an interface for the Query class. This has been done for my company's needs for building custom Query classes which add some behaviour to the base Lucene queries. It prevents multiple unnnecessary casts. I know this introduction is not wanted by the team, but it really makes our developments easier to maintain. If you don't want to use this, replace all /Queriable/ calls with standard /Query/.\n\n",
        "label": "NUG",
        "classified": "OTHER",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1143",
        "summary": "DocumentsWriter.abort fails to clear docStoreOffset",
        "description": "I hit this in working on LUCENE-1044.\n\nIf you disk full event during flush, then DocumentsWriter will abort\n(clear all buffered docs).  Then, if you then add another doc or two,\nand then close your writer, and this time succeed in flushing (say\nbecause it's only a couple buffered docs so the resulting segment is\nsmaller), you can flush a corrupt segment (that incorrectly has a\nnon-zero docStoreOffset).\n\nI modified the TestConcurrentMergeScheduler test to show this bug.\nI'll attach a patch shortly.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1936",
        "summary": "Remove deprecated charset support from Greek and Russian analyzers",
        "description": "This removes the deprecated support for custom charsets.\n\nOne thing I found is that once these charsets are removed, RussianLowerCaseFilter is the same as LowerCaseFilter.\nSo I marked it deprecated to be removed in 3.1\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1523",
        "summary": "isOpen needs to be accessible by subclasses of Directory",
        "description": "The Directory abstract class has a member variable named isOpen which is package accessible. The usage of the variable is such that it should be readable and must be writable (in order to implement close())  by any concrete implementation of directory. Because of the current accessibility of this variable is is not possible to create a Directory implementation that is not also in the org.apache.lucene.store.\n\nI propose that either the isOpen variable either needs to be declared protected or that there should be getter/setter methods that are protected.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1316",
        "summary": "Avoidable synchronization bottleneck in MatchAlldocsQuery$MatchAllScorer",
        "description": "The isDeleted() method on IndexReader has been mentioned a number of times as a potential synchronization bottleneck. However, the reason this  bottleneck occurs is actually at a higher level that wasn't focused on (at least in the threads I read).\n\nIn every case I saw where a stack trace was provided to show the lock/block, higher in the stack you see the MatchAllScorer.next() method. In Solr paricularly, this scorer is used for \"NOT\" queries. We saw incredibly poor performance (order of magnitude) on our load tests for NOT queries, due to this bottleneck. The problem is that every single document is run through this isDeleted() method, which is synchronized. Having an optimized index exacerbates this issues, as there is only a single SegmentReader to synchronize on, causing a major thread pileup waiting for the lock.\n\nBy simply having the MatchAllScorer see if there have been any deletions in the reader, much of this can be avoided. Especially in a read-only environment for production where you have slaves doing all the high load searching.\n\nI modified line 67 in the MatchAllDocsQuery\nFROM:\n  if (!reader.isDeleted(id)) {\nTO:\n  if (!reader.hasDeletions() || !reader.isDeleted(id)) {\n\nIn our micro load test for NOT queries only, this was a major performance improvement.  We also got the same query results. I don't believe this will improve the situation for indexes that have deletions. \n\nPlease consider making this adjustment for a future bug fix release.\n\n\n\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-900",
        "summary": "Enable Java asserts in the Junit tests",
        "description": "For background see http://www.mail-archive.com/java-dev@lucene.apache.org/msg10307.html",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2532",
        "summary": "improve test coverage of multi-segment indices",
        "description": "Simple patch that adds a test-only helper class, RandomIndexWriter, that lets you add docs, but it will randomly do things like use a different merge policy/scheduler, flush by doc count instead of RAM, flush randomly (so we get multi-segment indices) but also randomly optimize in the end (so we also sometimes test single segment indices).",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2831",
        "summary": "Revise Weight#scorer & Filter#getDocIdSet API to pass Readers context",
        "description": "Spinoff from LUCENE-2694 - instead of passing a reader into Weight#scorer(IR, boolean, boolean) we should / could revise the API and pass in a struct that has parent reader, sub reader, ord of that sub. The ord mapping plus the context with its parent would make several issues way easier. See LUCENE-2694, LUCENE-2348 and LUCENE-2829 to name some.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-695",
        "summary": "Improve BufferedIndexInput.readBytes() performance",
        "description": "During a profiling session, I discovered that BufferedIndexInput.readBytes(),\nthe function which reads a bunch of bytes from an index, is very inefficient\nin many cases. It is efficient for one or two bytes, and also efficient\nfor a very large number of bytes (e.g., when the norms are read all at once);\nBut for anything in between (e.g., 100 bytes), it is a performance disaster.\nIt can easily be improved, though, and below I include a patch to do that.\n\nThe basic problem in the existing code was that if you ask it to read 100\nbytes, readBytes() simply calls readByte() 100 times in a loop, which means\nwe check byte after byte if the buffer has another character, instead of just\nchecking once how many bytes we have left, and copy them all at once.\n\nMy version, attached below, copies these 100 bytes if they are available at\nbulk (using System.arraycopy), and if less than 100 are available, whatever\nis available gets copied, and then the rest. (as before, when a very large\nnumber of bytes is requested, it is read directly into the final buffer).\n\nIn my profiling, this fix caused amazing performance\nimprovement: previously, BufferedIndexInput.readBytes() took as much as 25%\nof the run time, and after the fix, this was down to 1% of the run time! However, my scenario is *not* the typical Lucene code, but rather a version of Lucene with added payloads, and these payloads average at 100 bytes, where the original readBytes() did worst. I expect that my fix will have less of an impact on \"vanilla\" Lucene, but it still can have an impact because it is used for things like reading fields. (I am not aware of a standard Lucene benchmark, so I can't provide benchmarks on a more typical case).\n\nIn addition to the change to readBytes(), my attached patch also adds a new\nunit test to BufferedIndexInput (which previously did not have a unit test).\nThis test simulates a \"file\" which contains a predictable series of bytes, and\nthen tries to read from it with readByte() and readButes() with various\nsizes (many thousands of combinations are tried) and see that exactly the\nexpected bytes are read. This test is independent of my new readBytes()\ninplementation, and can be used to check the old implementation as well.\n\nBy the way, it's interesting that BufferedIndexOutput.writeBytes was already efficient, and wasn't simply a loop of writeByte(). Only the reading code was inefficient. I wonder why this happened.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1768",
        "summary": "NumericRange support for new query parser",
        "description": "It would be good to specify some type of \"schema\" for the query parser in future, to automatically create NumericRangeQuery for different numeric types? It would then be possible to index a numeric value (double,float,long,int) using NumericField and then the query parser knows, which type of field this is and so it correctly creates a NumericRangeQuery for strings like \"[1.567..*]\" or \"(1.787..19.5]\".\n\nThere is currently no way to extract if a field is numeric from the index, so the user will have to configure the FieldConfig objects in the ConfigHandler. But if this is done, it will not be that difficult to implement the rest.\n\nThe only difference between the current handling of RangeQuery is then the instantiation of the correct Query type and conversion of the entered numeric values (simple Number.valueOf(...) cast of the user entered numbers). Evenerything else is identical, NumericRangeQuery also supports the MTQ rewrite modes (as it is a MTQ).\n\nAnother thing is a change in Date semantics. There are some strange flags in the current parser that tells it how to handle dates.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2134",
        "summary": "[PATCH] ant-task \"javadocs-all\" fails with OutOfMemoryError",
        "description": "Hi all,\n\nthe current nightly build's \"ant dist\" fails with an OutOfMemoryError at ant task javadocs-all (see below).\nApparently javadoc needs more memory.\n\nA similar case has been reported in HADOOP-5561 (add a maxmemory statement to the javadoc task), and I propose the same change for Lucene as well.\n\nCheers,\nChristian\n\n\njavadocs-all:\n  [javadoc] Generating Javadoc\n  [javadoc] Javadoc execution\n  [javadoc] Loading source files for package org.apache.lucene...\n  [javadoc] Loading source files for package org.apache.lucene.analysis...\n(...)\n  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.config...\n  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.nodes...\n  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.parser...\n  [javadoc] Loading source files for package org.apache.lucene.queryParser.standard.processors...\n  [javadoc] Constructing Javadoc information...\n  [javadoc] Standard Doclet version 1.6.0_15\n  [javadoc] Building tree for all the packages and classes...\n\n (takes a long time here until OOME)\n\n  [javadoc] java.lang.OutOfMemoryError: Java heap space\n  [javadoc] java.lang.OutOfMemoryError: Java heap space\n  [javadoc] \tat java.lang.Throwable.getStackTraceElement(Native Method)\n  [javadoc] \tat java.lang.Throwable.getOurStackTrace(Throwable.java:591)\n  [javadoc] \tat java.lang.Throwable.printStackTrace(Throwable.java:462)\n  [javadoc] \tat java.lang.Throwable.printStackTrace(Throwable.java:451)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:103)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.AbstractMemberBuilder.build(AbstractMemberBuilder.java:56)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildMemberSummary(ClassBuilder.java:279)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n  [javadoc] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n  [javadoc] \tat java.lang.reflect.Method.invoke(Method.java:597)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n  [javadoc] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n  [javadoc] \tat java.lang.reflect.Method.invoke(Method.java:597)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.build(ClassBuilder.java:108)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.HtmlDoclet.generateClassFiles(HtmlDoclet.java:155)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.AbstractDoclet.generateClassFiles(AbstractDoclet.java:164)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.AbstractDoclet.startGeneration(AbstractDoclet.java:106)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.AbstractDoclet.start(AbstractDoclet.java:64)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.HtmlDoclet.start(HtmlDoclet.java:42)\n  [javadoc] \tat com.sun.tools.doclets.standard.Standard.start(Standard.java:23)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n  [javadoc] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n  [javadoc] \tat java.lang.reflect.Method.invoke(Method.java:597)\n  [javadoc] \tat com.sun.tools.javadoc.DocletInvoker.invoke(DocletInvoker.java:269)\n  [javadoc] java.lang.OutOfMemoryError: Java heap space\n  [javadoc] \tat java.util.Arrays.copyOfRange(Arrays.java:3209)\n  [javadoc] \tat java.lang.String.<init>(String.java:215)\n  [javadoc] \tat com.sun.tools.javac.util.Convert.utf2string(Convert.java:131)\n  [javadoc] \tat com.sun.tools.javac.util.Name.toString(Name.java:164)\n  [javadoc] \tat com.sun.tools.javadoc.ClassDocImpl.getClassName(ClassDocImpl.java:341)\n  [javadoc] \tat com.sun.tools.javadoc.TypeMaker.getTypeName(TypeMaker.java:100)\n  [javadoc] \tat com.sun.tools.javadoc.ParameterizedTypeImpl.parameterizedTypeToString(ParameterizedTypeImpl.java:117)\n  [javadoc] \tat com.sun.tools.javadoc.TypeMaker.getTypeString(TypeMaker.java:121)\n  [javadoc] \tat com.sun.tools.javadoc.ExecutableMemberDocImpl.makeSignature(ExecutableMemberDocImpl.java:217)\n  [javadoc] \tat com.sun.tools.javadoc.ExecutableMemberDocImpl.signature(ExecutableMemberDocImpl.java:198)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.getMemberKey(VisibleMemberMap.java:485)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.access$1000(VisibleMemberMap.java:28)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.isOverridden(VisibleMemberMap.java:442)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.addMembers(VisibleMemberMap.java:316)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.build(VisibleMemberMap.java:278)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap$ClassMembers.access$100(VisibleMemberMap.java:230)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.util.VisibleMemberMap.<init>(VisibleMemberMap.java:93)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.ConstructorWriterImpl.<init>(ConstructorWriterImpl.java:38)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.WriterFactoryImpl.getConstructorWriter(WriterFactoryImpl.java:129)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.WriterFactoryImpl.getMemberSummaryWriter(WriterFactoryImpl.java:141)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.MemberSummaryBuilder.init(MemberSummaryBuilder.java:104)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.MemberSummaryBuilder.getInstance(MemberSummaryBuilder.java:64)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.BuilderFactory.getMemberSummaryBuilder(BuilderFactory.java:191)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.ClassWriterImpl.navSummaryLinks(ClassWriterImpl.java:474)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.ClassWriterImpl.printSummaryDetailLinks(ClassWriterImpl.java:456)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.HtmlDocletWriter.navLinks(HtmlDocletWriter.java:462)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.ClassWriterImpl.writeFooter(ClassWriterImpl.java:146)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassFooter(ClassBuilder.java:330)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n  [javadoc] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n  [javadoc] \tat java.lang.reflect.Method.invoke(Method.java:597)\n  [javadoc] java.lang.OutOfMemoryError: Java heap space\n  [javadoc] \tat java.util.LinkedHashMap.createEntry(LinkedHashMap.java:424)\n  [javadoc] \tat java.util.LinkedHashMap.addEntry(LinkedHashMap.java:406)\n  [javadoc] \tat java.util.HashMap.put(HashMap.java:385)\n  [javadoc] \tat sun.util.resources.OpenListResourceBundle.loadLookup(OpenListResourceBundle.java:118)\n  [javadoc] \tat sun.util.resources.OpenListResourceBundle.loadLookupTablesIfNecessary(OpenListResourceBundle.java:97)\n  [javadoc] \tat sun.util.resources.OpenListResourceBundle.handleGetObject(OpenListResourceBundle.java:58)\n  [javadoc] \tat sun.util.resources.TimeZoneNamesBundle.handleGetObject(TimeZoneNamesBundle.java:59)\n  [javadoc] \tat java.util.ResourceBundle.getObject(ResourceBundle.java:378)\n  [javadoc] \tat java.util.ResourceBundle.getObject(ResourceBundle.java:381)\n  [javadoc] \tat java.util.ResourceBundle.getStringArray(ResourceBundle.java:361)\n  [javadoc] \tat sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:100)\n  [javadoc] \tat sun.util.TimeZoneNameUtility.retrieveDisplayNames(TimeZoneNameUtility.java:81)\n  [javadoc] \tat java.util.TimeZone.getDisplayNames(TimeZone.java:399)\n  [javadoc] \tat java.util.TimeZone.getDisplayName(TimeZone.java:350)\n  [javadoc] \tat java.util.Date.toString(Date.java:1025)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.markup.HtmlDocWriter.today(HtmlDocWriter.java:337)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.HtmlDocletWriter.printHtmlHeader(HtmlDocletWriter.java:281)\n  [javadoc] \tat com.sun.tools.doclets.formats.html.ClassWriterImpl.writeHeader(ClassWriterImpl.java:122)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassHeader(ClassBuilder.java:164)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n  [javadoc] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n  [javadoc] \tat java.lang.reflect.Method.invoke(Method.java:597)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.buildClassDoc(ClassBuilder.java:124)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  [javadoc] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n  [javadoc] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n  [javadoc] \tat java.lang.reflect.Method.invoke(Method.java:597)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.ClassBuilder.invokeMethod(ClassBuilder.java:101)\n  [javadoc] \tat com.sun.tools.doclets.internal.toolkit.builders.AbstractBuilder.build(AbstractBuilder.java:90)\n  [javadoc] java.lang.OutOfMemoryError: Java heap space\n  [javadoc] java.lang.OutOfMemoryError: Java heap space\n  [javadoc] java.lang.OutOfMemoryError: Java heap space\n  [javadoc] java.lang.OutOfMemoryError: Java heap space\n  [javadoc] java.lang.OutOfMemoryError: Java heap space\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3906",
        "summary": "allow specifying -Dbootclasspath for javac/javadocs",
        "description": "So that you can compile/javadoc against the actual target JRE libraries\neven if you have a newer compiler.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-703",
        "summary": "Change QueryParser to use ConstantScoreRangeQuery in preference to RangeQuery by default",
        "description": "Change to QueryParser to default to using new ConstantScoreRangeQuery in preference to RangeQuery\nfor range queries. This implementation is generally preferable because it \na) Runs faster \nb) Does not have the scarcity of range terms unduly influence score \nc) avoids any \"TooManyBooleanClauses\" exception.\n\nHowever, if applications really need to use the old-fashioned RangeQuery and the above\npoints are not required then the  \"useOldRangeQuery\" property can be used to revert to old behaviour.\n\nThe patch includes extra Junit tests for this flag and all other Junit tests pass",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1444",
        "summary": "Broken javadocs->site docs links",
        "description": "See the java-dev mailing list discussion: [http://www.nabble.com/Broken-javadocs-%3Esite-docs-links-to20369092.html].\n\nWhen the Lucene Java website transitioned to versioning some of the documentation, links from some javadocs were not modified to follow the resources.  I found broken links to gettingstarted.html and queryparsersyntax.html.  Here is one example, to gettingstarted.html (the link text is \"demo\"): \n\n[http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/document/package-summary.html]\n\nThe attached patch converts absolute URLs from javadocs to versioned docs to be relative, and modifies the \"javadocs-all\" target in build.xml to add a path element named \"all\", so that both versions of the javadocs (all: core+contrib; and separated: core, contribs) can use the same relative URLs.  Adding a path element to the \"javadocs-all\" target is necessary because currently the \"all\" javadocs have one fewer path element than the separated javadocs.\n\nI left as-is one absolute URL, in the o.a.l.index.SegmentInfos javadocs, to fileformats.html, because SegmentInfos is a package-private class, and the javadocs targets in build.xml only generate javadocs for public classes.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3087",
        "summary": "highlighting exact phrase with overlapping tokens fails.",
        "description": "Fields with overlapping token are not highlighted in search results when searching exact phrases, when using TermVector.WITH_OFFSET.\n\nThe document builded in MemoryIndex for highlight does not preserve positions of tokens in this case. Overlapping tokens get \"flattened\" (position increment always set to 1), the spanquery used for searching relevant fragment will fail to identify the correct token sequence because the position shift.\n\nI corrected this by adding a position increment calculation in sub class StoredTokenStream. I added junit test covering this case.\n\nI used the eclipse codestyle from trunk, but style add quite a few format differences between repository and working copy files. I tried to reduce them, but some linewrapping rules still doesn't match.\n\nCorrection patch joined",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2634",
        "summary": "IndexReader.isCurrent() lies if documents were only removed by latest commit",
        "description": "Usecase is as following:\n\n1. Get indexReader via indexWriter.\n2. Delete documents by Term via indexWriter. \n3. Commit indexWriter.\n4. indexReader.isCurrent() returns true.\n\nUsually there is a check if index reader is current. If not then it is reopened (re-obtained via writer or ect.). But this causes the problem when documents can still be found through the search after deletion.\nTestcase is attached.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3205",
        "summary": "remove MultiTermQuery get/inc/clear totalNumberOfTerms",
        "description": "This method is not correct if the index has more than one segment.\nIts also not thread safe, and it means calling query.rewrite() modifies\nthe original query. \n\nAll of these things add up to confusion, I think we should remove this \nfrom multitermquery, the only thing that \"uses\" it is the NRQ tests, which \nconditionalizes all the asserts anyway.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3593",
        "summary": "Add a filter returning all document without a value in a field",
        "description": "In some situations it would be useful to have a Filter that simply returns all document that either have at least one or no value in a certain field. We don't have something like that out of the box and adding it seems straight forward.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2694",
        "summary": "MTQ rewrite + weight/scorer init should be single pass",
        "description": "Spinoff of LUCENE-2690 (see the hacked patch on that issue)...\n\nOnce we fix MTQ rewrite to be per-segment, we should take it further and make weight/scorer init also run in the same single pass as rewrite.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3146",
        "summary": "IndexReader.setNorms is no op if one of the field instances omits norms",
        "description": "If I add two documents to an index w/ same field, and one of them omit norms, then IndexReader.setNorms is no-op. I'll attach a patch w/ test case",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-845",
        "summary": "If you \"flush by RAM usage\" then IndexWriter may over-merge",
        "description": "I think a good way to maximize performance of Lucene's indexing for a\ngiven amount of RAM is to flush (writer.flush()) the added documents\nwhenever the RAM usage (writer.ramSizeInBytes()) has crossed the max\nRAM you can afford.\n\nBut, this can confuse the merge policy and cause over-merging, unless\nyou set maxBufferedDocs properly.\n\nThis is because the merge policy looks at the current maxBufferedDocs\nto figure out which segments are level 0 (first flushed) or level 1\n(merged from <mergeFactor> level 0 segments).\n\nI'm not sure how to fix this.  Maybe we can look at net size (bytes)\nof a segment and \"infer\" level from this?  Still we would have to be\nresilient to the application suddenly increasing the RAM allowed.\n\nThe good news is to workaround this bug I think you just need to\nensure that your maxBufferedDocs is less than mergeFactor *\ntypical-number-of-docs-flushed.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1003",
        "summary": "[PATCH] RussianAnalyzer's tokenizer skips numbers from input text,",
        "description": "RussianAnalyzer's tokenizer skips numbers from input text, so that resulting token stream miss numbers. Problem can be solved by adding numbers to RussianCharsets.UnicodeRussian. See test case below  for details.\n\n{code:title=TestRussianAnalyzer.java|borderStyle=solid}\n\npublic class TestRussianAnalyzer extends TestCase {\n\n  Reader reader = new StringReader(\"text 1000\");\n\n  // test FAILS\n  public void testStemmer() {\n    testAnalyzer(new RussianAnalyzer());\n  }\n\n  // test PASSES\n  public void testFixedRussianAnalyzer() {\n    testAnalyzer(new RussianAnalyzer(getRussianCharSet()));\n  }\n\n  private void testAnalyzer(RussianAnalyzer analyzer) {\n    try {\n      TokenStream stream = analyzer.tokenStream(\"text\", reader);\n      assertEquals(\"text\", stream.next().termText());\n      assertNotNull(stream.next());\n    } catch (IOException e) {\n      fail(e.getMessage());\n    }\n  }\n\n  private char[] getRussianCharSet() {\n    int length = RussianCharsets.UnicodeRussian.length;\n    final char[] russianChars = new char[length + 10];\n\n    System\n        .arraycopy(RussianCharsets.UnicodeRussian, 0, russianChars, 0, length);\n    russianChars[length++] = '0';\n    russianChars[length++] = '1';\n    russianChars[length++] = '2';\n    russianChars[length++] = '3';\n    russianChars[length++] = '4';\n    russianChars[length++] = '5';\n    russianChars[length++] = '6';\n    russianChars[length++] = '7';\n    russianChars[length++] = '8';\n    russianChars[length] = '9';\n    return russianChars;\n  }\n}\n\n{code} ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1435",
        "summary": "CollationKeyFilter: convert tokens into CollationKeys encoded using IndexableBinaryStringTools",
        "description": "Converts each token into its CollationKey using the provided collator, and then encodes the CollationKey with IndexableBinaryStringTools, to allow it to be stored as an index term.\n\nThis will allow for efficient range searches and Sorts over fields that need collation for proper ordering.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2954",
        "summary": "CheckIndex prints wrong version number on 3.1 indexes (and posibly also in trunk)",
        "description": "When you run CheckIndex on an index created/updated with 3.1, it prints about the SegmentInfos:\n\n{noformat}\nSegments file=segments_g19 numSegments=5 version=-11 [Lucene 1.3 or prior]\n{noformat}\n\nWe should fix CheckIndex and also verify other cases where version numbers are printed out. In trunk the issue may be more complicated!",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1050",
        "summary": "SimpleFSLockFactory ignores error on deleting the lock file",
        "description": "Spinoff from here:\n\n    http://www.gossamer-threads.com/lists/lucene/java-user/54438\n\nThe Lock.release for SimpleFSLockFactory ignores the return value of lockFile.delete().  I plan to throw a new LockReleaseFailedException, subclassing from IOException, when this returns false.  This is a very minor change to backwards compatibility because all methods in Lucene that release a lock already throw IOException.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-642",
        "summary": "GData Server IndexComponent",
        "description": "New Feature added:\n\n-> Indexcomponent.\n-> Content extraction from entries.\n-> Custom content ext. strategies added.\n-> user defined index schema.\n-> extended gdata-config.xml schema (xsd)\n-> Indexcomponent UnitTests\n-> Spellchecking on some JavaDoc.\n\n##############\nNew jars included:\n\nnekoHTML.jar \nxercesImpl.jar\n\n@yonik: don't miss the '+' button to add directories :)",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2782",
        "summary": "Possible rare thread hazard in IW.commit",
        "description": "I was seeing a very rare intermittent failure in TestIndexWriter.testCommitThreadSafety.\n\nThe issue happens if one thread calls commit while another is flushing, and is exacerbated at high flush rates (eg maxBufferedDocs=2).  The thread doing commit will first flush, and then it syncs the files.  However in between those two, if other threads manage to add enough docs and trigger another flush, a 2nd new segment can sneak into the SegmentInfos before we sync.\n\nThis is normally harmless, in that it just means the commit includes a few more docs that had been added by other threads, so it's fine. But, it can mean that a committed segment references the still-open doc store files.  Our tests now catch this (I changed MockDirWrapper to throw an exception in this case), and so testCommitThreadSafety can fail with this exception.  If you hardwire the maxBufferedDocs to 2 it happens quite often.\n\nIt's not clear this is really a problem in real apps vs just our anal MockDirWrapper but I think we should fix it...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3273",
        "summary": "Convert Lucene Core tests over to a simple MockQueryParser",
        "description": "Most tests use Lucene Core's QueryParser for convenience.  We want to consolidate it into a QP module which we can't have as a dependency.  We should add a simple MockQueryParser which does String.split() on the query string, analyzers the terms and builds a BooleanQuery if necessary.  Any more complex Queries (such as phrases) should be done programmatically. ",
        "label": "NUG",
        "classified": "TEST",
        "type": ""
    },
    {
        "key": "LUCENE-435",
        "summary": "[PATCH] BufferedIndexOutput - optimized writeBytes() method",
        "description": "I have created a patch that optimize writeBytes metod:\n\n  public void writeBytes(byte[] b, int length) throws IOException {\n    if (bufferPosition > 0) // flush buffer\n      flush();\n \n    if (length < BUFFER_SIZE) {\n      flushBuffer(b, length);\n      bufferStart += length;\n    } else {\n      int pos = 0;\n      int size;\n      while (pos < length) {\n        if (length - pos < BUFFER_SIZE) {\n          size = length - pos;\n        } else {\n          size = BUFFER_SIZE;\n        }\n        System.arraycopy(b, pos, buffer, 0, size);\n        pos += size;\n        flushBuffer(buffer, size);\n        bufferStart += size;\n      }\n    }\n  }\n\nIts a much more faster now. I know that for indexing this not help much, but for copying files in the IndexStore this is so big improvement. Its about 400% faster that old implementation.\n\nThe patch was tested with 300MB data, \"ant test\" sucessfuly finished with no errors.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1326",
        "summary": "Inflater.end() method not always called in FieldsReader",
        "description": "\nWe've just found an insidious memory leak in our own application as we did not always call Deflater.end() and Inflater.end(). As documented here;\n\nhttp://bugs.sun.com/view_bug.do?bug_id=4797189\n\nThe non-heap memory that the native zlib code uses is not freed in a timely manner.\n\nFieldsWriter appears safe as no exception can be thrown between the Deflater's creation and end() as it uses a ByteArrayOutputStream\n\nFieldsReader, however, is not safe. In the event of a DataFormatException the call to end() will not occur.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2711",
        "summary": "BooleanScorer.nextDoc should also delegate to sub-scorer's bulk scoring method",
        "description": "BooleanScorer uses the bulk score methods of its sub scorers, asking them to score each chunk of 2048 docs.\n\nHowever, its .nextDoc fails to do this, instead manually walking through the sub's docs (calling .nextDoc()), which is slower (though this'd be tiny in practice).\n\nAs far as I can tell it should delegate to the bulk scorer just like it does in its bulk scorer method.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2473",
        "summary": "Clicking on the \"More Results\" link in luceneweb.war demo results in ArrayIndexOutOfBoundsException",
        "description": "Summary says it all.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2386",
        "summary": "IndexWriter commits unnecessarily on fresh Directory",
        "description": "I've noticed IndexWriter's ctor commits a first commit (empty one) if a fresh Directory is passed, w/ OpenMode.CREATE or CREATE_OR_APPEND. This seems unnecessarily, and kind of brings back an autoCommit mode, in a strange way ... why do we need that commit? Do we really expect people to open an IndexReader on an empty Directory which they just passed to an IW w/ create=true? If they want, they can simply call commit() right away on the IW they created.\n\nI ran into this when writing a test which committed N times, then compared the number of commits (via IndexReader.listCommits) and was surprised to see N+1 commits.\n\nTried to change doCommit to false in IW ctor, but it got IndexFileDeleter jumping on me .. so the change might not be that simple. But I think it's manageable, so I'll try to attack it (and IFD specifically !) back :).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1470",
        "summary": "Add TrieRangeFilter to contrib",
        "description": "According to the thread in java-dev (http://www.gossamer-threads.com/lists/lucene/java-dev/67807 and http://www.gossamer-threads.com/lists/lucene/java-dev/67839), I want to include my fast numerical range query implementation into lucene contrib-queries.\n\nI implemented (based on RangeFilter) another approach for faster\nRangeQueries, based on longs stored in index in a special format.\n\nThe idea behind this is to store the longs in different precision in index\nand partition the query range in such a way, that the outer boundaries are\nsearch using terms from the highest precision, but the center of the search\nRange with lower precision. The implementation stores the longs in 8\ndifferent precisions (using a class called TrieUtils). It also has support\nfor Doubles, using the IEEE 754 floating-point \"double format\" bit layout\nwith some bit mappings to make them binary sortable. The approach is used in\nrather big indexes, query times are even on low performance desktop\ncomputers <<100 ms (!) for very big ranges on indexes with 500000 docs.\n\nI called this RangeQuery variant and format \"TrieRangeRange\" query because\nthe idea looks like the well-known Trie structures (but it is not identical\nto real tries, but algorithms are related to it).\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2872",
        "summary": "Terms dict should block-encode terms",
        "description": "With PrefixCodedTermsReader/Writer we now encode each term standalone,\nie its bytes, metadata, details for postings (frq/prox file pointers),\netc.\n\nBut, this is costly when something wants to visit many terms but pull\nmetadata for only few (eg respelling, certain MTQs).  This is\nparticularly costly for sep codec because it has more metadata to\nstore, per term.\n\nSo instead I think we should block-encode all terms between indexed\nterm, so that the metadata is stored \"column stride\" instead.  This\nmakes it faster to enum just terms.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1423",
        "summary": "InstantiatedTermEnum#skipTo(Term) throws ArrayIndexOutOfBoundsException on empty index",
        "description": "{code}\njava.lang.ArrayIndexOutOfBoundsException: 0\n\tat org.apache.lucene.store.instantiated.InstantiatedTermEnum.skipTo(InstantiatedTermEnum.java:105)\n\tat org.apache.lucene.store.instantiated.TestEmptyIndex.termEnumTest(TestEmptyIndex.java:73)\n\tat org.apache.lucene.store.instantiated.TestEmptyIndex.testTermEnum(TestEmptyIndex.java:54)\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2014",
        "summary": "position increment bug: smartcn",
        "description": "If i use LUCENE_VERSION >= 2.9 with smart chinese analyzer, it will crash indexwriter with any reasonable amount of chinese text.\n\nits especially annoying because it happens in 2.9.1 RC as well.\n\nthis is because the position increments for tokens after stopwords are bogus:\n\nHere's an example (from test case), where the position increment should be 2, but is instead 91975314!\n\n{code}\n  public void testChineseStopWords2() throws Exception {\n    Analyzer ca = new SmartChineseAnalyzer(Version.LUCENE_CURRENT); /* will load stopwords */\n    String sentence = \"Title:San\"; // : is a stopword\n    String result[] = { \"titl\", \"san\"};\n    int startOffsets[] = { 0, 6 };\n    int endOffsets[] = { 5, 9 };\n    int posIncr[] = { 1, 2 };\n    assertAnalyzesTo(ca, sentence, result, startOffsets, endOffsets, posIncr);\n  }\n{code}\n\njunit.framework.AssertionFailedError: posIncrement 1 expected:<2> but was:<91975314>\n\tat junit.framework.Assert.fail(Assert.java:47)\n\tat junit.framework.Assert.failNotEquals(Assert.java:280)\n\tat junit.framework.Assert.assertEquals(Assert.java:64)\n\tat junit.framework.Assert.assertEquals(Assert.java:198)\n\tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:83)\n\t...\n\n\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1323",
        "summary": "MultiReader should make a private copy of the subReaders array",
        "description": "Spinoff from here:\n\n    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200806.mbox/%3C88F3F6A4-FBFB-43DF-890D-DB5F0D9A2461@gmail.com%3E\n\nBecause MultiReader just holds a reference to the array that was passed in, it's possible to hit scary exceptions (that look like index corruption) if that array is later altered eg by reopening some of the readers.\n\nThe fix is trivial: just make a private copy.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2385",
        "summary": "Move NoDeletionPolicy from benchmark to core",
        "description": "As the subject says, but I'll also make it a singleton + add some unit tests, as well as some documentation. I'll post a patch hopefully today.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2761",
        "summary": "specialize payload processing from of DocsAndPositionsEnum",
        "description": "In LUCENE-2760 i started working to try to improve the speed of a few spanqueries.\nIn general the trick there is to avoid processing positions if you dont have to.\n\nBut, we can improve queries that read lots of positions further by cleaning up SegmentDocsAndPositionsEnum, \nin nextPosition() this has no less than 3 payloads-related checks.\n\nhowever, a large majority of users/fields have no payloads at all.\nI think we should specialize this case into a separate implementation and speed up the common case.\n\nedit: dyslexia with the jira issue number.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-461",
        "summary": "StandardTokenizer splitting all of Korean words into separate characters",
        "description": "StandardTokenizer splits all those Korean words inth separate character tokens. For example, \"?????\" is one Korean word that means \"Hello\", but StandardAnalyzer separates it into five tokens of \"?\", \"?\", \"?\", \"?\", \"?\".",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-591",
        "summary": "Add meta keywords to HTMLParser",
        "description": "\nIt would be good if the HTMLParser could give us the keywords specified in the meta tags, so that we can index them.\n\nIn HTMLParser.jj:\n\n  void addMetaTag() {\n      metaTags.setProperty(currentMetaTag, currentMetaContent);\n      currentMetaTag = null;\n      currentMetaContent = null;\n      return;\n  }\n\nOne way to do it:\n\n  void addMetaTag() throws IOException {\n      metaTags.setProperty(currentMetaTag, currentMetaContent);\n      if (currentMetaTag.equalsIgnoreCase(\"keywords\")) {\n          pipeOut.write(currentMetaContent);\n      }\n      currentMetaTag = null;\n      currentMetaContent = null;\n      return;\n  }\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1356",
        "summary": "Allow easy extensions of TopDocCollector",
        "description": "TopDocCollector's members and constructor are declared either private or package visible. It makes it hard to extend it as if you want to extend it you can reuse its *hq* and *totatlHits* members, but need to define your own. It also forces you to override getTotalHits() and topDocs().\nBy changing its members and constructor (the one that accepts a PQ) to protected, we allow users to extend it in order to get a different view of 'top docs' (like TopFieldCollector does), but still enjoy its getTotalHits() and topDocs() method implementations.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-657",
        "summary": "FuzzyQuery should not be final",
        "description": "I am trying to extend the FuzzyQuery to further filter the TermEnum. (I am indexing stem forms and original forms, but I only want to match original forms with a fuzzy term, otherwise I get to much noise). However, FuzzyQuery is a final class and I cannot extend it.  \n\nAs discussed in the mailing list (http://www.gossamer-threads.com/lists/lucene/java-dev/38756), we want to make the private variables and inner classes protected.\n\nI am attaching a patch for FuzzyQuery.java that implements this. I ran all unit tests and they passed without errors.\n\nAndreas.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1611",
        "summary": "Do not launch new merges if IndexWriter has hit OOME",
        "description": "if IndexWriter has hit OOME, it defends itself by refusing to commit changes to the index, including merges.  But this can lead to infinite merge attempts because we fail to prevent starting a merge.\n\nSpinoff from http://www.nabble.com/semi-infinite-loop-during-merging-td23036156.html.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2168",
        "summary": "benchmark alg can't handle negative relative priority",
        "description": "We can now set thread relative priority when we run BG tasks... but if you use a negative number it's parsing it as 0.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3061",
        "summary": "Open IndexWriter API to allow custom MergeScheduler implementation",
        "description": "IndexWriter's getNextMerge() and merge(OneMerge) are package-private, which makes it impossible for someone to implement his own MergeScheduler. We should open up these API, as well as any other that can be useful for custom MS implementations.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1870",
        "summary": "dists include analyzer contrib in src dist but not binary dist",
        "description": "dists include analyzer contrib in src dist but not binary dist",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3682",
        "summary": "Add deprecated 'transition' api for Document/Field",
        "description": "I think for 4.0 we should have a deprecated transition api for Field so you can do new Field(..., Field.Store.xxx, Field.Index.yyy) like before.\n\nThese combinations would just be some predefined fieldtypes that are used behind the scenes if you use these deprecated ctors\n\nSure it wouldn't be 'totally' backwards binary compat for Field.java, but why must it be all or nothing? I think this would eliminate a big\nhurdle for people that want to check out 4.x",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1210",
        "summary": "IndexWriter & ConcurrentMergeScheduler deadlock case if starting a merge hits an exception",
        "description": "If you're using CMS (the default) and mergeInit hits an exception (eg\nOOME), we are not properly clearing IndexWriter's internal tracking of\nrunning merges.  This causes IW.close() to hang while it incorrectly\nwaits for these non-started merges to finish.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2920",
        "summary": "Deprecate and remove ShingleMatrixFilter",
        "description": "Spin-off from LUCENE-1391: This filter is unmainatined and no longer up-to-date, has bugs nobody understands and does not work with attributes.\n\nThis issue deprecates it as of Lucene 3.1 and removes it from trunk.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1828",
        "summary": "MemoryIndex doesn't call TokenStream.reset() and TokenStream.end()",
        "description": "MemoryIndex from contrib/memory does not honor the contract for a consumer of a TokenStream\n\nwill work up a patch right quick",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3143",
        "summary": "SegmentMerger should assert .del and .s* files are not passed to createCompoundFile",
        "description": "Spinoff from LUCENE-3126. SegmentMerger.createCompoundFile does not document that it should not receive files that are not included in the .cfs, such as .del and .s* (separate norms). Today, that method is called from code which ensures that, but we should:\n# Add some documentation to clarify that.\n# Add some asserts so that if a test (or other code, running w/ -ea) does that, we catch it.\n\nWill post a patch soon",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2269",
        "summary": "don't download/extract 20,000 files when doing the build",
        "description": "When you build lucene, it downloads and extracts some data for contrib/benchmark, especially the 20,000+ files for the reuters corpus.\nthis is only needed for one test, and these 20,000 files drive IDEs and such crazy.\ninstead of doing this by default, we should only download/extract data if you specifically ask (like wikipedia, collation do, etc)\n\nfor the qualityrun test, instead use a linedoc formatted 587-line text file, similar to reuters.first20.lines.txt already used by benchmark.\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3636",
        "summary": "make it possible to use searchermanager with distributed stats",
        "description": "LUCENE-3555 added explicit stats methods to indexsearcher, but you must\nsubclass to override this (e.g. populate with distributed stats).\n\nIts also impossible to then do this with SearcherManager.\n\nOne idea is make this a factory method (or similar) on IndexSearcher instead,\nso you don't need to subclass it to override.\n\nThen you can initialize this in a SearcherWarmer, except there is currently\na lot of hair in what this warming should be. This is a prime example where\nSearcher has different meaning from Reader, we should clean this up.\n\nOtherwise, lets make NRT/SearcherManager subclassable in such a way that \nyou can return a custom indexsearcher.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2150",
        "summary": "Build should enable unchecked warnings in javac",
        "description": "Just have to uncomment this:\n{code}\n        <!-- for generics in Java 1.5: -->\n        <!--<compilerarg line=\"-Xlint:unchecked\"/>-->\n{code}\nin common-build.xml.  Test & core are clean, but contrib still has many warnings.  Either we fix contrib with this issue, or, conditionalize this (anyone anty who can do this?) so contrib is off until we can fix it.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2055",
        "summary": "Fix buggy stemmers and Remove duplicate analysis functionality",
        "description": "would like to remove stemmers in the following packages, and instead in their analyzers use a SnowballStemFilter instead.\n\n* analyzers/fr\n* analyzers/nl\n* analyzers/ru\n\nbelow are excerpts from this code where they proudly proclaim they use the snowball algorithm.\nI think we should delete all of this custom stemming code in favor of the actual snowball package.\n\n\n{noformat}\n/**\n * A stemmer for French words. \n * <p>\n * The algorithm is based on the work of\n * Dr Martin Porter on his snowball project<br>\n * refer to http://snowball.sourceforge.net/french/stemmer.html<br>\n * (French stemming algorithm) for details\n * </p>\n */\n\npublic class FrenchStemmer {\n\n/**\n * A stemmer for Dutch words. \n * <p>\n * The algorithm is an implementation of\n * the <a href=\"http://snowball.tartarus.org/algorithms/dutch/stemmer.html\">dutch stemming</a>\n * algorithm in Martin Porter's snowball project.\n * </p>\n */\npublic class DutchStemmer {\n\n/**\n * Russian stemming algorithm implementation (see http://snowball.sourceforge.net for detailed description).\n */\nclass RussianStemmer\n{noformat}\n\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3835",
        "summary": "MergeThread throws unchecked exceptions from toString()",
        "description": "This causes nearly all thread-dumping routines to fail and in the effect obscure the original problem. I think this\nshould return a string (always), possibly indicating the underlying writer has been closed or something.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1666",
        "summary": "Constants causing NullPointerException when fetching metadata \"Implementation Version\" in MANIFEST",
        "description": "If the MANIFEST.MF file does not contain the metadata IMPLEMENTATION_VERSION, a null value is returned, causing NullPointerException during commit:\n\nException in thread \"main\" java.lang.NullPointerException\n\tat org.apache.lucene.store.IndexOutput.writeString(IndexOutput.java:109)\n\tat org.apache.lucene.store.IndexOutput.writeStringStringMap(IndexOutput.java:229)\n\tat org.apache.lucene.index.SegmentInfo.write(SegmentInfo.java:558)\n\tat org.apache.lucene.index.SegmentInfos.write(SegmentInfos.java:337)\n\tat org.apache.lucene.index.SegmentInfos.prepareCommit(SegmentInfos.java:808)\n\tat org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:5319)\n\tat org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:3895)\n\tat org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3956)\n\nThis happened after having build a jar assembly using Maven, the original MANIFEST.MF of lucene jar has been overwritten, and didn't contain anynore the implementation version metadata.\n\nPath attached.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-814",
        "summary": "javacc on Win32 (cygwin) creates wrong line endings - fix them with 'ant replace'",
        "description": "\"ant javacc\" in Windows/Cygwin generates files with wrong line endings (\\r  or \\r\\n instead of *Nix's \\n). \nI managed to get rid of those using    perl -p -e 's/(\\r\\n|\\n|\\r)/\\n/g'\nSome useful info on line ending issues is in http://en.wikipedia.org/wiki/Newline\n\nAfter wasting some time to get rid of those, I modified javacc-QueryParser build.xml task to take care of that.\nSo now QueryParser files created with \"ant javacc\" are fixed (if required) to have \\n as line ends.\n\nShould probably do that also for the other javacc targets: javacc-HTMLParser and javacc-StandardAnalyzer(?)\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2718",
        "summary": "separate java code from .jj file",
        "description": "It would make development easier to move most of the java code out from the .jj file and into a real java file.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1695",
        "summary": "Update the Highlighter to use the new TokenStream API",
        "description": "",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-908",
        "summary": "MANIFEST.MF cleanup (main jar and luci customizations)",
        "description": "there are several problems with the MANIFEST.MF file used in the core jar, and some inconsistencies in th luci jar:\n\nLucli's build.xml has an own \"jar\" target and does not use the jar target from common-build.xml. The result\nis that the MANIFEST.MF file is not consistent and the META-INF dir does not contain LICENSE.TXT and NOTICE.TXT.\n\nIs there a reason why lucli behaves different in this regard? If not I think we should fix this.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1675",
        "summary": "Add a link to the release archive",
        "description": "It would be nice if the [Releases page|http://lucene.apache.org/java/docs/releases.html] contained a link to the release archive at http://archive.apache.org/dist/lucene/java/.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3771",
        "summary": "Rename some remaining tests for new IndexReader class hierarchy",
        "description": "",
        "label": "NUG",
        "classified": "TEST",
        "type": ""
    },
    {
        "key": "LUCENE-1693",
        "summary": "AttributeSource/TokenStream API improvements",
        "description": "This patch makes the following improvements to AttributeSource and\nTokenStream/Filter:\n\n- introduces interfaces for all Attributes. The corresponding\n  implementations have the postfix 'Impl', e.g. TermAttribute and\n  TermAttributeImpl. AttributeSource now has a factory for creating\n  the Attribute instances; the default implementation looks for\n  implementing classes with the postfix 'Impl'. Token now implements\n  all 6 TokenAttribute interfaces.\n\n- new method added to AttributeSource:\n  addAttributeImpl(AttributeImpl). Using reflection it walks up in the\n  class hierarchy of the passed in object and finds all interfaces\n  that the class or superclasses implement and that extend the\n  Attribute interface. It then adds the interface->instance mappings\n  to the attribute map for each of the found interfaces.\n\n- removes the set/getUseNewAPI() methods (including the standard\n  ones). Instead it is now enough to only implement the new API,\n  if one old TokenStream implements still the old API (next()/next(Token)),\n  it is wrapped automatically. The delegation path is determined via\n  reflection (the patch determines, which of the three methods was\n  overridden).\n\n- Token is no longer deprecated, instead it implements all 6 standard\n  token interfaces (see above). The wrapper for next() and next(Token)\n  uses this, to automatically map all attribute interfaces to one\n  TokenWrapper instance (implementing all 6 interfaces), that contains\n  a Token instance. next() and next(Token) exchange the inner Token\n  instance as needed. For the new incrementToken(), only one\n  TokenWrapper instance is visible, delegating to the currect reusable\n  Token. This API also preserves custom Token subclasses, that maybe\n  created by very special token streams (see example in Backwards-Test).\n\n- AttributeImpl now has a default implementation of toString that uses\n  reflection to print out the values of the attributes in a default\n  formatting. This makes it a bit easier to implement AttributeImpl,\n  because toString() was declared abstract before.\n\n- Cloning is now done much more efficiently in\n  captureState. The method figures out which unique AttributeImpl\n  instances are contained as values in the attributes map, because\n  those are the ones that need to be cloned. It creates a single\n  linked list that supports deep cloning (in the inner class\n  AttributeSource.State). AttributeSource keeps track of when this\n  state changes, i.e. whenever new attributes are added to the\n  AttributeSource. Only in that case will captureState recompute the\n  state, otherwise it will simply clone the precomputed state and\n  return the clone. restoreState(AttributeSource.State) walks the\n  linked list and uses the copyTo() method of AttributeImpl to copy\n  all values over into the attribute that the source stream\n  (e.g. SinkTokenizer) uses. \n\n- Tee- and SinkTokenizer were deprecated, because they use\nToken instances for caching. This is not compatible to the new API\nusing AttributeSource.State objects. You can still use the old\ndeprecated ones, but new features provided by new Attribute types\nmay get lost in the chain. A replacement is a new TeeSinkTokenFilter,\nwhich has a factory to create new Sink instances, that have compatible\nattributes. Sink instances created by one Tee can also be added to\nanother Tee, as long as the attribute implementations are compatible\n(it is not possible to add a sink from a tee using one Token instance\nto a tee using the six separate attribute impls). In this case UOE is thrown.\n\nThe cloning performance can be greatly improved if not multiple\nAttributeImpl instances are used in one TokenStream. A user can\ne.g. simply add a Token instance to the stream instead of the individual\nattributes. Or the user could implement a subclass of AttributeImpl that\nimplements exactly the Attribute interfaces needed. I think this\nshould be considered an expert API (addAttributeImpl), as this manual\noptimization is only needed if cloning performance is crucial. I ran\nsome quick performance tests using Tee/Sink tokenizers (which do\ncloning) and the performance was roughly 20% faster with the new\nAPI. I'll run some more performance tests and post more numbers then.\n\nNote also that when we add serialization to the Attributes, e.g. for\nsupporting storing serialized TokenStreams in the index, then the\nserialization should benefit even significantly more from the new API\nthan cloning. \n\nThis issue contains one backwards-compatibility break:\nTokenStreams/Filters/Tokenizers should normally be final\n(see LUCENE-1753 for the explaination). Some of these core classes are \nnot final and so one could override the next() or next(Token) methods.\nIn this case, the backwards-wrapper would automatically use\nincrementToken(), because it is implemented, so the overridden\nmethod is never called. To prevent users from errors not visible\nduring compilation or testing (the streams just behave wrong),\nthis patch makes all implementation methods final\n(next(), next(Token), incrementToken()), whenever the class\nitsself is not final. This is a BW break, but users will clearly see,\nthat they have done something unsupoorted and should better\ncreate a custom TokenFilter with their additional implementation\n(instead of extending a core implementation).\n\nFor further changing contrib token streams the following procedere should be used:\n\n    *  rewrite and replace next(Token)/next() implementations by new API\n    * if the class is final, no next(Token)/next() methods needed (must be removed!!!)\n    * if the class is non-final add the following methods to the class:\n{code:java}\n      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n       * not be overridden. Delegates to the backwards compatibility layer. */\n      public final Token next(final Token reusableToken) throws java.io.IOException {\n        return super.next(reusableToken);\n      }\n\n      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n       * not be overridden. Delegates to the backwards compatibility layer. */\n      public final Token next() throws java.io.IOException {\n        return super.next();\n      }\n{code}\nAlso the incrementToken() method must be final in this case\n(and the new method end() of LUCENE-1448)\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2728",
        "summary": "EnwikiContentSource does not properly identify the name/id of the Wikipedia article",
        "description": "The EnwikiContentSource does not properly identify the id (name in benchmark parlance) of the documents.  It currently produces assigns the id on the last <id> tag it sees in the document, as opposed to the id of the document.  Most documents have multiple <id> tags in them.  This prevents the ContentSource from being used effectively in producing documents for updating.\n\nExample doc:\n{quote}\n<page>\n    <title>AlgeriA</title>\n    <id>5</id>\n    <revision>\n      <id>133452200</id>\n      <timestamp>2007-05-25T17:11:48Z</timestamp>\n      <contributor>\n        <username>Gurch</username>\n        <id>241822</id>\n      </contributor>\n      <minor />\n      <comment>[[WP:AES|\u00e2<86><90>]]Redirected page to [[Algeria]]</comment>\n      <text xml:space=\"preserve\">#REDIRECT [[Algeria]] {{R from CamelCase}}</text>\n    </revision>\n  </page>\n{quote}\n\nIn this case, the getName() return 241822 instead of 5.  page/id is unique according to the schema at  http://www.mediawiki.org/xml/export-0.3.xsd, so we should just get that one.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1195",
        "summary": "Performance improvement for TermInfosReader",
        "description": "Currently we have a bottleneck for multi-term queries: the dictionary lookup is being done\ntwice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called.\nThe second time when the posting list is opened (TermDocs or TermPositions).\n\nThe dictionary lookup is not cheap, that's why a significant performance improvement is\npossible here if we avoid the second lookup. An easy way to do this is to add a small LRU \ncache to TermInfosReader. \n\nI ran some performance experiments with an LRU cache size of 20, and an mid-size index of\n500,000 documents from wikipedia. Here are some test results:\n\n50,000 AND queries with 3 terms each:\nold:                  152 secs\nnew (with LRU cache): 112 secs (26% faster)\n\n50,000 OR queries with 3 terms each:\nold:                  175 secs\nnew (with LRU cache): 133 secs (24% faster)\n\nFor bigger indexes this patch will probably have less impact, for smaller once more.\n\nI will attach a patch soon.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-452",
        "summary": "PrefixQuery is missing the equals() method",
        "description": "The PrefixQuery is inheriting the java.lang.Object's object default equals method. This makes it hard to have test working of PrefixFilter or any other task requiring equals to work proerply (insertion in Set, etc.). The equal method should be very similar, not to say identical except for class casting, to the equals() of TermQuery. ",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1072",
        "summary": "NullPointerException during indexing in DocumentsWriter$ThreadState$FieldData.addPosition",
        "description": "In my case during indexing sometimes appear documents with unusually large \"words\" - text-encoded images in fact.\nAttempt to add document that contains field with such token produces java.lang.IllegalArgumentException:\njava.lang.IllegalArgumentException: term length 37944 exceeds max term length 16383\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1492)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)\n        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)\n\nThis is expected, exception is caught and ignored. The problem is that after this IndexWriter becomes somewhat corrupted and subsequent attempts to add documents to the index fail as well, this time with NPE:\njava.lang.NullPointerException\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1497)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)\n        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)\n\nThis is 100% reproducible.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2054",
        "summary": "Remove \"System Properties\" page from release specific docs",
        "description": "We no longer use system properties to configure Lucene in version 3.0, the page is obsolete and should be removed before release.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1865",
        "summary": "Add a ton of missing license headers throughout test/demo/contrib",
        "description": "",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1392",
        "summary": "Some small javadocs/extra import fixes",
        "description": "Two things that Uwe Schindler caught, plus fixes for javadoc warnings in core.  I plan to commit to trunk & 2.4.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3484",
        "summary": "TaxonomyWriter parents array creation is not thread safe, can cause NPE",
        "description": "Following user list thread [TaxWriter leakage? | http://markmail.org/thread/jkkhemfzpnbdzoft] it appears that if two threads or more are asking for the parent array for the first time, a context switch after the first thread created the empty parents array but before it initialized it would cause the other array to use an uninitialized array, causing an NPE. Fix is simple: synchronize the method getParentArray()",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3648",
        "summary": "Speed up SegementDocsEnum by making it more friendly for JIT optimizations",
        "description": "Since we moved the bulk reading into the codec ie. make all  bulk reading codec private in LUCENE-3584 we have seen some performance [regression|http://people.apache.org/~mikemccand/lucenebench/Term.html] on different CPUs. I tried to optimize the implementation to make it more eligible for runtime optimizations, tried to make loops JIT friendly by moving out branches where I can, minimize member access in all loops, use final members where possible and specialize the two common cases With & Without LiveDocs.\n\nI will attache a patch and my benchmark results in a minute.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1586",
        "summary": "add IndexReader.getUniqueTermCount",
        "description": "Simple API to return number of unique terms (across all fields).  Spinoff from here:\n\nhttp://www.lucidimagination.com/search/document/536b22e017be3e27/term_limit",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-749",
        "summary": "ChainedFilter does not work well in the event of filters in ANDNOT",
        "description": "First ANDNOT operation takes place against a completely false bitset and will always return zero results.  ",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3024",
        "summary": "If index has more than Integer.MAX_VALUE terms, seeking can it AIOOBE due to long/int overflow",
        "description": "Tom hit a new long/int overflow case: http://markmail.org/thread/toyl2ujcl4suqvf3\n\nThis is a regression, in 3.1, introduced with LUCENE-2075.\n\nWorse, our Test2BTerms failed to catch this, so I've fixed that test to show the failure.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1564",
        "summary": "Field.setValue(...) doesn't properly handle switching between byte[] and other types",
        "description": "This came up in PyLucene testing, based on Lucene 2.4.1.  Thread here:\n\n  http://pylucene.markmail.org/message/75jzxzqi3smp2s4z\n\nThe problem is that Field.setValue does not fix up the isBinary\nboolean, so if you create a String field, and then do\nsetValue(byte[]), you'll get an exception when adding a document\ncontaining that field to the index.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3556",
        "summary": "Make DirectoryTaxonomyWriter's indexWriter member private",
        "description": "DirectoryTaxonomyWriter has a protected indexWriter member. As far as I can tell, for two reasons:\n\n# protected openIndexWriter method which lets you open your own IW (e.g. with a custom IndexWriterConfig).\n# protected closeIndexWriter which is a hook for letting you close the IW you opened in the previous one.\n\nThe fixes are trivial IMO:\n# Modify the method to return IW, and have the calling code set DTW's indexWriter member\n# Eliminate closeIW. DTW already has a protected closeResources() which lets you clean other resources you've allocated, so I think that's enough.\n\nI'll post a patch shortly.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2653",
        "summary": "ThaiAnalyzer assumes things about your jre",
        "description": "The ThaiAnalyzer/ThaiWordFilter depends on the fact that BreakIterator.getWordInstance(new Locale(\"th\")) returns a dictionary-based break iterator that can segment thai phrases into words (it does not use whitespace).\n\nBut this is non-standard that the JRE will specialize this locale in this way, its nice, but you can't depend on it.\nFor example, if you are running on IBM JRE, this analyzer/wordfilter is completely \"broken\" in the sense it won't do what it claims to do.\n\nAt the minimum, we need to document this and suggest users look at ICUTokenizer for thai, which always has this breakiterator and is not jre-dependent.\n\nBetter, would be to check statically that the thing actually works.\nwhen creating a new ThaiWordFilter we could clone() the BreakIterator, which is often cheaper than making a new one anyway.\nwe could throw an exception, if its not supported, and add a boolean so the user knows it works.\nand we could refer to this boolean with Assert.assume in its tests.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-822",
        "summary": "Make FieldSelector usable from Searchable ",
        "description": "Seems reasonable that you would want to be able to specify a FieldSelector from Searchable because many systems do not use IndexSearcher (where you can get a Reader), but instead use Searchable or Searcher so that Searchers and MultiSearchers can be used in a polymorphic manner.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2135",
        "summary": "IndexReader.close should forcefully evict entries from FieldCache",
        "description": "Spinoff of java-user thread \"heap memory issues when sorting by a string field\".\n\nWe rely on WeakHashMap to hold our FieldCache, keyed by reader.  But this lacks immediacy on releasing the reference, after a reader is closed.\n\nWeakHashMap can't free the key until the reader is no longer referenced by the app. And, apparently, WeakHashMap has a further impl detail that requires invoking one of its methods for it to notice that a key has just become only weakly reachable.\n\nTo fix this, I think on IR.close we should evict entries from the FieldCache, as long as the sub-readers are truly closed (refCount dropped to 0).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2973",
        "summary": "Source distribution packaging targets should make a tarball from \"svn export\"",
        "description": "Instead of picking and choosing which stuff to include from a local working copy, Lucene's dist-src/package-tgz-src target and Solr's package-src target should simply perform \"svn export\" with the same revision and URL as the local working copy.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3795",
        "summary": "Replace spatial contrib module with LSP's spatial-lucene module",
        "description": "I propose that Lucene's spatial contrib module be replaced with the spatial-lucene module within Lucene Spatial Playground (LSP).  LSP has been in development for approximately 1 year by David Smiley, Ryan McKinley, and Chris Male and we feel it is ready.  LSP is here: http://code.google.com/p/lucene-spatial-playground/  and the spatial-lucene module is intuitively in svn/trunk/spatial-lucene/.\n\nI'll add more comments to prevent the issue description from being too long.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1707",
        "summary": "Don't use ensureOpen() excessively in IndexReader and IndexWriter",
        "description": "A spin off from here: http://www.nabble.com/Excessive-use-of-ensureOpen()-td24127806.html.\n\nWe should stop calling this method when it's not necessary for any internal Lucene code. Currently, this code seems to hurt properly written apps, unnecessarily.\n\nWill post a patch soon",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1605",
        "summary": "Add subset method to BitVector",
        "description": "Recently I needed the ability to efficiently compute subsets of a BitVector. The method is:\n  public BitVector subset(int start, int end)\nwhere \"start\" is the starting index, inclusive and \"end\" is the ending index, exclusive.\n\nAttached is a patch including the subset method as well as relevant unit tests.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-551",
        "summary": "Make Lucene - Java 1.9.1 Available in Maven2 repository in iBibilio.org",
        "description": "Please upload 1.9.1 release to iBiblio so that Maven users can easily use the latest release.  Currently 1.4.3 is the most recently available version: http://www.ibiblio.org/maven2/lucene/lucene/\n\nPlease read the following FAQ for more information: http://maven.apache.org/project-faq.html",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2944",
        "summary": "BytesRef reuse bugs in QueryParser and analysis.jsp",
        "description": "Some code uses BytesRef as if it were a \"String\", in this case consumers of TermToBytesRefAttribute.\nThe thing is, while our general implementation works on char[] and then populates the consumers BytesRef,\nnot all TermToBytesRefAttribute implementations do this, specifically ICU collation, it reuses the bytes and simply sets the pointers:\n{noformat}\n  @Override\n  public int toBytesRef(BytesRef target) {\n    collator.getRawCollationKey(toString(), key);\n    target.bytes = key.bytes;\n    target.offset = 0;\n    target.length = key.size;\n    return target.hashCode();\n  }\n{noformat}\n\nMost of the blame falls on me as I added this to the queryparser in LUCENE-2514.\n\nAttached is a patch so that these consumers re-use a 'spare' and copy the bytes when they are going to make a long lasting object such as a Term.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-807",
        "summary": "Minor improvement to JavaDoc for ScoreDocComparator",
        "description": "About to attach a very small patch for ScoreDocComparator which broadens the contract of compare(ScoreDoc, ScoreDoc) to follow the same semantics as java.util.Comparator.compare() -- allow any integer to be returned, rather than specifically -1/0/-1.\n\nNote that this behaviour must already be acceptable; the anonymous ScoreDocComparators returned by FieldSortedHitQueue.comparatorStringLocale() already return the result of Collator.compare(), which is not tied to this -1/0/1 restriction.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1198",
        "summary": "Exception in DocumentsWriter.ThreadState.init leads to corruption",
        "description": "If an exception is hit in the init method, DocumentsWriter incorrectly\nincrements numDocsInRAM when in fact the document is not added.\n\nSpinoff of this thread:\n\n  http://markmail.org/message/e76hgkgldxhakuaa\n\nThe root cause that led to the exception in init was actually due to\nincorrect use of Lucene's APIs (one thread still modifying the\nDocument while IndexWriter.addDocument is adding it) but still we\nshould protect against any exceptions coming out of init.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3579",
        "summary": "DirectoryTaxonomyWriter should throw a proper exception if it was closed",
        "description": "DirTaxoWriter may throw random exceptions (NPE, Already Closed - depend on what API you call) after it has been closed/rollback. We should detect up front that it is already closed, and throw AlreadyClosedException.\n\nAlso, on LUCENE-3573 Doron pointed out a problem with DTW.rollback -- it should call close() rather than refreshReader. I will fix that as well in this issue.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-454",
        "summary": "lazily create SegmentMergeInfo.docMap",
        "description": "Since creating the docMap is expensive, and it's only used during segment merging, not searching, defer creation until it is requested.\n\nSegmentMergeInfo is also used in MultiTermEnum, the term enumerator for a MultiReader.  TermEnum is used by queries such as PrefixQuery, RangeQuery, WildcardQuery, as well as RangeFilter, DateFilter, and sorting the first time (filling the FieldCache).\n\nPerformance Results:\n  A simple single field index with 555,555 documents, and 1000 random deletions was queried 1000 times with a PrefixQuery matching a single document.\n\nPerformance Before Patch:\n  indexing time = 121,656 ms\n  querying time = 58,812 ms\n\nPerformance After Patch:\n  indexing time = 121,000 ms\n  querying time =         598 ms\n\nA 100 fold increase in query performance!\n\nAll lucene unit tests pass.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-431",
        "summary": "RAMInputStream and RAMOutputStream without further buffering",
        "description": "From java-dev, Doug's reply of 12 Sep 2005 \non Delaying buffer allocation in BufferedIndexInput: \n \nPaul Elschot wrote: \n... \n> I noticed that RAMIndexInput extends BufferedIndexInput. \n> It has all data in buffers already, so why is there another \n> layer of buffering? \n \nNo good reason: it's historical. \n \nTo avoid this either: (a) the BufferedIndexInput API would need to be  \nmodified to permit subclasses to supply the buffer; or (b)  \nRAMInputStream could subclass IndexInput directly, using its own  \nbuffers.  The latter would probably be simpler. \n \nEnd of quote. \n \nI made version (b) of RAMInputStream. \nUsing this RAMInputStream, TestTermVectorsReader failed as the only \nfailing test.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-675",
        "summary": "Lucene benchmark: objective performance test for Lucene",
        "description": "We need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests.\n\nRegarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2869",
        "summary": "remove Query.getSimilarity()",
        "description": "Spinoff of LUCENE-2854.\n\nSee LUCENE-2828 and LUCENE-2854 for reference.\n\nIn general, the SimilarityDelegator was problematic with regards to back-compat, and if queries\nwant to score differently, trying to runtime subclass Similarity only causes trouble.\n\nThe reason we could not fix this in LUCENE-2854 is because:\n{noformat}\nMichael McCandless added a comment - 08/Jan/11 01:53 PM\nbq. Is it possible to remove this method Query.getSimilarity also? I don't understand why we need this method!\n\nI would love to! But I think that's for another day...\n\nI looked into this and got stuck with BoostingQuery, which rewrites to an anon \nsubclass of BQ overriding its getSimilarity in turn override its coord method. \nRather twisted... if we can do this differently I think we could remove Query.getSimilarity.\n{noformat}\n\nhere is the method in question:\n\n{noformat}\n/** Expert: Returns the Similarity implementation to be used for this query.\n * Subclasses may override this method to specify their own Similarity\n * implementation, perhaps one that delegates through that of the Searcher.\n * By default the Searcher's Similarity implementation is returned.*/\npublic Similarity getSimilarity(IndexSearcher searcher) {\n  return searcher.getSimilarity();\n}\n{noformat}\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2400",
        "summary": "ShingleFilter: don't output all-filler shingles/unigrams; also, convert from TermAttribute to CharTermAttribute",
        "description": "When the input token stream to ShingleFilter has position increments greater than one, filler tokens are inserted for each position for which there is no token in the input token stream.  As a result, unigrams (if configured) and shingles can be filler-only.  Filler-only output tokens make no sense - these should be removed.\n\nAlso, because TermAttribute has been deprecated in favor of CharTermAttribute, the patch will also convert TermAttribute usages to CharTermAttribute in ShingleFilter.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-976",
        "summary": "MMapDirectory is missing newly added openInput method to FSDirectory",
        "description": "This issue was caused by the optimizations in LUCENE-888.  The new\nopenInput(String name, int bufferSize) added to FSDirectory was not\nalso overridden by MMapDirectory.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3609",
        "summary": "BooleanFilter changed behavior in 3.5, no longer acts as if \"minimum should match\" set to 1",
        "description": "The change LUCENE-3446 causes a change in behavior in BooleanFilter. It used to work as if minimum should match clauses is 1 (compared to BQ lingo), but now, if no should clauses match, then the should clauses are ignored, and for example, if there is a must clause, only that one will be used and returned.\n\nFor example, a single must clause and should clause, with the should clause not matching anything, should not match anything, but, it will match whatever the must clause matches.\n\nThe fix is simple, after iterating over the should clauses, if the aggregated bitset is null, return null.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-387",
        "summary": "Contrib: Main memory based SynonymMap and SynonymTokenFilter",
        "description": "- Contrib: Main memory based SynonymMap and SynonymTokenFilter\n- applies to SVN trunk as well as 1.4.3",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3058",
        "summary": "FST should allow more than one output for the same input",
        "description": "For the block tree terms dict, it turns out I need this case.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3520",
        "summary": "If the NRT reader hasn't changed then IndexReader.openIfChanged should return null",
        "description": "I hit a failure in TestSearcherManager (NOTE: doesn't always fail):\n\n{noformat}\n  ant test -Dtestcase=TestSearcherManager -Dtestmethod=testSearcherManager -Dtests.seed=459ac99a4256789c:-29b8a7f52497c3b4:145ae632ae9e1ecf\n{noformat}\n\nIt was tripping the assert inside SearcherLifetimeManager.record,\nbecause two different IndexSearcher instances had different IR\ninstances sharing the same version.  This was happening because\nIW.getReader always returns a new reader even when there are no\nchanges.  I think we should fix that...\n\nSeparately I found a deadlock in\nTestSearcherManager.testIntermediateClose, if the test gets\nSerialMergeScheduler and needs to merge during the second commit.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2838",
        "summary": "ConstantScoreQuery should directly support wrapping Query and simply strip off scores",
        "description": "Especially in MultiTermQuery rewrite modes we often simply need to strip off scores from Queries and make them constant score. Currently the code to do this looks quite ugly: new ConstantScoreQuery(new QueryWrapperFilter(query))\n\nAs the name says, QueryWrapperFilter should make any other Query constant score, so why does it not take a Query as ctor param? This question was aldso asked quite often by my customers and is simply correct, if you think about it.\n\nLooking closer into the code, it is clear that this would also speed up MTQs:\n- One additional wrapping and method calls can be removed\n- Maybe we can even deprecate QueryWrapperFilter in 3.1 now (it's now only used in tests and the use-case for this class is not really available) and LUCENE-2831 does not need the stupid hack to make Simon's assertions pass\n- CSQ now supports out-of-order scoring and topLevel scoring, so a CSQ on top-level now directly feeds the Collector. For that a small trick is used: The score(Collector) calls are directly delegated and the scores are stripped by wrapping the setScorer() method in Collector\n\nDuring that I found a visibility bug in Scorer (LUCENE-2839): The method \"boolean score(Collector collector, int max, int firstDocID)\" should be public not protected, as its not solely intended to be overridden by subclasses and is called from other classes, too! This leads to no compiler bugs as the other classes that calls it is mainly BooleanScorer(2) and thats in same package, but visibility is wrong. I will open an issue for that and fix it at least in trunk where we have no backwards-requirement.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-991",
        "summary": "BoostingTermQuery.explain() bugs",
        "description": "There are a couple of minor bugs in BoostingTermQuery.explain().\n\n1. The computation of average payload score produces NaN if no payloads were found. It should probably be:\nfloat avgPayloadScore = super.score() * (payloadsSeen > 0 ? (payloadScore / payloadsSeen) : 1);\n\n2. If the average payload score is zero, the value of the explanation is 0:\nresult.setValue(nonPayloadExpl.getValue() * avgPayloadScore);\nIf the query is part of a BooleanClause, this results in:\n\"no match on required clause...\"\n\"failure to meet condition(s) of required/prohibited clause(s)\"\n\nThe average payload score can be zero if the field boost = 0.\n\nI've attached a patch to 'TestBoostingTermQuery.java', however, the test 'testNoPayload' fails in 'SpanScorer.score()' because the doc = -1. It looks like 'setFreqCurrentDoc() should have been called before 'score()'. Maybe someone more knowledgable of spans could investigate this.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2637",
        "summary": "FSDirectory.copyBytes isn't safe for SimpleFSDirectory",
        "description": "the copyBytes optimization from LUCENE-2574 is not safe for SimpleFSDirectory, but works fine for NIOFSDirectory.\n\nWith SimpleFSDirectory, the copyBytes optimization causes index corruption.\n\nsee http://www.lucidimagination.com/search/document/36d2dbfc691909d5/bug_triggered_by_testindexwriter_testrandomstoredfields for background\n\nhere are my steps to reproduce (most of the time, at least on windows):\n{noformat}\n1. edit line 87 of TestIndexWriter to plugin the seed:\n    random = newRandom(3312389322103990899L);\n2. edit line 5138 of TestIndexWriter to force SimpleFSDirectory:\n    Directory dir = new SimpleFSDirectory(index);\n3. run this command:\n    ant clean test-core -Dtestcase=TestIndexWriter\n-Dtestmethod=testRandomStoredFields -Dtests.iter=10\n-Dtests.codec=\"MockVariableIntBlock(29)\"\n{noformat}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-661",
        "summary": "BUILD.txt instructions wrong for JavaCC",
        "description": "The text in BUILD.txt for javacc says to set the property to the bin directory in the javacc installation. It should actually be set to the javacc installation directory, the directory containing the bin directory. The comments common-build.xml correctly state this.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1116",
        "summary": "contrib.benchmark.quality package improvements",
        "description": "Few fixes and improvements for the search quality benchmark package:\n- flush report and logger at the end (otherwise long submission reports might miss last lines).\n- add run-tag-name to submission report (API change).\n- add control over max-#queries to run (useful at debugging a quality evaluation setup).\n- move control over max-docs-to-retrieve from benchmark constructor to a setter method (API change).\n- add computation of Mean Reciprocal Rank (MRR) in QualityStats.\n- QualityStats fixed to not fail if there are no results to average.\n- Add a TREC queries reader adequate for the 1MQ track (track started 2007).\n\nAll tests pass, will commit this in 1-2 days if there is no objection.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3361",
        "summary": "port url+email tokenizer to standardtokenizerinterface (or similar)",
        "description": "We should do this so that we can fix the LUCENE-3358 bug there, and preserve backwards.\nWe also want this mechanism anyway, for upgrading to new unicode versions in the future.\n\nWe can regenerate the new TLD list for 3.4 but, we should ensure the existing one is used for the urlemail33 or whatever,\nso that its exactly the same.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-564",
        "summary": "Class DisjunctionSumScorer does not need to be public.",
        "description": "See title, patch follows.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-508",
        "summary": "SegmentTermEnum.next() doesn't maintain prevBuffer at end",
        "description": "When you're iterating a SegmentTermEnum and you go past the end of the docs, you end up with a state where the nextBuffer = null and the prevBuffer is the penultimate term, not the last term.  This patch fixes it.  (It's also required for my Prefetching bug [LUCENE-506])\n\nIndex: java/org/apache/lucene/index/SegmentTermEnum.java\n===================================================================\n--- java/org/apache/lucene/index/SegmentTermEnum.java\t(revision 382121)\n+++ java/org/apache/lucene/index/SegmentTermEnum.java\t(working copy)\n@@ -109,6 +109,7 @@\n   /** Increments the enumeration to the next element.  True if one exists.*/\n   public final boolean next() throws IOException {\n     if (position++ >= size - 1) {\n+      prevBuffer.set(termBuffer);\n       termBuffer.reset();\n       return false;\n     }\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2006",
        "summary": "Optimization for FieldDocSortedHitQueue",
        "description": "When updating core for generics,  I found the following as a optimization of FieldDocSortedHitQueue:\n\nAll FieldDoc values are Compareables (also the score or docid, if they\nappear as SortField in a MultiSearcher or ParallelMultiSearcher). The code\nof lessThan seems very ineffective, as it has a big switch statement on the\nSortField type, then casts the value to the underlying numeric type Object,\ncalls Number.xxxValue() & co for it and then compares manually. As\nj.l.Number is itself Comparable, I see no reason to do this. Just call\ncompareTo on the Comparable interface and we are happy. The big deal is that\nit prevents casting and the two method calls xxxValue(), as Number.compareTo\nworks more efficient internally.\n\nThe only special cases are String sort, where the Locale may be used and the\nscore sorting which is backwards. But these are two if statements instead of\nthe whole switch.\n\nI had not tested it now for performance, but in my opinion it should be\nfaster for MultiSearchers. All tests still pass (because they should).\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3789",
        "summary": "Expose FilteredTermsEnum from MTQ ",
        "description": "MTQ#getEnum() is protected and in order to access it you need to be in the o.a.l.search package. \n\nhere is a relevant snipped from the mailing list discussion\n\n{noformat}\ngetEnum() is protected so it is intended to be called *only* by subclasses (that's the idea behind protected methods). They are also accessible by other classes from the same package, but that's more a Java bug than a feature. The problem with MTQ is that RewriteMethod is a separate *class* and *not a subclass* of MTQ, so the method cannot be called (it can because of the \"java bug\" called from same package). So theoretically it has to be public otherwise you cannot call getEnum().\n\nAnother cleaner fix would be to add a protected final method to RewriteMethod that calls this method from MTQ. So anything subclassing RewriteMethod can get the enum from inside the RewriteMethod class which is the \"correct\" way to handle it. Delegating to MTQ is then \"internal\".\n{noformat}",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-501",
        "summary": "need DOAP file for Lucene",
        "description": "Can someone please draft a DOAP file for Lucene, so that we're listed at http://projects.apache.org/?\n\nA DOAP generator is at:\n\nhttp://projects.apache.org/create.html\n\nPlease attach it to this bug report.  Thanks.",
        "label": "NUG",
        "classified": "TASK",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3324",
        "summary": "checkindex fails if docfreq >= skipInterval and term is indexed more than once at same position",
        "description": "This is a bad check in the skipping verification logic",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1728",
        "summary": "Move SmartChineseAnalyzer & resources to own contrib project",
        "description": "SmartChineseAnalyzer depends on  a large dictionary that causes the analyzer jar to grow up to 3MB. The dictionary is quite big compared to all the other resouces / class files contained in that jar. \nHaving a separate analyzer-cn contrib project enables footprint-sensitive users (e.g. using lucene on a mobile phone) to include analyzer.jar without getting into trouble with disk space.\n\nMoving SmartChineseAnalyzer to a separate project could also include a small refactoring as Robert mentioned in [LUCENE-1722|https://issues.apache.org/jira/browse/LUCENE-1722] several classes should be package protected, members and classes could be final, commented syserr and logging code should be removed etc.\n\nI set this issue target to 2.9 - if we can not make it until then feel free to move it to 3.0\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2620",
        "summary": "Queries with too many asterisks causing 100% CPU usage",
        "description": "If a search query has many adjacent asterisks (e.g. fo**************obar), I can get my webapp caught in a loop that does not seem to end in a reasonable amount of time and may in fact be infinite. For just a few asterisks the query eventually does return some results, but as I add more it takes a longer and longer amount of time. After about six or seven asterisks the query never seems to finish. Even if I abort the search, the thread handling the troublesome query continues running in the background and pinning a CPU.\n\nI found the problem in src/java/org/apache/lucene/search/WildcardTermEnum.java on Lucene 3.0.1 and it looks like 3.0.2 ought to be affected as well. I'm not sure about trunk, though. I have a patch that fixes the problem for me in 3.0.1.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2881",
        "summary": "Track FieldInfo per segment instead of per-IW-session",
        "description": "Currently FieldInfo is tracked per IW session to guarantee consistent global field-naming / ordering. IW carries FI instances over from previous segments which also carries over field properties like isIndexed etc. While having consistent field ordering per IW session appears to be important due to bulk merging stored fields etc. carrying over other properties might become problematic with Lucene's Codec support.  Codecs that rely on consistent properties in FI will fail if FI properties are carried over.\n\nThe DocValuesCodec (DocValuesBranch) for instance writes files per segment and field (using the field id within the file name). Yet, if a segment has no DocValues indexed in a particular segment but a previous segment in the same IW session had DocValues, FieldInfo#docValues will be true  since those values are reused from previous segments. \n\nWe already work around this \"limitation\" in SegmentInfo with properties like hasVectors or hasProx which is really something we should manage per Codec & Segment. Ideally FieldInfo would be managed per Segment and Codec such that its properties are valid per segment. It also seems to be necessary to bind FieldInfoS to SegmentInfo logically since its really just per segment metadata.  ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2384",
        "summary": "Reset zzBuffer in StandardTokenizerImpl* when lexer is reset.",
        "description": "When indexing large documents, the lexer buffer may stay large forever. This sub-issue resets the lexer buffer back to the default on reset(Reader).\n\nThis is done on the enclosing issue.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": ""
    },
    {
        "key": "LUCENE-821",
        "summary": "single norm file still uses up descriptors",
        "description": "The new index file format with a single .nrm file for all norms does not decrease file descriptor usage.\nThe .nrm file is opened once for each field with norms in the index segment.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3086",
        "summary": "add ElisionsFilter to ItalianAnalyzer",
        "description": "we set this up for french by default, but we don't for italian.\nwe should enable it with the standard italian contractions (e.g. definite articles).\n\nthe various stemmers for these languages assume this is already being taken care of\nand don't do anything about it... in general things like snowball assume really dumb\ntokenization, that you will split on the word-internal ', and they add these to stoplists.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2751",
        "summary": "add LuceneTestCase.newSearcher()",
        "description": "Most tests in the search package don't care about what kind of searcher they use.\n\nwe should randomly use MultiSearcher or ParallelMultiSearcher sometimes in tests.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3043",
        "summary": "o.a.l.analysis.de.GermanStemmer crashes on some inputs",
        "description": "See the tests from LUCENE-2560. \n\nGermanAnalyzer no longer uses this stemmer by default, but we should fix it.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1764",
        "summary": "SampleComparable doesn't work well in contrib/remote tests",
        "description": "As discovered in LUCENE-1749, when using identical instances of a SortComparator you get multiple entries in the FieldCache.\n\ndemonstrating this bug currently requires the patches in LUCENE-1749.\n\nSee markmiller's comment here...\nhttps://issues.apache.org/jira/browse/LUCENE-1749?focusedCommentId=12735190#action_12735190",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2165",
        "summary": "SnowballAnalyzer lacks a constructor that takes a Set of Stop Words",
        "description": "As discussed on the java-user list, the SnowballAnalyzer has been updated to use a Set of stop words. However, there is no constructor which accepts a Set, there's only the original String[] one\n\nThis is an issue, because most of the common sources of stop words (eg StopAnalyzer) have deprecated their String[] stop word lists, and moved over to Sets (eg StopAnalyzer.ENGLISH_STOP_WORDS_SET). So, for now, you either have to use a deprecated field on StopAnalyzer, or manually turn the Set into an array so you can pass it to the SnowballAnalyzer\n\nI would suggest that a constructor is added to SnowballAnalyzer which accepts a Set. Not sure if the old String[] one should be deprecated or not.\n\nA sample patch against 2.9.1 to add the constructor is:\n\n\n--- SnowballAnalyzer.java.orig  2009-12-15 11:14:08.000000000 +0000\n+++ SnowballAnalyzer.java       2009-12-14 12:58:37.000000000 +0000\n@@ -67,6 +67,12 @@\n     stopSet = StopFilter.makeStopSet(stopWords);\n   }\n \n+  /** Builds the named analyzer with the given stop words. */\n+  public SnowballAnalyzer(Version matchVersion, String name, Set stopWordsSet) {\n+    this(matchVersion, name);\n+    stopSet = stopWordsSet;\n+  }\n+\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-632",
        "summary": "The creation of a spell index from a LuceneDictionary via SpellChecker.indexDictionary (Dictionary dict) fails starting with 1.9.1 (up to current svn version)",
        "description": "Two different errors in 1.9.1/2.0.0 and current svn version.\n\n1.9.1/2.0.0:\nat the end of indexDictionary (Dictionary dict) \nthe IndexReader-instance reader is closed.\nThis causes a NullpointerException because reader has not been initialized before (neither in that method nor in the constructor).\nUncommenting this line (reader.close()) seems to resolve that issue.\n\ncurrent svn:\nthe constructor tries to create an IndexSearcher-instance for the specified path;\nas there is no index in that path - it is not created yet -  an exception is thrown.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2570",
        "summary": "Some improvements to _TestUtil and its usage",
        "description": "I've started this issue because I've noticed that _TestUtil.getRandomMultiplier() is called from many loops' condition check, sometimes hundreds and thousands of times. Each time it does Integer.parseInt after calling System.getProperty. This really can become a constant IMO, either in LuceneTestCase(J4) or _TestUtil, as it's not expected to change while tests are running ...\n\nI then reviewed the class and spotted some more things that I think can be fixed/improved:\n# getTestCodec() can become a constant as well\n# arrayToString is marked deprecated. I've checked an no one calls them, so I'll delete them. This is a 4.0 code branch + a test-only class. No need to deprecate anything.\n# getTempDir calls new Random(), instead of newRandom() in LuceneTestCaseJ4, which means that if something fails, we won't know the random seed used ...\n#* In that regard, we might want to output all the classes that obtained a static seed in reportAdditionalFailures(), instead of just the class that ran the test.\n# rmDir(String) can be removed IMO, and leave only rmDir(File)\n# I suggest we include some recursion in rmDir(File) to handle the deletion of nested directories.\n#* Also, it does not check whether the dir deletion itself succeeds (but it does so for the files). This can bite us on Windows, if some test did not close things properly.\n\nI'll work out a patch.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3765",
        "summary": "trappy ignoreCase behavior with StopFilter/ignoreCase",
        "description": "Spinoff from LUCENE-3751:\n\n{code}\n* If <code>stopWords</code> is an instance of {@link CharArraySet} (true if\n* <code>makeStopSet()</code> was used to construct the set) it will be\n* directly used and <code>ignoreCase</code> will be ignored since\n* <code>CharArraySet</code> directly controls case sensitivity.\n{code}\n\nThis is really confusing and trappy... we need to change something here.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2580",
        "summary": "MultiPhraseQuery throws AIOOBE",
        "description": "See thread \"MultiPhraseQuery throws ArrayIndexOutOfBounds Exception\" on dev@ by Jayendra Patil.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-446",
        "summary": "search.function - (1) score based on field value, (2) simple score customizability",
        "description": "FunctionQuery can return a score based on a field's value or on it's ordinal value.\n\nFunctionFactory subclasses define the details of the function.  There is currently a LinearFloatFunction (a line specified by slope and intercept).\n\nField values are typically obtained from FieldValueSourceFactory.  Implementations include FloatFieldSource, IntFieldSource, and OrdFieldSource.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1603",
        "summary": "Changes for TrieRange in FilteredTermEnum and MultiTermQuery improvement",
        "description": "This is a patch, that is needed for the MultiTermQuery-rewrite of TrieRange (LUCENE-1602):\n- Make the private members protected, to have access to them from the very special TrieRangeTermEnum \n- Fix a small inconsistency (docFreq() now only returns a value, if a valid term is existing)\n- Improvement of MultiTermFilter.getDocIdSet to return DocIdSet.EMPTY_DOCIDSET, if the TermEnum is empty (less memory usage) and faster.\n- Add the getLastNumberOfTerms() to MultiTermQuery for statistics on different multi term queries and how may terms they affect, using this new functionality, the improvement of TrieRange can be shown (extract from test case there, 10000 docs index, long values):\n{code}\n[junit] Average number of terms during random search on 'field8':\n[junit]  Trie query: 244.2\n[junit]  Classical query: 3136.94\n[junit] Average number of terms during random search on 'field4':\n[junit]  Trie query: 38.3\n[junit]  Classical query: 3018.68\n[junit] Average number of terms during random search on 'field2':\n[junit]  Trie query: 18.04\n[junit]  Classical query: 3539.42\n{code}\n\nAll core tests pass.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-836",
        "summary": "Benchmarks Enhancements (precision/recall, TREC, Wikipedia)",
        "description": "Would be great if the benchmark contrib had a way of providing precision/recall benchmark information ala TREC.  I don't know what the copyright issues are for the TREC queries/data (I think the queries are available, but not sure about the data), so not sure if the is even feasible, but I could imagine we could at least incorporate support for it for those who have access to the data.  It has been a long time since I have participated in TREC, so perhaps someone more familiar w/ the latest can fill in the blanks here.\n\nAnother option is to ask for volunteers to create queries and make judgments for the Reuters data, but that is a bit more complex and probably not necessary.  Even so, an Apache licensed set of benchmarks may be useful for the community as a whole.  Hmmm.... \n\nWikipedia might be another option instead of Reuters to setup as a download for benchmarking, as it is quite large and I believe the licensing terms are quite amenable.  Having a larger collection would be good for stressing Lucene more and would give many users a demonstration of how Lucene handles large collections.\n\nAt any rate, this kind of information could be useful for people looking at different indexing schemes, formats, payloads and different query strategies.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3615",
        "summary": "Make it easier to run Test2BTerms",
        "description": "Currently, Test2BTerms has an @Ignore annotation which means that the only way to run it as a test is to edit the file.\n\nThere are a couple of options to fix this:\n# Add a main() so it can be invoked via the command line outside of the test framework\n# Add some new annotations that mark it as slow or weekly or something like that and have the test target ignore @slow (or whatever) by default, but can also turn it on.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-353",
        "summary": "Locking bug",
        "description": "In org.apache.lucene.store.Lock, line 57 (lucene_1_4_final branch):\n\nif (++sleepCount == maxSleepCount)\n\nis incorrect, the sleepCount is incremented before the compare causing it\nthrowing the exception with out waiting for at least 1 interation.\n\nShould be changed instead to:\nif (sleepCount++ == maxSleepCount)\n\nAs this is a self-contained simple fix, I am not submitting a patch.\n\nThanks\n\n-John",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-951",
        "summary": "PATCH MultiLevelSkipListReader NullPointerException",
        "description": " When Reconstructing Document Using Luke Tool, received NullPointerException.\n\njava.lang.NullPointerException\n        at org.apache.lucene.index.MultiLevelSkipListReader.loadSkipLevels(MultiLevelSkipListReader.java:188)\n        at org.apache.lucene.index.MultiLevelSkipListReader.skipTo(MultiLevelSkipListReader.java:97)\n        at org.apache.lucene.index.SegmentTermDocs.skipTo(SegmentTermDocs.java:164)\n        at org.getopt.luke.Luke$2.run(Unknown Source)\n\nLuke version 0.7.1\n\nI emailed with Luke author Andrzej Bialecki and he suggested the attached patch file which fixed the problem.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2621",
        "summary": "Extend Codec to handle also stored fields and term vectors",
        "description": "Currently Codec API handles only writing/reading of term-related data, while stored fields data and term frequency vector data writing/reading is handled elsewhere.\n\nI propose to extend the Codec API to handle this data as well.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2787",
        "summary": "disable atime for DirectIOLinuxDirectory",
        "description": "In Linux's open():\nO_NOATIME\n    (Since Linux 2.6.8) Do not update the file last access time (st_atime in the inode) when the file is read(2). This flag is intended for use by indexing or backup programs, where its use can significantly reduce the amount of disk activity. This flag may not be effective on all filesystems. One example is NFS, where the server maintains the access time.\n\nSo we should do this in our linux-specific DirectIOLinuxDirectory.\n\nSeparately (offtopic), it would be better if this was a LinuxDirectory that only uses O_DIRECT when it should :)\nIt would be nice to think about an optional modules/native for common platforms similar to what tomcat provides\nIts easier to test directories like this now (-Dtests.directory)...\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3198",
        "summary": "Change default Directory impl on 64bit linux to MMap",
        "description": "Consistently in my NRT testing on Fedora 13 Linux, 64 bit JVM (Oracle 1.6.0_21) I see MMapDir getting better search and merge performance when compared to NIOFSDir.\n\nI think we should fix the default.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-383",
        "summary": "ConstantScoreRangeQuery - fixes \"too many clauses\" exception",
        "description": "ConstantScoreQuery wraps a filter (representing a set of documents) and returns\na constant score for each document in the set.\n\nConstantScoreRangeQuery implements a RangeQuery that works for any number of\nterms in the range.  It rewrites to a ConstantScoreQuery that wraps a RangeFilter.\n\nStill needed:\n  - unit tests (these classes have been tested and work fine in-house, but the\ncurrent tests rely on too much application specific code)\n  - code review of Weight() implementation (I'm unsure If I got all the score\nnormalization stuff right)\n  - explain() implementation\n\nNOTE: requires Java 1.4 for BitSet.nextSetBit()",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3540",
        "summary": "In 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.x",
        "description": "In 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.x:\n\nbq. This version of Lucene only supports indexes created with release 3.0 and later.\n\nIn 3.x it must be:\n\nbq. This version of Lucene only supports indexes created with release 1.9 and later.\n\nIndexes before 1.9 will throw this exception on reading SegmentInfos (LUCENE-3255).",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3244",
        "summary": "Contrib/Module-uptodate assume name matches path and jar",
        "description": "With adding a new 'queries' module, I am trying to change the project name of contrib/queries to queries-contrib.  However currently the contrib-uptodate assumes that the name property is used in the path and in the jar name.\n\nBy using the name in the path, I must set the value to 'queries' (since the path is contrib/queries).  However because the project name is now queries-contrib, the actual jar file will be lucene-queries-contrib-${version}.jar, not lucene-queries-${version}.jar, as is expected.\n\nConsequently I think we need to separate the path name from the jar name properties.  For simplicity I think adding a new jar-name property will suffice, which can be optional and if omitted, is filled in with the name property.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-672",
        "summary": "new merge policy",
        "description": "New merge policy developed in the course of \nhttp://issues.apache.org/jira/browse/LUCENE-565\nhttp://issues.apache.org/jira/secure/attachment/12340475/newMergePolicy.Sept08.patch",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-793",
        "summary": "Javadocs should explain possible causes for IOExceptions",
        "description": "\nMost methods in Lucene reserve the right to throw an IOException.  This can occur for nearly all methods from low level problems like wrong permissions, transient IO errors, bad hard drive or corrupted file system, corrupted index, etc, but for some methods there are also more interesting causes that we should try to document.\n\nSpinoff of this thread:\n\n    http://www.gossamer-threads.com/lists/lucene/java-user/44929",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2092",
        "summary": "BooleanQuery.hashCode and equals ignore isCoordDisabled",
        "description": "BooleanQuery.isCoordDisabled() is not considered by BooleanQuery's hashCode() or equals() methods ... this can cause serious badness to happen when caching BooleanQueries.\n\nbug traces back to at least 1.9",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-384",
        "summary": "Range Query works only with lower case terms",
        "description": "I am performing a range query that returns results if the terms are lower \ncase, but does not return result when the terms are mixed case.\n\nIn my collection, I have terms alpha, beta, delta, gamma.  I am using the \nStandardAnalyzer for both indexing and searching.\n\nThe query [alpha TO gamma] returns all four terms.  When I perform the query \n[Alpha TO Gamma], no results are returned.\n\nIt appears the lowerCaseFilter(), which is a part of the StandardAnalyzer, \ndoes not work properly on the search terms.  I've used Luke to peek at my \ncollection, and the terms are all lower case in the collection.\n\nI'm fairly new to Lucene, so I hope I'm not making a \"common mistake\".",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3216",
        "summary": "Store DocValues per segment instead of per field",
        "description": "currently we are storing docvalues per field which results in at least one file per field that uses docvalues (or at most two per field per segment depending on the impl.). Yet, we should try to by default pack docvalues into a single file if possible. To enable this we need to hold all docvalues in memory during indexing and write them to disk once we flush a segment. ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2629",
        "summary": "In modules/analysys/icu, ant gennorm2 does not work",
        "description": "\nCommand to run gennorm2 does not work at present.  Also, icupkg needs to be called to convert the binary file to big-endian.\n\nI will attach a patch.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1197",
        "summary": "IndexWriter can flush too early when flushing by RAM usage",
        "description": "There is a silly bug in how DocumentsWriter tracks its RAM usage:\nwhenever term vectors are enabled, it incorrectly counts the space\nused by term vectors towards flushing, when in fact this space is\nrecycled per document.\n\nThis is not a functionality bug.  All it causes is flushes to happen\ntoo frequently, and, IndexWriter will use less RAM than you asked it\nto.  To work around it you can simply give it a bigger RAM buffer.\n\nI will commit a fix shortly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3311",
        "summary": "Cleanup XML QueryParser Code",
        "description": "Before I move the XML QueryParser to the queryparser module, I want to pass over it and bring it up to module standards.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": ""
    },
    {
        "key": "LUCENE-1742",
        "summary": "Wrap SegmentInfos in public class ",
        "description": "Wrap SegmentInfos in a public class so that subclasses of MergePolicy do not need to be in the org.apache.lucene.index package.  ",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3068",
        "summary": "The repeats mechanism in SloppyPhraseScorer is broken when doc has tokens at same position",
        "description": "In LUCENE-736 we made fixes to SloppyPhraseScorer, because it was\nmatching docs that it shouldn't; but I think those changes caused it\nto fail to match docs that it should, specifically when the doc itself\nhas tokens at the same position.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2272",
        "summary": "PayloadNearQuery has hardwired explanation for 'AveragePayloadFunction'",
        "description": "The 'explain' method in PayloadNearSpanScorer assumes the AveragePayloadFunction was used. This patch adds the 'explain' method to the 'PayloadFunction' interface, where the Scorer can call it. Added unit tests for 'explain' and for {Min,Max}PayloadFunction.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2509",
        "summary": "Improve readability of StandardTermsDictWriter",
        "description": "One variable is named indexWriter, but it is a termsIndexWriter. Also some layout.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-698",
        "summary": "FilteredQuery ignores boost",
        "description": "Filtered query ignores it's own boost.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2309",
        "summary": "Fully decouple IndexWriter from analyzers",
        "description": "IndexWriter only needs an AttributeSource to do indexing.\n\nYet, today, it interacts with Field instances, holds a private\nanalyzers, invokes analyzer.reusableTokenStream, has to deal with a\nwide variety (it's not analyzed; it is analyzed but it's a Reader,\nString; it's pre-analyzed).\n\nI'd like to have IW only interact with attr sources that already\narrived with the fields.  This would be a powerful decoupling -- it\nmeans others are free to make their own attr sources.\n\nThey need not even use any of Lucene's analysis impls; eg they can\nintegrate to other things like [OpenPipeline|http://www.openpipeline.org].\nOr make something completely custom.\n\nLUCENE-2302 is already a big step towards this: it makes IW agnostic\nabout which attr is \"the term\", and only requires that it provide a\nBytesRef (for flex).\n\nThen I think LUCENE-2308 would get us most of the remaining way -- ie, if the\nFieldType knows the analyzer to use, then we could simply create a\ngetAttrSource() method (say) on it and move all the logic IW has today\nonto there.  (We'd still need existing IW code for back-compat).\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3713",
        "summary": "TestIndexWriterOnDiskFull.testAddIndexOnDiskFull fails with java.lang.IllegalStateException: CFS has pending open files ",
        "description": "{noformat}\n Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull\n    [junit] Testcase: testAddIndexOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):\tCaused an ERROR\n    [junit] CFS has pending open files\n    [junit] java.lang.IllegalStateException: CFS has pending open files\n    [junit] \tat org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:162)\n    [junit] \tat org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:206)\n    [junit] \tat org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:4099)\n    [junit] \tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3661)\n    [junit] \tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3260)\n    [junit] \tat org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)\n    [junit] \tat org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1902)\n    [junit] \tat org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1716)\n    [junit] \tat org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1670)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddIndexOnDiskFull(TestIndexWriterOnDiskFull.java:304)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n    [junit] \n    [junit] \n    [junit] Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 31.96 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddIndexOnDiskFull -Dtests.seed=-7dd066d256827211:127c018cbf5b0975:20481cd18a7d8b6e -Dtests.multiplier=3 -Dtests.nightly=true -Dargs=\"-Dfile.encoding=ISO8859-1\"\n    [junit] NOTE: test params are: codec=SimpleText, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {field=DFR GB1, id=DFR I(F)L1, content=IB SPL-D3(800.0), f=DFR G2}, locale=de_AT, timezone=America/Cambridge_Bay\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestAssertions, TestSearchForDuplicates, TestMockAnalyzer, TestDocValues, TestPerFieldPostingsFormat, TestDocument, TestAddIndexes, TestConcurrentMergeScheduler, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentsWriterDeleteQueue, TestFieldInfos, TestFilterIndexReader, TestFlex, TestIndexInput, TestIndexWriter, TestIndexWriterMergePolicy, TestIndexWriterMerging, TestIndexWriterNRTIsCurrent, TestIndexWriterOnDiskFull]\n    [junit] NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=39156976,total=180748288\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1066",
        "summary": "better explain output",
        "description": "Very simple patch that slightly improves output of idf: show both docFreq and numDocs.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3099",
        "summary": "Grouping module should allow subclasses to set the group key per document",
        "description": "The new grouping module can only group by a single-valued indexed field.\n\nBut, if we make the 'getGroupKey' a method that a subclass could override, then I think we could refactor Solr over to the module, because it could do function queries and normal queries via subclass (I think).\n\nThis also makes the impl more extensible to apps that might have their own interesting group values per document.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1941",
        "summary": "MinPayloadFunction returns 0 when only one payload is present",
        "description": "In some experiments with payload scoring through PayloadTermQuery, I'm seeing 0 returned when using MinPayloadFunction.  I believe there is a bug there.  No time at the moment to flesh out a unit test, but wanted to report it for tracking.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1466",
        "summary": "CharFilter - normalize characters before tokenizer",
        "description": "This proposes to import CharFilter that has been introduced in Solr 1.4.\n\nPlease see for the details:\n- SOLR-822\n- http://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-575",
        "summary": "SpellChecker min score is increased by time",
        "description": "The minimum score, an instance variable, is modified in a search. That is wrong, since it makes it 1. thread unsafe and 2. not working. \n\nLucky enought it is only used from the one and same method call, so I simply compied the instance variable to a local method variable.\n\n        float min = this.min; \n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3426",
        "summary": "optimizer for n-gram PhraseQuery",
        "description": "If 2-gram is used and the length of query string is 4, for example q=\"ABCD\", QueryParser generates (when autoGeneratePhraseQueries is true) PhraseQuery(\"AB BC CD\") with slop 0. But it can be optimized PhraseQuery(\"AB CD\") with appropriate positions.\n\nThe idea came from the Japanese paper \"N.M-gram: Implementation of Inverted Index Using N-gram with Hash Values\" by Mikio Hirabayashi, et al. (The main theme of the paper is different from the idea that I'm using here, though)",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2424",
        "summary": "FieldDoc.toString only returns super.toString",
        "description": "The FieldDoc.toString method very carefully builds a StringBuffer sb containing the information for the FieldDoc instance and then just returns super.toString() instead of sb.toString()",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3558",
        "summary": "SearcherManager and NRTManager should be in the same package",
        "description": "I didnt even know NRTManager was still around, because its in the .index package, whereas SearcherManager is in the .search package.\n\nSeparately, I don't like that this stuff is so 'hard' with core lucene... would it be so bad if this stuff was added to core?\n\nI suspect a lot of people have issues with this stuff (see http://www.lucidimagination.com/search/document/37964e5f0e5d733b) for example.\n\nWorst case is just that, combine mistakes with trying to manage this stuff with MMap unmapping and total lack of error detection\nfor searching closed readers (LUCENE-3439) and its a mess.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-840",
        "summary": "contrib/benchmark unit tests",
        "description": "The need came up in this thread: \nhttp://www.mail-archive.com/java-dev@lucene.apache.org/msg09260.html\n\n: We might want to start thinking about Unit Tests...  :-)  Seems kind\n: of weird to have tests for tests, but this is becoming sufficiently\n: complex that it should have some tests.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2630",
        "summary": "make the build more friendly to apache harmony",
        "description": "as part of improved testing, i thought it would be a good idea to make the build (ant test) more friendly\nto working under apache harmony.\n\ni'm not suggesting we de-optimize code for sun jvms or anything crazy like that, only use it as a tool.\n\nfor example:\n* bugs in tests/code: for example i found a test that expected ArrayIOOBE \n  when really the javadoc contract for the method is just IOOBE... it just happens to\n  pass always on sun jvm because thats the implementation it always throws.\n* better reproduction of bugs: for example [2 months out of the year|http://en.wikipedia.org/wiki/Unusual_software_bug#Phase_of_the_Moon_bug]\n  it seems TestQueryParser fails with thai locale in a difficult-to-reproduce way.\n  but i *always* get similar failures like this with harmony for this test class.\n* better stability and portability: we should try (if reasonable) to avoid depending\n  upon internal details. the same kinds of things that fail in harmony might suddenly\n  fail in a future sun jdk. because its such a different impl, it brings out a lot of interesting stuff.\n\nat the moment there are currently a lot of failures, I think a lot might be caused by this: http://permalink.gmane.org/gmane.comp.java.harmony.devel/39484\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-470",
        "summary": "Refactoring and slight extension of regex testing code.",
        "description": "",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-936",
        "summary": "Typo on query parser syntax web page.",
        "description": "On the web page http://lucene.apache.org/java/docs/queryparsersyntax.html#N10126 the text says:\n\n\"To search for documents that must contain \"jakarta\" and may contain \"lucene\" use the query:\"\n\nThe example says:\n\n+jakarta apache\n\nThe problem:\nThe example uses apache where the text says lucene.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-369",
        "summary": "Cannot use Lucene in an unsigned applet due to Java security restrictions",
        "description": "A few of the Lucene source files call System.getProperty and perform a couple of\nother operations within static initializers that assume full Java 2 permissions.\nThis prevents Lucene from being used from an unsigned applet embedded in a\nbrowser page, since the default security permissions for an applet will prohibit\nreading of most properties.\n\nI would suggest wrapping the initialization of the properties in try/catch\nblocks. This does mean that a couple properties would need to be made non-final,\nand in some cases, getter and setter methods might be desirable to allow the\napplet programmer to change the property values at runtime (some variables are\npublic static and could be changed directly without accessors).\n\nThis problem occurs with the shipping 1.4.3 version as well as the latest (as of\n 07-apr-2005) source code fetched from CVS.\n\nCurrently, the files that are affected are org.apache.lucene.index.IndexWriter,\norg.apache.lucene.index.SegmentReader, org.apache.lucene.search.BooleanQuery,\nand org.apache.lucene.store.FSDirectory.\n\nI have modified versions of these files with some suggested changes, plus a\nsimple test applet and associated files that demonstrate the situation. The\nsample applet can be launched in a browser either by double-clicking the file\nlocally or by putting it on a web server and launching it from an http URL. As\nsoon as I can figure out how to attach to a bug report, I'll do that.\n\nP.S. This topic came up in August, 2004 in lucene dev mailing list but as far as\nI can tell, has not yet been resolved.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2041",
        "summary": "Complete parallelizaton of ParallelMultiSearcher",
        "description": "ParallelMultiSearcher is parallel only for the method signatures of 'search'.\n\nPart of a query process calls the method docFreq(). There was a TODO comment to parallelize this. Parallelizing this method actually increases the performance of a query on multiple indexes, especially remotely.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3679",
        "summary": "Replace IndexReader.getFieldNames with IndexReader.getFieldInfos",
        "description": "",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-638",
        "summary": "Can't put non-index files (e.g. CVS, SVN directories) in a Lucene index directory",
        "description": "Lucene won't tolerate foreign files in its index directories.  This makes it impossible to keep an index in a CVS or Subversion repository.\n\nFor instance, this exception appears when creating a RAMDirectory from a java.io.File that contains a subdirectory called \".svn\".\n\njava.io.FileNotFoundException: /home/local/ejj/ic/.caches/.search/.index/.svn\n(Is a directory)\n        at java.io.RandomAccessFile.open(Native Method)\n        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\n        at\norg.apache.lucene.store.FSIndexInput$Descriptor.<init>(FSDirectory.java:425)\n        at org.apache.lucene.store.FSIndexInput.<init>(FSDirectory.java:434)\n        at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:324)\n        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:61)\n        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:86)",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2753",
        "summary": "IndexReader.listCommits should return a List and not an abstract Collection",
        "description": "Spinoff from here: http://www.mail-archive.com/dev@lucene.apache.org/msg07509.html",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3864",
        "summary": "support offsets in MemoryPostings",
        "description": "Really we should add this for Sep & Pulsing too... but this is one more",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-493",
        "summary": "Nightly build archives do not contain Java source code.",
        "description": "Under the Lucene News section of the Overview page, this item's link:\n\n26 January 2006 - Nightly builds available\nhttp://cvs.apache.org/dist/lucene/java/nightly/\n\ngoes to a directory with several 1.9M files, none of which have the src/java tree in them.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-601",
        "summary": "RAMDirectory implements Serializable",
        "description": "RAMDirectory is for some reason not serializable.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2834",
        "summary": "don't spawn thread statically in FSDirectory on Mac OS X",
        "description": "on the Mac, creating the digester starts up a PKCS11 thread.\n\nI do not think threads should be created statically (I have this same issue with TimeLimitedCollector and also FilterManager).\n\nUwe discussed removing this md5 digester, I don't care if we remove it or not, just as long as it doesn't create a thread,\nand just as long as it doesn't use the system default locale.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1995",
        "summary": "ArrayIndexOutOfBoundsException during indexing",
        "description": "http://search.lucidimagination.com/search/document/f29fc52348ab9b63/arrayindexoutofboundsexception_during_indexing",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2273",
        "summary": "FieldCacheImpl's getCacheEntries() is buggy as it uses WeakHashMap incorrectly and leads to ConcurrentModExceptions",
        "description": "The way how WeakHashMap works internally leads to the fact that it is not allowed to iterate over a WHM.keySet() and then get() the value. As each get() operation inspects the ReferenceQueue of the weak keys, they may suddenly disappear. If you use the entrySet() iterator you get key and value and no need to call get(), contains(),... that inspects the ReferenceQueue.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1036",
        "summary": "Unreleased 2.3 version of IndexWriter.optimize()  consistly throws java.lang.IllegalArgumentException out-of-the-box",
        "description": "Since the upcoming 2.3 version of Lucene has support for the setRAMBufferSizeMB() method in Index Writer,  I thought I would test its performance.   So, using my application that was built upon (and worked with) Lucene 2.2,  I downloaded the nightly build 2007-10-26_03-16-46 and rebuilt my application with new code setting setRAMBufferSizeMB() from a properties file.   My test data resides in a database table of 30 columns holding 1.25 million records.   The good news is that performance is superior to Lucene 2.2.  The indexing completes in roughly 1/3 the time.   The bad news is the Index Writer.optimize() step now throws an java.lang.IllegalArgumentException.\nI also run tests against various other tables.  Indexing smaller amounts of data did not throw the exception.  Indexing largers amounts of data did throw the exception.  Note, I also tested nightly builds dating back to 2007-10-05.\n\n...\nINFO:  SEIndexThread.commitCheck...\nINFO:    ----Commit point reached:  1200000\nINFO:  SEIndexThread.commitCheck...\nINFO:    ----Commit point reached:  1225000\nINFO:  SEIndexThread.commitCheck...\nINFO:    ----Commit point reached:  1250000\nINFO:  SEIndexThread.closeIndex()...\nINFO:    ----commit point reached:  1250659\nINFO:    ----optimize index\nINFO: SEIndexThread():  java.lang.IllegalArgumentException\n\njava.lang.IllegalArgumentException\n        at java.lang.Thread.setPriority(Thread.java(Compiled Code))\n        at org.apache.lucene.index.ConcurrentMergeScheduler.merge(ConcurrentMerg\neScheduler.java(Compiled Code))\n        at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1750)\n        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1686)\n        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1652)\n        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:643)\n        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:636)\n        at SEIndexThread.closeIndex(SEIndexThread.java:674)\n        at SEIndexThread.processSearchObject(SEIndexThread.java:487)\n        at SEIndexThread.prepareIndex(SEIndexThread.java:391)\n        at SEIndexThread.run(SEIndexThread.java:41)\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-348",
        "summary": "fileformats.xml doesn't document compound file streams",
        "description": "Current versions of Lucene generate segments in compound file stream format\nfiles, but the fileformats documentation does not have any description of the\nformat for those files.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-370",
        "summary": "BooleanQuery assumes everything else implements skipTo",
        "description": "skipTo seems to be optional functionality on the Scorer class (BooleanScorer\ndoesn't implement it).  BooleanQuery.scorer() tests all subclauses using\n\"instanceof BooleanQuery\" to determine if it can use a ConjunctionScorer that\nrequires skipTo functionality.\n\nThis means that any other new Query/Scorer that don't implement skipTo will get\ninto trouble when included in a BooleanQuery.\n\nIf skipTo is really optional, then there should be some way of telling by the\nScorer or the Query in a more generic manner.\n\nSome options:\n1) have a \"boolean Scorer.hasSkipTo()\" method\n2) have a \"boolean Query.hasSkipTo()\" method\n3) remove Scorer.skipTo and have a \"public interface ScorerSkipTo{boolean\nskipTo(int doc)}\" that scorers may implement",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3003",
        "summary": "Move UnInvertedField into Lucene core",
        "description": "Solr's UnInvertedField lets you quickly lookup all terms ords for a\ngiven doc/field.\n\nLike, FieldCache, it inverts the index to produce this, and creates a\nRAM-resident data structure holding the bits; but, unlike FieldCache,\nit can handle multiple values per doc, and, it does not hold the term\nbytes in RAM.  Rather, it holds only term ords, and then uses\nTermsEnum to resolve ord -> term.\n\nThis is great eg for faceting, where you want to use int ords for all\nof your counting, and then only at the end you need to resolve the\n\"top N\" ords to their text.\n\nI think this is a useful core functionality, and we should move most\nof it into Lucene's core.  It's a good complement to FieldCache.  For\nthis first baby step, I just move it into core and refactor Solr's\nusage of it.\n\nAfter this, as separate issues, I think there are some things we could\nexplore/improve:\n\n  * The first-pass that allocates lots of tiny byte[] looks like it\n    could be inefficient.  Maybe we could use the byte slices from the\n    indexer for this...\n\n  * We can improve the RAM efficiency of the TermIndex: if the codec\n    supports ords, and we are operating on one segment, we should just\n    use it.  If not, we can use a more RAM-efficient data structure,\n    eg an FST mapping to the ord.\n\n  * We may be able to improve on the main byte[] representation by\n    using packed ints instead of delta-vInt?\n\n  * Eventually we should fold this ability into docvalues, ie we'd\n    write the byte[] image at indexing time, and then loading would be\n    fast, instead of uninverting\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1490",
        "summary": "CJKTokenizer convert   HALFWIDTH_AND_FULLWIDTH_FORMS wrong",
        "description": "CJKTokenizer have these lines..\n                if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS) {\n                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */\n                    int i = (int) c;\n                    i = i - 65248;\n                    c = (char) i;\n                }\n\nThis is wrong. Some character in the block (e.g. U+ff68) have no BASIC_LATIN counterparts.\nOnly 65281-65374 can be converted this way.\n\nThe fix is\n\n             if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS && i <= 65474 && i> 65281) {\n                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */\n                    int i = (int) c;\n                    i = i - 65248;\n                    c = (char) i;\n                }",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1521",
        "summary": "\"fdx size mismatch\" exception in StoredFieldsWriter.closeDocStore() when closing index with 500M documents",
        "description": "When closing index that contains 500,000,000 randomly generated documents, an exception is thrown:\n\njava.lang.RuntimeException: after flush: fdx size mismatch: 500000000 docs vs 4000000004 length in bytes of _0.fdx\n\tat org.apache.lucene.index.StoredFieldsWriter.closeDocStore(StoredFieldsWriter.java:94)\n\tat org.apache.lucene.index.DocFieldConsumers.closeDocStore(DocFieldConsumers.java:83)\n\tat org.apache.lucene.index.DocFieldProcessor.closeDocStore(DocFieldProcessor.java:47)\n\tat org.apache.lucene.index.DocumentsWriter.closeDocStore(DocumentsWriter.java:367)\n\tat org.apache.lucene.index.IndexWriter.flushDocStores(IndexWriter.java:1688)\n\tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3518)\n\tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3442)\n\tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1623)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1588)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1562)\n        ...\n\nThis appears to be a bug at StoredFieldsWriter.java:93:\n\n      if (4+state.numDocsInStore*8 != state.directory.fileLength(state.docStoreSegmentName + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION))\n\nwhere the multiplication by 8 is causing integer overflow. The fix would be to cast state.numDocsInStore to long before multiplying.\n\nIt appears that this is another instance of the mistake that caused bug LUCENE-1519. I did a cursory seach for \\*8 against the code to see if there might be yet more instances of the same mistake, but found none. \n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1808",
        "summary": "make Query.createWeight public (or add back Query.createQueryWeight())",
        "description": "Now that the QueryWeight class has been removed, the public QueryWeight createQueryWeight() method on Query was also removed\n\ni have cases where i want to create a weight for a sub query (outside of the org.apache.lucene.search package) and i don't want the weight normalized (think BooleanQuery outside of the o.a.l.search package)\n\nin order to do this, i have to create a static Utils class inside o.a.l.search, pass in the Query and searcher, and have the static method call the protected createWeight method\nthis should not be necessary\n\nThis could be fixed in one of 2 ways:\n1. make createWeight() public on Query (breaks back compat)\n2. add the following method:\n{code}\npublic Weight createQueryWeight(Searcher searcher) throws IOException {\n  return createWeight(searcher);\n}\n{code}\n\ncreateWeight(Searcher) should then be deprectated in favor of the publicly accessible method\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1841",
        "summary": "Provide Summary Information on the Files in the Lucene index",
        "description": "I find myself often having to remember, by file extension, what is in a particular index file.  The information is all contained in the File Formats, but not summarized.  This patch provides a simple table that describes the extensions and provides links to the relevant section.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1253",
        "summary": "LengthFilter and other TokenFilters that skip tokens ignore relative positionIncrement",
        "description": "See for reference:\nhttp://www.nabble.com/WordDelimiterFilter%2BLenghtFilter-results-in-termPosition%3D%3D-1-td16306788.html\nand http://www.nabble.com/Lucene---Java-f24284.html\n\nIt seems that LengthFilter (at least) could produce a stream in which the first Token has a positionIncrement of 0, which make CheckIndex and Luke function \"Reconstruct&Edit\" to generate exception.\n\nShould something be done to avoid this situation, or could the error be ignored (by allowing Term with a position of -1, and relaxing CheckIndex checks?)\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1096",
        "summary": "Deleting docs of all returned Hits during search causes ArrayIndexOutOfBoundsException",
        "description": "For background user discussion:\nhttp://www.nabble.com/document-deletion-problem-to14414351.html\n\n{code}\nHits h = m_indexSearcher.search(q); // Returns 11475 documents \nfor(int i = 0; i < h.length(); i++) \n{ \n  int doc = h.id(i); \n  m_indexSearcher.getIndexReader().deleteDocument(doc);  <-- causes ArrayIndexOutOfBoundsException when i = 6400\n} \n{code}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2043",
        "summary": "Fix CommitIndexTask to also commit IndexReader changes",
        "description": "I'm setting up a benchmark for LUCENE-1458, and one limitation I hit is that the CommitIndexTask doesn't commit pending changes in the IndexReader (eg via DeleteByPercent), using a named commit point.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1068",
        "summary": "Invalid behavior of StandardTokenizerImpl",
        "description": "The following code prints the output of StandardAnalyzer:\n\n        Analyzer analyzer = new StandardAnalyzer();\n        TokenStream ts = analyzer.tokenStream(\"content\", new StringReader(\"<some text>\"));\n        Token t;\n        while ((t = ts.next()) != null) {\n            System.out.println(t);\n        }\n\nIf you pass \"www.abc.com\", the output is (www.abc.com,0,11,type=<HOST>) (which is correct in my opinion).\nHowever, if you pass \"www.abc.com.\" (notice the extra '.' at the end), the output is (wwwabccom,0,12,type=<ACRONYM>).\n\nI think the behavior in the second case is incorrect for several reasons:\n1. It recognizes the string incorrectly (no argue on that).\n2. It kind of prevents you from putting URLs at the end of a sentence, which is perfectly legal.\n3. An ACRONYM, at least to the best of my understanding, is of the form A.B.C. and not ABC.DEF.\n\nI looked at StandardTokenizerImpl.jflex and I think the problem comes from this definition:\n// acronyms: U.S.A., I.B.M., etc.\n// use a post-filter to remove dots\nACRONYM    =  {ALPHA} \".\" ({ALPHA} \".\")+\n\nNotice how the comment relates to acronym as U.S.A., I.B.M. and not something else. I changed the definition to\nACRONYM    =  {LETTER} \".\" ({LETTER} \".\")+\nand it solved the problem.\n\nThis was also reported here:\nhttp://www.nabble.com/Inconsistent-StandardTokenizer-behaviour-tf596059.html#a1593383\nhttp://www.nabble.com/Standard-Analyzer---Host-and-Acronym-tf3620533.html#a10109926\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3601",
        "summary": "testIWondiskfull unreferenced files failure",
        "description": "NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=aff9b14dd518cfb:4d2f112726e2947f:-2b03094a43a947ee -Dtests.multiplier=3 -Dargs=\"-Dfile.encoding=ISO8859-1\"\n\nReproduces some of the time...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-563",
        "summary": "IndexReader currently has javadoc errors",
        "description": "Current trunk has some javadoc errors in IndexReader and some more in contrib.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-524",
        "summary": "Current implementation of fuzzy and wildcard queries inappropriately implemented as Boolean query rewrites",
        "description": "The implementation of MultiTermQuery in terms of BooleanQuery introduces several problems:\n\n1) Collisions with maximum clause limit on boolean queries which throws an exception.  This is most problematic because it is difficult to ascertain in advance how many terms a fuzzy query or wildcard query might involve.\n\n2) The boolean disjunctive scoring is not appropriate for either fuzzy or wildcard queries.  In effect the score is divided by the number of terms in the query which has nothing to do with the relevancy of the results.\n\n3) Performance of disjunctive boolean queries for large term sets is quite sub-optimal",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-275",
        "summary": "Occur incompletely implemented for remote use.",
        "description": "Occur does not implement readResolve() creating problems for\nParallelMultiSearcher y.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2865",
        "summary": "Pass a context struct to Weight#scorer instead of naked booleans",
        "description": "Weight#scorer(AtomicReaderContext, boolean, boolean) is hard to extend if another boolean like \"needsScoring\" or similar flags / information need to be passed to Scorers. An immutable struct would make such an extension trivial / way easier. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-959",
        "summary": "Document Vector->ArrayList",
        "description": "Document Vector should be changed to ArrayList.\nDocument is not advertised to be thread safe, and it's doubtful that anyone modifies a Document from multiple threads.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2526",
        "summary": ".toString on empty MultiPhraseQuery hits NPE",
        "description": "Ross Woolf hit this on java-user thread \"MultiPhraseQuery.toString() throws null pointer exception\".  It's still present on trunk...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2672",
        "summary": "speed up automaton seeking in nextString",
        "description": "While testing, i found there are some queries (e.g. wildcard ?????????) that do quite a lot of backtracking.\n\nnextString doesn't handle this particularly well, when it walks the DFA, if it hits a dead-end and needs to backtrack, it increments the bytes, and starts over completely.\n\nalternatively it could save the path information in an int[], and backtrack() could return a position to restart from, instead of just a boolean.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-939",
        "summary": "Check for boundary conditions in FieldInfos",
        "description": "In FieldInfos there are three methods in which we don't check for\nboundary conditions but catch e. g. an IndexOutOfBoundsException\nor a NPE. I think this isn't good code style and is probably not\neven faster than checking explicitly.\n\n\"Exceptions should not be used to alter the flow of a program as \npart of normal execution.\"\n\nAlso this can be irritating when you're trying to debug an \nIndexOutOfBoundsException that is thrown somewhere else in your\nprogram and you place a breakpoint on that exception.\n\nThe three methods are:\n\n  public int fieldNumber(String fieldName) {\n    try {\n      FieldInfo fi = fieldInfo(fieldName);\n      if (fi != null)\n        return fi.number;\n    }\n    catch (IndexOutOfBoundsException ioobe) {\n      return -1;\n    }\n    return -1;\n  }\n  \n\n  public String fieldName(int fieldNumber) {\n    try {\n      return fieldInfo(fieldNumber).name;\n    }\n    catch (NullPointerException npe) {\n      return \"\";\n    }\n  }\n  \n  \n  public FieldInfo fieldInfo(int fieldNumber) {\n    try {\n      return (FieldInfo) byNumber.get(fieldNumber);\n    }\n    catch (IndexOutOfBoundsException ioobe) {\n      return null;\n    }\n  }",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-503",
        "summary": "Contrib: ThaiAnalyzer to enable Thai full-text search in Lucene",
        "description": "Thai text don't have space between words. Usually, a dictionary-based algorithm is used to break string into words. For Lucene to be usable for Thai, an Analyzer that know how to break Thai words is needed.\n\nI've implemented such Analyzer, ThaiAnalyzer, using ICU4j DictionaryBasedBreakIterator for word breaking. I'll upload the code later.\n\nI'm normally a C++ programmer and very new to Java. Please review the code for any problem. One possible problem is that it requires ICU4j. I don't know whether this is OK.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2066",
        "summary": "Add Highlighter test for RegexQuery",
        "description": "",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1157",
        "summary": "Formatable changes log  (CHANGES.txt is easy to edit but not so friendly to read by Lucene users)",
        "description": "Background in http://www.nabble.com/formatable-changes-log-tt15078749.html",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2251",
        "summary": "Change contrib tests to use the special LuceneTestCase(J4) constant for the current version used a matchVersion parameter",
        "description": "Sub issue for contrib changes",
        "label": "NUG",
        "classified": "TEST",
        "type": ""
    },
    {
        "key": "LUCENE-440",
        "summary": "FilteredQuery should have getFilter()",
        "description": "Unless you are in the same package, you can't access the filter in a FilteredQuery.\nA getFilter() method should be added.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1467",
        "summary": "Consolidate Solr's and Lucene's OpenBitSet classes",
        "description": "See SOLR-875 for details.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2332",
        "summary": "Merge CharTermAttribute and deprecations to stable",
        "description": "This should be merged to trunk until flex lands, so the analyzers can be ported to new api.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1943",
        "summary": "ChineseFilter is inefficient",
        "description": "trivial patch to use CharArraySet, so it can use termBuffer() instead of term()\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1480",
        "summary": "Wrap messages output with a check of InfoStream != null",
        "description": "I've found several places in the code where messages are output w/o first checking if infoStream != null. The result is that in most of the time, unnecessary strings are created but never output (because infoStream is not set). We should follow Java's logging best practices, where a log message is always output in the following format:\nif (logger.isLoggable(leve)) {\n    logger.log(level, msg);\n}\n\nLog messages are usually created w/o paying too much attention to performance (such as string concatenation using '+' instead of StringBuffer). Therefore, at runtime it is important to avoid creating those messages, if they will be discarded eventually.\n\nI will add a method to IndexWriter messagesEnabled() and then use it wherever a call to iw.message() is made.\n\nPatch will follow",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1311",
        "summary": "Add ability to open prior commits to IndexReader",
        "description": "If you use a customized DeletionPolicy, which keeps multiple commits\naround (instead of the default which is to only preserve the most\nrecent commit), it's useful to be able to list all such commits and\nthen open a reader against one of these commits.\n\nI've added this API to list commits:\n\n  public static Collection IndexReader.listCommits(Directory)\n\nand these two new open methods to IndexReader to open a specific commit:\n\n  public static IndexReader open(IndexCommit)\n  public static IndexReader open(IndexCommit, IndexDeletionPolicy)\n\nSpinoff from here:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3c85d3c3b60806161735o207a3238sa2e6c415171a8019@mail.gmail.com%3e\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-811",
        "summary": "Public API inconsistency",
        "description": "org.apache.lucene.index.SegmentInfos is public, and contains public methods (which is good for expert-level index manipulation tools such as Luke). However, SegmentInfo class has package visibility. This leads to a strange result that it's possible to read SegmentInfos, but it's not possible to access its details (SegmentInfos.info(int)) from a user application.\n\nThe solution is to make SegmentInfo class public.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3111",
        "summary": "TestFSTs.testRandomWords failure",
        "description": "Was running some while(1) tests on the docvalues branch (r1103705) and the following test failed:\n\n{code}\n    [junit] Testsuite: org.apache.lucene.util.automaton.fst.TestFSTs\n    [junit] Testcase: testRandomWords(org.apache.lucene.util.automaton.fst.TestFSTs):\tFAILED\n    [junit] expected:<771> but was:<TwoLongs:771,771>\n    [junit] junit.framework.AssertionFailedError: expected:<771> but was:<TwoLongs:771,771>\n    [junit] \tat org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.verifyUnPruned(TestFSTs.java:540)\n    [junit] \tat org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:496)\n    [junit] \tat org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:359)\n    [junit] \tat org.apache.lucene.util.automaton.fst.TestFSTs.doTest(TestFSTs.java:319)\n    [junit] \tat org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:940)\n    [junit] \tat org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:915)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)\n    [junit] \n    [junit] \n    [junit] Tests run: 7, Failures: 1, Errors: 0, Time elapsed: 7.628 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: Ignoring nightly-only test method 'testBigSet'\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestFSTs -Dtestmethod=testRandomWords -Dtests.seed=-269475578956012681:0\n    [junit] NOTE: test params are: codec=PreFlex, locale=ar, timezone=America/Blanc-Sablon\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestToken, TestCodecs, TestIndexReaderReopen, TestIndexWriterMerging, TestNoDeletionPolicy, TestParallelReaderEmptyIndex, TestParallelTermEnum, TestPerSegmentDeletes, TestSegmentReader, TestSegmentTermDocs, TestStressAdvance, TestTermVectorsReader, TestSurrogates, TestMultiFieldQueryParser, TestAutomatonQuery, TestBooleanScorer, TestFuzzyQuery, TestMultiTermConstantScore, TestNumericRangeQuery64, TestPositiveScoresOnlyCollector, TestPrefixFilter, TestQueryTermVector, TestScorerPerf, TestSloppyPhraseQuery, TestSpansAdvanced, TestWindowsMMap, TestRamUsageEstimator, TestSmallFloat, TestUnicodeUtil, TestFSTs]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=137329960,total=208207872\n    [junit] ------------- ---------------- ---------------\n    [junit] TEST org.apache.lucene.util.automaton.fst.TestFSTs FAILED\n{code}\n\nI am not able to reproduce",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3894",
        "summary": "Make BaseTokenStreamTestCase a bit more evil",
        "description": "Throw an exception from the Reader while tokenizing, stop after not consuming all tokens, sometimes spoon-feed chars from the reader...",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1492",
        "summary": "Allow readOnly OpenReader task",
        "description": "I'd like to change OpenReader in contrib/benchmark to open a readOnly reader by default, and take readOnly optional param if for some reason a \"writable IndexReader\" becomes necessary in the future.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1700",
        "summary": "LogMergePolicy.findMergesToExpungeDeletes need to get deletes from the SegmentReader",
        "description": "With LUCENE-1516, deletes are carried over in the SegmentReaders\nwhich means implementations of\nMergePolicy.findMergesToExpungeDeletes (such as LogMergePolicy)\nneed to obtain deletion info from the SR (instead of from the\nSegmentInfo which won't have the information).",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1850",
        "summary": "Update overview example code",
        "description": "See http://lucene.apache.org/java/2_4_1/api/core/overview-summary.html - need to update for non-deprecated best-practices/recommended API usage.\n\nAlso, double-check that the demo app works as documented.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2266",
        "summary": "problem with edgengramtokenfilter and highlighter",
        "description": "i ran into a problem while using the edgengramtokenfilter, it seems to report incorrect offsets when generating tokens, more specifically all the tokens have offset 0 and term length as start and end, this leads to goofy highlighting behavior when creating edge grams for tokens beyond the first one, i created a small patch that takes into account the start of the original token and adds that to the reported start/end offsets.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3465",
        "summary": "IndexSearcher fails to pass docBase to Collector when using ExecutorService",
        "description": "This bug is causing the failure in TestSearchAfter.\n\nWe are now always passing docBase 0 to Collector when you use ExecutorService with IndexSearcher.\n\nThis doesn't affect trunk (AtomicReaderContext carries the right docBase); only 3.x.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1567",
        "summary": "New flexible query parser",
        "description": "From \"New flexible query parser\" thread by Micheal Busch\n\nin my team at IBM we have used a different query parser than Lucene's in\nour products for quite a while. Recently we spent a significant amount\nof time in refactoring the code and designing a very generic\narchitecture, so that this query parser can be easily used for different\nproducts with varying query syntaxes.\n\nThis work was originally driven by Andreas Neumann (who, however, left\nour team); most of the code was written by Luis Alves, who has been a\nbit active in Lucene in the past, and Adriano Campos, who joined our\nteam at IBM half a year ago. Adriano is Apache committer and PMC member\non the Tuscany project and getting familiar with Lucene now too.\n\nWe think this code is much more flexible and extensible than the current\nLucene query parser, and would therefore like to contribute it to\nLucene. I'd like to give a very brief architecture overview here,\nAdriano and Luis can then answer more detailed questions as they're much\nmore familiar with the code than I am.\nThe goal was it to separate syntax and semantics of a query. E.g. 'a AND\nb', '+a +b', 'AND(a,b)' could be different syntaxes for the same query.\nWe distinguish the semantics of the different query components, e.g.\nwhether and how to tokenize/lemmatize/normalize the different terms or\nwhich Query objects to create for the terms. We wanted to be able to\nwrite a parser with a new syntax, while reusing the underlying\nsemantics, as quickly as possible.\nIn fact, Adriano is currently working on a 100% Lucene-syntax compatible\nimplementation to make it easy for people who are using Lucene's query\nparser to switch.\n\nThe query parser has three layers and its core is what we call the\nQueryNodeTree. It is a tree that initially represents the syntax of the\noriginal query, e.g. for 'a AND b':\n  AND\n /   \\\nA     B\n\nThe three layers are:\n1. QueryParser\n2. QueryNodeProcessor\n3. QueryBuilder\n\n1. The upper layer is the parsing layer which simply transforms the\nquery text string into a QueryNodeTree. Currently our implementations of\nthis layer use javacc.\n2. The query node processors do most of the work. It is in fact a\nconfigurable chain of processors. Each processors can walk the tree and\nmodify nodes or even the tree's structure. That makes it possible to\ne.g. do query optimization before the query is executed or to tokenize\nterms.\n3. The third layer is also a configurable chain of builders, which\ntransform the QueryNodeTree into Lucene Query objects.\n\nFurthermore the query parser uses flexible configuration objects, which\nare based on AttributeSource/Attribute. It also uses message classes that\nallow to attach resource bundles. This makes it possible to translate\nmessages, which is an important feature of a query parser.\n\nThis design allows us to develop different query syntaxes very quickly.\nAdriano wrote the Lucene-compatible syntax in a matter of hours, and the\nunderlying processors and builders in a few days. We now have a 100%\ncompatible Lucene query parser, which means the syntax is identical and\nall query parser test cases pass on the new one too using a wrapper.\n\n\nRecent posts show that there is demand for query syntax improvements,\ne.g improved range query syntax or operator precedence. There are\nalready different QP implementations in Lucene+contrib, however I think\nwe did not keep them all up to date and in sync. This is not too\nsurprising, because usually when fixes and changes are made to the main\nquery parser, people don't make the corresponding changes in the contrib\nparsers. (I'm guilty here too)\nWith this new architecture it will be much easier to maintain different\nquery syntaxes, as the actual code for the first layer is not very much.\nAll syntaxes would benefit from patches and improvements we make to the\nunderlying layers, which will make supporting different syntaxes much\nmore manageable.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1236",
        "summary": "EdgeNGram* documentation improvement",
        "description": "To clarify what \"edge\" means, I added some description. That edge means the beggining edge of a term or ending edge of a term.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1453",
        "summary": "When reopen returns a new IndexReader, both IndexReaders may now control the lifecycle of the underlying Directory which is managed by reference counting",
        "description": "Rough summary. Basically, FSDirectory tracks references to FSDirectory and when IndexReader.reopen shares a Directory with a created IndexReader and closeDirectory is true, FSDirectory's ref management will see two decrements for one increment. You can end up getting an AlreadyClosed exception on the Directory when the IndexReader is open.\n\nI have a test I'll put up. A solution seems fairly straightforward (at least in what needs to be accomplished).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3818",
        "summary": "TestIndexWriterNRTIsCurrent failure",
        "description": "found by jenkins: \n\nhttps://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12492/\n\nmake your computer busy (e.g. run tests in another checkout) then,\n\nant test-core -Dtests.iter=100 -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=\"-Dfile.encoding=ISO8859-1\"\n\ntakes a few tries till it pops...\n\n{noformat}\njunit-sequential:\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterNRTIsCurrent\n    [junit] Tests run: 100, Failures: 1, Errors: 1, Time elapsed: 277.818 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] WARNING: you are using -Dtests.iter=n where n > 1, not all tests support this option.\n    [junit] Some may crash or fail: this is not a bug.\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=\"-Dfile.encoding=ISO8859-1\"\n    [junit] The following exceptions were thrown by threads:\n    [junit] *** Thread: Lucene Merge Thread #17 ***\n    [junit] org.apache.lucene.index.MergePolicy$MergeException: java.lang.AssertionError\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:520)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:480)\n    [junit] Caused by: java.lang.AssertionError\n    [junit] \tat org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.initWritableLiveDocs(IndexWriter.java:580)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commitMergedDeletes(IndexWriter.java:3061)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commitMerge(IndexWriter.java:3137)\n    [junit] \tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3718)\n    [junit] \tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=\"-Dfile.encoding=ISO8859-1\"\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=\"-Dfile.encoding=ISO8859-1\"\n    [junit] NOTE: test params are: codec=Lucene40: {id=MockFixedIntBlock(blockSize=525)}, sim=DefaultSimilarity, locale=es_PY, timezone=Africa/Luanda\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestIndexWriterNRTIsCurrent]\n    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=74907448,total=255787008\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testIsCurrentWithThreads(org.apache.lucene.index.TestIndexWriterNRTIsCurrent):\tFAILED\n    [junit] info=_qx(4.0):C1/1 isn't live\n    [junit] junit.framework.AssertionFailedError: info=_qx(4.0):C1/1 isn't live\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n    [junit] \tat org.apache.lucene.index.IndexWriter$ReaderPool.infoIsLive(IndexWriter.java:663)\n    [junit] \tat org.apache.lucene.index.IndexWriter$ReaderPool.dropAll(IndexWriter.java:717)\n    [junit] \tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1136)\n    [junit] \tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1069)\n    [junit] \tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1033)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterNRTIsCurrent.testIsCurrentWithThreads(TestIndexWriterNRTIsCurrent.java:68)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:707)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:606)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:511)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:569)\n    [junit] \n    [junit] \n    [junit] Testcase: testIsCurrentWithThreads(org.apache.lucene.index.TestIndexWriterNRTIsCurrent):\tCaused an ERROR\n    [junit] java.lang.AssertionError: Some threads threw uncaught exceptions!\n    [junit] java.lang.RuntimeException: java.lang.AssertionError: Some threads threw uncaught exceptions!\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.tearDownInternal(LuceneTestCase.java:780)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.access$1000(LuceneTestCase.java:138)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:607)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:511)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:569)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.checkUncaughtExceptionsAfter(LuceneTestCase.java:808)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.tearDownInternal(LuceneTestCase.java:752)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.index.TestIndexWriterNRTIsCurrent FAILED\n\nBUILD FAILED\n{noformat}",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3577",
        "summary": "rename expungeDeletes",
        "description": "Similar to optimize(), expungeDeletes() has a misleading name.\n\nWe already had problems with this on the user list because TieredMergePolicy\ndidn't 'expunge' all their deletes.\n\nAlso I think expunge is the wrong word, because expunge makes it seem\nlike you just wrangle up the deletes and kick them out of the party and\nthat it should be fast.\n\n\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2972",
        "summary": "Intermittent failure in TestFieldCacheTermsFilter.testMissingTerms",
        "description": "Running tests in while(1) I hit this:\n\n{noformat}\n\nNOTE: reproduce with: ant test -Dtestcase=TestFieldCacheTermsFilter -Dtestmethod=testMissingTerms -Dtests.seed=-1046382732738729184:5855929314778232889\n\n1) testMissingTerms(org.apache.lucene.search.TestFieldCacheTermsFilter)\njava.lang.AssertionError: Must match 1 expected:<1> but was:<0>\n\tat org.junit.Assert.fail(Assert.java:91)\n\tat org.junit.Assert.failNotEquals(Assert.java:645)\n\tat org.junit.Assert.assertEquals(Assert.java:126)\n\tat org.junit.Assert.assertEquals(Assert.java:470)\n\tat org.apache.lucene.search.TestFieldCacheTermsFilter.testMissingTerms(TestFieldCacheTermsFilter.java:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n\tat org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n\tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1214)\n\tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1146)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n\tat org.junit.runners.Suite.runChild(Suite.java:128)\n\tat org.junit.runners.Suite.runChild(Suite.java:24)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:157)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:136)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:117)\n\tat org.junit.runner.JUnitCore.runMain(JUnitCore.java:98)\n\tat org.junit.runner.JUnitCore.runMainAndExit(JUnitCore.java:53)\n\tat org.junit.runner.JUnitCore.main(JUnitCore.java:45)\n{noformat}\n\nUnfortunately the seed doesn't [consistently] repro for me...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1696",
        "summary": "Added New Token API impl for ASCIIFoldingFilter",
        "description": "I added an implementation of incrementToken to ASCIIFoldingFilter.java and extended the existing  testcase for it.\nI will attach the patch shortly.\nBeside this improvement I would like to start up a small discussion about this filter. ASCIIFoldingFitler is meant to be a replacement for ISOLatin1AccentFilter which is quite nice as it covers a superset of the latter. I have used this filter quite often but never on a as it is basis. In the most cases this filter does the correct thing (replace a special char with its ascii correspondent) but in some cases like for German umlaut it does not return the expected result. A german umlaut  like '\u00e4' does not translate to a but rather to 'ae'. I would like to change this but I'n not 100% sure if that is expected by all users of that filter. Another way of doing it would be to make it configurable with a flag. This would not affect performance as we only check if such a umlaut char is found. \nFurther it would be really helpful if that filter could \"inject\" the original/unmodified token with the same position increment into the token stream on demand. I think its a valid use-case to index the modified and unmodified token. For instance, the german word \"s\u00fcd\" would be folded to \"sud\". In a query q:(s\u00fcd) the filter would also fold to sud and therefore find sud which has a totally different meaning. Folding works quite well but for special cases would could add those options to make users life easier. The latter could be done in a subclass while the umlaut problem should be fixed in the base class.\n\nsimon ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2249",
        "summary": "ParallelMultiSearcher should shut down thread pool on close",
        "description": "ParallelMultiSearcher does not shut down its internal thread pool on close. As a result, programs that create multiple instances of this class over their lifetime end up \"leaking\" threads.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-217",
        "summary": "[PATCH] new method: Document.remove()",
        "description": "Here's a patch that adds a remove() method to the Document class (+test case). This \nis very useful if you have converter classes that return a Lucene Document object but \nyou need to make changes to that object. \n \nIn my case, I wanted to index PDF files that were saved as BLOBs in a database. The \nfiles need to be saved to a temporary file and that file name is given to the PDF \nconverter class. The PDF converter then saves the name of the temporary file name \nas the file name, which doesn't make sense. So my code needs to remove the \n'filename' field and re-add it, this time with the columns primary ID. This is only possible \nwith the attached patch.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2027",
        "summary": "Deprecate Directory.touchFile",
        "description": "Lucene doesn't use this method, and, FindBugs reports that FSDirectory's impl shouldn't swallow the returned result from File.setLastModified.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2983",
        "summary": "FieldInfos should be read-only if loaded from disk",
        "description": "Currently FieldInfos create a private FieldNumberBiMap when they are loaded from a directory which is necessary due to some limitation we need to face with IW#addIndexes(Dir). If we add an index via a directory to an existing index field number can conflict with the global field numbers in the IW receiving the directories. Those field number conflicts will remain until those segments are merged and we stabilize again based on the IW global field numbers. Yet, we unnecessarily creating a BiMap here where we actually should enforce read-only semantics since nobody should modify this FieldInfos instance we loaded from the directory. If somebody needs to get a modifiable copy they should simply create a new one and all all FieldInfo instances to it.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3506",
        "summary": "tests for verifying that assertions are enabled do nothing since they ignore AssertionError",
        "description": "Follow-up from LUCENE-3501",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1344",
        "summary": "Make the Lucene jar an OSGi bundle",
        "description": "In order to use Lucene in an OSGi environment, some additional headers are needed in the manifest of the jar. As Lucene has no dependency, it is pretty straight forward and it ill be easy to maintain I think.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-557",
        "summary": "search vs explain - score discrepancies",
        "description": "I'm on a mission to demonstrate (and then hopefully fix) any inconsistencies between the score you get for a doc when executing a search, and the score you get when asking for an explanation of the query for that doc.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-802",
        "summary": "lucene jars should include LiCENSE and NOTICE",
        "description": "The Lucene jars created by the build should include the LICENSE and NOTICE files in META-INF.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2559",
        "summary": "reopen support for SegmentReader",
        "description": "Reopen for SegmentReader can be supported simply as the following:\n\n  @Override\n  public synchronized IndexReader reopen() throws CorruptIndexException,\n\t\tIOException {\n\treturn reopenSegment(this.si,false,readOnly);\n  }\n\n  @Override\n  public synchronized IndexReader reopen(boolean openReadOnly)\n\t\tthrows CorruptIndexException, IOException {\n\treturn reopenSegment(this.si,false,openReadOnly);\n  }\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-539",
        "summary": "Fix for deprecations in contrib/surround",
        "description": "Fix for deprecations in contrib/surround.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1430",
        "summary": "IndexReader.open(String|File) may incorrectly throw AlreadyClosedException",
        "description": "Spinoff from here:\n\n    http://www.nabble.com/Runtime-exception-when-creating-IndexSearcher-to20226279.html\n\nIf you open an IndexSearcher/Reader, passing in String or File, then\ncloseDirectory is set to true in the reader.\n\nIf the index has a single segment, then SegmentReader.get is used to\nopen the index.  If an IOException is hit in there, the SegmentReader\ncloses itself and then closes the directory since closeDirectory is\ntrue.\n\nThe problem is, the retry logic in SegmentInfos (to look for another\nsegments_N to try) kicks in and hits an AlreadyClosedException,\nmasking the original root cause.\n\nWorkaround is to separately get the Directory using\nFSDirectory.getDirectory, and then instantiate IndexSearcher/Reader\nfrom that.\n\nThis manifests as masking the root cause of a corrupted single-segment\nindex with a confusing AlreadyClosedException.  You could also hit\nthe false exception if the writer was in the process of committing\n(ie, a retry was really needed) or if there is some transient IO\nproblem opening the index (eg too many open files).\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1980",
        "summary": "Fix javadocs after deprecation removal",
        "description": "There are a lot of @links in Javadocs to methods/classes that no longer exist. javadoc target prints tons of warnings. We should fix that.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1530",
        "summary": "Support inclusive/exclusive for TrieRangeQuery/-Filter, remove default trie variant setters/getters",
        "description": "TrieRangeQuery/Filter is missing one thing: Ranges that have exclusive bounds. For TrieRangeQuery this may not be important for ranges on long or Date (==long) values (because [1..5] is the same like ]0..6[ or ]0..5]). This is not so simple for doubles because you must add/substract 1 from the trie encoded unsigned long.\n\nTo be conform with the other range queries, I will submit a patch that has two additional boolean parameters in the ctors to support inclusive/exclusive ranges for both ends. Internally it will be implemented using TrieUtils.incrementTrieCoded/decrementTrieCoded() but makes life simplier for double ranges (a simple exclusive replacement for the floating point range [0.0..1.0] is not possible without having the underlying unsigned long).\n\nIn December, when trie contrib was included (LUCENE-1470), 3 trie variants were supplied by TrieUtils. For new APIs a statically configureable default Trie variant does not conform to an API we want in Lucene (currently we want to deprecate all these static setters/getters). The important thing: It does not make code shorter or easier to understand, its more error prone. Before release of 2.9 it is a good time to remove the default trie variant and always force the parameter in TrieRangeQuery/Filter. It is better to choose the variant in the application and do not automatically manage it.\n\nAs Lucene 2.9 was not yet released, I will change the ctors and not preserve the old ones.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2351",
        "summary": "optimize automatonquery",
        "description": "Mike found a few cases in flex where we have some bad behavior with automatonquery.\nThe problem is similar to a database query planner, where sometimes simply doing a full table scan is faster than using an index.\n\nWe can optimize automatonquery a little bit, and get better performance for fuzzy,wildcard,regex queries.\n\nHere is a list of ideas:\n* create commonSuffixRef for infinite automata, not just really-bad linear scan cases\n* do a null check rather than populating an empty commonSuffixRef\n* localize the 'linear' case to not seek, but instead scan, when ping-ponging against loops in the state machine\n* add a mechanism to enable/disable the terms dict cache, e.g. we can disable it for infinite cases, and maybe fuzzy N>1 also.\n* change the use of BitSet to OpenBitSet or long[] gen for path-tracking\n* optimize the backtracking code where it says /* String is good to go as-is */, this need not be a full run(), I think...\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2293",
        "summary": "IndexWriter has hard limit on max concurrency",
        "description": "DocumentsWriter has this nasty hardwired constant:\n\n{code}\nprivate final static int MAX_THREAD_STATE = 5;\n{code}\n\nwhich probably I should have attached a //nocommit to the moment I\nwrote it ;)\n\nThat constant sets the max number of thread states to 5.  This means,\nif more than 5 threads enter IndexWriter at once, they will \"share\"\nonly 5 thread states, meaning we gate CPU concurrency to 5 running\nthreads inside IW (each thread must first wait for the last thread to\nfinish using the thread state before grabbing it).\n\nThis is bad because modern hardware can make use of more than 5\nthreads.  So I think an immediate fix is to make this settable\n(expert), and increase the default (8?).\n\nIt's tricky, though, because the more thread states, the less RAM\nefficiency you have, meaning the worse indexing throughput.  So you\nshouldn't up and set this to 50: you'll be flushing too often.\n\nBut... I think a better fix is to re-think how threads write state\ninto DocumentsWriter.  Today, a single docID stream is assigned across\nthreads (eg one thread gets docID=0, next one docID=1, etc.), and each\nthread writes to a private RAM buffer (living in the thread state),\nand then on flush we do a merge sort.  The merge sort is inefficient\n(does not currently use a PQ)... and, wasteful because we must\nre-decode every posting byte.\n\nI think we could change this, so that threads write to private RAM\nbuffers, with a private docID stream, but then instead of merging on\nflush, we directly flush each thread as its own segment (and, allocate\nprivate docIDs to each thread).  We can then leave merging to CMS\nwhich can already run merges in the BG without blocking ongoing\nindexing (unlike the merge we do in flush, today).\n\nThis would also allow us to separately flush thread states.  Ie, we\nneed not flush all thread states at once -- we can flush one when it\ngets too big, and then let the others keep running.  This should be a\ngood concurrency gain since is uses IO & CPU resources \"throughout\"\nindexing instead of \"big burst of CPU only\" then \"big burst of IO\nonly\" that we have today (flush today \"stops the world\").\n\nOne downside I can think of is... docIDs would now be \"less\nmonotonic\", meaning if N threads are indexing, you'll roughly get\nin-time-order assignment of docIDs.  But with this change, all of one\nthread state would get 0..N docIDs, the next thread state'd get\nN+1...M docIDs, etc.  However, a single thread would still get\nmonotonic assignment of docIDs.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3530",
        "summary": "Remove deprecated methods in CompoundTokenFilters",
        "description": "",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": ""
    },
    {
        "key": "LUCENE-3641",
        "summary": "MultiReader does not propagate readerFinishedListeners to clones/reopened readers",
        "description": "While working on refactoring MultiReader/DirectoryReader in trunk, I found out that MultiReader does not correctly pass readerFinishedListeners to its clones and reopened readers.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1115",
        "summary": "Some small fixes to contrib/benchmark",
        "description": "I've fixed a few small issues I've hit in contrib/benchmark.\n\nFirst, this alg was only doing work on the first round.  All\nsubsequent rounds immediately finished:\n\n{code}\nanalyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\ndoc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\nwork.dir = /lucene/work\ndocs.file=work/reuters.lines.txt\ndoc.maker.forever=false\ndirectory=FSDirectory\ndoc.add.log.step=3000\n\n{ \"Rounds\"\n  ResetSystemErase\n  CreateIndex\n  { \"AddDocs\" AddDoc > : *\n  CloseIndex\n  NewRound\n} : 3\n{code}\n\nI think this is because we are failing to reset \"exhausted\" to false\nin PerfTask.doLogic(), so I added that.  Plus I had to re-open the\nfile in LineDocMaker.\n\nSecond, I made a small optimization to not call updateExhausted unless\nany of the child tasks are TaskSequence or ResetInputsTask (which I\ncompute up-front).\n\nFinally, we were not allowing flushing by RAM and doc count, so I\nfixed the logic in Create/OpenIndexTask to set both RAMBufferSizeMB\nand MaxBufferedDocs.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-967",
        "summary": "Add \"tokenize documents only\" task to contrib/benchmark",
        "description": "I've been looking at performance improvements to tokenization by\nre-using Tokens, and to help benchmark my changes I've added a new\ntask called ReadTokens that just steps through all fields in a\ndocument, gets a TokenStream, and reads all the tokens out of it.\n\nEG this alg just reads all Tokens for all docs in Reuters collection:\n\n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker\n  doc.maker.forever=false\n  {ReadTokens > : *\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2290",
        "summary": "Remove unnecessary String concatenation in IndexWriter",
        "description": "I've noticed a couple of places in IndexWriter where a boolean string is created by bool + \"\", or integer by int + \"\". There are some places (in setDiagonstics) where a string is concatenated with an empty String ...\nThe patch uses Boolean.toString and Integer.toString, as well as remove the unnecessary str + \"\".",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2100",
        "summary": "Make contrib analyzers final",
        "description": "The analyzers in contrib/analyzers should all be marked final. None of the Analyzers should ever be subclassed - users should build their own analyzers if a different combination of filters and Tokenizers is desired.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2112",
        "summary": "Flex on non-flex emulation of TermsEnum incorrectly seeks/nexts beyond current field",
        "description": "Spinoff of LUCENE-2111, where Uwe found this issue with the flex on non-flex emulation.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-496",
        "summary": "New tool for  reseting the (length)norm of fields after changing Similarity",
        "description": "I've written a little tool that seems like it can/will be very handy as I tweak my custom similarity.  I think it would make a good addition to contrib/miscellaneous.\n\nClass and Tests to be attached shortly...",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3338",
        "summary": "Flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges",
        "description": "Flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges.\n\nThese two problems were found while developing LUCENE-1768.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1576",
        "summary": "Brazilian Analyzer doesn't remove stopwords when uppercase is given",
        "description": "The order of filters matter here, just need to apply lowercase token filter before removing stopwords\n\n\tresult = new StopFilter( result, stoptable );\n\t\tresult = new BrazilianStemFilter( result, excltable );\n\t\t// Convert to lowercase after stemming!\n\t\tresult = new LowerCaseFilter( result );\n\nLowercase must come before BrazilianStemFilter\n\nAt the end of day I'll attach a patch, it's straightforward",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-948",
        "summary": "Writers on two machines over NFS can hit FNFE due to stale NFS client caching",
        "description": "Issue spawned from this thread:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/50680\n\nWhen IndexFileDeleter lists the directory, looking for segments_X\nfiles to load, if it hits a FNFE on opening such a file it should\ncatch this and treat it as if the file does not exist.\n\nOn NFS (and possibly other file systems), a directory listing is not\nguaranteed to be \"current\"/coherent.  Specifically, if machine #1 has\njust removed file \"segments_n\" and shortly thereafer machine #2 does a\ndir listing, it's possible (likely?) that the dir listing will still\nshow that segments_n exists.\n\nI think the fix is simple: catch the FNFE and just handle it as if the\nsegments_n does not in fact exist.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1657",
        "summary": "Make \"boolean readOnly\" a required arg to IndexReader.open",
        "description": "Most apps don't need read/write IndexReader, and, a readOnly\nIndexReader has better concurrent performance.\n\nI'd love to simply default readOnly to true, and you'd have to specify\n\"false\" if you want a read/write reader (I think that's the natural\ndefault), but I think that'd break too many back-compat cases.\n\nSo the workaround is to make the parameter explicit, in 2.9.\n\nI think even for IndexSearcher's methods that open an IndexReader\nunder the hood, we should also make the parameter explicit.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2319",
        "summary": "IndexReader # doCommit - typo nit about v3.0 in trunk",
        "description": "Trunk is already in 3.0.1+ . But the documentation says -  \"In 3.0, this will become ... \".  Since it is already in 3.0, it might as well be removed. \n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2542",
        "summary": "TopDocsCollector should be abstract super class that is the real \"TopDocsCollector\" contract, a subclass should implement the priority-queue logic. e.g. PQTopDocsCollector",
        "description": "TopDocsCollector is both an abstract interface for producing TopDocs as well as a PriorityQueue based implementation.\nNot all Collectors that could produce TopDocs must use a PriorityQueue, and it would be advantageous to allow the TopDocsCollector to be an \"interface\" type abstract class, with a PQTopDocsCollector sub-class.\nWhile doing this, it'd be good to clean up the generics uses in these classes. As it's odd to create a TopFieldCollector and have to case the TopDocs object, when this can be fixed with generics.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2241",
        "summary": "Core Tests should call Version based ctors instead of deprecated default ctors",
        "description": "LUCENE-2183 introduced new ctors for all CharTokenizer subclasses. Core - tests should use those ctors with Version.LUCENE_CURRENT instead of the the deprecated ctors. Yet, LUCENE-2240 introduces more Version ctors For WhitespaceAnalyzer and SimpleAnalyzer. Test should also use their Version ctors instead the default ones.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1627",
        "summary": "SpellChecker has no \"close\" method",
        "description": "SpellChecker has no close method ... which means there is no way to force it to close the IndexSearcher it maintains when you are done using the SpellChecker.  (a quick skim of IndexSearcher doesn't even suggest there is a finalizer self closing in the event of GC)\n\nhttp://www.nabble.com/SpellChecker-locks-folder-to23171980.html#a23171980\n\nA hackish work around for people who want to force SpellChecker to close an IndexSearcher opened against a directory they care about doing something with... \n{code}yourSpellChecker.setSpellIndex(new RamDirecotry()){code}",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3453",
        "summary": "remove IndexDocValuesField",
        "description": "Its confusing how we present CSF functionality to the user, its actually not a \"field\" but an \"attribute\" of a field like  STORED or INDEXED.\n\nOtherwise, its really hard to think about CSF because there is a mismatch between the APIs and the index format.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2169",
        "summary": "Speedup of CharArraySet#copy if a CharArraySet instance is passed to copy.",
        "description": "the copy method should use the entries array itself to copy the set internally instead of iterating over all values. this would speedup copying even small set ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1427",
        "summary": "QueryWrapperFilter should not do scoring",
        "description": "The purpose of QueryWrapperFilter is to simply filter to include the docIDs that match the query.\n\nIts implementation is wasteful now because it computes scores for those matching docs even though the score is unused.  We could fix this by getting a Scorer and iterating through the docs without asking for the score:\n\n{code}\nIndex: src/java/org/apache/lucene/search/QueryWrapperFilter.java\n===================================================================\n--- src/java/org/apache/lucene/search/QueryWrapperFilter.java\t(revision 707060)\n+++ src/java/org/apache/lucene/search/QueryWrapperFilter.java\t(working copy)\n@@ -62,11 +62,9 @@\n   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n     final OpenBitSet bits = new OpenBitSet(reader.maxDoc());\n \n-    new IndexSearcher(reader).search(query, new HitCollector() {\n-      public final void collect(int doc, float score) {\n-        bits.set(doc);  // set bit for hit\n-      }\n-    });\n+    final Scorer scorer = query.weight(new IndexSearcher(reader)).scorer(reader);\n+    while(scorer.next())\n+      bits.set(scorer.doc());\n     return bits;\n   }\n{code}\n\nMaybe I'm missing something, but this seams like a simple win?\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3686",
        "summary": "EnhancementsPayloadIterator.getCategoryData(CategoryEnhancement) problematic usage of Object.equals()",
        "description": "EnhancementsPayloadIterator has an internal list of category enhancemnets, and in getCategoryData(CategoryEnhancement) there is a lookup of the given CategoryEnhancement in the list. In order to make sure this lookup works, CategoryEnhancement must override Object.equals(Object).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2986",
        "summary": "divorce defaultsimilarityprovider from defaultsimilarity",
        "description": "In LUCENE-2236 as a start, we made DefaultSimilarity which implements the factory interface (SimilarityProvider), and also extends Similarity.\n\nIts factory interface just returns itself always by default.\n\nDoron mentioned it would be cleaner to split the two, and I thought it would be good to revisit it later.\n\nToday as I was looking at SOLR-2338, it became pretty clear that we should do this, it makes things a lot cleaner. I think currently its confusing to users to see the two apis mixed if they are trying to subclass.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3129",
        "summary": "Single-pass grouping collector based on doc blocks",
        "description": "LUCENE-3112 enables adding/updating a contiguous block of documents to\nthe index, guaranteed (yet, experimental!) to retain adjacent docID\nassignment through the full life of the index as long the app doesn't\ndelete individual docs from the block.\n\nWhen an app does this, it can enable neat features like LUCENE-2454\n(nested documents), post-group facet counting (LUCENE-3097).\n\nIt also makes single-pass grouping possible, when you group by\nthe \"identifier\" field shared by the doc block, since we know we will\nsee a given group only once with all of its docs within one block.\n\nThis should be faster than the fully general two-pass collectors we\nalready have.\n\nI'm working on a patch but not quite there yet...\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-444",
        "summary": "StandardTokenizer loses Korean characters",
        "description": "While using StandardAnalyzer, exp. StandardTokenizer with Korean text stream, StandardTokenizer ignores the Korean characters. This is because the definition of CJK token in StandardTokenizer.jj JavaCC file doesn't have enough range covering Korean syllables described in Unicode character map.\nThis patch adds one line of 0xAC00~0xD7AF, the Korean syllables range to the StandardTokenizer.jj code.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1781",
        "summary": "Large distances in Spatial go beyond Prime MEridian",
        "description": "http://amidev.kaango.com/solr/core0/select?fl=*&json.nl=map&wt=json&radius=5000&rows=20&lat=39.5500507&q=honda&qt=geo&long=-105.7820674\n\nGet an error when using Solr when distance is calculated for the boundary box past 90 degrees.\n\n\nAug 4, 2009 1:54:00 PM org.apache.solr.common.SolrException log\nSEVERE: java.lang.IllegalArgumentException: Illegal lattitude value 93.1558669413734\n        at org.apache.lucene.spatial.geometry.FloatLatLng.<init>(FloatLatLng.java:26)\n        at org.apache.lucene.spatial.geometry.shape.LLRect.createBox(LLRect.java:93)\n        at org.apache.lucene.spatial.tier.DistanceUtils.getBoundary(DistanceUtils.java:50)\n        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoxShape(CartesianPolyFilterBuilder.java:47)\n        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoundingArea(CartesianPolyFilterBuilder.java:109)\n        at org.apache.lucene.spatial.tier.DistanceQueryBuilder.<init>(DistanceQueryBuilder.java:61)\n        at com.pjaol.search.solr.component.LocalSolrQueryComponent.prepare(LocalSolrQueryComponent.java:151)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:174)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1328)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:341)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:244)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)\n        at org.apache.coyote.http11.Http11AprProcessor.process(Http11AprProcessor.java:857)\n        at org.apache.coyote.http11.Http11AprProtocol$Http11ConnectionHandler.process(Http11AprProtocol.java:565)\n        at org.apache.tomcat.util.net.AprEndpoint$Worker.run(AprEndpoint.java:1509)\n        at java.lang.Thread.run(Thread.java:619)\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1098",
        "summary": "Small performance enhancement for StandardAnalyzer",
        "description": "The class StandardAnalyzer has an inner class, SavedStreams, which is used internally for maintaining some state. This class doesn't use the implicit reference to the enclosing class, so it can be made static and reduce some memory requirements. A patch will be attached shortly.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1380",
        "summary": "Patch for ShingleFilter.enablePositions (or PositionFilter)",
        "description": "Make it possible for *all* words and shingles to be placed at the same position, that is for _all_ shingles (and unigrams if included) to be treated as synonyms of each other.\n\nToday the shingles generated are synonyms only to the first term in the shingle.\nFor example the query \"abcd efgh ijkl\" results in:\n   (\"abcd\" \"abcd efgh\" \"abcd efgh ijkl\") (\"efgh\" efgh ijkl\") (\"ijkl\")\n\nwhere \"abcd efgh\" and \"abcd efgh ijkl\" are synonyms of \"abcd\", and \"efgh ijkl\" is a synonym of \"efgh\".\n\nThere exists no way today to alter which token a particular shingle is a synonym for.\nThis patch takes the first step in making it possible to make all shingles (and unigrams if included) synonyms of each other.\n\nSee http://comments.gmane.org/gmane.comp.jakarta.lucene.user/34746 for mailing list thread.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-437",
        "summary": "SnowballFilter loses token position offset",
        "description": "SnowballFilter doesn't set the token position increment (and thus it defaults to 1).\nThis also affetcs SnowballAnalyzer since it uses SnowballFilter.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2069",
        "summary": "fix LowerCaseFilter for unicode 4.0",
        "description": "lowercase suppl. characters correctly. \n\nthis only fixes the filter, the LowerCaseTokenizer is part of a more complex issue (CharTokenizer)\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1354",
        "summary": "Provide Programmatic Access to CheckIndex",
        "description": "Would be nice to have programmatic access to the CheckIndex tool, so that it can be used in applications like Solr.  \n\nSee SOLR-566",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1286",
        "summary": "LargeDocHighlighter - another span highlighter optimized for large documents",
        "description": "The existing Highlighter API is rich and well designed, but the approach taken is not very efficient for large documents.\n\nI believe that this is because the current Highlighter rebuilds the document by running through and scoring every every token in the tokenstream.\n\nWith a break in the current API, an alternate approach can be taken: rebuild the document by running through the query terms by using their offsets. The benefit is clear - a large doc will have a large tokenstream, but a query will likely be very small in comparison.\n\nI expect this approach to be quite a bit faster for very large documents, while still supporting Phrase and Span queries.\n\nFirst rough patch to follow shortly.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-935",
        "summary": "Improve maven artifacts",
        "description": "There are a couple of things we can improve for the next release:\n- \"*pom.xml\" files should be renamed to \"*pom.xml.template\"\n- artifacts \"lucene-parent\" should extend \"apache-parent\"\n- add source jars as artifacts\n- update <generate-maven-artifacts> task to work with latest version of maven-ant-tasks.jar\n- metadata filenames should not contain \"local\"",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1938",
        "summary": "Precedence query parser using the contrib/queryparser framework",
        "description": "Extend the current StandardQueryParser on contrib so it supports boolean precedence",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-768",
        "summary": "Exception in deleteDocument, undeleteAll or setNorm in IndexReader can fail to release write lock on close",
        "description": "I hit this while working on LUCENE-140\n\nWe have 3 cases in the IndexReader methods above where we have this pattern:\n\n  if (directoryOwner) acquireWriteLock();\n  doSomething();\n  hasChanges = true;\n\nThe problem is if you hit an exception in doSomething(), and hasChanges was not already true, then hasChanges will not have been set to true yet the write lock is held.  If you then try to close the reader without making any other changes, then the write lock is not released because in IndexReader.close() (well, in commit()) we only release write lock if hasChanges is true.\n\nI think the simple fix is to swap the order of hasChanges = true and doSomething().  I already fixed one case of this under LUCENE-140 commit yesterday; I will fix the other two under this issue.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2640",
        "summary": "add LuceneTestCase[J4].newField",
        "description": "I think it would be good to vary the different field options in tests.\n\nFor example, we do this with IW settings (newIndexWriterConfig), and directories (newDirectory).\n\nThis patch adds newField(), it works just like new Field(), except it will sometimes turns on extra options:\nStored fields, term vectors, additional term vectors data, etc.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2719",
        "summary": "Re-add SorterTemplate and use it to provide fast ArraySorting and replace BytesRefHash sorting",
        "description": "This patch adds back an optimized and rewritten SorterTemplate back to Lucene (removed after release of 3.0). It is of use for several components:\n\n- Automaton: Automaton needs to sort States and other things. Using Arrays.sort() is slow, because it clones internally to ensure stable search. This component is much faster. This patch adds Arrays.sort() replacements in ArrayUtil that work with natural order or using a Comparator<?>. You can choose between quickSort and mergeSort.\n- BytesRefHash uses another QuickSort algorithm without insertionSort for very short ord arrays. This class uses SorterTemplate to provide the same with insertionSort fallback in a very elegant way. Ideally this class can be used everywhere, where the sort algorithm needs to be separated from the underlying data and you can implement a swap() and compare() function (that get slot numbers instead of real values). This also applies to Solr (Yonik?).\n\nSorterTemplate provides quickSort and mergeSort algorithms. Internally for short arrays, it automatically chooses insertionSort (like JDK's Arrays). The quickSort algorith was copied modified from old BytesRefHash. This new class only shares MergeSort with the original CGLIB SorterTemplate, which is no longer maintained.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2891",
        "summary": "IndexWriterConfig does not allow readerTermsIndexDivisor to be -1, while the latest indicates the terms index should not be loaded",
        "description": "While you can pass -1 to IR.open(), and it's documented, you cannot do the same for IndexWriter's readers (b/c IWC blocks it). Need to allow this setting as well as add support for it in our tests, e.g. we should randomly set it to -1. Robert also suggested RandomIW use -1 randomly when it opens readers.\n\nI'll work on a patch",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1495",
        "summary": "Allow TaskSequence to run for certain time",
        "description": "To help the perf testing for LUCENE-1483, I added simple ability to specify a fixed run time (seconds) for a task sequence, eg:\n{code}\n{ \"XSearchWithSort\" SearchWithSort(doctitle:string) > : 2.7s\n{code}\niterates on that subtask until 2.7 seconds have elapsed, and then sets the repetition count to how many iterations were done.  This is useful when you are running searches whose runtime may vary drastically.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-871",
        "summary": "ISOLatin1AccentFilter a bit slow",
        "description": "The ISOLatin1AccentFilter is a bit slow giving 300+ ms responses when used in a highligher for output responses.\n\nPatch to follow",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-248",
        "summary": "[PATCH] Add StopFilter ignoreCase option",
        "description": "Wanted to have the ability to ignore case in the stop filter.  In some cases, I\ndon't want to have to lower case before passing through the stop filter, b/c I\nmay need case preserved for other analysis further down the stream, yet I don't\nneed the stopwords and I don't want to have to apply stopword filters twice.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1120",
        "summary": "Use bulk-byte-copy when merging term vectors",
        "description": "Indexing all of Wikipedia, with term vectors on, under the YourKit\nprofiler, shows that 26% of the time (!!) was spent merging the\nvectors.  This was without offsets & positions, which would make\nmatters even worse.\n\nDepressingly, merging, even with ConcurrentMergeScheduler, cannot in\nfact keep up with the flushing of new segments in this test, and this\nis on a strong IO system (Mac Pro with 4 drive RAID 0 array, 4 CPU\ncores).\n\nSo, just like Robert's idea to merge stored fields with bulk copying\nwhenever the field name->number mapping is \"congruent\" (LUCENE-1043),\nwe can do the same with term vectors.\n\nIt's a little trickier because the term vectors format doesn't quite\nmake it easy to bulk-copy because it doesn't directly encode the\noffset into the tvf file.\n\nI worked out a patch that changes the tvx format slightly, by storing\nthe absolute position in the tvf file for the start of each document\ninto the tvx file, just like it does for tvd now.  This adds an extra\n8 bytes (long) in the tvx file, per document.\n\nThen, I removed a vLong (the first \"position\" stored inside the tvd\nfile), which makes tvd contents fully position independent (so you can\njust copy the bytes).\n\nThis adds up to 7 bytes per document (less for larger indices) that\nhave term vectors enabled, but I think this small increase in index\nsize is acceptable for the gains in indexing performance?\n\nWith this change, the time spent merging term vectors dropped from 26%\nto 3%.  Of course, this only applies if your documents are \"regular\".\nI think in the future we could have Lucene try hard to assign the same\nfield number for a given field name, if it had been seen before in the\nindex...\n\nMerging terms now dominates the merge cost (~20% over overall time\nbuilding the Wikipedia index).\n\nI also beefed up TestBackwardsCompatibility unit test: test a non-CFS\nand a CFS of versions 1.9, 2.0, 2.1, 2.2 index formats, and added some\nterm vector fields to these indices.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1778",
        "summary": "Add log.step support per task",
        "description": "Following LUCENE-1774, this will add support for log.step per task name, rather than a single log.step setting for all tasks. The .alg file will support:\n* log.step - for all tasks.\n* log.step.<Task Class Name> - for a specific task. For example, log.step.AddDoc, or log.step.DeleteDoc\n\nI will post the patch soon",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-728",
        "summary": "Remove or deprecate contrib/similarity",
        "description": "Classes under contrib/similarity seem to be duplicates of classes under contrib/queries.\nI'd like to remove *.java from contrib/similarity without bothering with deprecation, since the same functionality exists in contrib/queries.\nAnyone minds?\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3539",
        "summary": "IndexFormatTooOld/NewExc should try to include fileName + directory when possible",
        "description": "(Spinoff from http://markmail.org/thread/t6s7nn3ve765nojc )\n\nWhen we throw a too old/new exc we should try to include the full path to the offending file, if possible.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1011",
        "summary": "Two or more writers over NFS can cause index corruption",
        "description": "When an index is used over NFS, and, more than one machine can be a\nwriter such that they swap roles quickly, it's possible for the index\nto become corrupt if the NFS client directory cache is stale.\n\nNot all NFS clients will show this.  Very recent versions of Linux's\nNFS client do not seem to show the issue, yet, slightly older ones do,\nand the latest Mac OS X one does as well.\n\nI've been working with Patrick Kimber, who provided a standalone test\nshowing the problem (thank you Patrick!).  This came out of this\nthread:\n\n  http://www.gossamer-threads.com/lists/engine?do=post_view_flat;post=50680;page=1;sb=post_latest_reply;so=ASC;mh=25;list=lucene\n\nNote that the first issue in that discussion has been resolved\n(LUCENE-948).  This is a new issue.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1008",
        "summary": "document with no term vector fields after documents with term vector fields corrupts the index",
        "description": "If a document with no term-vector-enabled fields is added after\ndocument(s) that did have term vectors, as part of a single set of\nbuffered docs, then the term-vector documents file is corrupted\nbecause we fail to write a \"0\" vInt.\n\nThanks to Grant for spotting this!\n\nSpinoff from this thread:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/53306\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2900",
        "summary": "make applying deletes optional when pulling a new NRT reader",
        "description": "Usually when you pull an NRT reader, you want all deletes to be applied.\n\nBut in some expert cases you may not need it (eg you just want to validate that the doc was indexed).  Since it's costly to apply deletes, and trivial to add this boolean (we already have a boolean internally), I think we should add it.\n\nThe deletes are still buffered, and you can always later pull another reader (for \"real\" searching) with deletes applied.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2695",
        "summary": "DisjunctionMaxScorer allocates 2 arrays per scored doc",
        "description": "It has this:\n{noformat}\n  @Override\n  public float score() throws IOException {\n    int doc = subScorers[0].docID();\n    float[] sum = { subScorers[0].score() }, max = { sum[0] };\n    int size = numScorers;\n    scoreAll(1, size, doc, sum, max);\n    scoreAll(2, size, doc, sum, max);\n    return max[0] + (sum[0] - max[0]) * tieBreakerMultiplier;\n  }\n{noformat}\n\nThey are thread-private arrays so possibly/likely JVM can optimize this case (allocate only on the stack) but still I think instead it should have private instance vars for the score/max.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3681",
        "summary": "FST.BYTE2 should save as fixed 2 byte not as vInt",
        "description": "We currently write BYTE1 as a single byte, but BYTE2/4 as vInt, but I think that's confusing.  Also, for the FST for the new Kuromoji analyzer (LUCENE-3305), writing as 2 bytes instead shrank the FST and ran faster, presumably because more values were >= 16384 than were < 128.\n\nSeparately the whole INPUT_TYPE is very confusing... really all it's doing is \"declaring\" the allowed range of the characters of the input alphabet, and then the only thing that uses that is the write/readLabel methods (well and some confusing sugar methods in Builder!).  Not sure how to fix that yet...\n\nIt's a simple change but it changes the FST binary format so any users w/ FSTs out there will have to rebuild (FST is marked experimental...).\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2764",
        "summary": "Allow tests to use random codec per field",
        "description": "Since we now have a real per field codec support we should enable to run the tests with a random codec per field. When I change something related to codecs internally I would like to ensure that whatever combination of codecs (except of preflex) I use the code works just fine. I created a RandomCodecProvider in LuceneTestCase that randomly selects the codec for fields when it sees them the first time. I disabled the test by default to leave the old randomize codec support in as it was / is.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1422",
        "summary": "New TokenStream API",
        "description": "This is a very early version of the new TokenStream API that \nwe started to discuss here:\n\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/66227\n\nThis implementation is a bit different from what I initially\nproposed in the thread above. I introduced a new class called\nAttributedToken, which contains the same termBuffer logic \nfrom Token. In addition it has a lazily-initialized map of\nClass<? extends Attribute> -> Attribute. Attribute is also a\nnew class in a new package, plus several implementations like\nPositionIncrementAttribute, PayloadAttribute, etc.\n\nSimilar to my initial proposal is the prototypeToken() method\nwhich the consumer (e. g. DocumentsWriter) needs to call.\nThe token is created by the tokenizer at the end of the chain\nand pushed through all filters to the end consumer. The \ntokenizer and also all filters can add Attributes to the \ntoken and can keep references to the actual types of the\nattributes that they need to read of modify. This way, when\nboolean nextToken() is called, no casting is necessary.\n\nI added a class called TestNewTokenStreamAPI which is not \nreally a test case yet, but has a static demo() method, which\ndemonstrates how to use the new API.\n\nThe reason to not merge Token and TokenStream into one class \nis that we might have caching (or tee/sink) filters in the \nchain that might want to store cloned copies of the tokens\nin a cache. I added a new class NewCachingTokenStream that\nshows how such a class could work. I also implemented a deep\nclone method in AttributedToken and a \ncopyFrom(AttributedToken) method, which is needed for the \ncaching. Both methods have to iterate over the list of \nattributes. The Attribute subclasses itself also have a\ncopyFrom(Attribute) method, which unfortunately has to down-\ncast to the actual type. I first thought that might be very\ninefficient, but it's not so bad. Well, if you add all\nAttributes to the AttributedToken that our old Token class\nhad (like offsets, payload, posIncr), then the performance\nof the caching is somewhat slower (~40%). However, if you \nadd less attributes, because not all might be needed, then\nthe performance is even slightly faster than with the old API.\nAlso the new API is flexible enough so that someone could\nimplement a custom caching filter that knows all attributes\nthe token can have, then the caching should be just as \nfast as with the old API.\n\n\nThis patch is not nearly ready, there are lot's of things \nmissing:\n\n- unit tests\n- change DocumentsWriter to use new API \n  (in backwards-compatible fashion)\n- patch is currently java 1.5; need to change before \n  commiting to 2.9\n- all TokenStreams and -Filters should be changed to use \n  new API\n- javadocs incorrect or missing\n- hashcode and equals methods missing in Attributes and \n  AttributedToken\n  \nI wanted to submit it already for brave people to give me \nearly feedback before I spend more time working on this.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3029",
        "summary": "MultiPhraseQuery assigns different scores to identical docs when using 0 pos-incr",
        "description": "If you have two identical docs with tokens a b c all zero pos-incr (ie\nthey occur on the same position), and you run a MultiPhraseQuery with\n[a, b] and [c] (all pos incr 0)... then the two docs will get\ndifferent scores despite being identical.\n\nAdmittedly it's a strange query... but I think the scorer ought to\ncount the phrase as having tf=1 for each doc.\n\nThe problem is that we are missing a tie-breaker for the PhraseQuery\nused by ExactPhraseScorer, and so the PQ ends up flip/flopping such\nthat every other document gets the same score.  Ie, even docIDs all\nget one score and odd docIDs all get another score.\n\nOnce I added the hard tie-breaker (ord) the scores are the same.\n\nHowever... there's a separate bug, that can over-count the tf, such\nthat if I create the MPQ like this:\n{noformat}\n  mpq.add(new Term[] {new Term(\"field\", \"a\")}, 0);\n  mpq.add(new Term[] {new Term(\"field\", \"b\"), new Term(\"field\", \"c\")}, 0);\n{noformat}\n\nI get tf=2 per doc, but if I create it like this:\n\n{noformat}\n  mpq.add(new Term[] {new Term(\"field\", \"b\"), new Term(\"field\", \"c\")}, 0);\n  mpq.add(new Term[] {new Term(\"field\", \"a\")}, 0);\n{noformat}\n\nI get tf=1 (which I think is correct?).\n\nThis happens because MultipleTermPositions freely returns the same\nposition more than once: it just unions the positions of the two\nstreams, so when both have their term at pos=0, you'll get pos=0\ntwice, which is not good and leads to over-counting tf.\n\nUnfortunately, I don't see a performant way to fix that... and I'm not\nsure that it really matters that much in practice.\n\n\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3062",
        "summary": "TestBytesRefHash#testCompact is broken",
        "description": "TestBytesRefHash#testCompact fails when run with ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360\n{noformat}\n\n    [junit] Testsuite: org.apache.lucene.util.TestBytesRefHash\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.454 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360\n    [junit] NOTE: test params are: codec=PreFlex, locale=et, timezone=Pacific/Tahiti\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestBytesRefHash]\n    [junit] NOTE: Linux 2.6.35-28-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=12,threads=1,free=363421800,total=379322368\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testCompact(org.apache.lucene.util.TestBytesRefHash):\tCaused an ERROR\n    [junit] bitIndex < 0: -27\n    [junit] java.lang.IndexOutOfBoundsException: bitIndex < 0: -27\n    [junit] \tat java.util.BitSet.set(BitSet.java:262)\n    [junit] \tat org.apache.lucene.util.TestBytesRefHash.testCompact(TestBytesRefHash.java:146)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1260)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1189)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.util.TestBytesRefHash FAILED\n{noformat}\n\nthe test expects that _TestUtil.randomRealisticUnicodeString(random, 1000); will never return the same string.\n\nI will upload a patch soon.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1734",
        "summary": "CharReader should delegate reset/mark/markSupported",
        "description": "The final class CharReader should delegate reset/mark/markSupported to its wrapped reader. Otherwise clients will get \"reset() not supported\" exception.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2842",
        "summary": "add Galician analyzer",
        "description": "Adds analyzer for Galician, based upon [\"Regras do lematizador para o galego\"|http://bvg.udc.es/recursos_lingua/stemming.jsp] , and a set of stopwords created in the usual fashion.\n\nThis is really just an adaptation of the Portuguese [RSLP|http://www.inf.ufrgs.br/~viviane/rslp/index.htm], so I added that too, and modified our existing hand-coded RSLP-S (RSLP's plural-only step) to just be a plural-only flow of RSLP.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1223",
        "summary": "lazy fields don't enforce binary vs string value",
        "description": "If you have a binary field, and load it lazy, and then ask that field\nfor its stringValue, it will incorrectly give you a String back (and\nthen will refuse to give a binaryValue).  And, vice-versa.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3032",
        "summary": "TestIndexWriterException fails with NPE on realtime",
        "description": "{noformat}\n   [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions\n    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):\tCaused an ERROR\n    [junit] (null)\n    [junit] java.lang.NullPointerException\n    [junit] \tat org.apache.lucene.index.DocumentsWriterPerThread.prepareFlush(DocumentsWriterPerThread.java:329)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:378)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:512)\n    [junit] \tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2619)\n    [junit] \tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2594)\n    [junit] \tat org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2464)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2537)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2519)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2503)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:230)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)\n    [junit] \n    [junit] \n    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 22.548 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-5079747362001734044:1572064802119081373\n    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _25(4.0):cv2/1 _29(4.0):cv2/1 _20(4.0):cv3/1 into _2m\n    [junit] RESOURCE LEAK: test method: 'testRandomExceptionsThreads' left 1 thread(s) running\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=Pulsing(freqCutoff=2), field=MockSep, id=Pulsing(freqCutoff=2), other=MockSep, contents=SimpleText, content1=MockSep, content2=SimpleText, content4=MockRandom, content5=MockRandom, content6=MockVariableIntBlock(baseBlockSize=41), crash=Standard, content7=MockFixedIntBlock(blockSize=1633)}, locale=en_GB, timezone=Europe/Vaduz\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes, TestFilterIndexReader, TestIndexWriterExceptions]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=155417240,total=292945920\n    [junit] ------------- ---------------- ---------------\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-861",
        "summary": "Contrib queries package Query implementations do not override equals()",
        "description": "Query implementations should override equals() so that Query instances can be cached and that Filters can know if a Query has been used before.  See the discussion in this thread.\n\nhttp://www.mail-archive.com/java-user@lucene.apache.org/msg13061.html\n\nFollowing 3 contrib Query implementations do no override equals()\n\norg.apache.lucene.search.BoostingQuery;\norg.apache.lucene.search.FuzzyLikeThisQuery;\norg.apache.lucene.search.similar.MoreLikeThisQuery;\n\nTest cases below show the problem.\n\npackage com.teamware.office.lucene.search;\n\nimport static org.junit.Assert.*;\n\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.BoostingQuery;\nimport org.apache.lucene.search.FuzzyLikeThisQuery;\nimport org.apache.lucene.search.TermQuery;\nimport org.apache.lucene.search.similar.MoreLikeThisQuery;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\npublic class ContribQueriesEqualsTest\n{\n    /**\n     * @throws java.lang.Exception\n     */\n    @Before\n    public void setUp() throws Exception\n    {\n    }\n\n    /**\n     * @throws java.lang.Exception\n     */\n    @After\n    public void tearDown() throws Exception\n    {\n    }\n    \n    /**\n     *  Show that the BoostingQuery in the queries contrib package \n     *  does not implement equals() correctly.\n     */\n    @Test\n    public void testBoostingQueryEquals()\n    {\n        TermQuery q1 = new TermQuery(new Term(\"subject:\", \"java\"));\n        TermQuery q2 = new TermQuery(new Term(\"subject:\", \"java\"));\n        assertEquals(\"Two TermQueries with same attributes should be equal\", q1, q2);\n        BoostingQuery bq1 = new BoostingQuery(q1, q2, 0.1f);\n        BoostingQuery bq2 = new BoostingQuery(q1, q2, 0.1f);\n        assertEquals(\"BoostingQuery with same attributes is not equal\", bq1, bq2);\n    }\n\n    /**\n     *  Show that the MoreLikeThisQuery in the queries contrib package \n     *  does not implement equals() correctly.\n     */\n    @Test\n    public void testMoreLikeThisQueryEquals()\n    {\n        String moreLikeFields[] = new String[] {\"subject\", \"body\"};\n        \n        MoreLikeThisQuery mltq1 = new MoreLikeThisQuery(\"java\", moreLikeFields, new StandardAnalyzer());\n        MoreLikeThisQuery mltq2 = new MoreLikeThisQuery(\"java\", moreLikeFields, new StandardAnalyzer());\n        assertEquals(\"MoreLikeThisQuery with same attributes is not equal\", mltq1, mltq2);\n    }\n    /**\n     *  Show that the FuzzyLikeThisQuery in the queries contrib package \n     *  does not implement equals() correctly.\n     */\n    @Test\n    public void testFuzzyLikeThisQueryEquals()\n    {\n        FuzzyLikeThisQuery fltq1 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());\n        fltq1.addTerms(\"javi\", \"subject\", 0.5f, 2);\n        FuzzyLikeThisQuery fltq2 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());\n        fltq2.addTerms(\"javi\", \"subject\", 0.5f, 2);\n        assertEquals(\"FuzzyLikeThisQuery with same attributes is not equal\", fltq1, fltq2);\n    }\n}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3677",
        "summary": "Remove old byte[] norms api from IndexReader",
        "description": "Followup to LUCENE-3628.\n\nWe should remove this api and just use docvalues everywhere, to allow for norms of arbitrary size in the future (not just byte[])",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1596",
        "summary": "optimize MultiTermEnum/MultiTermDocs",
        "description": "Optimize MultiTermEnum and MultiTermDocs to avoid seeks on TermDocs that don't match the term.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-608",
        "summary": "deprecate Document.fields(), add getFields()",
        "description": "A simple API improvement that I'm going to commit if nobody objects.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2602",
        "summary": "Default merge policy should take deletions into account",
        "description": "LUCENE-1634 added a calibrateSizeByDeletes; we had a TODO to default this to true as of 3.0 but we missed it.  I'll fix it now for 3.1 and 4.0.  While this is technically a change in back-compat (for 3.x), I think it's fine to make an exception here; this should be a big win for indices that have high doc turnover with time.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2239",
        "summary": "Revise NIOFSDirectory and its usage due to NIO limitations on Thread.interrupt",
        "description": "I created this issue as a spin off from http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201001.mbox/%3Cf18c9dde1001280051w4af2bc50u1cfd55f85e50914f@mail.gmail.com%3E\n\nWe should decide what to do with NIOFSDirectory, if we want to keep it as the default on none-windows platforms and how we want to document this.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3684",
        "summary": "Add offsets to postings (D&PEnum)",
        "description": "I think should explore making start/end offsets a first-class attr in the\npostings APIs, and fixing the indexer to index them into postings.\n\nThis will make term vector access cleaner (we now have to jump through\nhoops w/ non-first-class offset attr).  It can also enable efficient\nhighlighting without term vectors / reanalyzing, if the app indexes\noffsets into the postings.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-828",
        "summary": "Term's equals() throws ClassCastException if passed something other than a Term",
        "description": "Term.equals(Object) does a cast to Term without checking if the other object is a Term.\n\nIt's unlikely that this would ever crop up but it violates the implied contract of Object.equals().",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1937",
        "summary": "Add more methods to manipulate QueryNodeProcessorPipeline elements",
        "description": "QueryNodeProcessorPipeline allows the user to define a list of processors to process a query tree. However, it's not very flexible when the user wants to extend/modify an already created pipeline, because it only provides an add method, which only allows the user to append a new processor to the pipeline.\n\nSo, I propose to add new methods to manipulate the processor in a pipeline. I think the methods should not consider an index position when modifying the pipeline, hence the index position in a pipeline does not mean anything, a processor has a meaning when it's after or before another processor. Therefore, I suggest the methods should always consider another processor when inserting/modifying the pipeline. For example, insertAfter(processor, newProcessor), which will insert the \"newProcessor\" after the \"processor\".",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1058",
        "summary": "New Analyzer for buffering tokens",
        "description": "In some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline.\n\nFor example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.\n\nPatch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API.\n\nSee http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2529",
        "summary": "always apply position increment gap between values",
        "description": "I'm doing some fancy stuff with span queries that is very sensitive to term positions.  I discovered that the position increment gap on indexing is only applied between values when there are existing terms indexed for the document.  I suspect this logic wasn't deliberate, it's just how its always been for no particular reason.  I think it should always apply the gap between fields.  Reference DocInverterPerField.java line 82:\n\nif (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\nThis is checking fieldState.length.  I think the condition should simply be:  if (i > 0).\nI don't think this change will affect anyone at all but it will certainly help me.  Presently, I can either change this line in Lucene, or I can put in a hack so that the first value for the document is some dummy value which is wasteful.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-996",
        "summary": "Parsing mixed inclusive/exclusive range queries",
        "description": "The current query parser doesn't handle parsing a range query (i.e. ConstantScoreRangeQuery) with mixed inclusive/exclusive bounds.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1442",
        "summary": "NOT_ANALYZED fields can double-count offsets",
        "description": "If the same field name has 2 NOT_ANALYZED field instances then the offsets are double-counted.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1873",
        "summary": "Update site lucene-sandbox page",
        "description": "The page has misleading/bad info. One thing I would like to do - but I won't attempt now (prob good for the modules issue) - is commit to one word - contrib or sandbox. I think sandbox should be purged myself.\n\nThe current page says that the sandbox is kind of a rats nest with various early stage software that one day may make it into core - that info is outdated I think. We should replace it, and also specify how the back compat policy works in contrib eg each contrib can have its own policy, with the default being no policy.\n\nWe should also drop the piece about being open to Lucene's committers and others - a bit outdated.\n\nWe should also either include the other contribs, or change the wording to indicate that the list is only a sampling of the many contribs.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3390",
        "summary": "Incorrect sort by Numeric values for documents missing the sorting field",
        "description": "While sorting results over a numeric field, documents which do not contain a value for the sorting field seem to get 0 (ZERO) value in the sort. (Tested against Double, Float, Int & Long numeric fields ascending and descending order).\nThis behavior is unexpected, as zero is \"comparable\" to the rest of the values. A better solution would either be allowing the user to define such a \"non-value\" default, or always bring those document results as the last ones.\n\nExample scenario:\nAdding 3 documents, 1st with value 3.5d, 2nd with -10d, and 3rd without any value.\nSearching with MatchAllDocsQuery, with sort over that field in descending order yields the docid results of 0, 2, 1.\n\nAsking for the top 2 documents brings the document without any value as the 2nd result - which seems as a bug?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3741",
        "summary": "MockCharFilter offset correction is wrong",
        "description": "This is a fake charfilter used in basetokenstreamtestcase.\n\nit occasionally doubles some characters, and corrects offsets.\n\nits used to find bugs where analysis components would fail otherwise with charfilters,\nbut its correctOffset is actually wrong (harmless to any tests today, but still wrong).\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2975",
        "summary": "hotspot bug in readvint gives wrong results",
        "description": "When testing the 3.1-RC1 made by Yonik on the PANGAEA (www.pangaea.de) productive system I figured out that suddenly on a large segment (about 5 GiB) some stored fiels suddenly produce a strange deflate decompression problem (CompressionTools) although the stored fields are no longer pre-3.0 compressed. It seems that the header of the stored field is read incorrectly at the buffer boundary in MultiMMapDir and then FieldsReader just incorrectly detects a deflate-compressed field (CompressionTools).\n\nThe error occurs reproducible on CheckIndex with MMapDirectory, but not with NIODir or SimpleDir. The FDT file of that segment is 2.6 GiB, on Solaris the chunk size is Integer.MAX_VALUE, so we have 2 MultiMMap IndexInputs.\n\nRobert and me have the index ready as a tar file, we will do tests on our local machines and hopefully solve the bug, maybe introduced by Robert's recent changes to MMap.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-351",
        "summary": "More javadocs for Weight",
        "description": "From Doug's reply of 21 Feb 2005 in bug 31841",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1397",
        "summary": "When BG merge hits an exception, optimize sometimes throws an IOException missing the root cause",
        "description": "\nWhen IndexWriter.optimize() is called, ConcurrentMergeScheduler will\nrun the requested merges with background threads and optimize() will\nwait for these merges to complete.\n\nIf a merge hits an exception, it records the root cause exception such\nthat optimize can then retrieve this root cause and throw its own\nexception, with the root cause.\n\nBut there is a bug: sometimes, the fact that an exception occurred on\na merge is recorded, but the root cause is missing.  In this cause,\noptimize() still throws an exception (correctly indicating that the\noptimize() has not finished successfully), but it's not helpful\nbecause it's missing the root cause.  You must then go find the root\ncause in the JRE's stderr logs.\n\nThis has hit a few users on this lists, most recently:\n\n  http://www.nabble.com/Background-merge-hit-exception-td19540409.html#a19540409\n\nI found the isssue, and finally got a unit test to intermittently show\nit.  It's a simple thread safety issue: in a finally clause in\nIndexWriter.merge we record the fact that the merge hit an exception\nbefore actually setting the root cause, and then only in\nConcurrentMergeScheduler's exception handler do we set the root\ncause.  If the optimize thread is scheduled in between these two, it\ncan throw an exception missing its root cause.\n\nThe fix is straightforward.  I plan to commit to 2.4 & 2.9.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2957",
        "summary": "generate-maven-artifacts target should include all non-Mavenized Lucene & Solr dependencies",
        "description": "Currently, in addition to deploying artifacts for all of the Lucene and Solr modules to a repository (by default local), the {{generate-maven-artifacts}} target also deploys artifacts for the following non-Mavenized Solr dependencies (lucene_solr_3_1 version given here):\n\n# {{solr/lib/commons-csv-1.0-SNAPSHOT-r966014.jar}} as org.apache.solr:solr-commons-csv:3.1\n# {{solr/lib/apache-solr-noggit-r944541.jar}} as org.apache.solr:solr-noggit:3.1\n\\\\ \\\\\nThe following {{.jar}}'s should be added to the above list (lucene_solr_3_1 version given here):\n\\\\ \\\\\n# {{lucene/contrib/icu/lib/icu4j-4_6.jar}}\n# {{lucene/contrib/benchmark/lib/xercesImpl-2.9.1-patched-XERCESJ}}{{-1257.jar}}\n# {{solr/contrib/clustering/lib/carrot2-core-3.4.2.jar}}**\n# {{solr/contrib/uima/lib/uima-an-alchemy.jar}}\n# {{solr/contrib/uima/lib/uima-an-calais.jar}}\n# {{solr/contrib/uima/lib/uima-an-tagger.jar}}\n# {{solr/contrib/uima/lib/uima-an-wst.jar}}\n# {{solr/contrib/uima/lib/uima-core.jar}}\n\\\\ \\\\\nI think it makes sense to follow the same model as the current non-Mavenized dependencies:\n\\\\ \\\\\n* {{groupId}} = {{org.apache.solr/.lucene}}\n* {{artifactId}} = {{solr-/lucene-}}<original-name>,\n* {{version}} = <lucene-solr-release-version>.\n\n**The carrot2-core jar doesn't need to be included in trunk's release artifacts, since there already is a Mavenized Java6-compiled jar.  branch_3x and lucene_solr_3_1 will need this Solr-specific Java5-compiled maven artifact, though.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3730",
        "summary": "Improved Kuromoji search mode segmentation/decompounding",
        "description": "Kuromoji has a segmentation mode for search that uses a heuristic to promote additional segmentation of long candidate tokens to get a decompounding effect.  This heuristic has been improved.  Patch is coming up.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1388",
        "summary": "Add init method to CloseableThreadLocal",
        "description": "Java ThreadLocal has an init method that allows subclasses to easily instantiate an initial value.  ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-584",
        "summary": "Decouple Filter from BitSet",
        "description": "{code}\npackage org.apache.lucene.search;\n\npublic abstract class Filter implements java.io.Serializable \n{\n  public abstract AbstractBitSet bits(IndexReader reader) throws IOException;\n}\n\npublic interface AbstractBitSet \n{\n  public boolean get(int index);\n}\n\n{code}\n\nIt would be useful if the method =Filter.bits()= returned an abstract interface, instead of =java.util.BitSet=.\n\nUse case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible.\nSparsely populated =java.util.BitSet=s are not efficient and waste lots of memory. It would be desirable to have an alternative BitSet implementation with smaller memory footprint.\n\nThough it _is_ possibly to derive classes from =java.util.BitSet=, it was obviously not designed for that purpose.\nThat's why I propose to use an interface instead. The default implementation could still delegate to =java.util.BitSet=.\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2331",
        "summary": "Add NoOpMergePolicy",
        "description": "I'd like to add a simple and useful MP implementation which does .... nothing ! :). I've came across many places where either the following is documented or implemented: \"if you want to prevent merges, set mergeFactor to a high enough value\". I think a NoOpMergePolicy is just as good, and can REALLY allow you disable merges (except for maybe set mergeFactor to Int.MAX_VAL).\n\nAs such, NoOpMergePolicy will be introduced as a singleton, and can be used for convenience purposes only. Also, for Parallel Index it's important, because I'd like the slices to never do any merges, unless ParallelWriter decides so. So they should be set w/ that MP.\n\nI have a patch ready. Waiting for LUCENE-2320 to go in, so that I don't need to change it afterwards.\n\nAbout the name - I like the name, but suggestions are welcome. I thought of a NullMergePolicy, but I don't like 'Null' used for a NoOp.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3431",
        "summary": "Make QueryAutoStopWordAnalyzer immutable and reusable",
        "description": "Currently QueryAutoStopWordAnalyzer allows its list of stop words to be changed after instantiation through its addStopWords() methods.  This stops the Analyzer from being reusable since it must instantiate its StopFilters every time.\n\nHaving these methods means that although the Analyzer can be instantiated once and reused between IndexReaders, the actual analysis stack is not reusable (which is probably the more expensive part).\n\nSo lets change the Analyzer so that its stop words are set at instantiation time, facilitating reuse.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1222",
        "summary": "IndexWriter.doAfterFlush not being called when there are no deletions flushed",
        "description": "It should be called when flushing either added docs or deletions.  The fix is trivial.  I'll commit shortly to trunk & 2.3.2.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-920",
        "summary": "IndexModifier has incomplete Javadocs",
        "description": "A lot of public and protected members of org.apache.lucene.index.IndexModifier \ndon't have javadocs.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-3148",
        "summary": "TestIndexWriterExceptions reproducible AOOBE in MockVariableIntBlockCodec",
        "description": "{code}\n  [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.739 sec\n    [junit]\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testDocumentsWriterAbort -Dtests.seed=4579947455\n682149564:-7960989923752018504\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockVariableIntBlock(baseBlockSize=32)}, locale=bg_BG, timezone=Brazil\n/Acre\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestIndexWriterExceptions]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=94363216,total=125632512\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testDocumentsWriterAbort(org.apache.lucene.index.TestIndexWriterExceptions):      Caused an ERROR\n    [junit] 66\n    [junit] java.lang.ArrayIndexOutOfBoundsException: 66\n    [junit]     at org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockCodec$MockIntFactory$2.add(MockVariableIntBlockCodec.java:\n114)\n    [junit]     at org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexOutput.close(VariableIntBlockIndexOutput.java:118)\n    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl.close(SepPostingsWriterImpl.java:320)\n    [junit]     at org.apache.lucene.index.codecs.BlockTermsWriter.close(BlockTermsWriter.java:137)\n    [junit]     at org.apache.lucene.index.PerFieldCodecWrapper$FieldsWriter.close(PerFieldCodecWrapper.java:81)\n    [junit]     at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:103)\n    [junit]     at org.apache.lucene.index.TermsHash.flush(TermsHash.java:118)\n    [junit]     at org.apache.lucene.index.DocInverter.flush(DocInverter.java:80)\n    [junit]     at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:75)\n    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:457)\n    [junit]     at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:417)\n    [junit]     at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:309)\n    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:381)\n    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1469)\n    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1229)\n    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1210)\n    [junit]     at org.apache.lucene.index.TestIndexWriterExceptions.testDocumentsWriterAbort(TestIndexWriterExceptions.java:555)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1333)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1251)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.index.TestIndexWriterExceptions FAILED\n{code}\n\ntrunk: r1127871",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3397",
        "summary": "Cleanup Test TokenStreams so they are reusable",
        "description": "Many TokenStreams created in tests are not reusable.  Some do some really messy things which prevent their reuse so we may have to change the tests themselves.\n\nWe'll target back porting this to 3x.",
        "label": "NUG",
        "classified": "TEST",
        "type": ""
    },
    {
        "key": "LUCENE-2401",
        "summary": "Improve performance of CharTermAttribute(Impl) and also fully implement Appendable",
        "description": "The Appendable.append(CharSequence) method in CharTermAttributes is good for general use. But like StringBuilder has for some common use cases specialized methods, this does the same and adds separate append methods for String, StringBuilder and CharTermAttribute itsself. This methods enable the compiler to directly link the specialized methods and don't use the instanceof checks. The unspecialized method only does the instanceof checks for longer CharSequences (>8 chars), else it simply iterates.\n\nThis patch also fixes the required special \"null\" handling. append() methods are required by Appendable to append \"null\", if the argument is null. I dont like this, but its required. Maybe we should document, that we dont dont support it. Otherwise, JDK's formatter fails with formatting null.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1517",
        "summary": "Change superclass of TrieRangeQuery",
        "description": "This patch changes the superclass of TrieRangeQuery to ConstantScoreQuery. The current implementation is using rewrite() and was copied from early RangeQueries. But this is not needed, the TrieRangeQuery can easily subclassed from ConstantScoreQuery.\n\nIf LUCENE-1345 is solved, the whole TrieRangeQuery can be removed, as TrieRangeFilter can be added to BooleanQueries. The whole TrieRangeQuery class is just a convenience class for easier usage of the trie contrib.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3117",
        "summary": "yank SegmentReader.norm out of SegmentReader.java",
        "description": "While working on flex scoring branch and LUCENE-3012, I noticed it was difficult to navigate \nthe norms handling in SegmentReader's code.\n\nI think we should yank this inner class out into a separate file as a start.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1919",
        "summary": "Analysis back compat break",
        "description": "Old and new style token streams don't mix well.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2732",
        "summary": "Fix charset problems in XML loading in HyphenationCompoundWordTokenFilter (also Solr's loader from schema)",
        "description": "As said in LUCENE-2731, the handling of XML in HyphenationCompoundWordTokenFilter is broken and breaks XML 1.0 (5th edition) spec totally. You should never supply a Reader to any XML api, unless you have internal character data (e.g. created programmatically). Also you should supply a system id, as resolving external entities does not work. The loader from files is much more broken, it always open the file as a Reader and then passes it to InputSource. Instead it should point filename directly to InputSource.\n\nThis issue will fix it in trunk and use InputSource in Solr, but will still supply the Reader possibility in previous versions (deprecated).",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-502",
        "summary": "TermScorer caches values unnecessarily",
        "description": "TermScorer aggressively caches the doc and freq of 32 documents at a time for each term scored.  When querying for a lot of terms, this causes a lot of garbage to be created that's unnecessary.  The SegmentTermDocs from which it retrieves its information doesn't have any optimizations for bulk loading, and it's unnecessary.\n\nIn addition, it has a SCORE_CACHE, that's of limited benefit.  It's caching the result of a sqrt that should be placed in DefaultSimilarity, and if you're only scoring a few documents that contain those terms, there's no need to precalculate the SQRT, especially on modern VMs.\n\nEnclosed is a patch that replaces TermScorer with a version that does not cache the docs or feqs.  In the case of a lot of queries, that saves 196 bytes/term, the unnecessary disk IO, and extra SQRTs which adds up.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3913",
        "summary": "HTMLStripCharFilter produces invalid final offset",
        "description": "Nightly build found this... I boiled it down to a small test case that doesn't require the big line file docs.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1540",
        "summary": "Improvements to contrib.benchmark for TREC collections",
        "description": "The benchmarking utilities for  TREC test collections (http://trec.nist.gov) are quite limited and do not support some of the variations in format of older TREC collections.  \n\nI have been doing some benchmarking work with Lucene and have had to modify the package to support:\n* Older TREC document formats, which the current parser fails on due to missing document headers.\n* Variations in query format - newlines after <title> tag causing the query parser to get confused.\n* Ability to detect and read in uncompressed text collections\n* Storage of document numbers by default without storing full text.\n\nI can submit a patch if there is interest, although I will probably want to write unit tests for the new functionality first.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3250",
        "summary": "remove contrib/misc and contrib/wordnet's dependencies on analyzers module",
        "description": "These contribs don't actually analyze any text.\n\nAfter this patch, only the contrib/demo relies upon the analyzers module... we can separately try to figure that one out (I don't think any of these lucene contribs needs to reach back into modules/)\n\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1665",
        "summary": "Remove SortField.AUTO",
        "description": "I'd like to remove SortField.AUTO... it's dangerous for Lucene to\nguess the type of your field, based on the first term it encounters.\nIt can easily be wrong, and, whether it's wrong or right could\nsuddenly change as you index different documents.\n\nIt unexepctedly binds SortField to needing an IndexReader to do the\nguessing.\n\nIt's caused various problems in the past (most recently, for me on\nLUCENE-1656) as we fix other issues/make improvements.\n\nI'd prefer that users of Lucene's field sort be explicit about the\ntype that Lucene should cast the field to.  Someday, if we have\noptional strong[er] typing of Lucene's fields, such type information\nwould already be known.  But in the meantime, I think users should be\nexplicit.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2042",
        "summary": "Allow controllable printing of the hits",
        "description": "Adds \"print.hits.field\" property to the alg.  If set, then the hits retrieved by Search* tasks are printed, along with the value of the specified field, for each doc.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3045",
        "summary": "QueryNodeImpl.containsTag(String) should lowercase the tag key",
        "description": "QueryNodeImpl.containsTag(String key): tag keys are  supposed to be case insensitive, however QueryNodeImpl.containsTag method is considering the case when looking up for tag.\n\n*Bug found by Karsten Fissmer",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2760",
        "summary": "optimize spanfirstquery, spanpositionrangequery",
        "description": "SpanFirstQuery and SpanPositionRangeQuery (SpanFirst is just a special case of this), are currently inefficient.\n\nTake this worst case example: SpanFirstQuery(\"the\").\nCurrently the code reads all the positions for the term \"the\".\n\nBut when enumerating spans, once we have passed the allowable range we should move on to the next document (skipTo)\n ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2399",
        "summary": "Add support for ICU's Normalizer2",
        "description": "While there are separate Case Folding, Normalization, and Ignorable-removal filters in LUCENE-1488,\nthe new ICU Normalizer2 API does this all at once with nfkc_cf (based on the new NFKC_Casefold property in Unicode).\n\nThis is great, because it provides a ton of unicode functionality that is really needed.\nAnd the new Normalizer2 API takes CharSequence and writes to Appendable...\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2512",
        "summary": "DeleteByPercentTask hits NPE",
        "description": "I'm building up Wiki indices for testing search perf across 3.x/4.0, but hit NPE when creating deletions in 4.0 due to flex cutover...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3631",
        "summary": "Remove write access from SegmentReader and possibly move to separate class or IndexWriter/BufferedDeletes/...",
        "description": "After LUCENE-3606 is finished, there are some TODOs:\n\nSegmentReader still contains (package-private) all delete logic including crazy copyOnWrite for validDocs Bits. It would be good, if SegmentReader itsself could be read-only like all other IndexReaders.\n\nThere are two possibilities to do this:\n# the simple one: Subclass SegmentReader and make a RWSegmentReader that is only used by IndexWriter/BufferedDeletes/... DirectoryReader will only use the read-only SegmentReader. This would move all TODOs to a separate class. It's reopen/clone method would always create a RO-SegmentReader (for NRT).\n# Remove all write and commit stuff from SegmentReader completely and move it to IndexWriter's readerPool (it must be in readerPool as deletions need a not-changing view on an index snapshot).\n\nUnfortunately the code is so complicated and I have no real experience in those internals of IndexWriter so I did not want to do it with LUCENE-3606, I just separated the code in SegmentReader and marked with TODO. Maybe Mike McCandless can help :-)",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-368",
        "summary": "Surround query language",
        "description": "This is a copy of what I posted about a year ago. \n \nThe whole thing is hereby licenced under the Apache Licence 2.0, \ncopyright 2005 Apache Software Foundation. \n \nFor inclusion in Lucene (sandbox perhaps?) it will need \nat least the following adaptations: \n- renaming of package names \n  (org.surround to somewhere org.apache.lucene ) \n- moves of the source files to corresponding directories \n \nAlthough it uses the identifier sncf in some places \nI'm not associated with French railroads, but I like the TGV. \n \nRegards, \nPaul Elschot",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2819",
        "summary": "LuceneTestCase's check for uncaught exceptions in threads causes collateral damage?",
        "description": "Eg see these failures:\n\n    https://hudson.apache.org/hudson/job/Lucene-3.x/214/\n\nMultiple test methods failed in TestIndexWriterOnDiskFull, but, I think only 1 test had a real failure but somehow our \"thread hit exc\" tracking incorrectly blames the other 3 cases?\n\nI'm not sure about this but it seems like something like that is going on...\n\nSo, one problem is that LuceneTestCase.tearDown fails on any thread excs, but if CMS had also hit a failure, then fails to clear CMS's thread failures.  I think we should just remove CMS's thread failure tracking?  (It's static so it can definitely bleed across tests).  Ie, just rely on LuceneTestCase's tracking.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1966",
        "summary": "Arabic Analyzer: Stopwords list needs enhancement",
        "description": "The provided Arabic stopwords list needs some enhancements (e.g. it contains a lot of words that not stopwords, and some cleanup) . patch will be provided with this issue.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3526",
        "summary": "preflex codec returns wrong terms if you use an empty field name",
        "description": "spinoff from LUCENE-3473.\n\nI have a standalone test for this... the termsenum is returning a bogus extra empty-term (I assume it has no postings, i didnt try).\n\nThis causes the checkindex test in LUCENE-3473 to fail, because there are 4 terms instead of 3. \n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2608",
        "summary": "Allow for specification of spell checker accuracy when calling suggestSimilar",
        "description": "There is really no need for accuracy to be a class variable in the Spellchecker",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1679",
        "summary": "Make WildcardTermEnum#difference() non-final",
        "description": "The method WildcardTermEnum#difference() is declared final. I found it very useful to subclass WildcardTermEnum to implement different scoring for exact vs. partial matches. The change is rather trivial (attached)  but I guess it could make life easier for a couple of users.\n\nI attached two patches:\n - one which contains the single change to make difference() non-final (WildcardTermEnum.patch)\n - one which does also contain some minor cleanup of WildcardTermEnum. I removed unnecessary member initialization and made those final. ( WildcardTermEnum_cleanup.patch)\n\nThanks simon",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2668",
        "summary": "offset gap should be added regardless of existence of tokens in DocInverterPerField",
        "description": "Problem: If a multiValued field which contains a stop word (e.g. \"will\" in the following sample) only value is analyzed by StopAnalyzer when indexing, the offsets of the subsequent tokens are not correct.\n\n{code:title=indexing a multiValued field}\ndoc.add( new Field( F, \"Mike\", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );\ndoc.add( new Field( F, \"will\", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );\ndoc.add( new Field( F, \"use\", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );\ndoc.add( new Field( F, \"Lucene\", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );\n{code}\n\nIn this program (soon to be attached), if you use WhitespaceAnalyzer, you'll get the offset(start,end) for \"use\" and \"Lucene\" will be use(10,13) and Lucene(14,20). But if you use StopAnalyzer, the offsets will be use(9,12) and lucene(13,19). When searching, since searcher cannot know what analyzer was used at indexing time, this problem causes out of alignment of FVH.\n\nCause of the problem: StopAnalyzer filters out \"will\", anyToken flag set to false then offset gap is not added in DocInverterPerField:\n\n{code:title=DocInverterPerField.java}\nif (anyToken)\n  fieldState.offset += docState.analyzer.getOffsetGap(field);\n{code}\n\nI don't understand why the condition is there... If always the gap is added, I think things are simple.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1616",
        "summary": "add one setter for start and end offset to OffsetAttribute",
        "description": "add OffsetAttribute. setOffset(startOffset, endOffset);\n\ntrivial change, no JUnit needed\n\nChanged CharTokenizer to use it",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-830",
        "summary": "norms file can become unexpectedly enormous",
        "description": "\nSpinoff from this user thread:\n\n   http://www.gossamer-threads.com/lists/lucene/java-user/46754\n\nNorms are not stored sparsely, so even if a doc doesn't have field X\nwe still use up 1 byte in the norms file (and in memory when that\nfield is searched) for that segment.  I think this is done for\nperformance at search time?\n\nFor indexes that have a large # documents where each document can have\nwildly varying fields, each segment will use # documents times # fields\nseen in that segment.  When optimize merges all segments, that product\ngrows multiplicatively so the norms file for the single segment will\nrequire far more storage than the sum of all previous segments' norm\nfiles.\n\nI think it's uncommon to have a huge number of distinct fields (?) so\nwe would need a solution that doesn't hurt the more common case where\nmost documents have the same fields.  Maybe something analogous to how\nbitvectors are now optionally stored sparsely?\n\nOne simple workaround is to disable norms.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3432",
        "summary": "TieredMergePolicy expungeDeletes should not enforce maxMergedSegmentMB",
        "description": "",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3350",
        "summary": "trunk:  TestDocumentsWriterDeleteQueue.testStressDeleteQueue seed failure",
        "description": "fails 100% of the time for me, trunk r1152089\n\n{code}\n    [junit] Testsuite: org.apache.lucene.index.TestDocumentsWriterDeleteQueue\n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.585 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocumentsWriterDeleteQueue -Dtestmethod=testStressDeleteQueue -Dtests.seed=724635056932528964:-56\n53725200660632980\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {}, locale=en_US, timezone=Pacific/Port_Moresby\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestDocumentsWriterDeleteQueue]\n    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=86067624,total=125632512\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testStressDeleteQueue(org.apache.lucene.index.TestDocumentsWriterDeleteQueue):    FAILED\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3126",
        "summary": "IndexWriter.addIndexes can make any incoming segment into CFS if it isn't already",
        "description": "Today, IW.addIndexes(Directory) does not modify the CFS-mode of the incoming segments. However, if IndexWriter's MP wants to create CFS (in general), there's no reason why not turn the incoming non-CFS segments into CFS. We anyway copy them, and if MP is not against CFS, we should create a CFS out of them.\n\nWill need to use CFW, not sure it's ready for that w/ current API (I'll need to check), but luckily we're allowed to change it (@lucene.internal).\n\nThis should be done, IMO, even if the incoming segment is large (i.e., passes MP.noCFSRatio) b/c like I wrote above, we anyway copy it. However, if you think otherwise, speak up :).\n\nI'll take a look at this in the next few days.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2953",
        "summary": "PriorityQueue is inheriently broken if subclass attempts to use \"heap\" w/generic T bound to anything other then \"Object\"",
        "description": "as discovered in SOLR-2410 the fact that the protected \"heap\" variable in PriorityQueue is initialized using an Object[] makes it impossible for subclasses of PriorityQueue to exist and access the \"heap\" array unless they bind the generic to Object.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1872",
        "summary": "Improve javadocs for Numeric*",
        "description": "I'm working on improving Numeric* javadocs.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3042",
        "summary": "AttributeSource can have an invalid computed state",
        "description": "If you work a tokenstream, consume it, then reuse it and add an attribute to it, the computed state is wrong.\nthus for example, clearAttributes() will not actually clear the attribute added.\n\nSo in some situations, addAttribute is not actually clearing the computed state when it should.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-237",
        "summary": "[PATCH] fix compile errors in sandbox",
        "description": "Here's a patch that fixes the compile problems in sandbox/analyzers starting \nshortly before the 1.4 release. The deprecation warnings are also fixed. I \nhave not tested the changes (I don't use those analyzers) but the changes \nshould be trivial enough so they don't break anything. \n \nCould someone apply the patch and also fix FrenchAnalyzer? It's the same \nchange as for the other files, but I didn't manage to make a clean diff \nbecause of encoding problems.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1538",
        "summary": "ValueSourceQuery hits synchronization bottleneck in IndexReader.isDeleted",
        "description": "I plan to fix it the same way we did in LUCENE-1316 for MatchAllDocsQuery (use TermDocs(null)).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1776",
        "summary": "NearSpansOrdered does not lazy load payloads as the PayloadSpans javadoc implies",
        "description": "Best would be to lazy load, but I don't see how with the current algorithm. Short that, we should add an option to ignore payloads - otherwise, if you are doing non payload searching, but the payloads are present, they will be needlessly loaded.\n\nAlready added this to LUCENE-1748, but spinning from that issue to this - patch to follow when LUCENE-1748 is committed.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2298",
        "summary": "Polish Analyzer",
        "description": "Andrzej Bialecki has written a Polish stemmer and provided stemming tables for it under Apache License.\n\nYou can read more about it here: http://www.getopt.org/stempel/\n\nIn reality, the stemmer is general code and we could use it for more languages too perhaps.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2235",
        "summary": "implement PerFieldAnalyzerWrapper.getOffsetGap",
        "description": "PerFieldAnalyzerWrapper does not delegates calls to getOffsetGap(Fieldable), instead it returns the default values from the implementation of Analyzer. (Similar to LUCENE-659 \"PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap\")",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2852",
        "summary": "RAMInputStream hits false EOF if you seek to EOF then seek back then readBytes",
        "description": "TestLazyLoadThreadSafety fails in hudson, possibly an issue with RAMDirectory.\nIf you hack lucene testcase to return another directory, the same seed will pass.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-360",
        "summary": "Test compilation error",
        "description": "The Lucence test fails with the following error: (latest revision from SVN)\n\ncompile-test:\n   [mkdir] Created dir: /opt/lucene/lucene/build/classes/test\n   [javac] Compiling 79 source files to /opt/lucene/lucene/build/classes/test\n   [javac]\n/opt/lucene/lucene/src/test/org/apache/lucene/index/TermInfosTest.java:89:\ncannot resolve symbol\n   [javac] symbol  : constructor TermInfosWriter\n(org.apache.lucene.store.Directory,java.lang.String,org.apache.lucene.index.FieldInfos)\n   [javac] location: class org.apache.lucene.index.TermInfosWriter\n   [javac]     TermInfosWriter writer = new TermInfosWriter(store,\n\"words\", fis);\n\nI see that TermInfosWriter was changed two days back to add another\nargument (interval) to its constructor.\n\nMailing lists for Lucene do not seem to be responding (sending emails to\nsubscribe bounces back and also I do not see any mails after March 2nd being\narchived in either the user or dev lists). So I am using the bug database to\ninform the test failure and submit a simple patch that uses the value of 128\n(the default in the TermInfosWriter class) as interval in the test case.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1113",
        "summary": "fix for Document.getBoost() documentation",
        "description": "The attached patch fixes the javadoc to make clear that getBoost() will never return a useful value in most cases. I will commit this unless someone has a better wording or a real fix.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1426",
        "summary": "Next steps towards flexible indexing",
        "description": "In working on LUCENE-1410 (PFOR compression) I tried to prototype\nswitching the postings files to use PFOR instead of vInts for\nencoding.\n\nBut it quickly became difficult.  EG we currently mux the skip data\ninto the .frq file, which messes up the int blocks.  We inline\npayloads with positions which would also mess up the int blocks.\nSkipping offsets and TermInfo offsets hardwire the file pointers of\nfrq & prox files yet I need to change these to block + offset, etc.\n\nSeparately this thread also started up, on how to customize how Lucene\nstores positional information in the index:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/66264\n\nSo I decided to make a bit more progress towards \"flexible indexing\"\nby first modularizing/isolating the classes that actually write the\nindex format.  The idea is to capture the logic of each (terms, freq,\npositions/payloads) into separate interfaces and switch the flushing\nof a new segment as well as writing the segment during merging to use\nthe same APIs.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2164",
        "summary": "Make CMS smarter about thread priorities",
        "description": "Spinoff from LUCENE-2161...\n\nThe hard throttling CMS does (blocking the incoming thread that wants\nto launch a new merge) can be devastating when it strikes during NRT\nreopen.\n\nIt can easily happen if a huge merge is off and running, but then a\ntiny merge is needed to clean up recently created segments due to\nfrequent reopens.\n\nI think a small change to CMS, whereby it assigns a higher thread\npriority to tiny merges than big merges, should allow us to increase\nthe max merge thread count again, and greatly reduce the chance that\nNRT's reopen would hit this.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1802",
        "summary": "Un-deprecate QueryParser and remove documentation that says it will be replaced in 3.0",
        "description": "This looks like the consensus move at first blush. We can (of course) re-evaluate if things change.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3594",
        "summary": "Backport FieldCacheTermsFilter code duplication removal to 3.x",
        "description": "In trunk I already cleaned up FieldCacheTermsFilter to not duplicate code of FieldCacheRangeFilter. This issue simply backports this.",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-543",
        "summary": "Wildcard query with no wildcard characters in the term throws StringIndexOutOfBounds exception",
        "description": "\nQuery q1 = new WildcardQuery(new Term(\"Text\", \"a\"));\nHits hits = searcher.search(q1);\n\n\nCaught Exception\njava.lang.StringIndexOutOfBoundsException : String index out of range: -1\n    at java.lang.String.substring(Unknown Source)\n    at org.apache.lucene.search.WildcardTermEnum.<init>(WildcardTermEnum.java:65)\n    at org.apache.lucene.search.WildcardQuery.getEnum (WildcardQuery.java:38)\n    at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:54)\n    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:137)\n    at org.apache.lucene.search.Query.weight (Query.java:92)\n    at org.apache.lucene.search.Hits.<init>(Hits.java:41)\n    at org.apache.lucene.search.Searcher.search(Searcher.java:44)\n    at org.apache.lucene.search.Searcher.search(Searcher.java:36)\n    at QuickTest.main(QuickTest.java:45)\n\n\nFrom Erik Hatcher\n\nFeel free to log this as a bug report in our JIRA issue tracker.  It\nseems like a reasonable change to make, such that a WildcardQuery\nwithout a wildcard character would behave like TermQuery.",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2045",
        "summary": "FNFE hit when creating an empty index and infoStream is on",
        "description": "Shai just reported this on the dev list.  Simple test:\n{code}\nDirectory dir = new RAMDirectory();\nIndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), MaxFieldLength.UNLIMITED);\nwriter.setInfoStream(System.out);\nwriter.addDocument(new Document());\nwriter.commit();\nwriter.close();\n{code}\n\nhits this:\n\n{code}\nException in thread \"main\" java.io.FileNotFoundException: _0.prx\n    at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:149)\n    at org.apache.lucene.index.DocumentsWriter.segmentSize(DocumentsWriter.java:1150)\n    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:587)\n    at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3572)\n    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3483)\n    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3474)\n    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1940)\n    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1894)\n{code}\n\nTurns out it's just silly -- this is actually an issue I've already fixed on the flex (LUCENE-1458) branch.  DocumentsWriter has its own method to enumerate the flushed files and compute their size, but really it shouldn't do that -- it should use SegmentInfo's method, instead.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1320",
        "summary": "ShingleMatrixFilter, a three dimensional permutating shingle filter",
        "description": "Backed by a column focused matrix that creates all permutations of shingle tokens in three dimensions. I.e. it handles multi token synonyms.\n\nCould for instance in some cases be used to replaces 0-slop phrase queries with something speedier.\n\n{code:java}\nToken[][][]{\n  {{hello}, {greetings, and, salutations}},\n  {{world}, {earth}, {tellus}}\n}\n{code}\n\npasses the following test  with 2-3 grams:\n\n{code:java}\nassertNext(ts, \"hello_world\");\nassertNext(ts, \"greetings_and\");\nassertNext(ts, \"greetings_and_salutations\");\nassertNext(ts, \"and_salutations\");\nassertNext(ts, \"and_salutations_world\");\nassertNext(ts, \"salutations_world\");\nassertNext(ts, \"hello_earth\");\nassertNext(ts, \"and_salutations_earth\");\nassertNext(ts, \"salutations_earth\");\nassertNext(ts, \"hello_tellus\");\nassertNext(ts, \"and_salutations_tellus\");\nassertNext(ts, \"salutations_tellus\");\n{code}\n\nContains more and less complex tests that demonstrate offsets, posincr, payload boosts calculation and construction of a matrix from a token stream.\n\nThe matrix attempts to hog as little memory as possible by seeking no more than maximumShingleSize columns forward in the stream and clearing up unused resources (columns and unique token sets). Can still be optimized quite a bit though.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2203",
        "summary": "improved snowball testing",
        "description": "Snowball project has test vocabulary files for each language in their svn repository, along with expected output.\n\nWe should use these tests to ensure all languages are working correctly, and it might be helpful in the future for identifying back breaks/changes if we ever want to upgrade snowball, etc.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1189",
        "summary": "QueryParser does not correctly handle escaped characters within quoted strings",
        "description": "The Lucene query parser incorrectly handles escaped characters inside quoted strings; specifically, a quoted string that ends with an (escaped) backslash followed by any additional quoted string will not be properly tokenized. Consider the following example:\n\nbq. {{(name:\"///mike\\\\\\\\\\\\\") or (name:\"alphonse\")}}\n\nThis is not a contrived example -- it derives from an actual bug we've encountered in our system. Running this query will throw an exception, but removing the second clause resolves the problem. After some digging I've found that the problem is with the way quoted strings are processed by the lexer: you'll notice that Mike's name is followed by three escaped backslashes right before the ending quote; looking at the JavaCC code for the query parser highlights the problem:\n\n{code:title=QueryParser.jj|borderStyle=solid}\n<DEFAULT> TOKEN : {\n  <AND:       (\"AND\" | \"&&\") >\n| <OR:        (\"OR\" | \"||\") >\n| <NOT:       (\"NOT\" | \"!\") >\n| <PLUS:      \"+\" >\n| <MINUS:     \"-\" >\n| <LPAREN:    \"(\" >\n| <RPAREN:    \")\" >\n| <COLON:     \":\" >\n| <STAR:      \"*\" >\n| <CARAT:     \"^\" > : Boost\n| <QUOTED:     \"\\\"\" (~[\"\\\"\"] | \"\\\\\\\"\")* \"\\\"\">\n...\n{code}\n\nTake a look at the way the QUOTED token is constructed -- there is no lexical processing of the escaped characters within the quoted string itself. In the above query the lexer matches everything from the first quote through all the backslashes, _treating the end quote as an escaped character_, thus also matching the starting quote of the second term. This causes a lexer error, because the last quote is then considered the start of a new match.\n\nI've come to understand that the Lucene query handler is supposed to be able to handle unsanitized human input; indeed the lexer above would handle a query like {{\"blah\\\"}} without complaining, but that's a \"best-guess\" approach that results in bugs with legal, automatically generated queries. I've attached a patch that fixes the erroneous behavior but does not maintain leniency with malformed queries; I believe this is the correct approach because the two design goals are fundamentally at odds. I'd appreciate any comments.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-960",
        "summary": "SpanQueryFilter addition",
        "description": "Similar to the QueryFilter (or whatever it is called now) the SpanQueryFilter is a regular Lucene Filter, but it also can return Spans-like information.  This is useful if you not only want to filter based on a Query, but you then want to be able to compare how a given match from a new query compared to the positions of the filtered SpanQuery.  Patch to come shortly also contains a caching mechanism for the SpanQueryFilter",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3302",
        "summary": "Leftover legacy enum in IndexReader",
        "description": "In IndexReader we still have some leftover \"handmade\" enum from pre-Java5 times. Unfortunately the Generics/Java5 Policeman did not notice it.\n\nThis patch is just code cleanup, no baclkwards breaks, as code using this enum would not see any difference (because only superclass changes).\n\nI will commit this asap.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-325",
        "summary": "[PATCH] new method expungeDeleted() added to IndexWriter",
        "description": "We make use the docIDs in lucene. I need a way to compact the docIDs in segments\nto remove the \"holes\" created from doing deletes. The only way to do this is by\ncalling IndexWriter.optimize(). This is a very heavy call, for the cases where\nthe index is large but with very small number of deleted docs, calling optimize\nis not practical.\n\nI need a new method: expungeDeleted(), which finds all the segments that have\ndelete documents and merge only those segments.\n\nI have implemented this method and have discussed with Otis about submitting a\npatch. I don't see where I can attached the patch. I will do according to the\npatch guidleine and email the lucene mailing list.\n\nThanks\n\n-John\n\nI don't see a place where I can",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1584",
        "summary": "Callback for intercepting merging segments in IndexWriter",
        "description": "For things like merging field caches or bitsets, it's useful to\nknow which segments were merged to create a new segment.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1460",
        "summary": "Change all contrib TokenStreams/Filters to use the new TokenStream API",
        "description": "Now that we have the new TokenStream API (LUCENE-1422) we should change all contrib modules to use it.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2119",
        "summary": "If you pass Integer.MAX_VALUE as 2nd param to search(Query, int) you hit unexpected NegativeArraySizeException",
        "description": "Note that this is a nonsense value to pass in, since our PQ impl allocates the array up front.\n\nIt's because PQ takes 1+ this value (which wraps to -1), and attempts to allocate that.  We should bounds check it, and drop PQ size by one in this case.\n\nBetter, maybe: in IndexSearcher, if that n is ever > maxDoc(), set it to maxDoc().\n\nThis trips users up fairly often because they assume our PQ doesn't statically pre-allocate (a reasonable assumption...).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1179",
        "summary": "AssertionError on creating doc containing field with empty string as field name",
        "description": "Spinoff from here:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/58496\n\nPre-2.3 you were allowed to add Fields to a Document where the field name is the empty string.  In 2.3.0 it broke: you hit this during flush:\n\n{code}\njava.lang.AssertionError\n    at org.apache.lucene.index.TermInfosWriter.add(TermInfosWriter.java:143)\n    at org.apache.lucene.index.DocumentsWriter.appendPostings(DocumentsWriter.java:2290)\n    at org.apache.lucene.index.DocumentsWriter.writeSegment(DocumentsWriter.java:1985)\n    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:539)\n    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2497)\n    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2397)\n    at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1204)\n    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1178)\n    at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1153) \n{code}\n\nThe bug is just an over-aggressive assert statement.  I'll commit a fix shortly & port to 2.3 branch for 2.3.1 release.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1493",
        "summary": "Enable setting hits queue size in Search*Task in contrib/benchmark",
        "description": "In testing for LUCENE-1483, I'd like to try different collector queue\nsizes during benchmarking.  But currently contrib/benchmark uses\ndeprecated Hits with hardwired \"top 100\" queue size.  I'll switch it to\nthe TopDocs APIs.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2564",
        "summary": "wordlistloader is inefficient",
        "description": "WordListLoader is basically used for loading up stopwords lists, stem dictionaries, etc.\nUnfortunately the api returns Set<String> and sometimes even HashSet<String> or HashMap<String,String>\n\nI think we should break it and return CharArraySets and CharArrayMaps (but leave the return value as generic Set,Map).\n\nIf someone objects to breaking it in 3.1, then we can do this only in 4.0, but i think it would be good to fix it both places.\nThe reason is that if someone does new FooAnalyzer() a lot (probably not uncommon) i think its doing a bunch of useless copying.\n\nI think we should slap @lucene.internal on this API too, since thats mostly how its being used.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2692",
        "summary": "Position Checking Span Queries",
        "description": "I've created a bunch of new SpanQuery classes that allow one to do things like check to see if a SpanQuery falls between two positions (which is a more general form of SpanFirstQuery) and I've also added one that only includes a match if the payload located at the span match also matches a given payload.  With the latter, one can do queries for items w/ specific payloads.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1525",
        "summary": "Missing dependencies in two generated maven pom files ",
        "description": "There are some missing dependencies in generated maven pom.xml files (benchmark and highlighter)",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-944",
        "summary": "Remove deprecated methods in BooleanQuery",
        "description": "Remove deprecated methods setUseScorer14 and getUseScorer14 in BooleanQuery, and adapt javadocs.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2995",
        "summary": "factor out a shared spellchecking module",
        "description": "In lucene's contrib we have spellchecking support (index-based spellchecker, directspellchecker, etc). \nwe also have some things like pluggable comparators.\n\nIn solr we have auto-suggest support (with two implementations it looks like), some good utilities like HighFrequencyDictionary, etc.\n\nI think spellchecking is really important... google has upped the ante to what users expect.\nSo I propose we combine all this stuff into a shared modules/spellchecker, which will make it easier\nto refactor and improve the quality.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1424",
        "summary": "Change all multi-term querys so that they extend MultiTermQuery and allow for a constant score mode",
        "description": "Cleans up a bunch of code duplication, closer to how things should be - design wise, gives us constant score for all the multi term queries, and allows us at least the option of highlighting the constant score queries without much further work.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2374",
        "summary": "Add reflection API to AttributeSource/AttributeImpl",
        "description": "AttributeSource/TokenStream inspection in Solr needs to have some insight into the contents of AttributeImpls. As LUCENE-2302 has some problems with toString() [which is not structured and conflicts with CharSequence's definition for CharTermAttribute], I propose an simple API that get a default implementation in AttributeImpl (just like toString() current):\n\n- Iterator<Map.Entry<String,?>> AttributeImpl.contentsIterator() returns an iterator (for most attributes its a singleton) of a key-value pair, e.g. \"term\"->\"foobar\",\"startOffset\"->Integer.valueOf(0),...\n- AttributeSource gets the same method, it just concat the iterators of each getAttributeImplsIterator() AttributeImpl\n\nNo backwards problems occur, as the default toString() method will work like before (it just gets iterator and lists), but we simply remove the documentation for the format. (Char)TermAttribute gets a special impl fo toString() according to CharSequence and a corresponding iterator.\n\nI also want to remove the abstract hashCode() and equals() methods from AttributeImpl, as they are not needed and just create work for the implementor.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2209",
        "summary": "add @experimental javadocs tag",
        "description": "There are a lot of things marked experimental, api subject to change, etc. in lucene.\n\nthis patch simply adds a @experimental tag to common-build.xml so that we can use it, for more consistency.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3757",
        "summary": "Change AtomicReaderContext.leaves() to return itsself as only leave to simplify code and remove an otherwise unneeded ReaderUtil method",
        "description": "The documentation of IndexReaderContext.leaves() states that it returns (for convenience) all leave nodes, if the context is top-level (directly got from IndexReader), otherwise returns null. This is not correct for AtomicReaderContext, where it returns null always.\n\nTo make it consistent, the convenience method should simply return itsself as only leave for atomic contexts. This makes the utility method ReaderUtil.leaves() obsolete and simplifies code.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-788",
        "summary": "contrib/benchmark assumes Locale.US for parsing dates in Reuters collection",
        "description": "SimpleDateFormat used for parsing dates in Reuters documents is instantiated without specifying a locale. So it is using the default locale. If that happens to be US, it will work. But for another locale a parse exception is likely.\n\nAffects both StandardBenchmarker and ReutersDocMaker.\n\nFix is trivial - specify Locale.US for SimpleDateFormat's constructor.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2896",
        "summary": "in advance(), don't try to skip if there is evidence it will fail",
        "description": "There are TODO's about this in the code everywhere, and this was part of\nMike speeding up ExactPhraseScorer.\n\nI think the codec should do this.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-250",
        "summary": "Javadocs for Scorer.java and TermScorer.java",
        "description": "Javadocs for Scorer.java and TermScorer.java \nAlso changed build.xml to use package access for the \njavadocs target. That caused some minor error javadoc messages \nin CompoundFileReader.java and FieldInfos.java, which are also fixed. \n \nThe patch posted earlier for Weight.java \n(a broken javadoc link) is also included. \n \nThe attached patch is for all 5 files against the CVS top directory \nof 28 July 2004. The only dependency is that package access \nis needed for TermScorer.java. \n \nThis might be changed by declaring TermScorer as a public class, \nbut I preferred to use javadoc package access in build.xml \nover changing java code. \n \nUsing package access for javadocs shows some more undocumented \nclasses, eg. in the doc page of the search package. This might \nencourage folks to write more javadocs... \n \nRegards, \nPaul",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-239",
        "summary": "[PATCH] cleaner API for Field.Text",
        "description": "Currently there are four methods named Field.Text(). As those methods have \nthe same name and a very similar method signature, everyone will think these \nare just convenience methods that do the same thing. But they behave \ndifferently: the one that takes a Reader doesn't store the data, the one that \ntakes a String does. I know that this is documented, but it's still not a nice \nAPI. Methods that behave differently should have diffent names. The attached \npatch deprecates two of the old methods and adds two new ones named \nField.StoredText(). I think this is much easier to understand from the \nprogrammer's point-of-view and will help avoid bugs.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1610",
        "summary": "Preserve whitespace in <code> sections in the Changes.html generated from CHANGES.txt by changes2html.pl",
        "description": "The Trunk section of CHANGES.txt sports use of a new feature: <code> sections, for the two mentions of LUCENE-1575.\n\nThis looks fine in the text rendering, but looks crappy in the HTML version, since changes2html.pl escapes HTML metacharacters to appear as-is in the HTML rendering, but the newlines in the code are converted to a single space. \n\nI think this should be fixed by modifying changes2html.pl to convert <code> and </code> into (unescaped) <code><pre> and </pre></code>, respectively, since just passing through <code> and </code>, without </?pre>, while changing the font to monospaced (nice), still collapses whitespace (not nice). \n\nSee the java-dev thread that spawned this issue here: http://www.nabble.com/CHANGES.txt-td23102627.html",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1405",
        "summary": "Support for new Resources model in ant 1.7 in Lucene ant task.",
        "description": "Ant Task for Lucene should use modern Resource model (not only FileSet child element).\nThere is a patch with required changes.\n\nSupported by old (ant 1.6) and new (ant 1.7) resources model:\n<index ....> <!-- Lucene Ant Task -->\n  <fileset ... />\n</index> \n\nSupported only by new (ant 1.7) resources model:\n<index ....> <!-- Lucene Ant Task -->\n  <filelist ... />\n</index> \n\n<index ....> <!-- Lucene Ant Task -->\n  <userdefinied-filesource ... />\n</index> ",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2474",
        "summary": "Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey)",
        "description": "Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey).\n\nA spin of: https://issues.apache.org/jira/browse/LUCENE-2468. Basically, its make a lot of sense to cache things based on IndexReader#getFieldCacheKey, even Lucene itself uses it, for example, with the CachingWrapperFilter. FieldCache enjoys being called explicitly to purge its cache when possible (which is tricky to know from the \"outside\", especially when using NRT - reader attack of the clones).\n\nThe provided patch allows to plug a CacheEvictionListener which will be called when the cache should be purged for an IndexReader.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2240",
        "summary": "SimpleAnalyzer and WhitespaceAnalyzer should have Version ctors",
        "description": "Due to the Changes to CharTokenizer ( LUCENE-2183 ) WhitespaceAnalyzer and SimpleAnalyzer need a Version ctor. Default ctors must be deprecated",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1270",
        "summary": "After IW.addIndexesNoOptimize, IW.close may hang",
        "description": "Spinoff from here:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c43128.192.168.1.71.1208561409.webmail@192.168.1.71%3e\n\nThe addIndexesNoOptimize method first merges eligible segments\naccording to the MergePolicy, and then copies over one by one any\nremaining \"external\" segments.\n\nThat copy can possibly (rather rarely) result in new merges becoming\neligible because its size can change if the index being added was\ncreated with autoCommit=false.\n\nHowever, we fail to then invoke the MergeScheduler to run these\nmerges.  As a result, in close, where we wait until all running and\npending merges complete, we will never return.\n\nThe fix is simple: invoke the merge scheduler inside\ncopyExternalSegments() if any segments were copied.  I also added\ndefensive invocation of the merge scheduler during close, just in case\nother code paths could allow for a merge to be added to the pending\nqueue but not scheduled.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2565",
        "summary": "TestUTF32ToUTF8 can run forever",
        "description": "Stress testing this particular test uncovered that the testRandomRanges testcase can run forever, depending on the random numbers picked...",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1390",
        "summary": "add ASCIIFoldingFilter and deprecate ISOLatin1AccentFilter",
        "description": "The ISOLatin1AccentFilter is removing accents from accented characters in the ISO Latin 1 character set.\nIt does what it does and there is no bug with it.\n\nIt would be nicer, though, if there was a more comprehensive version of this code that included not just ISO-Latin-1 (ISO-8859-1) but the entire Latin 1 and Latin Extended A unicode blocks.\nSee: http://en.wikipedia.org/wiki/Latin-1_Supplement_unicode_block\nSee: http://en.wikipedia.org/wiki/Latin_Extended-A_unicode_block\n\nThat way, all languages using roman characters are covered.\nA new class, ISOLatinAccentFilter is attached. It is intended to supercede ISOLatin1AccentFilter which should get deprecated.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3503",
        "summary": "DisjunctionSumScorer gives slightly (float iotas) different scores when you .nextDoc vs .advance",
        "description": "Spinoff from LUCENE-1536.\n\nI dug into why we hit a score diff when using luceneutil to benchmark\nthe patch.\n\nAt first I thought it was BS1/BS2 difference, but because of a bug in\nthe patch it was still using BS2 (but should be BS1) -- Robert's last\npatch fixes that.\n\nBut it's actually a diff in BS2 itself, whether you next or advance\nthrough the docs.\n\nIt's because DisjunctionSumScorer, when summing the float scores for a\ngiven doc that matches multiple sub-scorers, might sum in a different\norder, when you had .nextDoc'd to that doc than when you had .advance'd\nto it.\n\nThis in turn is because the PQ used by that scorer (ScorerDocQueue)\nmakes no effort to break ties.  So, when the top N scorers are on the\nsame doc, the PQ doesn't care what order they are in.\n\nFixing ScorerDocQueue to break ties will likely be a non-trivial perf\nhit, though, so I'm not sure whether we should do anything here...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1723",
        "summary": "KeywordTokenizer does not properly set the end offset",
        "description": "KeywordTokenizer sets the Token's term length attribute but appears to omit the end offset. The issue was discovered while using a highlighter with the KeywordAnalyzer. KeywordAnalyzer delegates to KeywordTokenizer propagating the bug. \n\nBelow is a JUnit test (source is also attached) that exercises various analyzers via a Highlighter instance. Every analyzer but the KeywordAnazlyzer successfully wraps the text with the highlight tags, such as \"<b>thetext</b>\". When using KeywordAnalyzer the tags appear before the text, for example: \"<b></b>thetext\". \n\nPlease note NewKeywordAnalyzer and NewKeywordTokenizer classes below. When using NewKeywordAnalyzer the tags are properly placed around the text. The NewKeywordTokenizer overrides the next method of the KeywordTokenizer setting the end offset for the returned Token. NewKeywordAnalyzer utilizes KeywordTokenizer to produce proper token.\n\nUnless there is an objection I will gladly post a patch in the very near future . \n\n-----------------------------\npackage lucene;\n\nimport java.io.IOException;\nimport java.io.Reader;\n\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.KeywordAnalyzer;\nimport org.apache.lucene.analysis.KeywordTokenizer;\nimport org.apache.lucene.analysis.SimpleAnalyzer;\nimport org.apache.lucene.analysis.StopAnalyzer;\nimport org.apache.lucene.analysis.Token;\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.Tokenizer;\nimport org.apache.lucene.analysis.WhitespaceAnalyzer;\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.search.highlight.Highlighter;\nimport org.apache.lucene.search.highlight.QueryScorer;\nimport org.apache.lucene.search.highlight.SimpleHTMLFormatter;\nimport org.apache.lucene.search.highlight.WeightedTerm;\nimport org.junit.Test;\nimport static org.junit.Assert.*;\n\npublic class AnalyzerBug {\n\n\t@Test\n\tpublic void testWithHighlighting() throws IOException {\n\t\tString text = \"thetext\";\n\t\tWeightedTerm[] terms = { new WeightedTerm(1.0f, text) };\n\n\t\tHighlighter highlighter = new Highlighter(new SimpleHTMLFormatter(\n\t\t\t\t\"<b>\", \"</b>\"), new QueryScorer(terms));\n\n\t\tAnalyzer[] analazers = { new StandardAnalyzer(), new SimpleAnalyzer(),\n\t\t\t\tnew StopAnalyzer(), new WhitespaceAnalyzer(),\n\t\t\t\tnew NewKeywordAnalyzer(), new KeywordAnalyzer() };\n\n\t\t// Analyzers pass except KeywordAnalyzer\n\t\tfor (Analyzer analazer : analazers) {\n\t\t\tString highighted = highlighter.getBestFragment(analazer,\n\t\t\t\t\t\"CONTENT\", text);\n\t\t\tassertEquals(\"Failed for \" + analazer.getClass().getName(), \"<b>\"\n\t\t\t\t\t+ text + \"</b>\", highighted);\n\t\t\tSystem.out.println(analazer.getClass().getName()\n\t\t\t\t\t+ \" passed, value highlighted: \" + highighted);\n\t\t}\n\t}\n}\n\nclass NewKeywordAnalyzer extends KeywordAnalyzer {\n\n\t@Override\n\tpublic TokenStream reusableTokenStream(String fieldName, Reader reader)\n\t\t\tthrows IOException {\n\t\tTokenizer tokenizer = (Tokenizer) getPreviousTokenStream();\n\t\tif (tokenizer == null) {\n\t\t\ttokenizer = new NewKeywordTokenizer(reader);\n\t\t\tsetPreviousTokenStream(tokenizer);\n\t\t} else\n\t\t\ttokenizer.reset(reader);\n\t\treturn tokenizer;\n\t}\n\n\t@Override\n\tpublic TokenStream tokenStream(String fieldName, Reader reader) {\n\t\treturn new NewKeywordTokenizer(reader);\n\t}\n}\n\nclass NewKeywordTokenizer extends KeywordTokenizer {\n\tpublic NewKeywordTokenizer(Reader input) {\n\t\tsuper(input);\n\t}\n\n\t@Override\n\tpublic Token next(Token t) throws IOException {\n\t\tToken result = super.next(t);\n\t\tif (result != null) {\n\t\t\tresult.setEndOffset(result.termLength());\n\t\t}\n\t\treturn result;\n\t}\n}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2936",
        "summary": "score and explain don't match",
        "description": "I've faced this problem recently. I'll attach a program to reproduce the problem soon. The program outputs the following:\n\n{noformat}\n** score = 0.10003257\n** explain\n0.050016284 = (MATCH) product of:\n  0.15004885 = (MATCH) sum of:\n    0.15004885 = weight(f1:\"note book\" in 0), product of:\n      0.3911943 = queryWeight(f1:\"note book\"), product of:\n        0.61370564 = idf(f1: note=1 book=1)\n        0.6374299 = queryNorm\n      0.38356602 = fieldWeight(f1:\"note book\" in 0), product of:\n        1.0 = tf(phraseFreq=1.0)\n        0.61370564 = idf(f1: note=1 book=1)\n        0.625 = fieldNorm(field=f1, doc=0)\n  0.33333334 = coord(1/3)\n{noformat}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-510",
        "summary": "IndexOutput.writeString() should write length in bytes",
        "description": "We should change the format of strings written to indexes so that the length of the string is in bytes, not Java characters.  This issue has been discussed at:\n\nhttp://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html\n\nWe must increment the file format number to indicate this change.  At least the format number in the segments file should change.\n\nI'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-895",
        "summary": "Exclude PrecedenceQueryParser from build or disable failing test cases",
        "description": "As Erik commented in LUCENE-885 the PrecendenceQueryParser is currently\nunmaintained. Since some tests are failing we should either exclude PQP from the \nbuild or simply disable the failing tests.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2904",
        "summary": "non-contiguous LogMergePolicy should be careful to not select merges already running",
        "description": "Now that LogMP can do non-contiguous merges, the fact that it disregards which segments are already being merged is more problematic since it could result in it returning conflicting merges and thus failing to run multiple merges concurrently.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1012",
        "summary": "Problems with maxMergeDocs parameter",
        "description": "I found two possible problems regarding IndexWriter's maxMergeDocs value. I'm using the following code to test maxMergeDocs:\n\n{code:java} \n  public void testMaxMergeDocs() throws IOException {\n    final int maxMergeDocs = 50;\n    final int numSegments = 40;\n    \n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);      \n    writer.setMergePolicy(new LogDocMergePolicy());\n    writer.setMaxMergeDocs(maxMergeDocs);\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    for (int i = 0; i < numSegments * maxMergeDocs; i++) {\n      writer.addDocument(doc);\n      //writer.flush();      // uncomment to avoid the DocumentsWriter bug\n    }\n    writer.close();\n    \n    new SegmentInfos.FindSegmentsFile(dir) {\n\n      protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(directory, segmentFileName);\n        for (int i = 0; i < infos.size(); i++) {\n          assertTrue(infos.info(i).docCount <= maxMergeDocs);\n        }\n        return null;\n      }\n    }.run();\n  }\n{code} \n  \n- It seems that DocumentsWriter does not obey the maxMergeDocs parameter. If I don't flush manually, then the index only contains one segment at the end and the test fails.\n\n- If I flush manually after each addDocument() call, then the index contains more segments. But still, there are segments that contain more docs than maxMergeDocs, e. g. 55 vs. 50. The javadoc in IndexWriter says:\n{code:java}\n   /**\n   * Returns the largest number of documents allowed in a\n   * single segment.\n   *\n   * @see #setMaxMergeDocs\n   */\n  public int getMaxMergeDocs() {\n    return getLogDocMergePolicy().getMaxMergeDocs();\n  }\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2843",
        "summary": "Add variable-gap terms index impl.",
        "description": "PrefixCodedTermsReader/Writer (used by all \"real\" core codecs) already\nsupports pluggable terms index impls.\n\nThe only impl we have now is FixedGapTermsIndexReader/Writer, which\npicks every Nth (default 32) term and holds it in efficient packed\nint/byte arrays in RAM.  This is already an enormous improvement (RAM\nreduction, init time) over 3.x.\n\nThis patch adds another impl, VariableGapTermsIndexReader/Writer,\nwhich lets you specify an arbitrary IndexTermSelector to pick which\nterms are indexed, and then uses an FST to hold the indexed terms.\nThis is typically even more memory efficient than packed int/byte\narrays, though, it does not support ord() so it's not quite a fair\ncomparison.\n\nI had to relax the terms index plugin api for\nPrefixCodedTermsReader/Writer to not assume that the terms index impl\nsupports ord.\n\nI also did some cleanup of the FST/FSTEnum APIs and impls, and broke\nout separate seekCeil and seekFloor in FSTEnum.  Eg we need seekFloor\nwhen the FST is used as a terms index but seekCeil when it's holding\nall terms in the index (ie which SimpleText uses FSTs for).\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3476",
        "summary": "SearcherManager misses to close IR if manager is closed during reopen",
        "description": "if we close SM while there is a thread calling maybReopen() and swapSearcher throws already closed exception we miss to close the searcher / reader.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2791",
        "summary": "WindowsDirectory",
        "description": "We can use Windows' overlapped IO to do pread() and avoid the performance problems of SimpleFS/NIOFSDir.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2121",
        "summary": "add UnicodeUtil.nextValidUTF16String ",
        "description": "In flex branch, TermRef must not contain unpaired surrogates, etc.\nBut in trunk/previous releases, people could (and do) seek to these.\nAlso some lucene multitermqueries will generate these invalid seek locations, even now (which we should separately fix)\nI think the common case is already handled with a hack in SegmentReader.LegacyTermEnum, but we should clean up this hack and handle all cases.\n\nI would also like to use this nextValidUTF16String in LUCENE-1606, and there might be other places it could be used for better bw compat.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3002",
        "summary": "Add tests.iter.min to improve controlling tests.iter's behavior",
        "description": "As discussed here: http://lucene.472066.n3.nabble.com/Stop-iterating-if-testsFailed-td2747426.html, this issue proposes to add tests.iter.min in order to allow one better control over how many iterations are run:\n\n* Keep tests.iter as it is today\n* Add tests.iter.min (default to tests.iter) to denote that at least N instances of the test should run until there's either a failure or tests.iter is reached.\n\nIf one wants to run until the first failure, he can set tests.iter.min=1 and tests.iter=X -- up to X instances of the test will run, until the first failure.\n\nSimilarly, one can set tests.iter=N to denote that at least N instances should run, regardless if there were failures, but if after N runs a failure occurred, the test should stop.\n\nNote: unlike what's proposed on the thread, tests.iter.max is dropped from this proposal as it's exactly like tests.iter, so no point in having two similar parameters.\n\nI will work on a patch tomorrow.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1792",
        "summary": "new QueryParser fails to set AUTO REWRITE for multi-term queries",
        "description": "The old QueryParser defaults to constant score rewrite for Prefix,Fuzzy,Wildcard,TermRangeQuery, but the new one seems not to.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1162",
        "summary": "Improve architecture of FieldSortedHitQueue",
        "description": "Per the discussion (quite some time ago) on issue LUCENE-806, I'd like to propose an architecture change to the way FieldSortedHitQueue works, and in particular the way it creates SortComparatorSources. I think (I hope) that anyone who looks at the FSHQ code will agree that the class does a lot and much of it's repetitive stuff that really has no business being in that class.\n\nI am about to attach a patch which, in and of itself, doesn't really achieve much that's concrete but does tidy things up a great deal and makes it easier to plug in different behaviours. I then have a subsequent patch which provides a fairly simple and flexible example of how you might replace an implementation, in this case the field-local-String-comparator version from LUCENE-806.\n\nThe downside to this patch is that it involved changing the signature of SortComparatorSource.newComparator to take a Locale. There would be ways around this (letting FieldSortedHitQueue take in either a SortComparatorSource or some new, improved interface which takes a Locale (and possibly extends SortComparatorSource). I'm open to this but personally I think that the Locale version makes sense and would suggest that the code would be nicer by breaking the API (and hence targeting this to, presumably, 3.0 at a minimum).\n\nThis code does not include specific tests (I will add these, if people like the general idea I'm proposing here) but all current tests pass with this change.\n\nPatch to follow.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-785",
        "summary": "RAMDirectory not Serializable",
        "description": "The current implementation of RAMDirectory throws a NotSerializableException when trying to serialize, due to the inner class KeySet of HashMap not being serializable (god knows why)\n\njava.io.NotSerializableException: java.util.HashMap$KeySet\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)\n\nCaused by line 43:\n\nprivate Set fileNames = fileMap.keySet();\n\nEDIT:\n\nwhile we're at it: same goes for inner class Values \n\njava.io.NotSerializableException: java.util.HashMap$Values\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)\n\nCollection files = fileMap.values();\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-961",
        "summary": "RegexCapabilities is not Serializable",
        "description": "The class RegexQuery is marked Serializable by its super class, but it contains a RegexCapabilities which is not Serializable. Thus attempting to serialize the query results in an exception. \n\nMaking RegexCapabilities serializable should be no problem since its subclasses contain only serializable classes (java.util.regex.Pattern and org.apache.regexp.RE).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2754",
        "summary": "add spanquery support for all multitermqueries",
        "description": "I set fix version: 4.0, but possibly we could do this for 3.x too\n\nCurrently, we have a special SpanRegexQuery in contrib, and issues like LUCENE-522 open for SpanFuzzyQuery.\nThe SpanRegexQuery in contrib is a little messy additionally.\n\nFor any arbitrary MultiTermQueries to work as a SpanQuery, there are only 3 requirements:\n# The un-rewritten query must extend SpanQuery so it can be included in Span clauses\n# The rewritten query should be SpanOrQuery instead of BooleanQuery\n# The rewritten term clauses should be SpanTermQueries.\n\nInstead of having logic like this for each query, i suggest adding two rewrite methods:\n* ScoringSpanBoolean rewrite\n* TopTermsSpanBoolean rewrite\n\nas a start i wrote these up, and added a SpanMultiTermQueryWrapper that can be used to wrap any multitermquery this way.\nthere are a few kinks, but I think the MTQ policeman can probably help get through them.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1754",
        "summary": "Get rid of NonMatchingScorer from BooleanScorer2",
        "description": "Over in LUCENE-1614 Mike has made a comment about removing NonMatchinScorer from BS2, and return null in BooleanWeight.scorer(). I've checked and this can be easily done, so I'm going to post a patch shortly. For reference: https://issues.apache.org/jira/browse/LUCENE-1614?focusedCommentId=12715064&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12715064.\n\nI've marked the issue as 2.9 just because it's small, and kind of related to all the search enhancements done for 2.9.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1135",
        "summary": "Mark contrib/wikipedia as experimental",
        "description": "I am going to add javadocs to trunk and 2_3 branch that mark the WikipediaTokenizer as experimental.  I think it is fine to release, but I want people to know that the grammar may change in the next release (although I will try to keep it the same)",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3193",
        "summary": "TwoPhaseCommit interface",
        "description": "I would like to propose a TwoPhaseCommit interface which declares the methods necessary to implement a 2-phase commit algorithm:\n* prepareCommit()\n* commit()\n* rollback()\n\nThe prepare/commit ones have variants that take a (Map<String,String> commitData) following the ones we have in IndexWriter.\n\nIn addition, a TwoPhaseCommitTool which implements a 2-phase commit amongst several TPCs.\n\nHaving IndexWriter implement that interface will allow running the 2-phase commit algorithm on multiple IWs or IW + any other object that implements the interface.\n\nWe should mark the interface @lucene.internal so as to not block ourselves in the future. This is pretty advanced stuff anyway.\n\nWill post a patch soon",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1327",
        "summary": "TermSpans skipTo() doesn't always move forwards",
        "description": "In TermSpans (or the anonymous Spans class returned by SpansTermQuery, depending on the version), the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc:\n\n  public boolean skipTo(int target) throws IOException {\n          // are we already at the correct position?\n          if (doc >= target) {\n            return true;\n          }\n\n          ...\n\n\nThis violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:\n\nif (doc >= target) {\n  return next();\n}\n\nThis bug causes particular problems if one wants to use the payloads feature - this is because if one loads a payload, then performs a skipTo() to the same document, then tries to load the \"next\" payload, the spans hasn't changed position and it attempts to load the same payload again (which is an error).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2359",
        "summary": "CartesianPolyFilterBuilder doesn't handle edge case around the 180 meridian",
        "description": "Test case:  \nPoints all around the globe, plus two points at 0, 179.9 and 0,-179.9 (on each side of the meridian).  Then, do a Cartesian Tier filter on a point right near those two.  It will return all the points when it should just return those two.\n\nThe flawed logic is in the else clause below:\n{code}\nif (longX2 != 0.0) {\n\t\t//We are around the prime meridian\n\t\tif (longX == 0.0) {\n\t\t\tlongX = longX2;\n\t\t\tlongY = 0.0;\n        \tshape = getShapeLoop(shape,ctp,latX,longX,latY,longY);\n\t\t} else {//we are around the 180th longitude\n\t\t\tlongX = longX2;\n\t\t\tlongY = -180.0;\n\t\t\tshape = getShapeLoop(shape,ctp,latY,longY,latX,longX);\n\t}\n{code}\n\nBasically, the Y and X values are transposed.  This currently says go from longY (-180) all the way around  to longX which is the lower left longitude of the box formed.  Instead, it should go from the lower left long to -180.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3184",
        "summary": "add LuceneTestCase.rarely()/LuceneTestCase.atLeast()",
        "description": "in LUCENE-3175, the tests were sped up a lot by using reasonable number of iterations normally, but cranking up for NIGHTLY.\nwe also do crazy things more 'rarely' for normal builds (e.g. simpletext, payloads, crazy merge params, etc)\nalso, we found some bugs by doing this, because in general our parameters are too fixed.\n\nhowever, it made the code look messy... I propose some new methods:\ninstead of some crazy code in your test like:\n{code}\nint numdocs = (TEST_NIGHTLY ? 1000 : 100) * RANDOM_MULTIPLIER;\n{code}\n\nyou use:\n{code}\nint numdocs = atLeast(100);\n{code}\n\nthis will apply the multiplier, also factor in nightly, and finally add some random fudge... so e.g. in local runs its sometimes 127 docs, sometimes 113 docs, etc.\n\nadditionally instead of code like:\n{code}\nif ((TEST_NIGHTLY && random.nextBoolean()) || (random.nextInt(20) == 17)) {\n{code}\n\nyou do\n{code}\nif (rarely()) {\n{code}\n\nwhich applies NIGHTLY and also the multiplier (logarithmic growth).\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3807",
        "summary": "Cleanup suggester API",
        "description": "Currently the suggester api and especially TermFreqIterator don't play that nice with BytesRef and other paradigms we use in lucene, further the java iterator pattern isn't that useful when it gets to work with TermsEnum, BytesRef etc. We should try to clean up this api step by step moving over to BytesRef including the Lookup class and its interface...",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3053",
        "summary": "improve test coverage for Multi*",
        "description": "It seems like an easy win that when the test calls newSearcher(), \nit should sometimes wrap the reader with a SlowMultiReaderWrapper.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1387",
        "summary": "Add LocalLucene",
        "description": "Local Lucene (Geo-search) has been donated to the Lucene project, per https://issues.apache.org/jira/browse/INCUBATOR-77.  This issue is to handle the Lucene portion of integration.\n\nSee http://lucene.markmail.org/message/orzro22sqdj3wows?q=LocalLucene\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3421",
        "summary": "PayloadTermQuery's explain is broken when span score is not included",
        "description": "When setting includeSpanScore to false with PayloadTermQuery, the explain is broken.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-757",
        "summary": "Source packaging fails if ${dist.dir} does not exist",
        "description": "package-tgz-src and package-zip-src fail if ${dist.dir} does not exist, since these two targets do not call the package target, which is responsible for making the dir.\n\nI have a fix and will commit shortly.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-875",
        "summary": "javadocs creation has too many warnings/errors",
        "description": "Currently running 'ant javadocs' creates so many warnings that you have to grep the output to verify that new code did not add more.\n\nWhile most current errors might be minor, they might hide a few serious ones that we will never know abut until someone complains. \n\nBest if we fix all of them and keep it always clean...",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3021",
        "summary": "randomize skipInterval in tests",
        "description": "we probably don't test the multi-level skipping very well, but skipInterval etc is now private to the codec, so for better test coverage we should parameterize it to the postings writers, and randomize it via mockrandomcodec.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2244",
        "summary": "Improve StandardTokenizer's understanding of non ASCII punctuation and quotes",
        "description": "In the vein of LUCENE-1126 and LUCENE-1390, StandardTokenizerImpl.jflex should do a better job at understanding non-ASCII punctuation characters.\n\nFor example, its understanding of the single-quote character \"'\" is currently limited to that character only. It will set a token's type to APOSTROPHE only if the \"'\" was used.\nIn the patch attached, I added all the characters that ASCIIFoldingFilter would change into \"'\".\n\nI'm not sure that this is the right approach so I didn't write a complete patch for all the other hardcoded characters used in jflex rules such as \".\", \"-\" which have some variants in ASCIIFoldingFilter that could be used as well.\n\nMaybe a better approach would be to make it possible to have an ASCIIFoldingFilter-like reader as a character filter that could be in inserted in front of StandardTokenizer ?",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3381",
        "summary": "Sandbox remaining contrib queries",
        "description": "In LUCENE-3271, I moved the 'good' queries from the queries contrib to new destinations (primarily the queries module).  The remnants now need to find their home.  As suggested in LUCENE-3271, these classes are not bad per se, just odd.  So lets create a sandbox contrib that they and other 'odd' contrib classes can go to.  We can then decide their fate at another time.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-330",
        "summary": "[PATCH] Use filter bits for next() and skipTo() in FilteredQuery",
        "description": "This improves performance of FilteredQuery by not calling score() \non documents that do not pass the filter. \nThis passes the current tests for FilteredQuery, but these tests \nhave not been adapted/extended.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-580",
        "summary": "Pre-analyzed fields",
        "description": "Adds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue.\n\nThere might be some problems with mixing stored fields with the same name as a field with tokenStreamValue.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2315",
        "summary": "AttributeSource's methods for accessing attributes should be final, else its easy to corrupt the internal states",
        "description": "The methods that operate and modify the internal maps of AttributeSource should be final, which is a backwards break. But anybody that overrides such methods simply creates a buggy AS either case.\n\nI want to makeall impls final (in general the class should be final at all, but it is made for extension in TokenStream). So its important that the implementations are final!",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1830",
        "summary": "BoostingNearQuery doesn't have hashCode/equals",
        "description": "",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2050",
        "summary": "Improve contrib/benchmark for testing near-real-time search performance",
        "description": "It's not easy to test NRT performance right now w/ contrib/benchmark.\nI've made some initial fixes to improve this:\n\n  * Added new '&', that can follow any task within a serial sequence,\n    to \"background\" the task (just like a shell).  The test runs in\n    the BG, and then at the end of all serial tasks, any still running\n    BG tasks are stopped & joined.\n\n  * Added WaitTask that simply waits; useful for controlling how long\n    the BG'd tasks get to run.\n\n  * Added RollbackIndex task, which is real handy for using a given\n    index for an NRT test, doing a bunch of updates, then reverting it\n    all so your next run uses the same starting index\n\n  * Fixed the existing NearRealTimeReaderTask to simply periodically\n    open the new reader (previously it was also running a fixed\n    search), and removed its own threading (since & can do that\n    now). It periodically wakes up, opens the new reader, and swaps it\n    into the PerfRunData, at the schedule you specify.  I switched all\n    usage of PerfRunData's get/setIndexReader APIs to use ref\n    counting.\n\nWith these changes you can now make some very simple but powerful\nalgs, eg:\n\n{code}\nOpenIndex\n{\n  NearRealtimeReader(0.5) &\n  # Warm\n  Search\n  { \"Index1\" AddDoc > : * : 100/sec &\n  [ { \"Search\" Search > : * ] : 4 &\n  Wait(30.0)\n}\nCloseReader\nRollbackIndex\nRepSumByName\n{code}\n\nThis alg first opens the IndexWriter, then spawns the BG thread to\nreopen the NRT reader twice per second, does one warming Search (in\nthe FG), spans a new thread to index documents at the rate of 100 per\nsecond, then spawns 4 search threads that do as many searches as they\ncan.  We then wait for 30 seconds, then stop all the threads, revert\nthe index, and report.\n\nThe patch is a work in progress -- it generally works, but there're a\nfew nocommits, and, we may want to improve reporting (though I think\nthat's a separate issue).\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1790",
        "summary": "Add Boosting Function Term Query and Some Payload Query refactorings",
        "description": "Similar to the BoostingTermQuery, the BoostingFunctionTermQuery is a SpanTermQuery, but the difference is the payload score for a doc is not the average of all the payloads, but applies a function to them instead.  BoostingTermQuery becomes a BoostingFunctionTermQuery with an AveragePayloadFunction applied to it.\n\nAlso add marker interface to indicate PayloadQuery types.  Refactor Similarity.scorePayload to also take in the doc id.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1988",
        "summary": "CharacterCache - references deleted ",
        "description": "CharacterCache is deprecated by Character.valueOf(c) . Hence the latter is chosen over the former. ",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-635",
        "summary": "[PATCH] Decouple locking implementation from Directory implementation",
        "description": "This is a spinoff of http://issues.apache.org/jira/browse/LUCENE-305.\n\nI've opened this new issue to capture that it's wider scope than\nLUCENE-305.\n\nThis is a patch originally created by Jeff Patterson (see above link)\nand then modified as described here:\n\n  http://issues.apache.org/jira/browse/LUCENE-305#action_12418493\n\nwith some small additional changes:\n\n  * For each FSDirectory.getDirectory(), I made a corresponding\n    version that also accepts a LockFactory instance.  So, you can\n    construct an FSDirectory with your own LockFactory.\n\n  * Cascaded defaulting for FSDirectory's LockFactory implementation:\n    if you pass in a LockFactory instance, it's used; else if\n    setDisableLocks was called, we use NoLockFactory; else, if the\n    system property \"org.apache.lucene.store.FSDirectoryLockFactoryClass\"\n    is defined, we use that; finally, we'll use the original locking\n    implementation (SimpleFSLockFactory).\n\nThe gist is that all locking code has been moved out of *Directory and\ninto subclasses of a new abstract LockFactory class.  You can now set\nthe LockFactory of a Directory to change how it does locking.  For\nexample, you can create an FSDirectory but set its locking to\nSingleInstanceLockFactory (if you know all writing/reading will take\nplace a single JVM).\n\nThe changes pass all unit tests (on Ubuntu Linux Sun Java 1.5 and\nWindows XP Sun Java 1.4), and I added another TestCase to test the\nLockFactory code.\n\nNote that LockFactory defaults are not changed: FSDirectory defaults\nto SimpleFSLockFactory and RAMDirectory defaults to\nSingleInstanceLockFactory.\n\nNext step (separate issue) is to create a LockFactory that uses the OS\nnative locks (through java.nio).\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2615",
        "summary": "DirectIOLinuxDirectory hardwires buffer size and creates files with invalid permissions",
        "description": "TestDemo fails if I use the DirectIOLinuxDirectory (using Robert's new -Dtests.directory=XXX), because when it O_CREATs a file, it fails to specify the mode, so [depending on C stack!] you can get permission denied.\n\nAlso, we currently hardwire the buffer size to 1 MB (Mark found this)... I plan to add a \"forcedBufferSize\" to the DirectIOLinuxDir's ctor, to optionally override lucene's default buffer sizes (which are way too small for direct IO to get barely OK performance).  If you pass 0 for this then you get Lucene's default buffer sizes...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2682",
        "summary": "create test case to verify we support > 2.1B terms",
        "description": "I created a test case for this... I'm leaving it as @Ignore because it takes more than four hours on a faaast machine (beast) to run.  I think we should run this before each release.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2716",
        "summary": "Improve automaton's MinimizeOperations.minimizeHopcroft() to not create so many objects",
        "description": "MinimizeOperations.minimizeHopcroft() creates a lot of objects because of strange arrays and useless ArrayLists with fixed length. E.g. it created List<List<List<>>>. This patch minimizes this and makes the whole method much more GC friendler by using simple arrays or avoiding empty LinkedLists at all (inside reverse array). \n\nminimize() is called very very often, especially in tests (MockAnalyzer).\n\nA test for the method is prepared by Robert, we found a bug somewhere else in automaton, so this is pending until his issue and fix arrives.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-980",
        "summary": "Formatting error in ReportTask in contrib/benchmark",
        "description": "I am building a new Task, AnalyzerTask, that lets you change the Analyzer in the loop, thus allowing for the comparison of the same Analyzers over the set of documents.\n\nMy algorithm declaration looks like:\nNewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer)\n\nAnd it could be longer.\n\nThe exception is:\nError: cannot execute the algorithm! String index out of range: 85\njava.lang.StringIndexOutOfBoundsException: String index out of range: 85\n\tat java.lang.String.substring(String.java:1765)\n\tat org.apache.lucene.benchmark.byTask.utils.Format.format(Format.java:85)\n\tat org.apache.lucene.benchmark.byTask.tasks.ReportTask.tableTitle(ReportTask.java:85)\n\tat org.apache.lucene.benchmark.byTask.tasks.ReportTask.genPartialReport(ReportTask.java:140)\n\tat org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.reportSumByName(RepSumByNameTask.java:77)\n\tat org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.doLogic(RepSumByNameTask.java:39)\n\tat org.apache.lucene.benchmark.byTask.tasks.PerfTask.runAndMaybeStats(PerfTask.java:83)\n\tat org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doSerialTasks(TaskSequence.java:112)\n\tat org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doLogic(TaskSequence.java:93)\n\tat org.apache.lucene.benchmark.byTask.utils.Algorithm.execute(Algorithm.java:228)\n\tat org.apache.lucene.benchmark.byTask.Benchmark.execute(Benchmark.java:73)\n\tat org.apache.lucene.benchmark.byTask.Benchmark.main(Benchmark.java:109)\n\nThe error seems to be caused by the fact that ReportTask uses the OP (operation) column for the String, but then uses the length of the algorithm declaration to index into the String, resulting in the index out of bounds exception.\n\nThe line in question is:\nreturn (s + padd).substring(0, col.length());\n\nAnd probably should be changed to something like:\n    String s1 = (s + padd);\n    return s1.substring(0, Math.min(col.length(), s1.length()));\n\nEither that or the column should be trimmed.  The workaround is to explicitly name the task.\n\nIf no objections, I will make the change, tomorrow.  ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1813",
        "summary": "Add option to ReverseStringFilter to mark reversed tokens",
        "description": "This patch implements additional functionality in the filter to \"mark\" reversed tokens with a special marker character (Unicode 0001). This is useful when indexing both straight and reversed tokens (e.g. to implement efficient leading wildcards search).",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2255",
        "summary": "IndexWriter.getReader() allocates file handles",
        "description": "I am not sure if this is a \"bug\" or really just me not reading the Javadocs right...\n\nThe IR returned by IW.getReader() leaks file handles if you do not close() it, leading to starvation of the available file handles/process. If it was clear from the docs that this was a *new* reader and not some reference owned by the writer then this would probably be ok. But as I read the docs the reader is internally managed by the IW, which at first shot lead me to believe that I shouldn't close it.\n\nSo perhaps the docs should be amended to clearly state that this is a caller-owns reader that *must* be closed? Attaching a simple app that illustrates the problem.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1246",
        "summary": "Missing a null check in BooleanQuery.toString(String)",
        "description": "Our queryParser/tokenizer in some situations creates null query and was added as a clause to Boolean query.\nWhen we try to log the query, NPE is thrown from log(booleanQuery).\n\nIn BooleanQuery.toString(String), a simple null check is overlooked.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-518",
        "summary": "document field lengths count analyzer synonym overlays",
        "description": "Using a synonym expansion analyzer to add tokens with zero offset from the substituted token should not extend the length of the field in the document (for scoring purposes)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-621",
        "summary": "Default lock timeouts should have static setter/getters",
        "description": "\nWe recently stopped using Java system properties to derive defaults for things like the write/commit lock timeout, and switched to getter/setter's across all classes.  See here:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/27447\n\nBut, in the case at least of the write lock timeout, because it's marked \"public final static\", a consumer of this API can no longer change this value before instantiating the IndexWriter.  This is because the getter/setter for this is not static, which generally makes sense so you can change the timeout for each instance of IndexWriter.  But because IndexWriter on construction uses the timeout value, some uses cases need to change the value before getting an instance of IndexWriter.\n\nThis was actually a regression, in that Lucene users lost functionality they previously had, on upgrading.\n\nI would propose that that we add getter/setter for the default value of this timeout, which would be static.  I'll attach a patch file.\n\nSee this thread for context that led to this issue:\n\n   http://www.gossamer-threads.com/lists/lucene/java-dev/37421",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2633",
        "summary": "PackedInts does not support structures above 256MB",
        "description": "The PackedInts Packed32 and Packed64 fails when the internal structure exceeds 256MB. This is due to a missing cast that results in the bit position calculation being limited by Integer.MAX_VALUE (256MB * 8 = 2GB).",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3405",
        "summary": "Rename IOUtils.close methods",
        "description": "The closeSafely methods that take a boolean suppressExceptions are dangerous... I've renamed to .close (no suppression) and .closeWhileHandlingException (suppresses all exceptions).",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1578",
        "summary": "InstantiatedIndex supports non-optimized IndexReaders",
        "description": "InstantiatedIndex does not currently support non-optimized IndexReaders.  ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3496",
        "summary": "Support grouping by IndexDocValues",
        "description": "Although IDV is not yet finalized (More particular the SortedSource). I think we already can discuss / investigate implementing grouping by IDV.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1097",
        "summary": "IndexWriter.close(false) does not actually stop background merge threads",
        "description": "Right now when you close(false), IndexWriter marks any running merges\nas aborted but then does not wait for these merges to finish.  This\ncan cause problems because those threads still hold files open, so,\nsomeone might think they can call close(false) and then (say) delete\nall files from that directory, which would fail on Windows.\n\nInstead, close(false) should notify each running merge that it has\nbeen aborted, and not return until all running merges are done.  Then,\nSegmentMerger should periodically check whether it has been aborted\nand stop if so.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-506",
        "summary": "Optimize Memory Use for Short-Lived Indexes (Do not load TermInfoIndex if you know the queries ahead of time)",
        "description": "Summary: Provide a way to avoid loading the TermInfoIndex into memory if you know all the terms you are ever going to query.\n\nIn our search environment, we have a large number of indexes (many thousands), any of which may be queried by any number of hosts.  These indexes may be very large (~1M document), but since we have a low term/doc ratio, we have 7-11M terms.  With an index interval of 128, that means ~70-90K terms.  On loading the index, it instantiates a Term, a TermInfo, a String, and a char[].  When the document is long lived, this makes some sense because you can quickly search the list of terms using binary search.  However, since we throw away the Indexes very often, a lot of garbage is created per query\n\nHere's an example where we load a large index 10 times.  This corresponds to 7MB of garbage per query.\n          percent          live          alloc'ed  stack class\n rank   self  accum     bytes objs     bytes  objs trace name\n    1  4.48%  4.48%   4678736 128946  23393680 644730 387749 char[]\n    3  3.95% 12.61%   4126272 128946  20631360 644730 387751 org.apache.lucene.index.TermInfo\n    6  2.96% 22.71%   3094704 128946  15473520 644730 387748 java.lang.String\n    8  1.98% 26.97%   2063136 128946  10315680 644730 387750 org.apache.lucene.index.Term\n\nThis adds up after a while.  Since we know exactly which Terms we're going to search for before even opening the index, there's no need to allocate this much memory.  Upon opening the index, we can go through the TII in sequential order and retrieve the entries into the main term dictionary and reduce the storage requirements dramatically.  This reduces the amount of garbage generated by querying by about 60% if you only make 1 query/index with a 77% increase in throughput.\n\nThis is accomplished by factoring out the \"index loading\" aspects of TermInfosReader into a new file, SegmentTermInfosReader.  TermInfosReader becomes a base class to allow access to terms.  A new class, PrefetchedTermInfosReader will, upon startup, sort the passed in terms and retrieve the IndexEntries for those terms.  IndexReader and SegmentReader are modified to take new constructor methods that take a Collection of Terms that correspond to the total set of terms that will ever be searched in the life of the index.\n\nIn order to support the \"skipping\" behavior, some changes need to be made to SegmentTermEnum: specifically, we need to be able to go back an entry in order to retrieve the previous TermInfo and IndexPointer.  This is because, unlike the normal case, with the index  we want to return the value right before the intended field (so that we can be behind the desired termin the main dictionary).   For example, if we're looking for  \"apple\" in the index,  and the two adjacent values are \"abba\" and \"argon\", we want to return \"abba\" instead of \"argon\".  That way we won't miss any terms in the real index.   This code is confusing; it should probably be moved to an subclass of TermBuffer, but that required more code.  Not wanting to modify TermBuffer to keep it small, also lead to the odd NPE catch in SegmentTermEnum.java.  Stickler for contracts may want to rename SegmentTermEnum.skipTo() to a different name because it implements a different contract: but it would be useful for anyone trying to skip around in the TII, so I figured it was the right thing to do.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2928",
        "summary": "Improve LuceneTestCase javadocs",
        "description": "Now that the Lucene test-framework javadocs will be published, they should get some attention.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2212",
        "summary": "add a test for PorterStemFilter",
        "description": "There are no tests for PorterStemFilter, yet svn history reveals some (very minor) cleanups, etc.\nThe only thing executing its code in tests is a test or two in SmartChinese tests.\n\nThis patch runs the StemFilter against Martin Porter's test data set for this stemmer, checking for expected output.\n\nThe zip file is 100KB added to src/test, if this is too large I can change it to download the data instead.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1556",
        "summary": "some valid email address characters not correctly recognized",
        "description": "the EMAIL expression in StandardTokenizerImpl.jflex misses some unusual but valid characters in the left-hand-side of the email address. This causes an address to be broken into several tokens, for example:\n\nsomename+site@gmail.com gets broken into \"somename\" and \"site@gmail.com\"\nhusband&wife@talktalk.net gets broken into \"husband\" and \"wife@talktalk.net\"\n\nThese seem to be occurring more often. The first seems to be because of an anti-spam trick you can use with google (see: http://labnol.blogspot.com/2007/08/gmail-plus-smart-trick-to-find-block.html). I see the second in several domains but a disproportionate amount are from talktalk.net, so I expect it's a signup suggestion from the service.\n\nPerhaps a fix would be to change line 102 of StandardTokenizerImpl.jflex from:\nEMAIL      =  {ALPHANUM} ((\".\"|\"-\"|\"_\") {ALPHANUM})* \"@\" {ALPHANUM} ((\".\"|\"-\") {ALPHANUM})+\n\nto \n\nEMAIL      =  {ALPHANUM} ((\".\"|\"-\"|\"_\"|\"+\"|\"&\") {ALPHANUM})* \"@\" {ALPHANUM} ((\".\"|\"-\") {ALPHANUM})+\n\nI'm aware that the StandardTokenizer is meant to be more of a basic implementation rather than an implementation the full standard, but it is quite useful in places and hopefully this would improve it slightly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1208",
        "summary": "Deadlock case in IndexWriter on exception just before flush",
        "description": "If a document hits a non-aborting exception, eg something goes wrong\nin tokenStream.next(), and, that document had triggered a flush\n(due to RAM or doc count) then DocumentsWriter will deadlock because\nthat thread marks the flush as pending but fails to clear it on\nexception.\n\nI have a simple test case showing this, and a fix fixing it.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1662",
        "summary": "consolidate FieldCache and ExtendedFieldCache instances",
        "description": "It's confusing and error prone having two instances of FieldCache... FieldCache .DEFAULT and ExtendedFieldCache .EXT_DEFAULT.\nAccidentally use the wrong one and you silently double the memory usage for that field.  Since ExtendedFieldCache extends FieldCache, there's no reason not to share the same instance across both.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1673",
        "summary": "Move TrieRange to core",
        "description": "TrieRange was iterated many times and seems stable now (LUCENE-1470, LUCENE-1582, LUCENE-1602). There is lots of user interest, Solr added it to its default FieldTypes (SOLR-940) and if possible I want to move it to core before release of 2.9.\nBefore this can be done, there are some things to think about:\n# There are now classes called LongTrieRangeQuery, IntTrieRangeQuery, how should they be called in core? I would suggest to leave it as it is. On the other hand, if this keeps our only numeric query implementation, we could call it LongRangeQuery, IntRangeQuery or NumericRangeQuery (see below, here are problems). Same for the TokenStreams and Filters.\n# Maybe the pairs of classes for indexing and searching should be moved into one class: NumericTokenStream, NumericRangeQuery, NumericRangeFilter. The problem here: ctors must be able to pass int, long, double, float as range parameters. For the end user, mixing these 4 types in one class is hard to handle. If somebody forgets to add a L to a long, it suddenly instantiates a int version of range query, hitting no results and so on. Same with other types. Maybe accept java.lang.Number as parameter (because nullable for half-open bounds) and one enum for the type.\n# TrieUtils move into o.a.l.util? or document or?\n# Move TokenStreams into o.a.l.analysis, ShiftAttribute into o.a.l.analysis.tokenattributes? Somewhere else?\n# If we rename the classes, should Solr stay with Trie (because there are different impls)?\n# Maybe add a subclass of AbstractField, that automatically creates these TokenStreams and omits norms/tf per default for easier addition to Document instances?",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1465",
        "summary": "NearSpansOrdered.getPayload does not return the payload from the minimum match span",
        "description": "",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-837",
        "summary": "contrib/benchmark QueryMaker and Task Refactorings",
        "description": "Introduce an abstract QueryMaker implementation that shares much of the common code between the various QueryMaker implementations.\n\nAdd in a new QueryMaker for reading queries from a file that is specified in the properties.\n\nPatch shortly, and if no concerns, will commit tomorrow or Wed.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1791",
        "summary": "Enhance QueryUtils and CheckHIts to wrap everything they check in MultiReader/MultiSearcher",
        "description": "methods in CheckHits & QueryUtils are in a good position to take any Searcher they are given and not only test it, but also test MultiReader & MultiSearcher constructs built around them",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-794",
        "summary": "Extend contrib Highlighter to properly support PhraseQuery, SpanQuery,  ConstantScoreRangeQuery",
        "description": "This patch adds a new Scorer class (SpanQueryScorer) to the Highlighter package that scores just like QueryScorer, but scores a 0 for Terms that did not cause the Query hit. This gives 'actual' hit highlighting for the range of SpanQuerys, PhraseQuery, and  ConstantScoreRangeQuery. New Query types are easy to add. There is also a new Fragmenter that attempts to fragment without breaking up Spans.\n\nSee http://issues.apache.org/jira/browse/LUCENE-403 for some background.\n\nThere is a dependency on MemoryIndex.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2020",
        "summary": "Remove unused imports",
        "description": "With all of the churn recently, now seems like the opportune time to do some import cleanup",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3054",
        "summary": "SorterTemplate.quickSort stack overflows on broken comparators that produce only few disticnt values in large arrays",
        "description": "Looking at Otis's sort problem on the mailing list, he said:\n{noformat}\n* looked for other places where this call is made - found it in\nMultiPhraseQuery$MultiPhraseWeight and changed that call from\nArrayUtil.quickSort to ArrayUtil.mergeSort\n* now we no longer see SorterTemplate.quickSort in deep recursion when we do a\nthread dump\n{noformat}\n\nI thought this was interesting because PostingsAndFreq's comparator\nlooks like it needs a tiebreaker.\n\nI think in our sorts we should add some asserts to try to catch some of these broken comparators.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2494",
        "summary": "Modify ParallelMultiSearcher to use a CompletionService instead of slowly polling for results",
        "description": "Right now, the parallel multi searcher creates an array/list of Future<V> representing each of the searchables that's being concurrently searched (and its corresponding search task).\n\nAs it stands, once the tasks are all submitted to the executor, the array is iterated over, FIFO, and Future.get() is called iteratively.  This obviously works, but isn't ideal.  It's entirely possible (a situation I've run into) where one of the first searchables represents a large index that takes a long time to search, so the results of the other searchables can't be processed until the large index is done searching.  In my case, we have two indexes with several million records that get searched in front of some other indexes, the smallest of which has only a few ten thousand entries and I didn't think it was ideal for the results of the other indexes to wait.\n\nI've modified ParallelMultiSearcher to use CompletionServices instead, so that results are processed in the order they are completed, rather than the order that they are submitted.  All the tests still pass, and to the best of my knowledge this won't break anything.  This have several advantages:\n1) Speed - the thread owning the executor doesn't have to wait for the first submitted task to finish in order to process the results of the other tasks, which may have finished first\n2) Removed several warnings (even if they are annotated away) due to the ugliness of typecasting generic arrays.\n3) Decreased the complexity of the code in some cases, usually by removing the necessity of allocating and filling arrays.\n\nWith a primed \"cache\" of searchables, I was getting 700-1200 ms per search, and using the same phrases, with this patch, I am now getting 400-500ms per search :)\n\nPatch is attached.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-653",
        "summary": "GData-server storage fix activation depth",
        "description": "Fixed nullpointer exception while rendering feeds with big amount of extensions. DB4O context.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2561",
        "summary": "Fix exception handling and thread safety in realtime branch",
        "description": "Several tests are currently failing in the realtime branch - most of them due to thread safety problems (often exceptions in ConcurrentMergeScheduler) and in tests that test for aborting and non-aborting exceptions.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1583",
        "summary": "SpanOrQuery skipTo() doesn't always move forwards",
        "description": "In SpanOrQuery the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc, since skipTo() may not be called for any of the clauses' spans:\n\n    public boolean skipTo(int target) throws IOException {\n          if (queue == null) {\n            return initSpanQueue(target);\n          }\n\n          while (queue.size() != 0 && top().doc() < target) {\n            if (top().skipTo(target)) {\n              queue.adjustTop();\n            } else {\n              queue.pop();\n            }\n          }\n          \n        \treturn queue.size() != 0;\n        }\n\nThis violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:\n\n    public boolean skipTo(int target) throws IOException {\n          if (queue == null) {\n            return initSpanQueue(target);\n          }\n\n          boolean skipCalled = false;\n          while (queue.size() != 0 && top().doc() < target) {\n            if (top().skipTo(target)) {\n              queue.adjustTop();\n            } else {\n              queue.pop();\n            }\n            skipCalled = true;\n          }\n          \n          if (skipCalled) {\n        \treturn queue.size() != 0;\n          }\n          return next();\n        }",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-532",
        "summary": "[PATCH] Indexing on Hadoop distributed file system",
        "description": "In my current project we needed a way to create very large Lucene indexes on Hadoop distributed file system. When we tried to do it directly on DFS using Nutch FsDirectory class - we immediately found that indexing fails because DfsIndexOutput.seek() method throws UnsupportedOperationException. The reason for this behavior is clear - DFS does not support random updates and so seek() method can't be supported (at least not easily).\n \nWell, if we can't support random updates - the question is: do we really need them? Search in the Lucene code revealed 2 places which call IndexOutput.seek() method: one is in TermInfosWriter and another one in CompoundFileWriter. As we weren't planning to use CompoundFileWriter - the only place that concerned us was in TermInfosWriter.\n \nTermInfosWriter uses IndexOutput.seek() in its close() method to write total number of terms in the file back into the beginning of the file. It was very simple to change file format a little bit and write number of terms into last 8 bytes of the file instead of writing them into beginning of file. The only other place that should be fixed in order for this to work is in SegmentTermEnum constructor - to read this piece of information at position = file length - 8.\n \nWith this format hack - we were able to use FsDirectory to write index directly to DFS without any problems. Well - we still don't index directly to DFS for performance reasons, but at least we can build small local indexes and merge them into the main index on DFS without copying big main index back and forth. \n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1797",
        "summary": "new QueryParser over-increment position for MultiPhraseQuery",
        "description": "If the new QP is parsing a phrase, and when the analyzer runs on the text within the phrase it produces some tokens with posIncr=0, a MultiPhraseQuery is produced.  But, the positions of the added terms are over-incremented, and don't match what the current QueryParser does.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2303",
        "summary": "Remove code duplication from Token class, just extend TermAttributeImpl",
        "description": "This issue removes the code duplication from Token, as it shares the whole char[] buffer handling code with TermAttributeImpl. This issue removes this duplication by just extending TermAttributeImpl.\n\nWhen the parent issue LUCENE-2302 will extend TermAttribute to support CharSequence and Appendable and also the new BytesRefAttribute gets added, Token will automatically provide this too, so no further code duplication.\n\nThis code should also be committed to trunk, as it has nothing to do with flex.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": ""
    },
    {
        "key": "LUCENE-2907",
        "summary": "automaton termsenum bug when running with multithreaded search",
        "description": "This one popped in hudson (with a test that runs the same query against fieldcache, and with a filter rewrite, and compares results)\n\nHowever, its actually worse and unrelated to the fieldcache: you can set both to filter rewrite and it will still fail.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-813",
        "summary": "leading wildcard's don't work with trailing wildcard",
        "description": "As reported by Antony Bowesman, leading wildcards don't work when there is a trailing wildcard character -- instead a PrefixQuery is constructed.\n\n\nhttp://www.nabble.com/QueryParser-bug--tf3270956.html",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2966",
        "summary": "SegmentReader.doCommit should be sync'd; norms methods need not be sync'd",
        "description": "I fixed the failure in TestNRTThreads, but in the process tripped an assert because SegmentReader.doCommit isn't sync'd.\n\nSo I sync'd it, but I don't think the norms APIs need to be sync'd -- we populate norms up front and then never change them.  Un-sync'ing them is important so that in the NRT case calling IW.commit doesn't block searches trying to pull norms.\n\nAlso some small code refactoring.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2265",
        "summary": "improve automaton performance by running on byte[]",
        "description": "Currently, when enumerating terms, automaton must convert entire terms from flex's native utf-8 byte[] to char[] first, then step each char thru the state machine.\n\nwe can make this more efficient, by allowing the state machine to run on byte[], so it can return true/false faster.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1588",
        "summary": "Update Spatial Lucene sort to use FieldComparatorSource",
        "description": "Update distance sorting to use FieldComparator sorting as opposed to SortComparator",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3327",
        "summary": "TestFSTs.testRandomWords throws AIOBE when \"verbose\"=true",
        "description": "Seems like invalid utf-8 sometimes gets passed to Bytesref.utf8ToString() in the verbose \"println\"s.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-949",
        "summary": "AnalyzingQueryParser can't work with leading wildcards.",
        "description": "The getWildcardQuery mehtod in AnalyzingQueryParser.java need the following changes to accept leading wildcards:\n\n\tprotected Query getWildcardQuery(String field, String termStr) throws ParseException\n\t{\n\t\tString useTermStr = termStr;\n\t\tString leadingWildcard = null;\n\t\tif (\"*\".equals(field))\n\t\t{\n\t\t\tif (\"*\".equals(useTermStr))\n\t\t\t\treturn new MatchAllDocsQuery();\n\t\t}\n\t\tboolean hasLeadingWildcard = (useTermStr.startsWith(\"*\") || useTermStr.startsWith(\"?\")) ? true : false;\n\n\t\tif (!getAllowLeadingWildcard() && hasLeadingWildcard)\n\t\t\tthrow new ParseException(\"'*' or '?' not allowed as first character in WildcardQuery\");\n\n\t\tif (getLowercaseExpandedTerms())\n\t\t{\n\t\t\tuseTermStr = useTermStr.toLowerCase();\n\t\t}\n\n\t\tif (hasLeadingWildcard)\n\t\t{\n\t\t\tleadingWildcard = useTermStr.substring(0, 1);\n\t\t\tuseTermStr = useTermStr.substring(1);\n\t\t}\n\n\t\tList tlist = new ArrayList();\n\t\tList wlist = new ArrayList();\n\t\t/*\n\t\t * somewhat a hack: find/store wildcard chars in order to put them back\n\t\t * after analyzing\n\t\t */\n\t\tboolean isWithinToken = (!useTermStr.startsWith(\"?\") && !useTermStr.startsWith(\"*\"));\n\t\tisWithinToken = true;\n\t\tStringBuffer tmpBuffer = new StringBuffer();\n\t\tchar[] chars = useTermStr.toCharArray();\n\t\tfor (int i = 0; i < useTermStr.length(); i++)\n\t\t{\n\t\t\tif (chars[i] == '?' || chars[i] == '*')\n\t\t\t{\n\t\t\t\tif (isWithinToken)\n\t\t\t\t{\n\t\t\t\t\ttlist.add(tmpBuffer.toString());\n\t\t\t\t\ttmpBuffer.setLength(0);\n\t\t\t\t}\n\t\t\t\tisWithinToken = false;\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tif (!isWithinToken)\n\t\t\t\t{\n\t\t\t\t\twlist.add(tmpBuffer.toString());\n\t\t\t\t\ttmpBuffer.setLength(0);\n\t\t\t\t}\n\t\t\t\tisWithinToken = true;\n\t\t\t}\n\t\t\ttmpBuffer.append(chars[i]);\n\t\t}\n\t\tif (isWithinToken)\n\t\t{\n\t\t\ttlist.add(tmpBuffer.toString());\n\t\t}\n\t\telse\n\t\t{\n\t\t\twlist.add(tmpBuffer.toString());\n\t\t}\n\n\t\t// get Analyzer from superclass and tokenize the term\n\t\tTokenStream source = getAnalyzer().tokenStream(field, new StringReader(useTermStr));\n\t\torg.apache.lucene.analysis.Token t;\n\n\t\tint countTokens = 0;\n\t\twhile (true)\n\t\t{\n\t\t\ttry\n\t\t\t{\n\t\t\t\tt = source.next();\n\t\t\t}\n\t\t\tcatch (IOException e)\n\t\t\t{\n\t\t\t\tt = null;\n\t\t\t}\n\t\t\tif (t == null)\n\t\t\t{\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!\"\".equals(t.termText()))\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttlist.set(countTokens++, t.termText());\n\t\t\t\t}\n\t\t\t\tcatch (IndexOutOfBoundsException ioobe)\n\t\t\t\t{\n\t\t\t\t\tcountTokens = -1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ttry\n\t\t{\n\t\t\tsource.close();\n\t\t}\n\t\tcatch (IOException e)\n\t\t{\n\t\t\t// ignore\n\t\t}\n\n\t\tif (countTokens != tlist.size())\n\t\t{\n\t\t\t/*\n\t\t\t * this means that the analyzer used either added or consumed\n\t\t\t * (common for a stemmer) tokens, and we can't build a WildcardQuery\n\t\t\t */\n\t\t\tthrow new ParseException(\"Cannot build WildcardQuery with analyzer \" + getAnalyzer().getClass()\n\t\t\t\t\t+ \" - tokens added or lost\");\n\t\t}\n\n\t\tif (tlist.size() == 0)\n\t\t{\n\t\t\treturn null;\n\t\t}\n\t\telse if (tlist.size() == 1)\n\t\t{\n\t\t\tif (wlist.size() == 1)\n\t\t\t{\n\t\t\t\t/*\n\t\t\t\t * if wlist contains one wildcard, it must be at the end,\n\t\t\t\t * because: 1) wildcards at 1st position of a term by\n\t\t\t\t * QueryParser where truncated 2) if wildcard was *not* in end,\n\t\t\t\t * there would be *two* or more tokens\n\t\t\t\t */\n\t\t\t\tStringBuffer sb = new StringBuffer();\n\t\t\t\tif (hasLeadingWildcard)\n\t\t\t\t{\n\t\t\t\t\t// adding leadingWildcard\n\t\t\t\t\tsb.append(leadingWildcard);\n\t\t\t\t}\n\t\t\t\tsb.append((String) tlist.get(0));\n\t\t\t\tsb.append(wlist.get(0).toString());\n\t\t\t\treturn super.getWildcardQuery(field, sb.toString());\n\t\t\t}\n\t\t\telse if (wlist.size() == 0 && hasLeadingWildcard)\n\t\t\t{\n\t\t\t\t/*\n\t\t\t\t * if wlist contains no wildcard, it must be at 1st position\n\t\t\t\t */\n\t\t\t\tStringBuffer sb = new StringBuffer();\n\t\t\t\tif (hasLeadingWildcard)\n\t\t\t\t{\n\t\t\t\t\t// adding leadingWildcard\n\t\t\t\t\tsb.append(leadingWildcard);\n\t\t\t\t}\n\t\t\t\tsb.append((String) tlist.get(0));\n\t\t\t\tsb.append(wlist.get(0).toString());\n\t\t\t\treturn super.getWildcardQuery(field, sb.toString());\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t/*\n\t\t\t\t * we should never get here! if so, this method was called with\n\t\t\t\t * a termStr containing no wildcard ...\n\t\t\t\t */\n\t\t\t\tthrow new IllegalArgumentException(\"getWildcardQuery called without wildcard\");\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\t/*\n\t\t\t * the term was tokenized, let's rebuild to one token with wildcards\n\t\t\t * put back in postion\n\t\t\t */\n\t\t\tStringBuffer sb = new StringBuffer();\n\t\t\tif (hasLeadingWildcard)\n\t\t\t{\n\t\t\t\t// adding leadingWildcard\n\t\t\t\tsb.append(leadingWildcard);\n\t\t\t}\n\t\t\tfor (int i = 0; i < tlist.size(); i++)\n\t\t\t{\n\t\t\t\tsb.append((String) tlist.get(i));\n\t\t\t\tif (wlist != null && wlist.size() > i)\n\t\t\t\t{\n\t\t\t\t\tsb.append((String) wlist.get(i));\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn super.getWildcardQuery(field, sb.toString());\n\t\t}\n\t}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1351",
        "summary": "Add some ligatures (ff, fi, fl, ft, st) to ISOLatin1AccentFilter",
        "description": "ISOLatin1AccentFilter removes common diacritics and some ligatures. This patch adds support for additional common ligatures: ff, fi, fl, ft, st.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2531",
        "summary": "FieldComparator.TermOrdValComparator compares by value unnecessarily",
        "description": "Digging on LUCENE-2504, I noticed that TermOrdValComparator's compareBottom method falls back on compare-by-value when it needn't.\n\nSpecifically, if we know the current bottom ord \"matches\" the current segment, we can skip the value comparison when the ords are the same (ie, return 0) because the ords are exactly comparable.\n\nThis is hurting string sort perf especially for optimized indices (and also unoptimized indices), and especially for highly redundant (not many unique values) fields.  This affects all releases >= 2.9.x, but trunk is likely more severely affected since looking up a value is more costly.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3121",
        "summary": "FST should offer lookup-by-output API when output strictly increases",
        "description": "Spinoff from \"FST and FieldCache\" java-dev thread http://lucene.markmail.org/thread/swoawlv3fq4dntvl\n\nFST is able to associate arbitrary outputs with the sorted input keys, but in the special (and, common) case where the function is strictly monotonic (each output only \"increases\" vs prior outputs), such as mapping to term ords or mapping to file offsets in the terms dict, we should offer a lookup-by-output API that efficiently walks the FST and locates input key (exact or floor or ceil) matching that output.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2208",
        "summary": "Token div exceeds length of provided text sized 4114",
        "description": "I have a doc which contains html codes. I want to strip html tags and make the test clear after then apply highlighter on the clear text . But highlighter throws an exceptions if I strip out the html characters  , if i don't strip out , it works fine. It just confuses me at the moment \n\nI copy paste 3 thing here from the console as it may contain special characters which might cause the problem.\n\n\n1 -) Here is the html text \n\n          <h2>Starter</h2>\n          <div id=\"tab1-content\" class=\"tabContent selected\">\n            <div class=\"head\"></div>\n            <div class=\"body\">\n             <div class=\"subject-header\">Learning path: History</div>\n              <h3>Key question</h3>\n              <p>Did transport fuel the industrial revolution?</p>\n              <h3>Learning Objective</h3>\n\t      <ul>\n              <li>To categorise points as for or against an argument</li>\n              </ul>\n\t      <p>\n              <h3>What to do?</h3>\n              <ul>\n                <li>Watch the clip: <em>Transport fuelled the industrial revolution.</em></li>\n              </ul>\n              <p>The clips claims that transport fuelled the industrial revolution. Some historians argue that the industrial revolution only happened because of developments in transport.</p>\n\t\t\t  <ul>\n\t\t\t  \t<li>Read the statements below and decide which points are <em>for</em> and which points are <em>against</em> the argument that industry expanded in the 18th and 19th centuries because of developments in transport.</li>\n\t\t\t</ul>\n\t\t\t\n\t\t\t<ol type=\"a\">\n\t\t\t\t<li>Industry expanded because of inventions and the discovery of steam power.</li>\n\t\t\t\t<li>Improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for.</li>\n\t\t\t\t<li>Developments in transport allowed resources, such as coal from mines and cotton from America to come together to manufacture products.</li>\n\t\t\t\t<li>Transport only developed because industry needed it. It was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry.</li>\n\t\t\t</ol>\n\t\t\t\n\t\t\t<p>Now try to think of 2 more statements of your own.</p>\n\t\t\t\n            </div>\n            <div class=\"foot\"></div>\n          </div>\n          <h2>Main activity</h2>\n          <div id=\"tab2-content\" class=\"tabContent\">\n            <div class=\"head\"></div>\n            <div class=\"body\"><div class=\"subject-header\">Learning path: History</div>\n              <h3>Learning Objective</h3>\n              <ul>\n                <li>To select evidence to support points</li>\n              </ul>\n              <h3>What to do?</h3>\n              <!--<ul>\n                <li>Watch the clip: <em>Windmill and water mill</em></li>\n              </ul>-->\n              <ul><li>Choose the 4 points that you think are most important - try to be balanced by having two <strong>for</strong> and two <strong>against</strong>.</li>\n\t\t\t  <li>Write one in each of the point boxes of the paragraphs on the sheet <a href=\"lp_history_industry_transport_ws1.html\" class=\"link-internal\">Constructing a balanced argument</a>.</li></ul> <p>You might like to re write the points in your own words and use connectives to link the paragraphs.</p>\n              \n\t\t\t  <p>In history and in any argument, you need evidence to support your points.</p>\n\t\t\t  <ul><li>Find evidence from these sources and from your own knowledge to support each of your points:</li></ul>\n\t\t\t  <ol>\n                <li><a href=\"../servlet/link?template=vid&macro=setResource&resourceID=2044\" class=\"link-internal\">At a toll gate</a></li>\n                <li><a href=\"../servlet/link?macro=setResource&template=vid&resourceID=2046\" class=\"link-internal\">Canals</a></li>\n                <li><a href=\"../servlet/link?macro=setResource&template=vid&resourceID=2043\" class=\"link-internal\">Growing cities: traffic</a></li>\n\t\t\t\t<li><a href=\"../servlet/link?macro=setResource&template=vid&resourceID=2047\" class=\"link-internal\">Impact of the railway</a> </li>\n\t\t\t\t<li><a href=\"../servlet/link?macro=setResource&template=vid&resourceID=2048\" class=\"link-internal\">Sailing ships</a> </li>\n\t\t\t\t<li><a href=\"../servlet/link?macro=setResource&template=vid&resourceID=2050\" class=\"link-internal\">Liverpool: Capital of Culture</a> </li>\n              </ol>\n\t\t\t  <p>Try to be specific in your evidence - use named examples of places or people. Use dates if you can.</p>\n            </div>\n            <div class=\"foot\"></div>\n          </div>\n          <h2>Plenary</h2>\n          <div id=\"tab3-content\" class=\"tabContent\">\n            <div class=\"head\"></div>\n            <div class=\"body\"><div class=\"subject-header\">Learning path: History</div>\n              <h3>Learning Objective</h3>\n              <ul>\n                <li>To judge which of the arguments is most valid</li>\n              </ul>\n              <h3>What to do?</h3>\n<!--              <ul>\n                <li>Watch the clip: <em>Food of the rich</em></li>\n              </ul>-->\n              <p>In order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. Having been through the evidence which point do you think is most important? Why? Is there more evidence? Is the evidence more convincing?</p>\n\t\t\t  <ul><li>In the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution.</li></ul>\n            </div>\n            <div class=\"foot\"></div>\n          </div>\n          <h2>Extension</h2>\n          <div id=\"tab4-content\" class=\"tabContent\">\n            <div class=\"head\"></div>\n            <div class=\"body\"><div class=\"subject-header\">Learning path: History</div>\n              <h3>What to do?</h3>\n              <p>Watch the clip <em>Stress in a ski resort</em></p>\n\t\t\t  <p>New industries, such as tourism, can now be said to be fuelled by transport improvements.</p>\n              <ul><li>Search Clipbank, using the Related clip lists as well as the search function, to find examples from around the world of how transport has helped industry.</li></ul>              \n            </div>\n            <div class=\"foot\"></div>\n          </div>\n          \n          \n2-) here is the text after stripped html tags  out \n\n           Starter \n           \n              \n             \n              Learning path: History \n               Key question \n               Did transport fuel the industrial revolution? \n               Learning Objective \n\t       \n               To categorise points as for or against an argument \n               \n\t       \n               What to do? \n               \n                 Watch the clip:  Transport fuelled the industrial revolution.  \n               \n               The clips claims that transport fuelled the industrial revolution. Some historians argue that the industrial revolution only happened because of developments in transport. \n\t\t\t   \n\t\t\t  \t Read the statements below and decide which points are  for  and which points are  against  the argument that industry expanded in the 18th and 19th centuries because of developments in transport. \n\t\t\t \n\t\t\t\n\t\t\t \n\t\t\t\t Industry expanded because of inventions and the discovery of steam power. \n\t\t\t\t Improvements in transport allowed goods to be sold all over the country and all over the world so there were more customers to develop industry for. \n\t\t\t\t Developments in transport allowed resources, such as coal from mines and cotton from America to come together to manufacture products. \n\t\t\t\t Transport only developed because industry needed it. It was slow to develop as money was spent on improving roads, then building canals and the replacing them with railways in order to keep up with industry. \n\t\t\t \n\t\t\t\n\t\t\t Now try to think of 2 more statements of your own. \n\t\t\t\n             \n              \n           \n           Main activity \n           \n              \n              Learning path: History \n               Learning Objective \n               \n                 To select evidence to support points \n               \n               What to do? \n               \n                Choose the 4 points that you think are most important - try to be balanced by having two  for  and two  against . \n\t\t\t   Write one in each of the point boxes of the paragraphs on the sheet  Constructing a balanced argument .    You might like to re write the points in your own words and use connectives to link the paragraphs. \n              \n\t\t\t   In history and in any argument, you need evidence to support your points. \n\t\t\t    Find evidence from these sources and from your own knowledge to support each of your points:  \n\t\t\t   \n                  At a toll gate  \n                  Canals  \n                  Growing cities: traffic  \n\t\t\t\t  Impact of the railway   \n\t\t\t\t  Sailing ships   \n\t\t\t\t  Liverpool: Capital of Culture   \n               \n\t\t\t   Try to be specific in your evidence - use named examples of places or people. Use dates if you can. \n             \n              \n           \n           Plenary \n           \n              \n              Learning path: History \n               Learning Objective \n               \n                 To judge which of the arguments is most valid \n               \n               What to do? \n \n               In order to be a good historian, and get good marks in exams, you need to show your evaluation skills and make a judgement. Having been through the evidence which point do you think is most important? Why? Is there more evidence? Is the evidence more convincing? \n\t\t\t    In the final box on your worksheet write a conclusion explaining whether on balance the evidence is enough to convince you that transport fuelled the industrial revolution.  \n             \n              \n           \n           Extension \n           \n              \n              Learning path: History \n               What to do? \n               Watch the clip  Stress in a ski resort  \n\t\t\t   New industries, such as tourism, can now be said to be fuelled by transport improvements. \n                Search Clipbank, using the Related clip lists as well as the search function, to find examples from around the world of how transport has helped industry.                \n             \n              \n           \n          \n         3-) here is the exception I get\n\norg.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token div exceeds length of provided text sized 4114\n\tat org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:228)\n\tat org.apache.lucene.search.highlight.Highlighter.getBestFragments(Highlighter.java:158)\n\tat org.apache.lucene.search.highlight.Highlighter.getBestFragments(Highlighter.java:462)\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1077",
        "summary": "New Analysis  Contributions",
        "description": "With the advent of the new TeeTokenFilter and SinkTokenizer, there now exists some interesting new things that can be done in the analysis phase of indexing.  See LUCENE-1058.\n\nThis patch provides some new implementations of SinkTokenizer that may be useful.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1522",
        "summary": "another highlighter",
        "description": "I've written this highlighter for my project to support bi-gram token stream (general token stream (e.g. WhitespaceTokenizer) also supported. see test code in patch). The idea was inherited from my previous project with my colleague and LUCENE-644. This approach needs highlight fields to be TermVector.WITH_POSITIONS_OFFSETS, but is fast and can support N-grams. This depends on LUCENE-1448 to get refined term offsets.\n\nusage:\n{code:java}\nTopDocs docs = searcher.search( query, 10 );\nHighlighter h = new Highlighter();\nFieldQuery fq = h.getFieldQuery( query );\nfor( ScoreDoc scoreDoc : docs.scoreDocs ){\n  // fieldName=\"content\", fragCharSize=100, numFragments=3\n  String[] fragments = h.getBestFragments( fq, reader, scoreDoc.doc, \"content\", 100, 3 );\n  if( fragments != null ){\n    for( String fragment : fragments )\n      System.out.println( fragment );\n  }\n}\n{code}\n\nfeatures:\n- fast for large docs\n- supports not only whitespace-based token stream, but also \"fixed size\" N-gram (e.g. (2,2), not (1,3)) (can solve LUCENE-1489)\n- supports PhraseQuery, phrase-unit highlighting with slops\n{noformat}\nq=\"w1 w2\"\n<b>w1 w2</b>\n---------------\nq=\"w1 w2\"~1\n<b>w1</b> w3 <b>w2</b> w3 <b>w1 w2</b>\n{noformat}\n- highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS\n- easy to apply patch due to independent package (contrib/highlighter2)\n- uses Java 1.5\n- looks query boost to score fragments (currently doesn't see idf, but it should be possible)\n- pluggable FragListBuilder\n- pluggable FragmentsBuilder\n\nto do:\n- term positions can be unnecessary when phraseHighlight==false\n- collects performance numbers\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3374",
        "summary": "move nrtcachingdir to core in 4.0",
        "description": "in 4.0 with the IOContext changes this implementation is clean and I think we should move it to core and use it in our tests etc.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1468",
        "summary": "FSDirectory.list() is inconsistent",
        "description": "LUCENE-638 added a check to the FSDirectory.list() method to only return files that are Lucene related. I think this change made the FSDirectory implementation inconsistent with all other methods in Directory. E.g. you can create a file with an arbitrary name using FSDirectory, fileExists() will report that it is there, deleteFile() will remove it, but the array returned by list() will not contain the file.\n\nThe actual issue that was reported in LUCENE-638 was about sub directories. Those should clearly not be listed, but IMO it is not the responsibility of a Directory implementation to decide what kind of files can be created or listed. The Directory class is an abstraction of a directory and it should't to more than that.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1970",
        "summary": "Remove deprecated DocIdSetIterator methods",
        "description": "",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3483",
        "summary": "Move Function grouping collectors from Solr to grouping module",
        "description": "Move the Function*Collectors from Solr (inside Grouping source file) to grouping module.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1194",
        "summary": "Add deleteByQuery to IndexWriter",
        "description": "This has been discussed several times recently:\n\n  http://markmail.org/message/awlt4lmk3533epbe\n  http://www.gossamer-threads.com/lists/lucene/java-user/57384#57384\n\nIf we add deleteByQuery to IndexWriter then this is a big step towards\nallowing IndexReader to be readonly.\n\nI took the approach suggested in that first thread: I buffer delete\nqueries just like we now buffer delete terms, holding the max docID\nthat the delete should apply to.\n\nThen, I also decoupled flushing deletes (mapping term or query -->\nactual docIDs that need deleting) from flushing added documents, and\nnow I flush deletes only when a merge is started, or on commit() or\nclose().  SegmentMerger now exports the docID map it used when\nmerging, and I use that to renumber the max docIDs of all pending\ndeletes.\n\nFinally, I turned off tracking of memory usage of pending deletes\nsince they now live beyond each flush.  Deletes are now only\nexplicitly flushed if you set maxBufferedDeleteTerms to something\nother than DISABLE_AUTO_FLUSH.  Otherwise they are flushed at the\nstart of every merge.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3326",
        "summary": "MoreLikeThis reuses a reader after it has already closed it",
        "description": "MoreLikeThis has a fatal bug whereby it tries to reuse a reader for multiple fields:\n\n{code}\n    Map<String,Int> words = new HashMap<String,Int>();\n    for (int i = 0; i < fieldNames.length; i++) {\n        String fieldName = fieldNames[i];\n        addTermFrequencies(r, words, fieldName);\n    }\n{code}\n\nHowever, addTermFrequencies() is creating a TokenStream for this reader:\n\n{code}\n    TokenStream ts = analyzer.reusableTokenStream(fieldName, r);\n    int tokenCount=0;\n    // for every token\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    ts.reset();\n    while (ts.incrementToken()) {\n        /* body omitted */\n    }\n    ts.end();\n    ts.close();\n{code}\n\nWhen it closes this analyser, it closes the underlying reader.  Then the second time around the loop, you get:\n\n{noformat}\nCaused by: java.io.IOException: Stream closed\n\tat sun.nio.cs.StreamDecoder.ensureOpen(StreamDecoder.java:27)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:128)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:167)\n\tat com.acme.util.CompositeReader.read(CompositeReader.java:101)\n\tat org.apache.lucene.analysis.standard.StandardTokenizerImpl.zzRefill(StandardTokenizerImpl.java:803)\n\tat org.apache.lucene.analysis.standard.StandardTokenizerImpl.getNextToken(StandardTokenizerImpl.java:1010)\n\tat org.apache.lucene.analysis.standard.StandardTokenizer.incrementToken(StandardTokenizer.java:178)\n\tat org.apache.lucene.analysis.standard.StandardFilter.incrementTokenClassic(StandardFilter.java:61)\n\tat org.apache.lucene.analysis.standard.StandardFilter.incrementToken(StandardFilter.java:57)\n\tat com.acme.storage.index.analyser.NormaliseFilter.incrementToken(NormaliseFilter.java:51)\n\tat org.apache.lucene.analysis.LowerCaseFilter.incrementToken(LowerCaseFilter.java:60)\n\tat org.apache.lucene.search.similar.MoreLikeThis.addTermFrequencies(MoreLikeThis.java:931)\n\tat org.apache.lucene.search.similar.MoreLikeThis.retrieveTerms(MoreLikeThis.java:1003)\n\tat org.apache.lucene.search.similar.MoreLikeThis.retrieveInterestingTerms(MoreLikeThis.java:1036)\n{noformat}\n\nMy first thought was that it seems like a \"ReaderFactory\" of sorts should be passed in so that a new Reader can be created for the second field (maybe the factory could be passed the field name, so that if someone wanted to pass a different reader to each, they could.)\n\nInterestingly, the methods taking File and URL exhibit the same issue.  I'm not sure what to do about those (and we're not using them.)  The method taking File could open the file twice, but the method taking a URL probably shouldn't fetch the same URL twice.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3090",
        "summary": "DWFlushControl does not take active DWPT out of the loop on fullFlush",
        "description": "We have seen several OOM on TestNRTThreads and all of them are caused by DWFlushControl missing DWPT that are set as flushPending but can't full due to a full flush going on. Yet that means that those DWPT are filling up in the background while they should actually be checked out and blocked until the full flush finishes. Even further we currently stall on the maxNumThreadStates while we should stall on the num of active thread states. I will attach a patch tomorrow.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1283",
        "summary": "Factor out ByteSliceWriter from DocumentsWriterFieldData",
        "description": "DocumentsWriter uses byte slices into shared byte[]'s to hold the\ngrowing postings data for many different terms in memory.  This is\nprobably the trickiest (most confusing) part of DocumentsWriter.\n\nRight now it's not cleanly factored out and not easy to separately\ntest.  In working on this issue:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c126142c0805061426n1168421ya5594ef854fae5e4@mail.gmail.com%3e\n\nwhich eventually turned out to be a bug in Oracle JRE's JIT compiler,\nI factored out ByteSliceWriter and created a unit test to stress test\nthe writing & reading of byte slices.  The test just randomly writes N\nstreams interleaved into shared byte[]'s, then reads them back\nverifying the results are correct.\n\nI created the stress test to try to find any bugs in that code.  The\ntest ran fine (no bugs were found) but I think the refactoring is\nstill very much worthwhile.\n\nI expected the changes to reduce indexing throughput, so I ran a test\nindexing first 200K Wikipedia docs using this alg:\n\n{code}\nanalyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\ndoc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n\ndocs.file=/Volumes/External/lucene/wiki.txt\ndoc.stored = true\ndoc.term.vector = true\ndoc.add.log.step=2000\n\ndirectory=FSDirectory\nautocommit=false\ncompound=true\n\nram.flush.mb=256\n\n{ \"Rounds\"\n  ResetSystemErase\n  { \"BuildIndex\"\n    - CreateIndex\n     { \"AddDocs\" AddDoc > : 200000\n    - CloseIndex\n  }\n  NewRound\n} : 4\n\nRepSumByPrefRound BuildIndex\n\n{code}\n\nOk trunk it produces these results:\n{code}\nOperation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\nBuildIndex      0        1       200000        791.7      252.63   338,552,096  1,061,814,272\nBuildIndex -  - 1 -  -   1 -  -  200000 -  -   793.1 -  - 252.18 - 605,262,080  1,061,814,272\nBuildIndex      2        1       200000        794.8      251.63   601,966,528  1,061,814,272\nBuildIndex -  - 3 -  -   1 -  -  200000 -  -   782.5 -  - 255.58 - 608,699,712  1,061,814,272\n{code}\n\nand with the patch:\n\n{code}\nOperation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\nBuildIndex      0        1       200000        745.0      268.47   338,318,784  1,061,814,272\nBuildIndex -  - 1 -  -   1 -  -  200000 -  -   792.7 -  - 252.30 - 605,331,776  1,061,814,272\nBuildIndex      2        1       200000        786.7      254.24   602,915,712  1,061,814,272\nBuildIndex -  - 3 -  -   1 -  -  200000 -  -   795.3 -  - 251.48 - 602,378,624  1,061,814,272\n{code}\n\nSo it looks like the performance cost of this change is negligible (in\nthe noise).\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-723",
        "summary": "QueryParser support for MatchAllDocs",
        "description": "It seems like there really should be QueryParser support for MatchAllDocsQuery.\nI propose *:* (brings back memories of DOS :-)\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1788",
        "summary": "Cleanup highlighter test class",
        "description": "cleanup highlighter test class - did some of this in another issue, but there is a bit more to do",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1406",
        "summary": "new Arabic Analyzer (Apache license)",
        "description": "I've noticed there is no Arabic analyzer for Lucene, most likely because Tim Buckwalter's morphological dictionary is GPL.\n\nHowever, it is not necessary  to have full morphological analysis engine for a quality arabic search. \nThis implementation implements the light-8s algorithm present in the following paper: http://ciir.cs.umass.edu/pubfiles/ir-249.pdf\n\nAs you can see from the paper, improvement via this method over searching surface forms (as lucene currently does) is significant, with almost 100% improvement in average precision.\n\nWhile I personally don't think all the choices were the best, and some easily improvements are still possible, the major motivation for implementing it exactly the way it is presented in the paper is that the algorithm is TREC-tested, so the precision/recall improvements to lucene are already documented.\n\nFor a stopword list, I used a list present at http://members.unine.ch/jacques.savoy/clef/index.html simply because the creator of this list documents the data as BSD-licensed.\n\nThis implementation (Analyzer) consists of above mentioned stopword list plus two filters:\n ArabicNormalizationFilter: performs orthographic normalization (such as hamza seated on alif, alif maksura, teh marbuta, removal of harakat, tatweel, etc)\n ArabicStemFilter: performs arabic light stemming\n\nBoth filters operate directly on termbuffer for maximum performance. There is no object creation in this Analyzer.\n\nThere are no external dependencies. I've indexed about half a billion words of arabic text and tested against that.\n\nIf there are any issues with this implementation I am willing to fix them. I use lucene on a daily basis and would like to give something back. Thanks.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-973",
        "summary": "Token of  \"\" returns in CJKTokenizer + new TestCJKTokenizer",
        "description": "The \"\" string returns as Token in the boundary of two byte character and one byte character. \n\nThere is no problem in CJKAnalyzer. \nWhen CJKTokenizer is used with the unit, it becomes a problem. (Use it with \nSolr etc.)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3871",
        "summary": "Stack traces from failed tests are messed up on ANT 1.7.x",
        "description": "",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2088",
        "summary": "AttributeSource.addAttribute should only accept interfaces, the missing test leads to problems with Token.TOKEN_ATTRIBUTE_FACTORY",
        "description": "This is a blocker, because you can call addAttribute(Token.class) without getting an error message.\n\nI will commit the fix and restart the vote for 3.0. This also applies to 2.9, but there is no Token Attribute Factory. But I will merge to 2.9, too, if a 2.9.2 comes.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2223",
        "summary": "ShingleFilter benchmark",
        "description": "Spawned from LUCENE-2218: a benchmark for ShingleFilter, along with a new task to instantiate (non-default-constructor) ShingleAnalyzerWrapper: NewShingleAnalyzerTask.\n\nThe included shingle.alg runs ShingleAnalyzerWrapper, wrapping the default StandardAnalyzer, with 4 different configurations over 10,000 Reuters documents each.  To allow ShingleFilter timings to be isolated from the rest of the pipeline, StandardAnalyzer is also run over the same set of Reuters documents.  This set of 5 runs is then run 5 times.\n\nThe patch includes two perl scripts, the first to output JIRA table formatted timing information, with the minimum elapsed time for each of the 4 ShingleAnalyzerWrapper runs and the StandardAnalyzer run, and the second to compare two runs' JIRA output, producing another JIRA table showing % improvement.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3827",
        "summary": "Make term offsets work in MemoryIndex",
        "description": "Fix the logic for retrieving term offsets from DocsAndPositionsEnum on a MemoryIndex, and allow subclasses to access them.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3488",
        "summary": "Factor out SearcherManager from NRTManager",
        "description": "Currently we have NRTManager and SearcherManager while NRTManager contains a big piece of the code that is already in SearcherManager. Users are kind of forced to use NRTManager if they want to have SearcherManager goodness with NRT. The integration into NRTManager also forces you to maintain two instances even if you know you always want deletes. To me NRTManager tries to do more than necessary and mixes lots of responsibilities ie. handling searchers and handling indexing generations. NRTManager should use a SearcherManager by aggregation rather than duplicate a lot of logic. SearcherManager should have a NRT and Directory based implementation users can simply choose from.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1714",
        "summary": "WriteLineDocTask incorrectly normalizes fields",
        "description": "WriteLineDocTask normalizes the body, title and date fields by replacing any \"\\t\" with a space. However, if any one of them contains newlines, LineDocMaker will fail, since the first line read will include some of the text, however the second line, which it now expects to be a new document, will include other parts of the text.\n\nI don't know how we didn't hit it so far. Maybe the wikipedia text doesn't have such lines, however when I ran over the TREC collection I hit a lot of those.\n\nI will attach a patch shortly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-374",
        "summary": "You cannot sort on fields that don't exist",
        "description": "While it's possible to search for fields that don't exist (you'll get 0 hits),  \nyou'll get an exception if you try to sort by a field that has no values. The  \nexception is this:  \n  \nif (termEnum.term() == null) {  \n  throw new RuntimeException (\"no terms in field \" + field);  \n}  \n  \nI'll attach a change suggested by Yonik Seeley that removes this exception. \n \nAlso, the if-condition above is incomplete anyway, so currently the exception \nis not always thrown (as termEnum .term() might well be != null but point to a \nterm in a different field already)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-332",
        "summary": "nightly build/javadocs for sandbox",
        "description": "This isn't something i think is crucial, but since i've been on the lucene-users\nmailing list (less then 2 months) I've seen several people post questions asking\nwhere they can find documentation on some module available in the sandbox.\n\nthe answer of course is usually that they should download the source and build\nthe javadocs themselves, but since it keeps coming up, I figured i'd suggest\nsetting up a nightly \"build\" of the whole sandbox, including the javadocs.\n\nif nothing else, it will cut down on the number of questions -- but i think it\nmay also have an added benefit to the size of the user base.  In my experience,\npeople tend to be more willing to download/install something and try it out\nafter they've read the docs online.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3644",
        "summary": "problems with IR's readerFinishedListener",
        "description": "There are two major problems:\n1. The listener api does not really apply all indexreaders. for example segmentreaders dont fire it on close, only segmentcorereaders. this is wrong, a segmentcorereader is *not* an indexreader. Furthermore, if you register it on a top-level reader you get events for anything under the reader tree (sometimes, unless they are segmentreaders as mentioned above, where it doesnt work correctly at all).\n2. Furthermore your listener is 'passed along' in a viral fashion from clone() and reopen(). This means for example, if you are trying to listen to readers in NRT search you are just accumulating reader listeners, all potentially keeping references to old indexreaders (because, in order to deal with #1 your listener must 'keep' a reference to the IR it was registered on, so it can check if thats *really* the one).\n\nWe should discuss how to fix #1. \n\nI will create a patch for #2 shortly and commit it, its just plain wrong.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1855",
        "summary": "Change AttributeSource API to use generics",
        "description": "The AttributeSource API will be easier to use with JDK 1.5 generics.\n\nUwe, if you started working on a patch for this already feel free to assign this to you.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1353",
        "summary": "javacc ant task for contrib/misc precedence query parser",
        "description": "Add a javacc task in contrib/misc for the precedence query parser.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3687",
        "summary": "Allow similarity to encode norms other than a single byte",
        "description": "LUCENE-3628 cut over norms to docvalues. This removes the long standing limitation that norms are a single byte. Yet, we still need to expose this functionality to Similarity to write / encode norms in a different format. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3051",
        "summary": "don't call SegmentInfo.sizeInBytes for the merging segments",
        "description": "Selckin has been running Lucene's tests on the RT branch, and hit this:\n{noformat}\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter\n    [junit] Testcase: testDeleteAllSlowly(org.apache.lucene.index.TestIndexWriter):\tFAILED\n    [junit] Some threads threw uncaught exceptions!\n    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:535)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)\n    [junit] \n    [junit] \n    [junit] Tests run: 67, Failures: 1, Errors: 0, Time elapsed: 38.357 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testDeleteAllSlowly -Dtests.seed=-4291771462012978364:4550117847390778918\n    [junit] The following exceptions were thrown by threads:\n    [junit] *** Thread: Lucene Merge Thread #1 ***\n    [junit] org.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: _4_1.del\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)\n    [junit] Caused by: java.io.FileNotFoundException: _4_1.del\n    [junit] \tat org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:290)\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:549)\n    [junit] \tat org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:287)\n    [junit] \tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3280)\n    [junit] \tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2956)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=Pulsing(freqCutoff=15), f7=MockFixedIntBlock(blockSize=1606), f8=SimpleText, f9=MockSep, f1=MockVariableIntBlock(baseBlockSize=99), f0=MockFixedIntBlock(blockSize=1606), f3=Pulsing(freqCutoff=15), f2=MockSep, f5=SimpleText, f4=Standard, f=MockFixedIntBlock(blockSize=1606), c=MockSep, termVector=MockRandom, d9=MockFixedIntBlock(blockSize=1606), d8=Pulsing(freqCutoff=15), d5=SimpleText, d4=Standard, d7=MockRandom, d6=MockVariableIntBlock(baseBlockSize=99), d25=MockRandom, d0=MockRandom, c29=MockFixedIntBlock(blockSize=1606), d24=MockVariableIntBlock(baseBlockSize=99), d1=Standard, c28=Standard, d23=SimpleText, d2=MockFixedIntBlock(blockSize=1606), c27=MockRandom, d22=Standard, d3=MockVariableIntBlock(baseBlockSize=99), d21=Pulsing(freqCutoff=15), d20=MockSep, c22=MockFixedIntBlock(blockSize=1606), c21=Pulsing(freqCutoff=15), c20=MockRandom, d29=MockFixedIntBlock(blockSize=1606), c26=Standard, d28=Pulsing(freqCutoff=15), c25=MockRandom, d27=MockRandom, c24=MockSep, d26=MockVariableIntBlock(baseBlockSize=99), c23=SimpleText, e9=MockRandom, e8=MockSep, e7=SimpleText, e6=MockFixedIntBlock(blockSize=1606), e5=Pulsing(freqCutoff=15), c17=MockFixedIntBlock(blockSize=1606), e3=Standard, d12=MockVariableIntBlock(baseBlockSize=99), c16=Pulsing(freqCutoff=15), e4=SimpleText, d11=MockFixedIntBlock(blockSize=1606), c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=SimpleText, e2=Pulsing(freqCutoff=15), d13=MockSep, e0=MockVariableIntBlock(baseBlockSize=99), d10=Standard, d19=MockVariableIntBlock(baseBlockSize=99), c11=SimpleText, c10=Standard, d16=Pulsing(freqCutoff=15), c13=MockRandom, c12=MockVariableIntBlock(baseBlockSize=99), d15=MockSep, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1606), d17=Standard, c14=Pulsing(freqCutoff=15), b3=MockSep, b2=SimpleText, b5=Standard, b4=MockRandom, b7=MockVariableIntBlock(baseBlockSize=99), b6=MockFixedIntBlock(blockSize=1606), d50=MockFixedIntBlock(blockSize=1606), b9=Pulsing(freqCutoff=15), b8=MockSep, d43=MockSep, d42=SimpleText, d41=MockFixedIntBlock(blockSize=1606), d40=Pulsing(freqCutoff=15), d47=MockVariableIntBlock(baseBlockSize=99), d46=MockFixedIntBlock(blockSize=1606), b0=MockVariableIntBlock(baseBlockSize=99), d45=Standard, b1=MockRandom, d44=MockRandom, d49=MockVariableIntBlock(baseBlockSize=99), d48=MockFixedIntBlock(blockSize=1606), c6=Pulsing(freqCutoff=15), c5=MockSep, c4=MockVariableIntBlock(baseBlockSize=99), c3=MockFixedIntBlock(blockSize=1606), c9=MockVariableIntBlock(baseBlockSize=99), c8=SimpleText, c7=Standard, d30=SimpleText, d32=MockRandom, d31=MockVariableIntBlock(baseBlockSize=99), c1=SimpleText, d34=MockFixedIntBlock(blockSize=1606), c2=MockSep, d33=Pulsing(freqCutoff=15), d36=MockSep, c0=MockFixedIntBlock(blockSize=1606), d35=SimpleText, d38=MockSep, d37=SimpleText, d39=MockRandom, e92=MockFixedIntBlock(blockSize=1606), e93=MockVariableIntBlock(baseBlockSize=99), e90=MockRandom, e91=Standard, e89=MockVariableIntBlock(baseBlockSize=99), e88=SimpleText, e87=Standard, e86=Pulsing(freqCutoff=15), e85=MockSep, e84=MockVariableIntBlock(baseBlockSize=99), e83=MockFixedIntBlock(blockSize=1606), e80=MockFixedIntBlock(blockSize=1606), e81=SimpleText, e82=MockSep, e77=MockVariableIntBlock(baseBlockSize=99), e76=MockFixedIntBlock(blockSize=1606), e79=Pulsing(freqCutoff=15), e78=MockSep, e73=MockSep, e72=SimpleText, e75=Standard, e74=MockRandom, binary=MockFixedIntBlock(blockSize=1606), f98=Pulsing(freqCutoff=15), f97=MockSep, f99=Standard, f94=Standard, f93=MockRandom, f96=MockVariableIntBlock(baseBlockSize=99), f95=MockFixedIntBlock(blockSize=1606), e95=SimpleText, e94=Standard, e97=MockRandom, e96=MockVariableIntBlock(baseBlockSize=99), e99=MockFixedIntBlock(blockSize=1606), e98=Pulsing(freqCutoff=15), id=MockFixedIntBlock(blockSize=1606), f34=MockSep, f33=SimpleText, f32=MockFixedIntBlock(blockSize=1606), f31=Pulsing(freqCutoff=15), f30=MockRandom, f39=MockFixedIntBlock(blockSize=1606), f38=Standard, f37=MockRandom, f36=MockSep, f35=SimpleText, f43=Standard, f42=MockRandom, f45=MockVariableIntBlock(baseBlockSize=99), f44=MockFixedIntBlock(blockSize=1606), f41=MockSep, f40=SimpleText, f47=MockVariableIntBlock(baseBlockSize=99), f46=MockFixedIntBlock(blockSize=1606), f49=Pulsing(freqCutoff=15), f48=MockSep, content=Pulsing(freqCutoff=15), e19=Standard, e18=MockRandom, e17=MockSep, f12=Pulsing(freqCutoff=15), e16=SimpleText, f11=MockSep, f10=MockVariableIntBlock(baseBlockSize=99), e15=MockFixedIntBlock(blockSize=1606), e14=Pulsing(freqCutoff=15), f16=SimpleText, e13=MockFixedIntBlock(blockSize=1606), f15=Standard, e12=Pulsing(freqCutoff=15), e11=MockRandom, f14=Pulsing(freqCutoff=15), e10=MockVariableIntBlock(baseBlockSize=99), f13=MockSep, f19=Pulsing(freqCutoff=15), f18=MockRandom, f17=MockVariableIntBlock(baseBlockSize=99), e29=MockSep, e26=Standard, f21=SimpleText, e25=MockRandom, f20=Standard, e28=MockVariableIntBlock(baseBlockSize=99), f23=MockRandom, e27=MockFixedIntBlock(blockSize=1606), f22=MockVariableIntBlock(baseBlockSize=99), f25=MockRandom, e22=MockSep, f24=MockVariableIntBlock(baseBlockSize=99), e21=SimpleText, f27=MockFixedIntBlock(blockSize=1606), e24=Standard, f26=Pulsing(freqCutoff=15), e23=MockRandom, f29=MockSep, f28=SimpleText, e20=MockFixedIntBlock(blockSize=1606), field=Pulsing(freqCutoff=15), string=MockVariableIntBlock(baseBlockSize=99), e30=Pulsing(freqCutoff=15), e31=MockFixedIntBlock(blockSize=1606), a98=MockFixedIntBlock(blockSize=1606), e34=MockRandom, a99=MockVariableIntBlock(baseBlockSize=99), e35=Standard, f79=Pulsing(freqCutoff=15), e32=SimpleText, e33=MockSep, b97=Pulsing(freqCutoff=15), f77=Pulsing(freqCutoff=15), e38=MockFixedIntBlock(blockSize=1606), b98=MockFixedIntBlock(blockSize=1606), f78=MockFixedIntBlock(blockSize=1606), e39=MockVariableIntBlock(baseBlockSize=99), b99=SimpleText, f75=MockVariableIntBlock(baseBlockSize=99), e36=MockRandom, f76=MockRandom, e37=Standard, f73=Standard, f74=SimpleText, f71=MockSep, f72=Pulsing(freqCutoff=15), f81=Pulsing(freqCutoff=15), f80=MockSep, e40=MockSep, e41=MockRandom, e42=Standard, e43=MockFixedIntBlock(blockSize=1606), e44=MockVariableIntBlock(baseBlockSize=99), e45=MockSep, e46=Pulsing(freqCutoff=15), f86=SimpleText, e47=MockSep, f87=MockSep, e48=Pulsing(freqCutoff=15), f88=MockRandom, e49=Standard, f89=Standard, f82=MockVariableIntBlock(baseBlockSize=99), f83=MockRandom, f84=Pulsing(freqCutoff=15), f85=MockFixedIntBlock(blockSize=1606), f90=SimpleText, f92=MockRandom, f91=MockVariableIntBlock(baseBlockSize=99), str=MockFixedIntBlock(blockSize=1606), a76=MockVariableIntBlock(baseBlockSize=99), e56=MockVariableIntBlock(baseBlockSize=99), f59=MockSep, a77=MockRandom, e57=MockRandom, a78=Pulsing(freqCutoff=15), e54=Standard, f57=MockFixedIntBlock(blockSize=1606), a79=MockFixedIntBlock(blockSize=1606), e55=SimpleText, f58=MockVariableIntBlock(baseBlockSize=99), e52=MockSep, e53=Pulsing(freqCutoff=15), e50=MockFixedIntBlock(blockSize=1606), e51=MockVariableIntBlock(baseBlockSize=99), f51=SimpleText, f52=MockSep, f50=MockFixedIntBlock(blockSize=1606), f55=MockFixedIntBlock(blockSize=1606), f56=MockVariableIntBlock(baseBlockSize=99), f53=MockRandom, e58=MockVariableIntBlock(baseBlockSize=99), f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=99), e60=MockVariableIntBlock(baseBlockSize=99), a82=Pulsing(freqCutoff=15), a81=MockSep, a84=SimpleText, a83=Standard, a86=MockRandom, a85=MockVariableIntBlock(baseBlockSize=99), a89=MockRandom, f68=Standard, e65=Pulsing(freqCutoff=15), f69=SimpleText, e66=MockFixedIntBlock(blockSize=1606), a87=SimpleText, e67=SimpleText, a88=MockSep, e68=MockSep, e61=Standard, e62=SimpleText, e63=MockVariableIntBlock(baseBlockSize=99), e64=MockRandom, f60=MockRandom, f61=Standard, f62=MockFixedIntBlock(blockSize=1606), f63=MockVariableIntBlock(baseBlockSize=99), e69=SimpleText, f64=MockSep, f65=Pulsing(freqCutoff=15), f66=Standard, f67=SimpleText, f70=Standard, a93=MockRandom, a92=MockVariableIntBlock(baseBlockSize=99), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockSep, a96=SimpleText, a95=MockFixedIntBlock(blockSize=1606), a94=Pulsing(freqCutoff=15), c58=MockRandom, a63=Pulsing(freqCutoff=15), a64=MockFixedIntBlock(blockSize=1606), c59=Standard, c56=SimpleText, d59=MockVariableIntBlock(baseBlockSize=99), a61=MockVariableIntBlock(baseBlockSize=99), c57=MockSep, a62=MockRandom, c54=Pulsing(freqCutoff=15), c55=MockFixedIntBlock(blockSize=1606), a60=SimpleText, c52=MockVariableIntBlock(baseBlockSize=99), c53=MockRandom, d53=MockSep, d54=Pulsing(freqCutoff=15), d51=MockFixedIntBlock(blockSize=1606), d52=MockVariableIntBlock(baseBlockSize=99), d57=MockVariableIntBlock(baseBlockSize=99), b62=MockSep, d58=MockRandom, b63=Pulsing(freqCutoff=15), d55=Standard, b60=MockFixedIntBlock(blockSize=1606), d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=99), b56=SimpleText, b55=Standard, b54=Pulsing(freqCutoff=15), b53=MockSep, d61=MockVariableIntBlock(baseBlockSize=99), b59=Pulsing(freqCutoff=15), d60=MockFixedIntBlock(blockSize=1606), b58=MockRandom, b57=MockVariableIntBlock(baseBlockSize=99), c62=MockRandom, c61=MockVariableIntBlock(baseBlockSize=99), a59=Standard, c60=SimpleText, a58=MockRandom, a57=MockSep, a56=SimpleText, a55=MockFixedIntBlock(blockSize=1606), a54=Pulsing(freqCutoff=15), a72=SimpleText, c67=MockFixedIntBlock(blockSize=1606), a73=MockSep, c68=MockVariableIntBlock(baseBlockSize=99), a74=MockRandom, c69=MockSep, a75=Standard, c63=SimpleText, c64=MockSep, a70=Pulsing(freqCutoff=15), c65=MockRandom, a71=MockFixedIntBlock(blockSize=1606), c66=Standard, d62=Standard, d63=SimpleText, d64=MockVariableIntBlock(baseBlockSize=99), b70=Pulsing(freqCutoff=15), d65=MockRandom, b71=Standard, d66=Pulsing(freqCutoff=15), b72=SimpleText, d67=MockFixedIntBlock(blockSize=1606), b73=MockVariableIntBlock(baseBlockSize=99), d68=SimpleText, b74=MockRandom, d69=MockSep, b65=MockRandom, b64=MockVariableIntBlock(baseBlockSize=99), b67=MockFixedIntBlock(blockSize=1606), b66=Pulsing(freqCutoff=15), d70=Pulsing(freqCutoff=15), b69=MockSep, b68=SimpleText, d72=SimpleText, d71=Standard, c71=MockFixedIntBlock(blockSize=1606), c70=Pulsing(freqCutoff=15), a69=MockSep, c73=MockSep, c72=SimpleText, a66=Standard, a65=MockRandom, a68=MockVariableIntBlock(baseBlockSize=99), a67=MockFixedIntBlock(blockSize=1606), c32=MockFixedIntBlock(blockSize=1606), c33=MockVariableIntBlock(baseBlockSize=99), c30=MockRandom, c31=Standard, c36=Standard, a41=MockFixedIntBlock(blockSize=1606), c37=SimpleText, a42=MockVariableIntBlock(baseBlockSize=99), a0=MockSep, c34=MockSep, c35=Pulsing(freqCutoff=15), a40=Standard, b84=SimpleText, d79=MockFixedIntBlock(blockSize=1606), b85=MockSep, b82=Pulsing(freqCutoff=15), d77=MockRandom, c38=Standard, b83=MockFixedIntBlock(blockSize=1606), d78=Standard, c39=SimpleText, b80=MockVariableIntBlock(baseBlockSize=99), d75=SimpleText, b81=MockRandom, d76=MockSep, d73=Pulsing(freqCutoff=15), d74=MockFixedIntBlock(blockSize=1606), d83=MockFixedIntBlock(blockSize=1606), a9=MockVariableIntBlock(baseBlockSize=99), d82=Pulsing(freqCutoff=15), d81=MockRandom, d80=MockVariableIntBlock(baseBlockSize=99), b79=MockFixedIntBlock(blockSize=1606), b78=Standard, b77=MockRandom, b76=MockSep, b75=SimpleText, a1=MockFixedIntBlock(blockSize=1606), a35=Pulsing(freqCutoff=15), a2=MockVariableIntBlock(baseBlockSize=99), a34=MockSep, a3=MockSep, a33=MockVariableIntBlock(baseBlockSize=99), a4=Pulsing(freqCutoff=15), a32=MockFixedIntBlock(blockSize=1606), a5=Standard, a39=MockRandom, c40=Standard, a6=SimpleText, a38=MockVariableIntBlock(baseBlockSize=99), a7=MockVariableIntBlock(baseBlockSize=99), a37=SimpleText, a8=MockRandom, a36=Standard, c41=MockSep, c42=Pulsing(freqCutoff=15), c43=Standard, c44=SimpleText, c45=MockVariableIntBlock(baseBlockSize=99), a50=MockSep, c46=MockRandom, a51=Pulsing(freqCutoff=15), c47=Pulsing(freqCutoff=15), a52=Standard, c48=MockFixedIntBlock(blockSize=1606), a53=SimpleText, b93=MockRandom, d88=MockSep, c49=Pulsing(freqCutoff=15), b94=Standard, d89=Pulsing(freqCutoff=15), b95=MockFixedIntBlock(blockSize=1606), b96=MockVariableIntBlock(baseBlockSize=99), d84=MockRandom, b90=MockFixedIntBlock(blockSize=1606), d85=Standard, b91=SimpleText, d86=MockFixedIntBlock(blockSize=1606), b92=MockSep, d87=MockVariableIntBlock(baseBlockSize=99), d92=MockSep, d91=SimpleText, d94=Standard, d93=MockRandom, b87=MockVariableIntBlock(baseBlockSize=99), b86=MockFixedIntBlock(blockSize=1606), d90=MockFixedIntBlock(blockSize=1606), b89=Pulsing(freqCutoff=15), b88=MockSep, a44=SimpleText, a43=Standard, a46=MockRandom, a45=MockVariableIntBlock(baseBlockSize=99), a48=MockFixedIntBlock(blockSize=1606), a47=Pulsing(freqCutoff=15), c51=Pulsing(freqCutoff=15), a49=SimpleText, c50=MockSep, d98=MockVariableIntBlock(baseBlockSize=99), d97=MockFixedIntBlock(blockSize=1606), d96=Standard, d95=MockRandom, d99=MockSep, a20=MockSep, c99=MockRandom, c98=MockVariableIntBlock(baseBlockSize=99), c97=SimpleText, c96=Standard, b19=MockRandom, a16=MockSep, a17=Pulsing(freqCutoff=15), b17=SimpleText, a14=MockFixedIntBlock(blockSize=1606), b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=99), a12=MockRandom, a13=Standard, a10=SimpleText, a11=MockSep, b11=MockVariableIntBlock(baseBlockSize=99), b12=MockRandom, b10=SimpleText, b15=SimpleText, b16=MockSep, a18=MockSep, b13=Pulsing(freqCutoff=15), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1606), b30=MockFixedIntBlock(blockSize=1606), a31=MockVariableIntBlock(baseBlockSize=99), a30=MockFixedIntBlock(blockSize=1606), b28=MockFixedIntBlock(blockSize=1606), a25=Standard, b29=MockVariableIntBlock(baseBlockSize=99), a26=SimpleText, a27=MockVariableIntBlock(baseBlockSize=99), a28=MockRandom, a21=MockFixedIntBlock(blockSize=1606), a22=MockVariableIntBlock(baseBlockSize=99), a23=MockSep, a24=Pulsing(freqCutoff=15), b20=Pulsing(freqCutoff=15), b21=MockFixedIntBlock(blockSize=1606), b22=SimpleText, b23=MockSep, a29=MockVariableIntBlock(baseBlockSize=99), b24=MockRandom, b25=Standard, b26=MockFixedIntBlock(blockSize=1606), b27=MockVariableIntBlock(baseBlockSize=99), b41=Standard, b40=MockRandom, c77=Standard, c76=MockRandom, c75=MockSep, c74=SimpleText, c79=MockVariableIntBlock(baseBlockSize=99), c78=MockFixedIntBlock(blockSize=1606), c80=MockRandom, c83=SimpleText, c84=MockSep, c81=Pulsing(freqCutoff=15), b39=Standard, c82=MockFixedIntBlock(blockSize=1606), b37=Standard, b38=SimpleText, b35=MockSep, b36=Pulsing(freqCutoff=15), b33=MockFixedIntBlock(blockSize=1606), b34=MockVariableIntBlock(baseBlockSize=99), b31=MockRandom, b32=Standard, str2=MockFixedIntBlock(blockSize=1606), b50=MockVariableIntBlock(baseBlockSize=99), b52=Pulsing(freqCutoff=15), str3=SimpleText, b51=MockSep, c86=MockVariableIntBlock(baseBlockSize=99), tvtest=MockSep, c85=MockFixedIntBlock(blockSize=1606), c88=Pulsing(freqCutoff=15), c87=MockSep, c89=Standard, c90=SimpleText, c91=MockSep, c92=MockRandom, c93=Standard, c94=MockFixedIntBlock(blockSize=1606), c95=MockVariableIntBlock(baseBlockSize=99), content1=Pulsing(freqCutoff=15), b46=MockVariableIntBlock(baseBlockSize=99), b47=MockRandom, content3=MockVariableIntBlock(baseBlockSize=99), b48=Pulsing(freqCutoff=15), content4=MockFixedIntBlock(blockSize=1606), b49=MockFixedIntBlock(blockSize=1606), content5=Standard, b42=MockSep, b43=Pulsing(freqCutoff=15), b44=Standard, b45=SimpleText}, locale=tr, timezone=MET\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestMergeSchedulerExternal, TestCharTokenizers, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=69508608,total=127336448\n{noformat}\n\nSimon dug and it looks like this is a trunk issue, caused by LUCENE-1076 (only committed to trunk so far).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1904",
        "summary": "move wordnet based synonym code out of contrib/memory and into contrib/wordnet (or somewhere else)",
        "description": "see LUCENE-387 ... some synonym related code has been living in contrib/memory for a very long time ... it should be refactored out.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3394",
        "summary": "TestIndexFileDeleter checkIndex fail",
        "description": "found on 3.x\n\n{noformat}\nant test-tag -Dtestcase=TestIndexFileDeleter -Dtestmethod=testDeleteLeftoverFiles -Dtests.seed=7631088157098800527:4270221915205524915\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2700",
        "summary": "Expose DocValues via Fields",
        "description": "DocValues Reader are currently exposed / accessed directly via IndexReader. To integrate the new feature in a more \"native\" way we should expose the DocValues via Fields on a perSegment level and on MultiFields in the multi reader case. DocValues should be side by side with Fields.terms  enabling access to Source, SortedSource and ValuesEnum something like that:\n\n{code}\npublic abstract class Fields {\n...\n\n  public DocValues values();\n\n}\n\npublic abstract class DocValues {\n  /** on disk enum based API */\n  public abstract ValuesEnum getEnum() throws IOException;\n  /** in memory Random Access API - with enum support - first call loads values in ram*/\n  public abstract Source getSource() throws IOException;\n  /** sorted in memory Random Access API - optional operation */\n  public SortedSource getSortedSource(Comparator<BytesRef> comparator) throws IOException, UnsupportedOperationException;\n  /** unloads previously loaded source only but keeps the doc values open */\n  public abstract unload();\n  /** closes the doc values */\n  public abstract close();\n}\n{code}\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-754",
        "summary": "FieldCache keeps hard references to readers, doesn't prevent multiple threads from creating same instance",
        "description": "",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1744",
        "summary": "BooleanScorer2 fails to update this.doc when its the top scorer",
        "description": "When BooleanScorer2 runs the top collection loop (one of its\nscore(Collector)) methods, it uses a local \"doc\" var, ie:\n\n{code}\npublic void score(Collector collector) throws IOException {\n    collector.setScorer(this);\n    int doc;\n    while ((doc = countingSumScorer.nextDoc()) != NO_MORE_DOCS) {\n      collector.collect(doc);\n    }\n}\n{code}\n\nThe problem is, if the child collector calls scorer.doc() it will\nalways get -1.  Most Collectors don't actually call scorer.doc(), but\none important one that does is ScoreCachingWrapperScorer, as it uses\nthe doc to know when to invalidate its cache.  Since this always\nreturns -1, the ScoreCachingWrapperScorer keeps returning score=0.0 to\nits caller, thus messing up a SortField.SCORE comparator instance if\nit's included in the sort fields.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-984",
        "summary": "remove TermVectorsWriter (it's no longer used)",
        "description": "We should remove TermVectorsWriter: it's no longer used now that\nDocumentsWriter writes the term vectors directly to the index.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-629",
        "summary": "Performance improvement for merging stored, compressed fields",
        "description": "Hello everyone,\n\ncurrently the merging of stored, compressed fields is not optimal for the following reason: every time a stored, compressed field is being merged, the FieldsReader uncompresses the data, hence the FieldsWriter has to compress it again when it writes the merged fields data (.fdt) file. The uncompress/compress step is unneccessary and slows down the merge performance significantly.\n\nThis patch improves the merge performance by avoiding the uncompress/compress step. In the following I give an overview of the changes I made:\n   * Added a new FieldSelectorResult constant named \"LOAD_FOR_MERGE\" to org.apache.lucene.document.FieldSelectorResult\n   * SegmentMerger now uses an FieldSelector to get stored fields from the FieldsReader. This FieldSelector's accept() method returns the FieldSelectorResult \"LOAD_FOR_MERGE\" for every field.\n   * Added a new inner class to FieldsReader named \"FieldForMerge\", which extends  org.apache.lucene.document.AbstractField. This class holds the field properties and its data. If a field has the FieldSelectorResult \"LOAD_FOR_MERGE\", then the FieldsReader creates an instance of \"FieldForMerge\" and does not uncompress the field's data.\n   * FieldsWriter checks if the field it is about to write is an instanceof FieldsReader.FieldForMerge. If true, then it does not compress the field data.\n\n\nTo test the performance I index about 350,000 text files and store the raw text in a stored, compressed field in the lucene index. I use a merge factor of 10. The final index has a size of 366MB. After building the index, I optimize it to measure the pure merge performance.\n\nHere are the performance results:\n\nold version:\n   * Time for Indexing:  36.7 minutes\n   * Time for Optimizing: 4.6 minutes\n\npatched version:\n   * Time for Indexing:  20.8 minutes\n   * Time for Optimizing: 0.5 minutes\n\nThe results show that the index build time improved by about 43%, and the optimizing step is more than 8x faster. \n\nA diff of the final indexes (old and patched version) shows, that they are identical. Furthermore, all junit testcases succeeded with the patched version. \n\nRegards,\n  Michael Busch",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-381",
        "summary": "Contributing a High-performance single-document main memory Apache Lucene fulltext search index.",
        "description": "Here is my contribution: a High-performance single-document main memory Apache Lucene fulltext \nsearch index. I'll try to attach the files, hoping for comments on how to proceed with this...",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1086",
        "summary": "DocMakers setup for the \"docs.dir\" property fails when passing an absolute path.",
        "description": "setConfig in TrecDocMaker assumes docs.dir is a relative path. Therefore it create new File(workDir, docs.dir). However, if docs.dir is an absolute path, this works incorrectly and results in No txt files in dataDir exception.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3627",
        "summary": "CorruptIndexException on indexing after a failure occurs after segments file creation but before any bytes are written",
        "description": "FSDirectory.createOutput(..) uses a RandomAccessFile to do its work.  On my system the default FSDirectory.open(..) creates an NIOFSDirectory.  If createOutput is called on a segments_* file and a crash occurs between RandomAccessFile creation (file system shows a segments_* file exists but has zero bytes) but before any bytes are written to the file, subsequent IndexWriters cannot proceed.  The difficulty is that it does not know how to clear the empty segments_* file.  None of the file deletions will happen on such a segment file because the opening bytes cannot not be read to determine format and version.\n\nAn initial proposed patch file is attached below.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1095",
        "summary": "StopFilter should have option to incr positionIncrement after stop word",
        "description": "I've seen this come up on the mailing list a few times in the last month, so i'm filing a known bug/improvement arround it...\n\nStopFilter should have an option that if set, records how many stop words are \"skipped\" in a row, and then sets that value as the positionIncrement on the \"next\" token that StopFilter does return.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3905",
        "summary": "BaseTokenStreamTestCase should test analyzers on real-ish content",
        "description": "We already have LineFileDocs, that pulls content generated from europarl or wikipedia... I think sometimes BTSTC should test the analyzers on that as well.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2204",
        "summary": "FastVectorHighlighter: some classes and members should be publicly accessible to implement FragmentsBuilder",
        "description": "I intended to design custom FragmentsBuilder can be written and pluggable, though, when I tried to write it out of the FVH package, it came out that some classes and members should be publicly accessible.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-231",
        "summary": "Can't build lucene 06/13/2004 CVS under jdk 1.5.0",
        "description": "[root@shilo jakarta-lucene]# /usr/local/ant/bin/ant build\nBuildfile: build.xml\n\nBUILD FAILED\nTarget `build' does not exist in this project. \n\nTotal time: 1 second\n[root@shilo jakarta-lucene]# /usr/local/ant/bin/ant \nBuildfile: build.xml\n\ninit:\n    [mkdir] Created dir: /usr/src/jakarta-lucene/build\n    [mkdir] Created dir: /usr/src/jakarta-lucene/dist\n\ncompile-core:\n    [mkdir] Created dir: /usr/src/jakarta-lucene/build/classes/java\n    [javac] Compiling 160 source files to /usr/src/jakarta-\nlucene/build/classes/java\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of \nrelease 1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]     public void seek(TermEnum enum) throws IOException { in.seek\n(enum); }\n    [javac]                               ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of \nrelease 1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]     public void seek(TermEnum enum) throws IOException { in.seek\n(enum); }\n    [javac]                                                                  ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:55: as of release \n1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]   public void seek(TermEnum enum) throws IOException {\n    [javac]                             ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release \n1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) \nenum).fieldInfos == parent.fieldInfos)          // optimized case\n    [javac]         ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release \n1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) \nenum).fieldInfos == parent.fieldInfos)          // optimized case\n    [javac]                                                               ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:60: as of release \n1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]       ti = ((SegmentTermEnum) enum).termInfo();\n    [javac]                               ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:62: as of release \n1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]       ti = parent.tis.get(enum.term());\n    [javac]                           ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:63: as of release \n1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]     SegmentTermEnum enum = (SegmentTermEnum)enumerators.get();\n    [javac]                     ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:64: as of release \n1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]     if (enum == null) {\n    [javac]         ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: enum types \nmust not be local\n    [javac]       enum = terms();\n    [javac]       ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: <identifier> \nexpected\n    [javac]       enum = terms();\n    [javac]            ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: '{' expected\n    [javac]       enum = terms();\n    [javac]                     ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> \nexpected\n    [javac]       enumerators.set(enum);\n    [javac]                      ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: ';' expected\n    [javac]       enumerators.set(enum);\n    [javac]                       ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> \nexpected\n    [javac]       enumerators.set(enum);\n    [javac]                           ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: '{' expected\n    [javac]       enumerators.set(enum);\n    [javac]                            ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: illegal start \nof type\n    [javac]     return enum;\n    [javac]     ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: as of release \n1.5, 'enum' is a keyword, and may not be used as an identifier\n    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)\n    [javac]     return enum;\n    [javac]            ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:75: illegal start \nof expression\n    [javac]   private final void readIndex() throws IOException {\n    [javac]   ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:199: ';' expected\n    [javac] }\n    [javac] ^\n    [javac] /usr/src/jakarta-\nlucene/src/java/org/apache/lucene/index/TermInfosReader.java:200: '}' expected\n    [javac] ^\n    [javac] 21 errors\n\nBUILD FAILED\n/usr/src/jakarta-lucene/build.xml:140: Compile failed; see the compiler error \noutput for details.\n\nTotal time: 4 seconds\n[root@shilo jakarta-lucene]#",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-240",
        "summary": "bug form doesn't list latest version",
        "description": " ",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1010",
        "summary": "Document with no term vectors mixed with ones that have term vectors cause EOFException during merge",
        "description": "Another spinoff from here:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/53306\n\nThank you to Andi Vajda for capturing the issue in a compact test!\n\nThis is the same logical error from LUCENE-1008, but in this case the\nbug is in TermVectorsWriter: we are failing to write the \"0\" field\ncount to the tvd file when the document has no vectors.  I have a unit\ntest showing the issue & simple fix.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3159",
        "summary": "lucene benchmark has some unnecessary files",
        "description": "lucene/contrib/benchmark/.rsync-filter is only in the source pack (and in SVN), I was not aware of this file, though it was added long ago in https://issues.apache.org/jira/browse/LUCENE-848?focusedCommentId=12491404&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12491404\nNot a blocker for this RC, just interesting to note.\n\nmaybe this is related to LUCENE-3155 too, in that we could consider this one for automatic exclusion (like DS_Store), but we should fix it if its committed in SVN too.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3093",
        "summary": "Build failed in the flexscoring branch because of Javadoc warnings",
        "description": "Ant build log:\n  [javadoc] Standard Doclet version 1.6.0_24\n  [javadoc] Building tree for all the packages and classes...\n  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/Similarity.java:93: warning - Tag @link: can't find tf(float) in org.apache.lucene.search.Similarity\n  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument \"term\" is not a parameter name.\n  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument \"docFreq\" is not a parameter name.\n  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:618: warning - @param argument \"terms\" is not a parameter name.\n  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated//package-summary.html...\n  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...\n  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/HitCollectionBench.jpg to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...\n  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...\n  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/serialized-form.html...\n  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/prettify/stylesheet+prettify.css to file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/stylesheet+prettify.css...\n  [javadoc] Building index for all the packages and classes...\n  [javadoc] Building index for all classes...\n  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/help-doc.html...\n  [javadoc] 4 warnings\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3555",
        "summary": "Add support for distributed stats",
        "description": "(its a bug in a way, since we broke this, temporarily).\n\nThere is no way to do this now (distributed IDF, etc) with the new API.\n\nBut we should do it right:\n* having the sim ask the searcher for docfreq of a term is wasteful and dangerous, \n  usually we have already seek'd to the term and already collected the 'raw' stuff.\n* the situation is more than just docfreq, because you should be able to implement\n  distributed scoring for all of the new sim models (or your own), that use any\n  of Lucene's stats.\n",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3027",
        "summary": "TestOmitTf.testMixedMerge random seed failure",
        "description": "Version: trunk r1091638\n\nant test -Dtests.seed=-6595054217575280191:5576532348905930588\n\n\n    [junit] ------------- Standard Error -----------------\n    [junit] WARNING: test method: 'testDeMorgan' left thread running: Thread[NRT search threads-1691-thread-2,5,main]\n    [junit] RESOURCE LEAK: test method: 'testDeMorgan' left 1 thread(s) running\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestBooleanQuery -Dtestmethod=testDeMorgan -Dtests.seed=-6595054217575280191:5576532348905930588\n    [junit] ------------- ---------------- ---------------\n    [junit] Testsuite: org.apache.lucene.index.TestNorms\n    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 5.064 sec\n    [junit] \n    [junit] Testsuite: org.apache.lucene.index.TestOmitTf\n    [junit] Testcase: testMixedMerge(org.apache.lucene.index.TestOmitTf):\tCaused an ERROR\n    [junit] CheckIndex failed\n    [junit] java.lang.RuntimeException: CheckIndex failed\n    [junit] \tat org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:152)\n    [junit] \tat org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)\n    [junit] \tat org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)\n    [junit] \n    [junit] \n    [junit] Tests run: 5, Failures: 0, Errors: 1, Time elapsed: 0.851 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] CheckIndex failed\n    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]\n    [junit]   1 of 1: name=_12 docCount=60\n    [junit]     codec=SegmentCodecs [codecs=[MockRandom, MockVariableIntBlock(baseBlockSize=112)], provider=RandomCodecProvider: {f1=MockRandom, f2=MockVariableIntBlock(baseBlockSize=112)}]\n    [junit]     compound=false\n    [junit]     hasProx=false\n    [junit]     numFiles=16\n    [junit]     size (MB)=0,01\n    [junit]     diagnostics = {optimize=true, mergeFactor=2, os.version=2.6.37-gentoo, os=Linux, lucene.version=4.0-SNAPSHOT, source=merge, os.arch=amd64, java.version=1.6.0_24, java.vendor=Sun Microsystems Inc.}\n    [junit]     no deletions\n    [junit]     test: open reader.........OK\n    [junit]     test: fields..............OK [2 fields]\n    [junit]     test: field norms.........OK [2 fields]\n    [junit]     test: terms, freq, prox...ERROR: java.io.IOException: Read past EOF\n    [junit] java.io.IOException: Read past EOF\n    [junit] \tat org.apache.lucene.store.RAMInputStream.switchCurrentBuffer(RAMInputStream.java:90)\n    [junit] \tat org.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:63)\n    [junit] \tat org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:105)\n    [junit] \tat org.apache.lucene.store.DataInput.readVInt(DataInput.java:94)\n    [junit] \tat org.apache.lucene.index.codecs.sep.SepSkipListReader.readSkipData(SepSkipListReader.java:188)\n    [junit] \tat org.apache.lucene.index.codecs.MultiLevelSkipListReader.loadNextSkip(MultiLevelSkipListReader.java:142)\n    [junit] \tat org.apache.lucene.index.codecs.MultiLevelSkipListReader.skipTo(MultiLevelSkipListReader.java:112)\n    [junit] \tat org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsEnum.advance(SepPostingsReaderImpl.java:454)\n    [junit] \tat org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:782)\n    [junit] \tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:495)\n    [junit] \tat org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:148)\n    [junit] \tat org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)\n    [junit] \tat org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)\n    [junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit] \tat java.lang.reflect.Method.invoke(Method.java:597)\n    [junit] \tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit] \tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit] \tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit] \tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit] \tat org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit] \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit] \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit] \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)\n    [junit] \tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit] \tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit] \tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit] \tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit] \tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit] \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit] \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit] \tat org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit] \tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)\n    [junit]     test: stored fields.......OK [60 total field count; avg 1 fields per doc]\n    [junit]     test: term vectors........OK [120 total vector count; avg 2 term/freq vector fields per doc]\n    [junit] FAILED\n    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:\n    [junit] java.lang.RuntimeException: Term Index test failed\n    [junit] \tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:508)\n    [junit] \tat org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:148)\n    [junit] \tat org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:138)\n    [junit] \tat org.apache.lucene.index.TestOmitTf.testMixedMerge(TestOmitTf.java:155)\n    [junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit] \tat java.lang.reflect.Method.invoke(Method.java:597)\n    [junit] \tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit] \tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit] \tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit] \tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit] \tat org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit] \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit] \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit] \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1232)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1160)\n    [junit] \tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit] \tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit] \tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit] \tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit] \tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit] \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit] \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit] \tat org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit] \tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)\n    [junit] \n    [junit] WARNING: 1 broken segments (containing 60 documents) detected\n    [junit] \n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestOmitTf -Dtestmethod=testMixedMerge -Dtests.seed=-6595054217575280191:5576532348905930588\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {noTf=MockSep, tf=Standard, f1=MockRandom, f2=MockVariableIntBlock(baseBlockSize=112)}, locale=cs_CZ, timezone=Chile/Continental\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestAssertions, TestCachingTokenFilter, TestDocument, TestDirectoryReader, TestFlex, TestIndexWriterConfig, TestIndexWriterMerging, TestIndexWriterOnJRECrash, TestMultiReader, TestNewestSegment, TestNorms, TestOmitTf]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=94021800,total=126484480\n    [junit] ------------- ---------------- ---------------",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1645",
        "summary": "Deleted documents are visible across reopened MSRs",
        "description": "",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1684",
        "summary": "Add matchVersion to StandardAnalyzer",
        "description": "I think we should add a matchVersion arg to StandardAnalyzer.  This\nallows us to fix bugs (for new users) while keeping precise back\ncompat (for users who upgrade).\n\nWe've discussed this on java-dev, but I'd like to now make it concrete\n(patch attached).  I think it actually works very well, and is a\nsimple tool to help us carry out our back-compat policy.\n\nI coded up an example with StandardAnalyzer:\n\n  * The ctor now takes a required arg (Version matchVersion).  You\n    pass Version.LUCENE_CURRENT to always get lates & greatest, or eg\n    Version.LUCENE_24 to match 2.4's bugs/settings/behavior.\n\n  * StandardAalyzer conditionalizes the \"replace invalid acronym\" and\n    \"enable position increment in StopFilter\" based on matchVersion.\n\n  * It also prevents creating zillions of ctors, over time, as we need\n    to change settings in the class.  EG StandardAnalyzer now has 2\n    settings that are version dependent, and there's at least another\n    2 issues open on fixing some more of its bugs.\n\nThe migration is also very clean: we'd only add this to classes on an\n\"as needed\" basis.  On the first release that adds the arg, the\ndefault remains back compatible with the prior release.  Then, going\nforward, we are free to fix issues on that class and conditionalize by\nmatchVersion.\n\nThe javadoc at the top of StandardAnalyzer clearly calls out what\nversion specific behavior is done:\n\n{code}\n * <p>You must specify the required {@link Version}\n * compatibility when creating StandardAnalyzer:\n * <ul>\n *   <li> As of 2.9, StopFilter preserves position\n *        increments by default\n *   <li> As of 2.9, Tokens incorrectly idenfied as acronyms\n *        are corrected (see <a href=\"https://issues.apache.org/jira/browse/LUCENE-1068\">LUCENE-1608</a>\n * </ul>\n *\n{code}\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2377",
        "summary": "Enable the use of NoMergePolicy and NoMergeScheduler by Benchmark",
        "description": "Benchmark allows one to set the MP and MS to use, by defining the class name and then use reflection to instantiate them. However NoMP and NoMS are singletons and therefore reflection does not work for them. Easy fix in CreateIndexTask. I'll post a patch soon.",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2022",
        "summary": "remove contrib deprecations",
        "description": "there aren't too many deprecations in contrib to remove for 3.0, but we should get rid of them.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2502",
        "summary": "Remove some unused code in Surround query parser",
        "description": "",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1672",
        "summary": "Deprecate all String/File ctors/opens in IndexReader/IndexWriter/IndexSearcher",
        "description": "During investigation of LUCENE-1658, I found out, that even LUCENE-1453 is not completely fixed.\nAs 1658 deprecates all FSDirectory.getDirectory() static factories, we should not use them anymore. As the user is now free to choose the correct directory implementation using direct instantiation or using FSDir.open() he should no longer use all ctors/methods in IndexWriter/IndexReader/IndexSearcher & Co. that simply take path names as String or File and always instantiate the Directory himself.\n\nLUCENE-1453 currently works for the cached directory implementations from FSDir.getDirectory, but not with uncached, non refcounting FSDirs. Sometime reopen() closes the directory (as far as I see, when a SegmentReader changes to a MultiSegmentReader and/or deletes apply). This is hard to track. In Lucene 3.0 we then can remove the whole bunch of closeDirectory parameters/fields in these classes and simply do not care anymore about closing directories.\n\nTo remove this closeDirectory parameter now (before 3.0) and also fix 1453 correctly, an additional idea would be to change these factories that take the File/String to return the IndexReader wrapped by a FilteredIndexReader, that keeps track on closing the underlying directory after close and reopen. This is simplier than passing this boolean between different DirectoryIndexReader instances. The small performance impact by wrapping with FilterIndexReader should not be so bad, because the method is deprecated and we can state, that it is better to user the factory method with Directory parameter.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2702",
        "summary": "BytesRefHash#get() should expect a BytesRef instances for consistency",
        "description": "BytesRefHash#get should use a provided BytesRef instances instead of the internally used scratch. This is how all other APIs currently work and we should be consistent.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-365",
        "summary": "[PATCH] Performance improvement to DisjunctionSumScorer",
        "description": "A recent profile of the new BooleanScorer2 showed that \nquite a bit of CPU time is spent in the advanceAfterCurrent method \nof DisjunctionScorer, and in the PriorityQueue of scorers that \nis used there. \n \nThis patch reduces the internal overhead of DisjunctionScorer \nto about 70% of the current one (ie. 30% saving in cpu time). \nIt also reduces the number of calls to the subscorers, but \nthat was not measured. \n \nTo get this, it was necessary to specialize the PriorityQueue \nfor a Scorer and to add move some code fragments from DisjunctionScorer \nto this specialized queue.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3419",
        "summary": "Resolve JUnit assert deprecations",
        "description": "Many tests use assertEquals methods which have been deprecated.  The culprits are assertEquals(float, float), assertEquals(double, double) and assertEquals(Object[], Object[]).  Although not a big issue, they annoy me every time I see them so I'm going to fix them.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1985",
        "summary": "DisjunctionMaxQuery -  Iterator code to  for ( A  a : container ) construct",
        "description": "For better readability  - converting the Iterable<T> to  for ( A  a : container ) constructs that is more intuitive to read. ",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1604",
        "summary": "Stop creating huge arrays to represent the absense of field norms",
        "description": "Creating and keeping around huge arrays that hold a constant value is very inefficient both from a heap usage standpoint and from a localility of reference standpoint. It would be much more efficient to use null to represent a missing norms table.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3486",
        "summary": "Add SearcherLifetimeManager, so you can retrieve the same searcher you previously used",
        "description": "The idea is similar to SOLR-2809 (adding searcher leases to Solr).\n\nThis utility class sits above whatever your source is for \"the\ncurrent\" searcher (eg NRTManager, SearcherManager, etc.), and records\n(holds a reference to) each searcher in recent history.\n\nThe idea is to ensure that when a user does a follow-on action (clicks\nnext page, drills down/up), or when two or more searcher invocations\nwithin a single user search need to happen against the same searcher\n(eg in distributed search), you can retrieve the same searcher you\nused \"last time\".\n\nI think with the new searchAfter API (LUCENE-2215), doing follow-on\nsearches on the same searcher is more important, since the \"bottom\"\n(score/docID) held for that API can easily shift when a new searcher\nis opened.\n\nWhen you do a \"new\" search, you record the searcher you used with the\nmanager, and it returns to you a long token (currently just the\nIR.getVersion()), which you can later use to retrieve the same\nsearcher.\n\nSeparately you must periodically call prune(), to prune the old\nsearchers, ideally from the same thread / at the same time that\nyou open a new searcher.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3354",
        "summary": "Extend FieldCache architecture to multiple Values",
        "description": "I would consider this a bug. It appears lots of people are working around this limitation, \nwhy don't we just change the underlying data structures to natively support multiValued fields in the FieldCache architecture?\n\nThen functions() will work properly, and we can do things like easily geodist() on a multiValued field.\n\nThoughts?",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2658",
        "summary": "TestIndexWriterExceptions random failure: AIOOBE in ByteBlockPool.allocSlice",
        "description": "TestIndexWriterExceptions threw this today, and its reproducable",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2058",
        "summary": "benchmark pkg: specify trec_eval submission output from the command line",
        "description": "the QueryDriver for the trec benchmark currently requires 4 command line arguments.\nthe third argument is ignored (i typically populate this with \"bogus\")\nInstead, allow the third argument to specify the submission.txt file for trec_eval.\n\nwhile I am here, add a usage() documenting what the arguments to this driver program do.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-300",
        "summary": "[PATCH] Refactoring of SpanScorer",
        "description": "Refactored some common code in next() and skipTo(). \nRemoved dependency on score value for next() and skipTo(). \nPasses all current tests at just about the same speed \nas the current version. Added minimal javadoc. \n \nIirc, there has been some discussion on the dependency of next() \nand skipTo() on the score value, but I don't remember the conclusion. \nIn case that dependency should stay in, it can be adapted \nin the refactored code.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2631",
        "summary": "Fix small perf issues with String/TermOrdValComparator",
        "description": "Uncovered some silliness when working on LUCENE-2504, eg we are doing unnecessary binarySearch on a single-segment reader.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1416",
        "summary": "Ant contrib test can fail if there is a space in path to lucene project",
        "description": "A couple contrib ant tests get the path to test files through a URL object, and so the path is URL encoded. Normally fine, but if you have a space in your path (/svn stuff/lucene/contrib/ant) then it will have %20 for the space and (at least on my Ubuntu system) the test will fail with filenotfound. This patch simply replaces all %20 with \" \". Not sure if we want/need to take it any further.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3344",
        "summary": "Add workaround for ICU bug in combination with Java7 to LuceneTestCase",
        "description": "There is a bug in ICU that makes it fail to load it ULocale class in Java7: http://bugs.icu-project.org/trac/ticket/8734\n\nThe problem is caused by some new locales in Java 7, that lead to a chicken-and-egg problem in the static initializer of ULocale. It initializes its default locale from the JDK locale in a static ctor. Until the default ULocale instance is created, the default is not set in ULocale. But ULocales ctor itsself needs the default locale to fetch some ressource bundles and throws NPE.\n\nThe code in LuceneTestCase that randomizes the default locale should classload ULocale before it tries to set another random locale, using a defined, safe locale (Locale.US). Patch is easy.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3074",
        "summary": "SimpleTextCodec needs SimpleText DocValues impl",
        "description": "currently SimpleTextCodec uses binary docValues we should move that to a simple text impl.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3748",
        "summary": "EnglishPossessiveFilter should work with Unicode right single quotation mark",
        "description": "The current EnglishPossessiveFilter (used in EnglishAnalyzer) removes possessives using only the '\\'' character (plus 's' or 'S'), but some common systems (German?) insert the Unicode \"\\u2019\" (RIGHT SINGLE QUOTATION MARK) instead and this is not removed when processing UTF-8 text. I propose to change EnglishPossesiveFilter to support '\\u2019' as an alternative to '\\''.",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2421",
        "summary": "Hardening of NativeFSLock",
        "description": "NativeFSLock create a test lock file which its name might collide w/ another JVM that is running. Very unlikely, but still it happened a couple of times already, since the tests were parallelized. This may result in a false exception thrown from release(), when the lock file's delete() is called and returns false, because the file does not exist (deleted by another JVM already). In addition, release() should give a second attempt to delete() if it fails, since the file may be held temporarily by another process (like AntiVirus) before it fails. The proposed changes are:\n\n1) Use ManagementFactory.getRuntimeMXBean().getName() as part of the test lock name (should include the process Id)\n2) In release(), if delete() fails, check if the file indeed exists. If it is, let's attempt a re-delete() few ms later.\n3) If (3) still fails, throw an exception. Alternatively, we can attempt a deleteOnExit.\n\nI'll post a patch later today.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2741",
        "summary": "Several Codecs use the same files - PerFieldCodecWrapper can not hold two codec using the same files",
        "description": "Currently we have a rather simple file naming scheme which prevents us from using more than one codec in a segment that relies on the same file.  For instance pulsing and standard codec can not be used together since they both need the .frq .tii .tis etc. To make this work we either need to write distinct per codec files or set a per field / codec file ID. While the first solution seems to be quiet verbose the second one seems to be more flexible too.\n\nOne possibility to do that would be to assign a unique id to each SegmentsWriteState when opening the FieldsConsumer and write the IDs into the segments file to eventually load it once the segment is opened. Otherwise our PerFieldCodec feature will not be really flexible nor useful though.  ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3430",
        "summary": "TestParser.testSpanTermXML fails with some sims",
        "description": "here is why this test sometimes fails (my explanation in the test i wrote):\n\n{noformat}\n  /** make sure all sims work with spanOR(termX, termY) where termY does not exist */\n  public void testCrazySpans() throws Exception {\n    // The problem: \"normal\" lucene queries create scorers, returning null if terms dont exist\n    // This means they never score a term that does not exist.\n    // however with spans, there is only one scorer for the whole hierarchy:\n    // inner queries are not real queries, their boosts are ignored, etc.\n{noformat}\n\nBasically, SpanQueries aren't really queries, you just get one scorer. it calls extractTerms on the whole hierarchy and computes weights (e.g. IDF) on\nthe whole bag of terms, even if they don't exist.\n\nThis is fine, we already have tests that sim's won't bug-out in computeStats() here: however they don't expect to actually score documents based on\nthese terms that don't exist... however this is exactly what happens in Spans because it doesn't use sub-scorers.\n\nLucene's sim avoids this with the (docFreq + 1)\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3462",
        "summary": "Jenkins builds hang quite often in TestIndexWriterWithThreads.testCloseWithThreads",
        "description": "Last hung test run: [https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/10638/console]\n\n{noformat}\n[junit] \"main\" prio=5 tid=0x0000000801ef3800 nid=0x1965c waiting on condition [0x00007fffffbfd000]\n[junit]    java.lang.Thread.State: WAITING (parking)\n[junit] \tat sun.misc.Unsafe.park(Native Method)\n[junit] \t- parking to wait for  <0x0000000825d853a8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\n[junit] \tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n[junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)\n[junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:871)\n[junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1201)\n[junit] \tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\n[junit] \tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\n[junit] \tat org.apache.lucene.index.DocumentsWriterFlushControl.markForFullFlush(DocumentsWriterFlushControl.java:403)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:557)\n[junit] \t- locked <0x0000000825d81998> (a org.apache.lucene.index.DocumentsWriter)\n[junit] \tat org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2776)\n[junit] \t- locked <0x0000000825d7d840> (a java.lang.Object)\n[junit] \tat org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2904)\n[junit] \t- locked <0x0000000825d7d830> (a java.lang.Object)\n[junit] \tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1156)\n[junit] \tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1099)\n[junit] \tat org.apache.lucene.index.TestIndexWriterWithThreads.testCloseWithThreads(TestIndexWriterWithThreads.java:200)\n[junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n[junit] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n[junit] \tat java.lang.reflect.Method.invoke(Method.java:616)\n[junit] \tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n[junit] \tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n[junit] \tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n[junit] \tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n[junit] \tat org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n[junit] \tat org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)\n[junit] \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n[junit] \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n[junit] \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n[junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)\n[junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)\n[junit] \tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n[junit] \tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n[junit] \tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n[junit] \tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n[junit] \tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n[junit] \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n[junit] \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n[junit] \tat org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n[junit] \tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n[junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)\n[junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)\n[junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2481",
        "summary": "Enhance SnapshotDeletionPolicy to allow taking multiple snapshots",
        "description": "A spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/99161?do=post_view_threaded#99161\n\nI will:\n# Replace snapshot() with snapshot(String), so that one can name/identify the snapshot\n# Add some supporting methods, like release(String), getSnapshots() etc.\n# Some unit tests of course.\n\nThis is mostly written already - I want to contribute it. I've also written a PersistentSDP, which persists the snapshots on stable storage (a Lucene index in this case) to support opening an IW with existing snapshots already, so they don't get deleted. If it's interesting, I can contribute it as well.\n\nPorting my patch to the new API. Should post it soon.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1318",
        "summary": "InstantiatedIndexReader.norms called from MultiReader bug",
        "description": "Small bug in InstantiatedIndexReader.norms(String field, byte[] bytes, int offset) where the offset is not applied properly in the System.arraycopy",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-630",
        "summary": "results.jsp in luceneweb.war uses unknown parse-Method",
        "description": "results.jsp in luceneweb.war demo throws JasperException:\n\norg.apache.jasper.JasperException: Unable to compile class for JSP\n\nAn error occurred at line: 60 in the jsp file: /results.jsp\nGenerated servlet error:\nThe method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)\n\nI think, the code in line 81 of results.jsp should maybe look like the following ?\n\nQueryParser qp = new QueryParser(\"contents\", analyzer);\nquery = qp.parse(queryString);",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3477",
        "summary": "Fix JFlex tokenizer compiler warnings",
        "description": "We get lots of distracting fallthrough warnings running \"ant compile\"\nin modules/analysis, from the tokenizers generated from JFlex.\n\nDigging a bit, they actually do look spooky.\n\nSo I managed to edit the JFlex inputs to insert a bunch of break\nstatements in our rules, but I have no idea if this is\nright/dangerous, and it seems a bit weird having to do such insertions\nof \"naked\" breaks.\n\nBut, this does fix all the warnings, and all tests pass...\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-215",
        "summary": "addIndexes unexpectedly closes index",
        "description": "It seems that in 1.4rc2, a call to IndexWriter.addIndexes (IndexReader[]) will\nclose the provided IndexReader; in 1.3-final this does not happen.  So my code\nwhich uses addIndexes to merge new information into an index and then calls\nclose() on the IndexReader now crashes with an \"already closed\" exception.  I\ncan attach test code which works in 1.3 but not in 1.4rc2 if that would be helpful.\n\nIf this is an intentional change in behavior, it needs to be documented.  Thanks!",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2712",
        "summary": "FieldBoostMapAttribute in contrib/qp is broken.",
        "description": "While looking for more SuppressWarnings in lucene, i came across two of them in contrib/queryparser.\n\neven worse, i found these revolved around using maps with CharSequence as key.\n\nFrom the javadocs for CharSequence:\n\nThis interface does not refine the general contracts of the equals and hashCode methods. The result of comparing two objects that implement CharSequence is therefore, in general, undefined. Each object may be implemented by a different class, and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. {color:red} It is therefore inappropriate to use arbitrary CharSequence instances as elements in a set or as keys in a map. {color}\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3372",
        "summary": "TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull seed failure",
        "description": "version: trunk r1155278\nreproduce-able: always\n\n{code}\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.847 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=-3cc23002ebad518d:70ae722281b31c9f:57406021f8789a22\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockFixedIntBlock(blockSize=1081)}, locale=hr_HR, timezone=Atlantic/Jan_Mayen\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestIndexWriterOnDiskFull]\n    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=85252968,total=125632512\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testAddDocumentOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):     Caused an ERROR\n    [junit] no segments* file found in MockDirWrapper(org.apache.lucene.store.RAMDirectory@65dcc2a3 lockFactory=MockLockFactoryWrapper(org.apache.lucene.store.SingleInstanceLockFactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]\n    [junit] org.apache.lucene.index.IndexNotFoundException: no segments* file found in MockDirWrapper(org.apache.lucene.store.RAMDirectory@65dcc2a3 lockFactory=MockLockFactoryWrapper(org.apache.lucene.store.SingleInstanceLockFactory@6e8f94)): files: [_1.cfs, _1.cfe, _0.cfe, _0.cfs]\n    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:657)\n    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:534)\n    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:284)\n    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:277)\n    [junit]     at org.apache.lucene.index.TestIndexWriter.assertNoUnreferencedFiles(TestIndexWriter.java:158)\n    [junit]     at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull(TestIndexWriterOnDiskFull.java:114)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1526)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1428)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.index.TestIndexWriterOnDiskFull FAILED\n{code}\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1702",
        "summary": "Thai token type() bug",
        "description": "While adding tests for offsets & type to ThaiAnalyzer, i discovered it does not type Thai numeric digits correctly.\nThaiAnalyzer uses StandardTokenizer, and this is really an issue with the grammar, which adds the entire [:Thai:] block to ALPHANUM.\n\ni propose that alphanum be described a little bit differently in the grammar.\nInstead, [:letter:] should be allowed to have diacritics/signs/combining marks attached to it.\n\nthis would allow the [:thai:] hack to be completely removed, would allow StandardTokenizer to parse complex writing systems such as Indian languages, and would fix LUCENE-1545.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3342",
        "summary": "make frozenbuffereddeletes more efficient for terms",
        "description": "when looking at LUCENE-3340, I thought its also ridiculous how much ram we use for delete by term.\n\nso we can save a lot of memory, especially object overhead by being a little more efficient.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1896",
        "summary": "Modify confusing javadoc for queryNorm",
        "description": "See http://markmail.org/message/arai6silfiktwcer\n\nThe javadoc confuses me as well.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-832",
        "summary": "NPE when calling isCurrent() on a ParallellReader",
        "description": "As demonstrated by the test case below, if you call isCurrent() on a ParallelReader it causes an NPE. Fix appears to be to add an isCurrent() to ParallelReader which calls it on the underlying indexes but I'm not sure what other problems may be lurking here. Do methods such as getVersion(), lastModified(), isOptimized() also have to be rewritten or is this a use case where ParallelReader will never mimic IndexReader perfectly? At the very least this behavior should be documented so others know what to expect.\n\n\n    [junit] Testcase: testIsCurrent(org.apache.lucene.index.TestParallelReader):        Caused an ERROR\n    [junit] null\n    [junit] java.lang.NullPointerException\n    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:502)\n    [junit]     at org.apache.lucene.index.SegmentInfos.readCurrentVersion(SegmentInfos.java:336)\n    [junit]     at org.apache.lucene.index.IndexReader.isCurrent(IndexReader.java:316)\n    [junit]     at org.apache.lucene.index.TestParallelReader.testIsCurrent(TestParallelReader.java:146)\n\n\n\nIndex: src/test/org/apache/lucene/index/TestParallelReader.java\n===================================================================\n--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 518122)\n+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)\n@@ -135,6 +135,15 @@\n       assertEquals(docParallel.get(\"f4\"), docSingle.get(\"f4\"));\n     }\n   }\n+  \n+  public void testIsCurrent() throws IOException {\n+    Directory dir1 = getDir1();\n+    Directory dir2 = getDir2();\n+    ParallelReader pr = new ParallelReader();\n+    pr.add(IndexReader.open(dir1));\n+    pr.add(IndexReader.open(dir2));\n+    assertTrue(pr.isCurrent());\n+  }\n \n   // Fiels 1-4 indexed together:\n   private Searcher single() throws IOException {\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2075",
        "summary": "Share the Term -> TermInfo cache across threads",
        "description": "Right now each thread creates its own (thread private) SimpleLRUCache,\nholding up to 1024 terms.\n\nThis is rather wasteful, since if there are a high number of threads\nthat come through Lucene, you're multiplying the RAM usage.  You're\nalso cutting way back on likelihood of a cache hit (except the known\nmultiple times we lookup a term within-query, which uses one thread).\nIn NRT search we open new SegmentReaders (on tiny segments) often\nwhich each thread must then spend CPU/RAM creating & populating.\n\nNow that we are on 1.5 we can use java.util.concurrent.*, eg\nConcurrentHashMap.  One simple approach could be a double-barrel LRU\ncache, using 2 maps (primary, secondary).  You check the cache by\nfirst checking primary; if that's a miss, you check secondary and if\nyou get a hit you promote it to primary.  Once primary is full you\nclear secondary and swap them.\n\nOr... any other suggested approach?\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1377",
        "summary": "Add HTMLStripReader and WordDelimiterFilter from SOLR",
        "description": "SOLR has two classes HTMLStripReader and WordDelimiterFilter which are very useful for a wide variety of use cases.  It would be good to place them into core Lucene.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2243",
        "summary": "FastVectorHighlighter: support DisjunctionMaxQuery",
        "description": "Add DisjunctionMaxQuery support in FVH. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2803",
        "summary": "FieldCache should not pay attention to deleted docs when creating entries",
        "description": "The FieldCache uses a key that ignores deleted docs, so it's actually a bug to use deleted docs when creating an entry.  It can lead to incorrect values when the same entry is used with a different reader.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2589",
        "summary": "Add a variable-sized int block codec",
        "description": "We already have support for fixed block size int block codecs, making it very simple to create a codec from a int encoder algorithms like FOR/PFOR.\n\nBut algorithms like Simple9/16 are not fixed -- they encode a variable number of adjacent ints at once, depending on the specific values of those ints.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-736",
        "summary": "Sloppy Phrase Scorer matches the doc \"A B C D E\" for query = \"B C B\"~2",
        "description": "This is an extension of https://issues.apache.org/jira/browse/LUCENE-697\n\nIn addition to abnormalities Yonik pointed out in 697, there seem to be other issues with slopy phrase search and scoring.\n\n1) A phrase with a repeated word would be detected in a document although it is not there.\nI.e. document = A B D C E , query = \"B C B\" would not find this document (as expected), but query \"B C B\"~2 would find it. \nI think that no matter how large the slop is, this document should not be a match.\n\n2) A document containing both orders of a query, symmetrically, would score differently for the queru and for its reveresed form.\nI.e. document = A B C B A would score differently for queries \"B C\"~2 and \"C B\"~2, although it is symmetric to both.\n\nI will attach test cases that show both these problems and the one reported by Yonik in 697. ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2053",
        "summary": "When thread is interrupted we should throw a clear exception",
        "description": "This is the 3.0 followon from LUCENE-1573.  We should throw a dedicated exception, not just RuntimeException.\n\nRecent discussion from java-dev \"Thread.interrupt()\" subject: http://www.lucidimagination.com/search/document/8423f9f0b085034e/thread_interrupt",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3100",
        "summary": "IW.commit() writes but fails to fsync the N.fnx file",
        "description": "In making a unit test for NRTCachingDir (LUCENE-3092) I hit this surprising bug!\n\nBecause the new N.fnx file is written at the \"last minute\" along with the segments file, it's not included in the sis.files() that IW uses to figure out which files to sync.\n\nThis bug means one could call IW.commit(), successfully, return, and then the machine could crash and when it comes back up your index could be corrupted.\n\nWe should hopefully first fix TestCrash so that it hits this bug (maybe it needs more/better randomization?), then fix the bug....",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2004",
        "summary": "Constants.LUCENE_MAIN_VERSION is inlined in code compiled against Lucene JAR, so version detection is incorrect",
        "description": "When you compile your own code against the Lucene 2.9 version of the JARs and use the LUCENE_MAIN_VERSION constant and then run the code against the 3.0 JAR, the constant still contains 2.9, because javac inlines primitives and Strings into the class files if they are public static final and are generated by a constant (not method).\n\nThe attached fix will fix this by using a ident(String) functions that return the String itsself to prevent this inlining.\n\nWill apply to 2.9, trunk and 2.9 BW branch. No I can also reenable one test I removed because of this.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3339",
        "summary": "TestNRTThreads hangs in nightly 3.x builds",
        "description": "Maybe we have a problem, maybe its a bug in the test.\n\nBut its strange that lately the 3.x nightlies have been hanging here.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1175",
        "summary": "occasional MergeException while indexing",
        "description": "TestStressIndexing2.testMultiConfig occasionally hits merge exceptions",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2258",
        "summary": "Remove \"synchonized\" from FuzzyTermEnum#similarity(final String target)",
        "description": "The similarity method in FuzzyTermEnum is synchronized which is stupid because of:\n- TermEnums are the iterator pattern and so are single-thread per definition\n- The method is private, so nobody could ever create a fake FuzzyTermEnum just to have this method and use it multithreaded.\n- The method is not static and has no static fields - so instances do not affect each other\n\nThe root of this comes from LUCENE-296, but was never reviewd and simply committed. The argument for making it synchronized is wrong.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2102",
        "summary": "LowerCaseFilter for Turkish language",
        "description": "java.lang.Character.toLowerCase() converts 'I' to 'i' however in Turkish alphabet lowercase of 'I' is not 'i'. It is LATIN SMALL LETTER DOTLESS I.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3295",
        "summary": "BitVector never skips fully populated bytes when writing ClearedDgaps",
        "description": "When writing cleared DGaps in BitVector we compare a byte against 0xFF (255) yet the byte is casted into an int (-1) and the comparison will never succeed. We should mask the byte with 0xFF before comparing or compare against -1",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1833",
        "summary": "When we move to java 1.5 in 3.0 we should replace all Interger, Long, etc construction with .valueOf",
        "description": "-128 to 128 are guaranteed to be cached and using valueOf in that case is 3.5 times faster than using contructor",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1739",
        "summary": "2.4.x index cannot be opened with 2.9-dev",
        "description": "Sorry for the lack of proper testcase.\n\nIn 2.4.1, if you created an index with the (stupid) options below, then it will not create a .prx file. 2.9 expects this file and will not open the index.\nThe reason i used these stupid options is because i changed the field from indexed=yes to indexed=no, but forgot to remove the .setOmitTf()\n\n{code}\npublic class Testcase {\n\tpublic static void main(String args[]) throws Exception {\n\t\t/* run this part with lucene 2.4.1 */\n\t\tIndexWriter iw = new IndexWriter(\"test\", new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n\t\tiw.setUseCompoundFile(false);\n\t\tDocument doc = new Document();\n\t\tField field1 = new Field(\"field1\", \"foo\", Field.Store.YES, Field.Index.NO);\n\t\tfield1.setOmitTf(true); // 2.9 will create a 0-byte .prx file, but 2.4.x will NOT. This is the problem. 2.9 expects this file!\n\t\tdoc.add(field1);\n\t\tiw.addDocument(doc);\n\t\tiw.close(); \n\t\t/* run this with lucene 2.9 */\n\t\tIndexReader ir = IndexReader.open(FSDirectory.getDirectory(\"test\"), true); \n\t}\n}\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2242",
        "summary": "Contrib CharTokenizer classes should be instantiated using their new Version based ctors",
        "description": "Contrib CharTokenizer classes should be instantiated using their new Version based ctors introduced by LUCENE-2183 and LUCENE-2240",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2248",
        "summary": "Tests using Version.LUCENE_CURRENT will produce problems in backwards branch, when development for 3.2 starts",
        "description": "A lot of tests for the most-recent functionality in Lucene use Version.LUCENE_CURRENT, which is fine in trunk, as we use the most recent version without hassle and changing this in later versions.\n\nThe problem is, if we copy these tests to backwards branch after 3.1 is out and then start to improve analyzers, we then will have the maintenance hell for backwards tests. And we loose backward compatibility testing for older versions. If we would specify a specific version like LUCENE_31 in our tests, after moving to backwards they must work without any changes!\n\nTo not always modify all tests after a new version comes out (e.g. after switching to 3.2 dev), I propose to do the following:\n- declare a static final Version TEST_VERSION = Version.LUCENE_CURRENT (or better) Version.LUCENE_31 in LuceneTestCase(4J).\n- change all tests that use Version.LUCENE_CURRENT using eclipse refactor to use this constant and remove unneeded import statements.\n\nWhen we then move the tests to backward we must only change one line, depending on how we define this constant:\n- If in trunk LuceneTestCase it's Version.LUCENE_CURRENT, we just change the backwards branch to use the version numer of the released thing.\n- If trunk already uses the LUCENE_31 constant (I prefer this), we do not need to change backwards, but instead when switching version numbers we just move trunk forward to the next major version (after added to Version enum).",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-804",
        "summary": "build.xml: result of \"dist-src\" should support \"build-contrib\"",
        "description": "Currently the packed src distribution would fail to run \"ant build-contrib\".\nIt would be much nicer if that work.\nIn fact, would be nicer if you could even \"re-pack\" with it.\n\nFor now I marked this for 2.1, although I am not yet sure if this is a stopper.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1200",
        "summary": "IndexWriter.addIndexes* can deadlock in rare cases",
        "description": "In somewhat rare cases it's possible for addIndexes to deadlock\nbecause it is a synchronized method.\n\nNormally the merges that are necessary for addIndexes are done\nserially (with the primary thread) because they involve segments from\nan external directory.  However, if mergeFactor of these merges\ncomplete then a merge becomes necessary for the merged segments, which\nare not external, and so it can run in the background.  If too many BG\nthreads need to run (currently > 4) then the \"pause primary thread\"\napproach adopted in LUCENE-1164 will deadlock, because the addIndexes\nmethod is holding a lock on IndexWriter.\n\nThis was appearing as a intermittant deadlock in the\nTestIndexWriterMerging test case.\n\nThis issue is not present in 2.3 (it was caused by LUCENE-1164).\n\nThe solution is to shrink the scope of synchronization: don't\nsynchronize on the whole method & wrap synchronized(this) in the right\nplaces inside the methods.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2980",
        "summary": "Benchmark's ContentSource should not rely on file suffixes to be lower cased when detecting file type (gzip/bzip2/text)",
        "description": "file.gz is correctly handled as gzip, but file.GZ handled as text which is wrong.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1898",
        "summary": "Decide if we should remove lines numbers from latest Changes",
        "description": "As Lucene dev has grown, a new issue has arisen - many times, new changes invalidate old changes. A proper changes file should just list the changes from the last version, not document the dev life of the issues. Keeping changes in proper order now requires a lot of renumbering sometimes. The numbers have no real meaning and could be added to more rich versions (such as the html version) automatically if desired.\n\nI think an * makes a good replacement myself. The issues already have ids that are stable, rather than the current, decorational numbers which are subject to change over a dev cycle.\n\nI think we should replace the numbers with an asterix for the 2.9 section and going forward (ie 4. becomes *).\n\nIf we don't get consensus very quickly, this issue won't block.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-706",
        "summary": "Index File Format - Example for frequency file .frq is wrong",
        "description": "Reported by Johan Stuyts - http://www.nabble.com/Possible-documentation-error--p7012445.html - \n\nFrequency file example says: \n\n     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: \n         15, 22, 3 \n\nIt should be: \n\n     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: \n         15, 8, 3 \n\n\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2188",
        "summary": "A handy utility class for tracking deprecated overridden methods",
        "description": "This issue provides a new handy utility class that keeps track of overridden deprecated methods in non-final sub classes. This class can be used in new deprecations.\n\nSee the javadocs for an example.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3135",
        "summary": "backport suggest module to branch 3.x",
        "description": "It would be nice to develop a plan to expose the autosuggest functionality to Lucene users in 3.x\n\nThere are some complications, such as seeing if we can backport the FST-based functionality,\nwhich might require a good bit of work. But I think this would be well-worth it.\n",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2295",
        "summary": "Create a MaxFieldLengthAnalyzer to wrap any other Analyzer and provide the same functionality as MaxFieldLength provided on IndexWriter",
        "description": "A spinoff from LUCENE-2294. Instead of asking the user to specify on IndexWriter his requested MFL limit, we can get rid of this setting entirely by providing an Analyzer which will wrap any other Analyzer and its TokenStream with a TokenFilter that keeps track of the number of tokens produced and stop when the limit has reached.\n\nThis will remove any count tracking in IW's indexing, which is done even if I specified UNLIMITED for MFL.\n\nLet's try to do it for 3.1.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2586",
        "summary": "move intblock/sep codecs into test",
        "description": "The intblock and sep codecs in core exist to make it easy for people to try different low-level algos for encoding ints.\n\nSep breaks docs, freqs, pos, skip data, payloads into 5 separate files (vs 2 files that standard codec uses).\n\nIntblock further enables the docs, freqs, pos files to encode fixed-sized blocks of ints at a time.\n\nSo an app can easily \"subclass\" these codecs, using their own int encoder.\n\nBut these codecs are now concrete, and they use dummy low-level block int encoder (eg encoding 128 ints as separate vints).\n\nI'd like to change these to be abstract, and move these dummy codecs into test.\n\nThe tests would still test these dummy codecs, by rotating them in randomly for all tests.\n\nI'd also like to rename IntBlock -> FixedIntBlock, because I'm trying to get a VariableIntBlock working well (for int encoders like Simple9, Simple16, whose block size varies depending on the particular values).\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-849",
        "summary": "contrib/benchmark:  configurable HTML Parser, external classes to path, exhaustive doc maker",
        "description": "\"doc making\" enhancements:\n\n1. Allow configurable html parser, with a new html.parser property.\nCurrently TrecDocMaker is using the Demo html parser. With this new property this can be overridden.\n\n2. allow to add external class path, so the benchmark can be used with modified makers/parsers without having to add code to Lucene.\nRun benchmark with e.g. \"ant run-task -Dbenchmark.ext.classpath=/myproj/myclasses\"\n\n3. allow to crawl a doc maker until exhausting all its files/docs once, without having to know in advance how many docs it can make. \nThis can be useful for instance if the input data is in zip files.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-349",
        "summary": "Documentation for tii and tis files seems to be out of sync with code",
        "description": "The documentation on the .tii file in fileformats.xml seems to be out of sync\nwith the actual code in TermInfosReader.java.\n\nSpecifically, the docs for the TermInfosIndex file seems to leave out several\nfields that are read from the file in the readIndex() method (well, specifically\nthey're read in by the SegmentTermEnum constructor, but you get the idea).",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-488",
        "summary": "adding docs with large (binary) fields of 5mb causes OOM regardless of heap size",
        "description": "as reported by George Washington in a message to java-user@lucene.apache.org with subect \"Storing large text or binary source documents in the index and memory usage\" arround 2006-01-21 there seems to be a problem with adding docs containing really large fields.\n\nI'll attach a test case in a moment, note that (for me) regardless of how big i make my heap size, and regardless of what value I set  MIN_MB to, once it starts trying to make documents of containing 5mb of data, it can only add 9 before it rolls over and dies.\n\nhere's the output from the code as i will attach in a moment...\n\n    [junit] Testsuite: org.apache.lucene.document.TestBigBinary\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 78.656 sec\n\n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: directory will not be cleaned up automatically...\n    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.4mb\n    [junit] iters completed: 100\n    [junit] totalBytes Allocated: 419430400\n    [junit] NOTE: directory will not be cleaned up automatically...\n    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.5mb\n    [junit] iters completed: 9\n    [junit] totalBytes Allocated: 52428800\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testBigBinaryFields(org.apache.lucene.document.TestBigBinary):    Caused an ERROR\n    [junit] Java heap space\n    [junit] java.lang.OutOfMemoryError: Java heap space\n\n\n    [junit] Test org.apache.lucene.document.TestBigBinary FAILED\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-207",
        "summary": "[PATCH] npe if java.io.tmpdir does not exist",
        "description": "In org.apache.lucene.store.FSDirectory from Lucene-1.3-final, on line 170-171:\n\nFile tmpdir = new File(System.getProperty(\"java.io.tmpdir\"));\nfiles = tmpdir.list();\n\nif the directory specified by the property \"java.io.tmpdir\" does not exist, a\nnull pointer exception is thrown.  Perhaps a check to see if the directory\nexists is in order, and if it doesn't, use a directory you know exists (e.g. a\n/temp directory in the directory created earlier in the create() method).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-430",
        "summary": "Reducing buffer sizes for TermDocs.",
        "description": "From java-dev: \n \nOn Friday 09 September 2005 00:34, Doug Cutting wrote: \n> Paul Elschot wrote: \n> > I suppose one of these cases are when many terms are used in a query.  \n> > Would it be easily possible to make the buffer size for a term iterator \n> > depend on the numbers of documents to be iterated? \n> > Many terms only occur in a few documents, so this could be a  \n> > nice win on total buffer size for the many terms case. \n>  \n> This would not be too difficult. \n>  \n> Look in SegmentTermDocs.java. \u00a0The buffer may be allocated when the  \n> parent's stream is first cloned, but clone() won't allocate a buffer if  \n> the source hasn't had a buffer allocated yet, and nothing should perform  \n> i/o directly on the parent's freqStream, so in practice a buffer should  \n> not be allocated until the first read is performed on the clone. \n \nI tried delaying the buffer allocation in BufferedIndexInput by \nusing this clone() method: \n \n\u00a0 public Object clone() { \n\u00a0 \u00a0 BufferedIndexInput clone = (BufferedIndexInput)super.clone(); \n\u00a0 \u00a0 clone.buffer = null; \n\u00a0 \u00a0 clone.bufferLength = 0; \n\u00a0 \u00a0 clone.bufferPosition = 0; \n\u00a0 \u00a0 clone.bufferStart = getFilePointer();  \n\u00a0 \u00a0 return clone; \n\u00a0 } \n \nWith this all term document iterators seem to be empty, no \nquery in the test cases gives any results, for example TestDemo \nand TestBoolean2. \nAs far as I can see, this delaying should work, but it doesn't and \nI have no idea why. \n \nEnd of quote from java-dev. \n \nDoug replied that at a glance this clone method looks good. \nWithout this delayed buffer allocation, a reduced buffer size \nfor TermDocs cannot be implemented easily.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1359",
        "summary": "FrenchAnalyzer's tokenStream method does not honour the contract of Analyzer",
        "description": "In {{Analyzer}} :\n{code}\n/** Creates a TokenStream which tokenizes all the text in the provided\n    Reader.  Default implementation forwards to tokenStream(Reader) for \n    compatibility with older version.  Override to allow Analyzer to choose \n    strategy based on document and/or field.  Must be able to handle null\n    field name for backward compatibility. */\n  public abstract TokenStream tokenStream(String fieldName, Reader reader);\n{code}\n\n\nand in {{FrenchAnalyzer}}\n\n{code}\npublic final TokenStream tokenStream(String fieldName, Reader reader) {\n\n    if (fieldName == null) throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (reader == null) throw new IllegalArgumentException(\"reader must not be null\");\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-541",
        "summary": "The DisjunctionMaxQuery lacks an implementation of extractTerms().",
        "description": "The DisjunctionMaxQuery lacks an implementation of extractTerms(). ",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3742",
        "summary": "SynFilter doesn't set offsets for outputs that hang off the end of the input tokens",
        "description": "If you have syn rule a -> x y and input a then output is a/x y but... what should y's offsets be?  Right now we set to 0/0.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1199",
        "summary": "NullPointerException in IndexModifier.close()",
        "description": "We upgraded from Lucene 2.0.0. to 2.3.1 hoping this would resolve this issue.\n\nhttp://jira.codehaus.org/browse/MRM-715\n\nTrace is as below for Lucene 2.3.1:\njava.lang.NullPointerException\nat org.apache.lucene.index.IndexModifier.close(IndexModifier.java:576)\nat org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.closeQuietly(LuceneRepositoryContentIndex.java:416)\nat org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.modifyRecord(LuceneRepositoryContentIndex.java:152)\nat org.apache.maven.archiva.consumers.lucene.IndexContentConsumer.processFile(IndexContentConsumer.java:169)\nat org.apache.maven.archiva.repository.scanner.functors.ConsumerProcessFileClosure.execute(ConsumerProcessFileClosure.java:51)\nat org.apache.commons.collections.functors.IfClosure.execute(IfClosure.java:117)\nat org.apache.commons.collections.CollectionUtils.forAllDo(CollectionUtils.java:388)\nat org.apache.maven.archiva.repository.scanner.RepositoryContentConsumers.executeConsumers(RepositoryContentConsumers.java:283)\nat org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.transferFile(DefaultRepositoryProxyConnectors.java:597)\nat org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.fetchFromProxies(DefaultRepositoryProxyConnectors.java:157)\nat org.apache.maven.archiva.web.repository.ProxiedDavServer.applyServerSideRelocation(ProxiedDavServer.java:447)\nat org.apache.maven.archiva.web.repository.ProxiedDavServer.fetchContentFromProxies(ProxiedDavServer.java:354)\nat org.apache.maven.archiva.web.repository.ProxiedDavServer.process(ProxiedDavServer.java:189)\nat org.codehaus.plexus.webdav.servlet.multiplexed.MultiplexedWebDavServlet.service(MultiplexedWebDavServlet.java:119)\nat org.apache.maven.archiva.web.repository.RepositoryServlet.service(RepositoryServlet.java:155)\nat javax.servlet.http.HttpServlet.service(HttpServlet.java:803)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-753",
        "summary": "Use NIO positional read to avoid synchronization in FSIndexInput",
        "description": "As suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file.\nThis could mitigate any MT performance drop caused by reducing the number of files in the index format.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2500",
        "summary": "A Linux-specific Directory impl that bypasses the buffer cache",
        "description": "I've been testing how we could prevent Lucene's merges from evicting\npages from the OS's buffer cache.  I tried fadvise/madvise (via JNI)\nbut (frustratingly), I could not get them to work (details at\nhttp://chbits.blogspot.com/2010/06/lucene-and-fadvisemadvise.html).\n\nThe only thing that worked was to use Linux's O_DIRECT flag, which\nforces all IO to bypass the buffer cache entirely... so I created a\nLinux-specific Directory impl to do this.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1887",
        "summary": "o.a.l.messages should be moved to core",
        "description": "contrib/queryParser contains an org.apache.lucene.messages package containing some generallized code that (claims in it's javadocs) is not specific to the queryParser.\n\nIf this is truely general purpose code, it should probably be moved out of hte queryParser contrib -- either into it's own contrib, or into the core (it's very small)\n\n*EDIT:* alternate suggestion to rename package to fall under the o.a.l.queryParser namespace retracted due to comments in favor of (eventually) promoting to it's own contrib",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-693",
        "summary": "ConjunctionScorer - more tuneup",
        "description": "(See also: #LUCENE-443)\nI did some profile testing with the new ConjuctionScorer in 2.1 and discovered a new bottleneck in ConjunctionScorer.sortScorers. The java.utils.Arrays.sort method is cloning the Scorers array on every sort, which is quite expensive on large indexes because of the size of the 'norms' array within, and isn't necessary. \n\nHere is one possible solution:\n\n  private void sortScorers() {\n// squeeze the array down for the sort\n//    if (length != scorers.length) {\n//      Scorer[] temps = new Scorer[length];\n//      System.arraycopy(scorers, 0, temps, 0, length);\n//      scorers = temps;\n//    }\n    insertionSort( scorers,length );\n    // note that this comparator is not consistent with equals!\n//    Arrays.sort(scorers, new Comparator() {         // sort the array\n//        public int compare(Object o1, Object o2) {\n//          return ((Scorer)o1).doc() - ((Scorer)o2).doc();\n//        }\n//      });\n  \n    first = 0;\n    last = length - 1;\n  }\n  private void insertionSort( Scorer[] scores, int len)\n  {\n      for (int i=0; i<len; i++) {\n          for (int j=i; j>0 && scores[j-1].doc() > scores[j].doc();j-- ) {\n              swap (scores, j, j-1);\n          }\n      }\n      return;\n  }\n  private void swap(Object[] x, int a, int b) {\n    Object t = x[a];\n    x[a] = x[b];\n    x[b] = t;\n  }\n \nThe squeezing of the array is no longer needed. \nWe also initialized the Scorers array to 8 (instead of 2) to avoid having to grow the array for common queries, although this probably has less performance impact.\n\nThis change added about 3% to query throughput in my testing.\n\nPeter\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3661",
        "summary": "move deletes under codec",
        "description": "After LUCENE-3631, this should be easier I think.\n\nI haven't looked at it much myself but i'll play around a bit, but at a glance:\n* SegmentReader to have Bits liveDocs instead of BitVector\n* address the TODO in the IW-using ctors so that SegmentReader doesn't take a parent but just an existing core.\n* we need some type of minimal \"MutableBits\" or similar subinterface of bits. BitVector and maybe Fixed/OpenBitSet could implement it\n* BitVector becomes an impl detail and moves to codec (maybe we have a shared base class and split the 3.x/4.x up rather than the conditional backwards)\n* I think the invertAll should not be used by IndexWriter, instead we define the codec interface to say \"give me a new MutableBits, by default all are set\" ?\n* redundant internally-consistent checks in checkLiveCounts should be done in the codec impl instead of in SegmentReader.\n* plain text impl in SimpleText.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1829",
        "summary": "'ant javacc' in root project should also properly create contrib/queryparser Java files",
        "description": "'ant javacc' in the project root doesn't run javacc in contrib/queryparser\n'ant javacc' in contrib/queryparser does not properly create the Java files. What still needs to be done by hand is (partly!) described in contrib/queryparser/README.javacc. I think this process should be automated. Patch provided.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3717",
        "summary": "Add fake charfilter to BaseTokenStreamTestCase to find offsets bugs",
        "description": "Recently lots of issues have been fixed about broken offsets, but it would be nice to improve the\ntest coverage and test that they work across the board (especially with charfilters).\n\nin BaseTokenStreamTestCase.checkRandomData, we can sometimes pass the analyzer a reader wrapped\nin a \"MockCharFilter\" (the one in the patch sometimes doubles characters). If the analyzer does\nnot call correctOffsets or does incorrect \"offset math\" (LUCENE-3642, etc) then eventually\nthis will create offsets and the test will fail.\n\nOther than tests bugs, this found 2 real bugs: ICUTokenizer did not call correctOffset() in its end(),\nand ThaiWordFilter did incorrect offset math.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3213",
        "summary": "Use AtomicReaderContext also for CustomScoreProvider",
        "description": "When moving to AtomicReaderContext, one place was not changed to use it: CustomScoreQuery's CustomScoreProvider. It should also take AtomicReaderContext instead of IndexReader, as this may help users to effectively implement custom scoring there absolute DocIds are needed.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2218",
        "summary": "ShingleFilter improvements",
        "description": "ShingleFilter should allow configuration of minimum shingle size (in addition to maximum shingle size), so that it's possible to (e.g.) output only trigrams instead of bigrams mixed with trigrams.  The token separator used in composing shingles should be configurable too.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-361",
        "summary": "FieldSortedHitQueue.lessThan() should not be final",
        "description": "The final seems to provide little benefit and it takes away the ability to\nspecialize this method (which I need to do, forcing a customization of Lucene to\nremove the final).",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3634",
        "summary": "remove old static main methods in core",
        "description": "We have a few random static main methods that I think are very rarely used... we should remove them (IndexReader, UTF32ToUTF8, English).\n\nThe IndexReader main lets you list / extract the sub-files from a CFS... I think we should move this to a new tool in contrib/misc.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3650",
        "summary": "move o.a.l.index.codecs.* -> o.a.l.codecs.*",
        "description": "These package names are getting pretty long, e.g.:\n\norg.apache.lucene.index.codecs.lucene40.values.XXXXYYYY\n\nI think we should move it to just the codecs package now while it won't cause anyone any trouble.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3578",
        "summary": "TestSort testParallelMultiSort reproducible seed failure",
        "description": "trunk r1202157\n{code}\n    [junit] Testsuite: org.apache.lucene.search.TestSort\n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.978 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSort -Dtestmethod=testParallelMultiSort -Dtests.seed=-2996f3e0f5d118c2:32c8e62dd9611f63:7a90f44586ae8263 -Dargs=\"-Dfile.encoding=UTF-8\"\n    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-1,5,main]\n    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-2,5,main]\n    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-3,5,main]\n    [junit] NOTE: test params are: codec=Lucene40: {short=Lucene40(minBlockSize=98 maxBlockSize=214), contents=PostingsFormat(name=MockSep), byte=PostingsFormat(name=SimpleText), int=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), string=PostingsFormat(name=NestedPulsing), i18n=Lucene40(minBlockSize=98 maxBlockSize=214), long=PostingsFormat(name=Memory), double=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), parser=MockVariableIntBlock(baseBlockSize=88), float=Lucene40(minBlockSize=98 maxBlockSize=214), custom=PostingsFormat(name=MockRandom)}, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {short=BM25(k1=1.2,b=0.75), tracer=DFR I(ne)B2, byte=DFR I(ne)B3(800.0), contents=IB LL-LZ(0.3), int=DFR I(n)BZ(0.3), string=IB LL-D3(800.0), i18n=DFR GB2, double=DFR I(ne)B2, long=DFR GB1, parser=DFR GL2, float=BM25(k1=1.2,b=0.75), custom=DFR I(ne)Z(0.3)}, locale=ga_IE, timezone=America/Louisville\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestSort]\n    [junit] NOTE: Linux 3.0.6-gentoo amd64/Sun Microsystems Inc. 1.6.0_29 (64-bit)/cpus=8,threads=4,free=78022136,total=125632512\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testParallelMultiSort(org.apache.lucene.search.TestSort): FAILED\n    [junit] expected:<[ZJ]I> but was:<[JZ]I>\n    [junit] junit.framework.AssertionFailedError: expected:<[ZJ]I> but was:<[JZ]I>\n    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1245)\n    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)\n    [junit]     at org.apache.lucene.search.TestSort.runMultiSorts(TestSort.java:1202)\n    [junit]     at org.apache.lucene.search.TestSort.testParallelMultiSort(TestSort.java:855)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)\n    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)\n    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.search.TestSort FAILED\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1191",
        "summary": "If IndexWriter hits OutOfMemoryError it should not commit",
        "description": "While progress has been made making IndexWriter robust to OOME, I\nthink there is still a real risk that an OOME at a bad time could put\nIndexWriter into a bad state such that if close() is called and\nsomehow it succeeds without hitting another OOME, it risks\nintroducing messing up the index.\n\nI'd like to detect if OOME has been hit in any of the methods that\nalter IW's state, and if so, do not commit changes to the index.  If\nclose is called after hitting OOME, I think writer should instead\nabort.\n\nAttached patch just adds try/catch clauses to catch OOME, note that\nit was hit, and re-throw it.  Then, sync() refuses to commit a new\nsegments_N if OOME was hit, and close instead calls abort when OOME\nwas hit.  All tests pass.  I plan to commit in a day or two.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2740",
        "summary": "PerFieldCodecWrapper causes crashes if not all per field codes have been used",
        "description": "If a PerFieldCodecWrapper is used an SegmentMerger tries to merge two segments where one segment only has a subset of the field PerFieldCodecWrapper defines SegmentMerger tries to open non-existing files since Codec#files(Directory, SegmentInfo, Set<String>) blindly copies the expected files into the given set. This also hits exceptions in CheckIndex and addIndexes(). \nThe reason for this is that PerFieldCodecWrapper simply iterates over the codecs it knows and adds all files without checking if they are present in the given Directory. We need to have some mechnanism that check if the \"required\" files for a codec are present and only add the files to the set if that field is really there.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-296",
        "summary": "[PATCH] FuzzyTermEnum optimization and refactor",
        "description": "I took a look at it to see if it could be improved.  I saw speed improvements of\n20% - 40% by making a couple changes.  \n\nThe patch is here: http://www.hagerfamily.com/patches/FuzzyTermEnumOptimizePatch.txt\n\nThe Patch is based on the HEAD of the CVS tree as of Oct 22, 2004.\n\nWhat Changed?\n\nSince the word was discarded if the edit distance for the word was\nabove a certain threshold, I updated the distance algorithm to abort\nif at any time during the calculation it is determined that the best\npossible outcome of the edit distance algorithm is above this\nthreshold.  The source code has a great explanation.\n\nI also reduced the amount of floating point math, reduced the amount\nof potential space the array takes in its first dimension, removed the\npotential divide by 0 error when one term is an empty string, and\nfixed a bug where an IllegalArgumentException was thrown if the class\nwas somehow initialized wrong, instead of looking at the arguments.\n\nThe behavior is almost identical.  The exception is that similarity is\nset to 0.0 when it is guaranteed to be below the minimum similarity.\n\nResults\n\nI saw the biggest improvement from longer words, which makes a sense.\nMy long word was \"bridgetown\" and I saw a 60% improvement on this.\nThe biggest improvement are for words that are farthest away from the\nmedian length of the words in the index.  Short words (1-3 characters)\nsaw a 30% improvement.  Medium words saw a 10% improvement (5-7\ncharacters).  These improvements are with the prefix set to 0.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2643",
        "summary": "StringHelper#stringDifference is wrong about supplementary chars ",
        "description": "StringHelper#stringDifference does not take supplementary characters into account. Since this is not used internally at all we should think about removing it but I guess since it is not too complex we should just or fix it for bwcompat reasons. For released versions we should really fix it since folks might use it though. For trunk we could just drop it.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2582",
        "summary": "allow an alg file to specify the default codec",
        "description": "I already committed this one by accident so I better open an issue!\n\nI added this:\n\n  default.codec = Pulsing\n\nso that your alg file can specify the codec to be used when writing new segments in an index.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-834",
        "summary": "Payload Queries",
        "description": "Now that payloads have been implemented, it will be good to make them searchable via one or more Query mechanisms.  See http://wiki.apache.org/lucene-java/Payload_Planning for some background information and https://issues.apache.org/jira/browse/LUCENE-755 for the issue that started it all.  ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1868",
        "summary": "update NOTICE.txt",
        "description": "From the java-dev discussion, NOTICE.txt should be up-to-date.\n\nOne thing I know, is that the persian stopwords file (analyzers/fa) came from the same source as the arabic stopwords file, and is BSD-licensed. \n\nThere might be others (I think ICU has already been added)",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-450",
        "summary": "MatchAllDocsQuery doesn't honor boost or queryNorm",
        "description": "MatchAllDocsQuery doesn't pay attention to either it's own boost, or lucene's query normalization factor.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3596",
        "summary": "DirectoryTaxonomyWriter extensions should be able to set internal index writer config attributes such as info stream",
        "description": "Current protected openIndexWriter(Directory directory, OpenMode openMode) does not provide access to the IWC it creates.\nSo extensions must reimplement this method completely in order to set e.f. info stream for the internal index writer.\nThis came up in [user question: Taxonomy indexer debug |http://lucene.472066.n3.nabble.com/Taxonomy-indexer-debug-td3533341.html]",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-756",
        "summary": "Maintain norms in a single file .nrm",
        "description": "Non-compound indexes are ~10% faster at indexing, and perform 50% IO activity comparing to compound indexes. But their file descriptors foot print is much higher. \n\nBy maintaining all field norms in a single .nrm file, we can bound the number of files used by non compound indexes, and possibly allow more applications to use this format.\n\nMore details on the motivation for this in: http://www.nabble.com/potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-tf2826909.html (in particular http://www.nabble.com/Re%3A-potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-p7910403.html).\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2404",
        "summary": "Improve speed of ThaiWordFilter by CharacterIterator, factor out LowerCasing and also fix some bugs (empty tokens stop iteration)",
        "description": "The ThaiWordFilter creates new Strings out of term buffer before passing to The BreakIterator., But BreakIterator can take a CharacterIterator and directly process on it without buffer copying.\nAs Java itsself does not provide a CharacterIterator implementation in java.text, we can use the javax.swing.text.Segment class, that operates on a char[] and is even reuseable! This class is very strange but it works and is in JDK 1.4+ and not deprecated.\n\nThe filter also had a bug: It stopped iterating tokens when an empty token occurred. Also the lowercasing for non-thai words was removed and put into the Analyzer by adding LowerCaseFilter.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3480",
        "summary": "refactoring of docvalues params in Codec.java",
        "description": "While working on LUCENE-2621 I am trying to do some cleanup of the Codec APIs, currently Codec.java has a boolean for getDocValuesUseCFS()\n\nI think this is an impl detail that should not be in Codec.java: e.g. i might make a SimpleText impl that uses only 1 file and then the param\nis awkward.\n\nSo, instead I created Sep impls that dont use CFS (use separate files) and placed them under the sep package, if you don't want to use\nCFS you can just use these implementations in your codec.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1971",
        "summary": "Remove deprecated RangeQuery classes",
        "description": "Remove deprecated RangeQuery classes",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1968",
        "summary": "Remove deprecated methods in PriorityQueue",
        "description": "",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2874",
        "summary": "Highlighting overlapping tokens outputs doubled words",
        "description": "If for the text \"the fox did not jump\" we generate following tokens :\n(the, 0, 0-3),({fox},0,0-7),(fox,1,4-7),(did,2,8-11),(not,3,12,15),(jump,4,16,18)\n\nIf TermVector for field is stored WITH_OFFSETS and not WITH_POSITIONS_OFFSETS, highlighing would output\n\"the<em>the fox</em> did not jump\"\n\nI join a patch with 2 additive JUnit tests and a fix of TokenSources class where token ordering by offset did'nt manage well overlapping tokens.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2155",
        "summary": "random localization test failures",
        "description": "Some tests fail randomly (hard to reproduce). It appears to me that this is caused by uninitialized date fields. For example Uwe reported a failure today in this test of TestQueryParser:\n\n{code}\n /** for testing legacy DateField support */\n  public void testLegacyDateRange() throws Exception {\n    String startDate = getLocalizedDate(2002, 1, 1, false);\n    String endDate = getLocalizedDate(2002, 1, 4, false);\n{code}\n\nif you look at the helper getLocalizedDate, you can see if the 4th argument is false, it does not initialize all date field functions.\n{code}\n  private String getLocalizedDate(int year, int month, int day, boolean extendLastDate) {\n Calendar calendar = new GregorianCalendar();\n calendar.set(year, month, day);\n if (extendLastDate) {\n      calendar.set(Calendar.HOUR_OF_DAY, 23);\n      calendar.set(Calendar.MINUTE, 59);\n      calendar.set(Calendar.SECOND, 59);\n ...\n}\n{code}\n\nI think the solution to this is that in all tests, whereever we create new GregorianCalendar(), it should be followed by a call to Calendar.clear().\nThis will ensure that we always initialize unused calendar fields to zero, rather than being dependent on the local time.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1893",
        "summary": "All spatial contrib shape classes implement equals but not hashCode",
        "description": "violates contract - at a min, need to implement return constant.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3576",
        "summary": "TestBackwardsCompatibility needs terms with U+E000 to U+FFFF",
        "description": "we changed sort order in 4.0, and have sophisticated backwards compatibility (e.g. surrogates dance),\nbut we don't test this at all in TestBackwardsCompatibility.\n\nfor example, nothing handles this case for term vectors...",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3356",
        "summary": "trunk TestRollingUpdates.testRollingUpdates seed failure",
        "description": "trunk r1152892\nreproducable: always\n\n{code}\njunit-sequential:\n    [junit] Testsuite: org.apache.lucene.index.TestRollingUpdates\n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 1.168 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestRollingUpdates -Dtestmethod=testRollingUpdates -Dtests.seed=-5322802004404580273:-4001225075726350391\n    [junit] WARNING: test method: 'testRollingUpdates' left thread running: merge thread: _c(4.0):cv3/2 _h(4.0):cv3 into _k\n    [junit] RESOURCE LEAK: test method: 'testRollingUpdates' left 1 thread(s) running\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {docid=Standard, body=SimpleText, title=MockSep, titleTokenized=Pulsing(freqCutoff=20), date=MockFixedIntBlock(blockSize=1474)}, locale=lv_LV, timezone=Pacific/Fiji\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestRollingUpdates]\n    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=128782656,total=158400512\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testRollingUpdates(org.apache.lucene.index.TestRollingUpdates):   FAILED\n    [junit] expected:<20> but was:<21>\n    [junit] junit.framework.AssertionFailedError: expected:<20> but was:<21>\n    [junit]     at org.apache.lucene.index.TestRollingUpdates.testRollingUpdates(TestRollingUpdates.java:76)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1522)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1427)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.index.TestRollingUpdates FAILED\n\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1880",
        "summary": "Make contrib/collation/(ICU)CollationKeyAnalyzer constructors public",
        "description": "In contrib/collation, the constructors for CollationKeyAnalyzer and ICUCollationKeyAnalyzer are package private, and so are effectively unusable.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3467",
        "summary": "Cut over numeric docvalues to fixed straight bytes",
        "description": "Currently numeric docvalues types are encoded and stored individually which creates massive duplication of writing / indexing code. Yet, almost all of them (except packed ints) are essentially a fixed straight bytes variant. ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3423",
        "summary": "add Terms.docCount",
        "description": "spinoff from LUCENE-3290, where yonik mentioned:\n\n{noformat}\nIs there currently a way to get the number of documents that have a value in the field?\nThen one could compute the average length of a (sparse) field via sumTotalTermFreq(field)/docsWithField(field)\ndocsWithField(field) would be useful in other contexts that want to know how sparse a field is (automatically selecting faceting algorithms, etc).\n{noformat}\n\nI think this is a useful stat to add, in case you have sparse fields for heuristics or scoring.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1806",
        "summary": "Add args to test-macro",
        "description": "Add passing args to JUnit.  (Like Solr and mainly for debugging).  ",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3202",
        "summary": "Add DataInput/DataOutput subclasses that delegate to an InputStream/OutputStream.",
        "description": "Such classes would be handy for FST serialization/deserialization.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2021",
        "summary": "French elision filter should use CharArraySet",
        "description": "French elision filter creates new strings, lowercases them, etc just to check against a Set<String>.\ntrivial patch to use chararrayset instead.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-225",
        "summary": "ClassCastException MultiReader",
        "description": "(See original message below)\nSure.  'Bugzilla it', please.\n\nOtis\nP.S.\nThat line 274 should be line 273 in the CVS HEAD as of now.\n\n--- Rasik Pandey <rasik.pandey@ajlsm.com> wrote:\n> Howdy,\n> \n> This exception was thrown with 1.4rc3. Do you need a test case for \n> this one?\n> \n> java.lang.ClassCastException\n>         at\n> org.apache.lucene.index.MultiTermEnum.<init>(MultiReader.java:274)\n>         at\n> org.apache.lucene.index.MultiReader.terms(MultiReader.java:187)\n> \n> \n> Regards,\n> RBP\n> \n> \n> \n> ---------------------------------------------------------------------\n> To unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org\n> For additional commands, e-mail: lucene-dev-help@jakarta.apache.org\n> \n\n\n---------------------------------------------------------------------\nTo unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org\nFor additional commands, e-mail: lucene-dev-help@jakarta.apache.org",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-620",
        "summary": "GData Server - TestCase Deadlock  StorageModifier",
        "description": "Solfed the racecondition deadlock while closing the StorageController.\nThis occured the first time hossman tried to run the test cases. \n\nConcurrent Modification Exception while iteration over a collection in a sepereate thread -- ModifiedEntryFilter replaced list with array.\n\n@Hossman if you can get to it, could you try the testcases again.\n\n@all If you guys do have time you could run the testcases on different environment, that would help to resolve bugs in the test cases and the server.\n\nsimon",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-952",
        "summary": "GData's TestGdataIndexer.testDestroy() intermittently hits spin loop & causes build timeout",
        "description": "Several nightly builds (at least #136, #143 and #144) have failed due\nto timeout at 45 minutes while running the TestGdataIndexer.testDestroy()\ntest case.\n\nI tracked it down to this line:\n\n      // wait active for the commit\n      while(this.indexer.writer != null){}\n\nIntermittently, that while loop will spin forever.  I can only get the\nfailure to happen on Linux: it doesn't happen on Mac OS X (haven't\ntried windows).  The nightly build runs on Solaris 10, so it also\nhappens there.\n\nIt turns out, this is due to the fact that \"writer\" is not declared as\n\"volatile\".  This is because one thread is closing the indexer, which\nsets writer to null, but another thread is running the while loop.\nIf this.indexer.writer was set to null before that while loop starts,\nthe test will run through fine; else, it won't.\n\nI plan to fix this by adding this method to GDataIndexer class:\n\n    // Used only for testing\n    protected synchronized IndexWriter getWriter() {\n      return this.writer;\n    }\n\nand changing unit test to call that method.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-317",
        "summary": "[PATCH] When locks are disabled, IndexWriter.close() throws NullPointerException",
        "description": "If locks are disabled (via setting the System property 'disableLuceneLocks' to\ntrue), IndexWriter throws a NullPointerException on closing. The reason is that\nthe attempt to call writeLock.release() fails because writeLock is null.\nTo correct this, just check for this case before releasing. A (trivial) patch is\nattached.",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3584",
        "summary": "bulk postings should be codec private",
        "description": "In LUCENE-2723, a lot of work was done to speed up Lucene's bulk postings read API.\n\nThere were some upsides:\n* you could specify things like 'i dont care about frequency data up front'.\n  This made things like multitermquery->filter and other consumers that don't\n  care about freqs faster. But this is unrelated to 'bulkness' and we have a\n  separate patch now for this on LUCENE-2929.\n* the buffersize for standardcodec was increased to 128, increasing performance\n  for TermQueries, but this was unrelated too.\n\nBut there were serious downsides/nocommits:\n* the API was hairy because it tried to be 'one-size-fits-all'. This made consumer code crazy.\n* the API could not really be specialized to your codec: e.g. could never take advantage that e.g. docs and freqs are aligned.\n* the API forced codecs to implement delta encoding for things like documents and positions. \n  But this is totally up to the codec how it wants to encode! Some codecs might not use delta encoding.\n* using such an API for positions was only theoretical, it would have been super complicated and I doubt ever\n  performant or maintainable.\n* there was a regression with advance(), probably because the api forced you to do both a linear scan thru\n  the remaining buffer, then refill...\n\nI think a cleaner approach is to let codecs do whatever they want to implement the DISI\ncontract. This lets codecs have the freedom to implement whatever compression/buffering they want\nfor the best performance, and keeps consumers simple. If a codec uses delta encoding, or if it wants\nto defer this to the last possible minute or do it at decode time, thats its own business. Maybe a codec\ndoesn't want to do any buffering at all.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-946",
        "summary": "replace text from an online collection (used in few test cases) with text that is surely 100% free.",
        "description": "Text from an online firstaid collection (firstaid . ie . eu . org)  is used as arbitrary text for test documents creation, in:\n   o.a.l.analysis.Analyzer.FunctionTestSetup.DOC_TEXT_LINES\n   o.a.l.benchmark.byTask.feeds.SimpleDocMaker.DOC_TEXT\n\nI once got this text from Project Gutenberg and was sure that it is free. But now the referred Web site does not seem to respond, and I can no more find that firstaid eBook in the Project Gutenberg site.\n\nSince it doesn't matter what text we use there, I will just replace that with some of my own words...",
        "label": "NUG",
        "classified": "OTHER",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1600",
        "summary": "Reduce usage of String.intern(), performance is terrible",
        "description": "I profiled a simple MatchAllDocsQuery() against ~1.5 million documents (8 fields of short text, Field.Store.YES,Field.Index.NOT_ANALYZED_NO_NORMS), then retrieved all documents via searcher.doc(i, fs). String.intern() showed up as a top hotspot (see attached screenshot), so i implemented a small optimization to not intern() for every new Field(), instead forcing the intern in the FieldInfos class and adding a optional \"internName\" constructor to Field. This reduced execution time for searching and iterating through all documents by 35%. Results were similar for -server and -client.\n\n\nTRUNK (2.9) w/out patch: matched 1435563 in 8884 ms/search\nTRUNK (2.9) w/patch: matched 1435563 in 5786 ms/search",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3598",
        "summary": "Improve InfoStream class in trunk to be more consistent with logging-frameworks like slf4j/log4j/commons-logging",
        "description": "Followup on a [thread by Shai Erea on java-dev@lao|http://lucene.472066.n3.nabble.com/IndexWriter-infoStream-is-final-td3537485.html]: I already discussed with Robert about that, that there is one thing missing. Currently the IW only checks if the infoStream!=null and then passes the message to the method, and that *may* ignore it. For your requirement it is the case that this is enabled or disabled dynamically. Unfortunately if the construction of the message is heavy, then this wastes resources.\n\nI would like to add another method to this class: abstract boolean isEnabled() that can also be implemented. I would then replace all null checks in IW by this method. The default config in IW would be changed to use a NoOutputInfoStream that returns false here and ignores the message.\n\nA simple logger wrapper for e.g. log4j / slf4j then could look like (ignoring component, could be enabled):\n\n{code:java}\nLoger log = YourLoggingFramework.getLogger(IndexWriter.class);\n\npublic void message(String component, String message) {\n  log.debug(component + \": \" + message);\n}\n\npublic boolean isEnabled(String component) {\n  return log.isDebugEnabled();\n}\n{code}\n\nUsing this you could enable/disable logging live by e.g. the log4j management console of your app server by enabling/disabling IndexWriter.class logging.\n\nThe changes are really simple:\n- PrintStreamInfoStream returns true, always, mabye make it dynamically enable/disable to allow Shai's request\n- infoStream.getDefault() is never null and can never be set to null. Instead the default is a singleton NoOutputInfoStream that returns false of isEnabled(component).\n- All null checks on infoStream should be replaced by infoStream.isEanbled(component), this is possible as always != null. There are no slowdowns by this - it's like Collections.emptyList() instead stupid null checks.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3622",
        "summary": "separate IndexDocValues interface from implementation",
        "description": "Currently the o.a.l.index.values contains both the abstract apis and Lucene40's current implementation.\n\nI think we should move the implementation underneath Lucene40Codec, leaving only the abstract apis.\n\nFor example, simpletext might have a different implementation, and we might make a int8 implementation\nunderneath preflexcodec to support norms.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-781",
        "summary": "NPE in MultiReader.isCurrent() and getVersion()",
        "description": "I'm attaching a fix for the NPE in MultiReader.isCurrent() plus a testcase. For getVersion(), we should throw a better exception that NPE. I will commit unless someone objects or has a better idea.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2067",
        "summary": "Czech Stemmer",
        "description": "Currently, the CzechAnalyzer is merely stopwords, and there isn't a czech stemmer in snowball.\n\nThis patch implements the light stemming algorithm described in: http://portal.acm.org/citation.cfm?id=1598600\n\nIn their measurements, it improves MAP 42%\n\nThe analyzer does not use this stemmer if LUCENE_VERSION <= 3.0, for back compat.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3271",
        "summary": "Move 'good' contrib/queries classes to Queries module",
        "description": "With the Queries module now filled with the FunctionQuery stuff, we should look at closing down contrib/queries.  While not a huge contrib, it contains a number of pretty useful classes and some that should go elsewhere.\n\nHeres my proposed plan:\n\n- similar.* -> suggest module\n- regex.* -> queries module\n- BooleanFilter -> queries module under .filters package\n- BoostingQuery -> queries module\n- ChainedFilter -> queries module under .filters package\n- DuplicateFilter -> queries module under .filters package\n- FieldCacheRewriteMethod -> This doesn't belong in this contrib or the queries module.  I think we should push it to contrib/misc for the time being.  It seems to have quite a few constraints on when its useful.  If indeed CONSTANT_SCORE_AUTO rewrite is better, then I dont see a purpose for it.\n- FilterClause -> class inside BooleanFilter\n- FuzzyLikeThisQuery -> suggest module. This class seems a mess with its Similarity hardcoded.  With all that said, it does seem to do what it claims and with some cleanup, it could be good.\n- TermsFilter -> queries module under .filters package\n- SlowCollated* -> They can stay in the module till we have a better place to nuke them.\n\nOne of the implications of the above moves, is that the xml-query-parser, which supports many of the queries, will need to have a dependency on the queries module.  But that seems unavoidable at this stage.\n\n\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1957",
        "summary": "Remove deprecated Filter.bits() and make Filter.getDocIdSet() abstract.",
        "description": "",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1546",
        "summary": "Add IndexReader.flush(commitUserData)",
        "description": "IndexWriter offers a commit(String commitUserData) method.\nIndexReader can commit as well using the flush/close methods and so\nneeds an analogous method that accepts commitUserData.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1383",
        "summary": "Work around ThreadLocal's \"leak\"",
        "description": "Java's ThreadLocal is dangerous to use because it is able to take a\nsurprisingly very long time to release references to the values you\nstore in it.  Even when a ThreadLocal instance itself is GC'd, hard\nreferences to the values you had stored in it are easily kept for\nquite some time later.\n\nWhile this is not technically a \"memory leak\", because eventually\n(when the underlying Map that stores the values cleans up its \"stale\"\nreferences) the hard reference will be cleared, and GC can proceed,\nits end behavior is not different from a memory leak in that under the\nright situation you can easily tie up far more memory than you'd\nexpect, and then hit unexpected OOM error despite allocating an\nextremely large heap to your JVM.\n\nLucene users have hit this many times.  Here's the most recent thread:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200809.mbox/%3C6e3ae6310809091157j7a9fe46bxcc31f6e63305fcdc%40mail.gmail.com%3E\n\nAnd here's another:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3CF5FC94B2-E5C7-40C0-8B73-E12245B91CEE%40mikemccandless.com%3E\n\nAnd then there's LUCENE-436 and LUCENE-529 at least.\n\nA google search for \"ThreadLocal leak\" yields many compelling hits.\n\nSun does this for performance reasons, but I think it's a terrible\ntrap and we should work around it with Lucene.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2289",
        "summary": "Calls to SegmentInfos.message should be wrapped w/ infoStream != null checks",
        "description": "To avoid the expensive message creation (which involves the '+' operator on strings, calls to message should be wrapped w/ infoStream != null check, rather than inside message(). I'll attach a patch which does that.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3645",
        "summary": "Remove unnecessary array wrapping when calling varargs methods",
        "description": "varargs method callers don't have to wrap args in arrays",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2795",
        "summary": "Genericize DirectIOLinuxDir -> UnixDir",
        "description": "Today DirectIOLinuxDir is tricky/dangerous to use, because you only want to use it for indexWriter and not IndexReader (searching).  It's a trap.\n\nBut, once we do LUCENE-2793, we can make it fully general purpose because then a single native Dir impl can be used.\n\nI'd also like to make it generic to other Unices, if we can, so that it becomes UnixDirectory.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-587",
        "summary": "Explanation.toHtml outputs invalid HTML",
        "description": "If you want an HTML representation of an Explanation, you might call the toHtml() method.  However, the output of this method looks like the following:\n\n<ul>\n  <li>some value = some description</li>\n  <ul>\n    <li>some nested value = some description</li>\n  </ul>\n</ul>\n\nAs it is illegal in HTML to nest a UL directly inside a UL, this method will always output unparseable HTML if there are nested explanations.\n\nWhat Lucene probably means to output is the following, which is valid HTML:\n\n<ul>\n  <li>some value = some description\n    <ul>\n      <li>some nested value = some description</li>\n    </ul>\n  </li>\n</ul>\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2144",
        "summary": "InstantiatedIndexReader does not handle #termDocs(null) correct (AllTermDocs)",
        "description": "This patch contains core changes so someone else needs to commit it.\n\nDue to the incompatible #termDocs(null) behaviour at least MatchAllDocsQuery, FieldCacheRangeFilter and ValueSourceQuery fails using II since 2.9.\n\nAllTermDocs now has a superclass, AbstractAllTermDocs that also InstantiatedAllTermDocs extend.\n\nAlso:\n\n * II-tests made less plausable to pass on future incompatible changes to TermDocs and TermEnum\n * IITermDocs#skipTo and #next mimics the behaviour of document posisioning from SegmentTermDocs#dito when returning false\n * II now uses BitVector rather than sets for deleted documents\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3234",
        "summary": "Provide limit on phrase analysis in FastVectorHighlighter",
        "description": "With larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document.  If one is willing to accept\nless-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible.  This is analogous to the Highlighter limit on the number of characters to analyze.\n\nThe patch includes an artifical test case that shows > 1000x speedup.  In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document.  Most of our sites operate in this way (just show the first snippet), so this would be a big win for us.\n\nWith phraseLimit = -1, you get the existing FVH behavior. At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-324",
        "summary": "org.apache.lucene.analysis.cn.ChineseTokenizer missing offset decrement",
        "description": "Apparently, in ChineseTokenizer, offset should be decremented like bufferIndex\nwhen Character is OTHER_LETTER.  This directly affects startOffset and endOffset\nvalues.\n\nThis is critical to have Highlighter working correctly because Highlighter marks\nmatching text based on these offset values.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1543",
        "summary": "Field specified norms in MatchAllDocumentsScorer ",
        "description": "This patch allows for optionally setting a field to use for norms factoring when scoring a MatchingAllDocumentsQuery.\n\nFrom the test case:\n{code:java}\n.\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    iw.setMaxBufferedDocs(2);  // force multi-segment\n    addDoc(\"one\", iw, 1f);\n    addDoc(\"two\", iw, 20f);\n    addDoc(\"three four\", iw, 300f);\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir);\n    IndexSearcher is = new IndexSearcher(ir);\n    ScoreDoc[] hits;\n\n    // assert with norms scoring turned off\n\n    hits = is.search(new MatchAllDocsQuery(), null, 1000).scoreDocs;\n    assertEquals(3, hits.length);\n    assertEquals(\"one\", ir.document(hits[0].doc).get(\"key\"));\n    assertEquals(\"two\", ir.document(hits[1].doc).get(\"key\"));\n    assertEquals(\"three four\", ir.document(hits[2].doc).get(\"key\"));\n\n    // assert with norms scoring turned on\n\n    MatchAllDocsQuery normsQuery = new MatchAllDocsQuery(\"key\");\n    assertEquals(3, hits.length);\n//    is.explain(normsQuery, hits[0].doc);\n    hits = is.search(normsQuery, null, 1000).scoreDocs;\n\n    assertEquals(\"three four\", ir.document(hits[0].doc).get(\"key\"));    \n    assertEquals(\"two\", ir.document(hits[1].doc).get(\"key\"));\n    assertEquals(\"one\", ir.document(hits[2].doc).get(\"key\"));\n{code}",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-877",
        "summary": "2.1 Locking documentation in \"Apache Lucene - Index File Formats\" section \"6.2 Lock File\" out dated",
        "description": "I am in the process to migrate from Lucene 2.0 to Lucene 2.1.\n\nFrom reading the Changes document I understand that the write locks are now written into the index folder instead of the java.io.tmpdir. \n\nIn the \"Apache Lucene - Index File Formats\" document in section \"6.2 Lock File\" I read that there is a write lock used to indicate that another process is writing into the index and that this file is stored in the java.io.tempdir.\n\nThis is confusing to me.  I had the impression all lock files go into the index folder now.  And using the the java.io.tempdir is only local and does not support access to shared index folders.\n\nDo I miss something here or is the documentation not updated?\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2318",
        "summary": "Add System.getProperty(\"tempDir\") as final static to LuceneTestCase(J4)",
        "description": "Almost every test calls System.getProperty(\"tempDir\") and some of them check the return value for null. In other cases the test simply fails from within eclipse.\n\nWe should add this to LuceneTestCase(J4) as a static final constant. For enabling tests run in eclipse, we can add a fallback to \".\", if the Sysprop is not defined.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1238",
        "summary": "intermittent failures of  TestTimeLimitedCollector.testTimeoutMultiThreaded in nightly tests",
        "description": "Occasionly TestTimeLimitedCollector.testTimeoutMultiThreaded fails. e.g. with this output:\n\n{noformat}\n   [junit] ------------- Standard Error -----------------\n   [junit] Exception in thread \"Thread-97\" junit.framework.AssertionFailedError: no hits found!\n   [junit]     at junit.framework.Assert.fail(Assert.java:47)\n   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)\n   [junit] Exception in thread \"Thread-85\" junit.framework.AssertionFailedError: no hits found!\n   [junit]     at junit.framework.Assert.fail(Assert.java:47)\n   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)\n   [junit] ------------- ---------------- ---------------\n   [junit] Testcase: testTimeoutMultiThreaded(org.apache.lucene.search.TestTimeLimitedCollector):      FAILED\n   [junit] some threads failed! expected:<50> but was:<48>\n   [junit] junit.framework.AssertionFailedError: some threads failed! expected:<50> but was:<48>\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestMultiThreads(TestTimeLimitedCollector.java:255)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutMultiThreaded(TestTimeLimitedCollector.java:220)\n   [junit]\n{noformat}\n\nProblem either in test or in TimeLimitedCollector.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1481",
        "summary": "Sort and SortField does not have equals() and hashCode()",
        "description": "During developing for my project panFMP I had the following issue:\nI have a cache for queries (like Solr has, too)  for query results. This cache also uses the Sort/SortField as key into the cache. The problem is, because Sort/SortField does not implement equals() and hashCode(), you cannot store them as cache keys. To workaround, currently I use Sort.toString() as cache key, but this is not so nice.\n\nIn corelation with issue LUCENE-1478, I could fix this there in one patch together with the other improvements.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-436",
        "summary": "[PATCH] TermInfosReader, SegmentTermEnum Out Of Memory Exception",
        "description": "We've been experiencing terrible memory problems on our production search server, running lucene (1.4.3).\n\nOur live app regularly opens new indexes and, in doing so, releases old IndexReaders for garbage collection.\n\nBut...there appears to be a memory leak in org.apache.lucene.index.TermInfosReader.java.\nUnder certain conditions (possibly related to JVM version, although I've personally observed it under both linux JVM 1.4.2_06, and 1.5.0_03, and SUNOS JVM 1.4.1) the ThreadLocal member variable, \"enumerators\" doesn't get garbage-collected when the TermInfosReader object is gc-ed.\n\nLooking at the code in TermInfosReader.java, there's no reason why it _shouldn't_ be gc-ed, so I can only presume (and I've seen this suggested elsewhere) that there could be a bug in the garbage collector of some JVMs.\n\nI've seen this problem briefly discussed; in particular at the following URL:\n  http://java2.5341.com/msg/85821.html\nThe patch that Doug recommended, which is included in lucene-1.4.3 doesn't work in our particular circumstances. Doug's patch only clears the ThreadLocal variable for the thread running the finalizer (my knowledge of java breaks down here - I'm not sure which thread actually runs the finalizer). In our situation, the TermInfosReader is (potentially) used by more than one thread, meaning that Doug's patch _doesn't_ allow the affected JVMs to correctly collect garbage.\n\nSo...I've devised a simple patch which, from my observations on linux JVMs 1.4.2_06, and 1.5.0_03, fixes this problem.\n\nKieran\nPS Thanks to daniel naber for pointing me to jira/lucene\n\n@@ -19,6 +19,7 @@\n import java.io.IOException;\n\n import org.apache.lucene.store.Directory;\n+import java.util.Hashtable;\n\n /** This stores a monotonically increasing set of <Term, TermInfo> pairs in a\n  * Directory.  Pairs are accessed either by Term or by ordinal position the\n@@ -29,7 +30,7 @@\n   private String segment;\n   private FieldInfos fieldInfos;\n\n-  private ThreadLocal enumerators = new ThreadLocal();\n+  private final Hashtable enumeratorsByThread = new Hashtable();\n   private SegmentTermEnum origEnum;\n   private long size;\n\n@@ -60,10 +61,10 @@\n   }\n\n   private SegmentTermEnum getEnum() {\n-    SegmentTermEnum termEnum = (SegmentTermEnum)enumerators.get();\n+    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());\n     if (termEnum == null) {\n       termEnum = terms();\n-      enumerators.set(termEnum);\n+      enumeratorsByThread.put(Thread.currentThread(), termEnum);\n     }\n     return termEnum;\n   }\n@@ -195,5 +196,15 @@\n   public SegmentTermEnum terms(Term term) throws IOException {\n     get(term);\n     return (SegmentTermEnum)getEnum().clone();\n+  }\n+\n+  /* some jvms might have trouble gc-ing enumeratorsByThread */\n+  protected void finalize() throws Throwable {\n+    try {\n+        // make sure gc can clear up.\n+        enumeratorsByThread.clear();\n+    } finally {\n+        super.finalize();\n+    }\n   }\n }\n\n\n\nTermInfosReader.java, full source:\n======================================\npackage org.apache.lucene.index;\n\n/**\n * Copyright 2004 The Apache Software Foundation\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.IOException;\n\nimport org.apache.lucene.store.Directory;\nimport java.util.Hashtable;\n\n/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a\n * Directory.  Pairs are accessed either by Term or by ordinal position the\n * set.  */\n\nfinal class TermInfosReader {\n  private Directory directory;\n  private String segment;\n  private FieldInfos fieldInfos;\n\n  private final Hashtable enumeratorsByThread = new Hashtable();\n  private SegmentTermEnum origEnum;\n  private long size;\n\n  TermInfosReader(Directory dir, String seg, FieldInfos fis)\n       throws IOException {\n    directory = dir;\n    segment = seg;\n    fieldInfos = fis;\n\n    origEnum = new SegmentTermEnum(directory.openFile(segment + \".tis\"),\n                                   fieldInfos, false);\n    size = origEnum.size;\n    readIndex();\n  }\n\n  public int getSkipInterval() {\n    return origEnum.skipInterval;\n  }\n\n  final void close() throws IOException {\n    if (origEnum != null)\n      origEnum.close();\n  }\n\n  /** Returns the number of term/value pairs in the set. */\n  final long size() {\n    return size;\n  }\n\n  private SegmentTermEnum getEnum() {\n    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());\n    if (termEnum == null) {\n      termEnum = terms();\n      enumeratorsByThread.put(Thread.currentThread(), termEnum);\n    }\n    return termEnum;\n  }\n\n  Term[] indexTerms = null;\n  TermInfo[] indexInfos;\n  long[] indexPointers;\n\n  private final void readIndex() throws IOException {\n    SegmentTermEnum indexEnum =\n      new SegmentTermEnum(directory.openFile(segment + \".tii\"),\n\t\t\t  fieldInfos, true);\n    try {\n      int indexSize = (int)indexEnum.size;\n\n      indexTerms = new Term[indexSize];\n      indexInfos = new TermInfo[indexSize];\n      indexPointers = new long[indexSize];\n\n      for (int i = 0; indexEnum.next(); i++) {\n\tindexTerms[i] = indexEnum.term();\n\tindexInfos[i] = indexEnum.termInfo();\n\tindexPointers[i] = indexEnum.indexPointer;\n      }\n    } finally {\n      indexEnum.close();\n    }\n  }\n\n  /** Returns the offset of the greatest index entry which is less than or equal to term.*/\n  private final int getIndexOffset(Term term) throws IOException {\n    int lo = 0;\t\t\t\t\t  // binary search indexTerms[]\n    int hi = indexTerms.length - 1;\n\n    while (hi >= lo) {\n      int mid = (lo + hi) >> 1;\n      int delta = term.compareTo(indexTerms[mid]);\n      if (delta < 0)\n\thi = mid - 1;\n      else if (delta > 0)\n\tlo = mid + 1;\n      else\n\treturn mid;\n    }\n    return hi;\n  }\n\n  private final void seekEnum(int indexOffset) throws IOException {\n    getEnum().seek(indexPointers[indexOffset],\n\t      (indexOffset * getEnum().indexInterval) - 1,\n\t      indexTerms[indexOffset], indexInfos[indexOffset]);\n  }\n\n  /** Returns the TermInfo for a Term in the set, or null. */\n  TermInfo get(Term term) throws IOException {\n    if (size == 0) return null;\n\n    // optimize sequential access: first try scanning cached enum w/o seeking\n    SegmentTermEnum enumerator = getEnum();\n    if (enumerator.term() != null                 // term is at or past current\n\t&& ((enumerator.prev != null && term.compareTo(enumerator.prev) > 0)\n\t    || term.compareTo(enumerator.term()) >= 0)) {\n      int enumOffset = (int)(enumerator.position/enumerator.indexInterval)+1;\n      if (indexTerms.length == enumOffset\t  // but before end of block\n\t  || term.compareTo(indexTerms[enumOffset]) < 0)\n\treturn scanEnum(term);\t\t\t  // no need to seek\n    }\n\n    // random-access: must seek\n    seekEnum(getIndexOffset(term));\n    return scanEnum(term);\n  }\n\n  /** Scans within block for matching term. */\n  private final TermInfo scanEnum(Term term) throws IOException {\n    SegmentTermEnum enumerator = getEnum();\n    while (term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}\n    if (enumerator.term() != null && term.compareTo(enumerator.term()) == 0)\n      return enumerator.termInfo();\n    else\n      return null;\n  }\n\n  /** Returns the nth term in the set. */\n  final Term get(int position) throws IOException {\n    if (size == 0) return null;\n\n    SegmentTermEnum enumerator = getEnum();\n    if (enumerator != null && enumerator.term() != null &&\n        position >= enumerator.position &&\n\tposition < (enumerator.position + enumerator.indexInterval))\n      return scanEnum(position);\t\t  // can avoid seek\n\n    seekEnum(position / enumerator.indexInterval); // must seek\n    return scanEnum(position);\n  }\n\n  private final Term scanEnum(int position) throws IOException {\n    SegmentTermEnum enumerator = getEnum();\n    while(enumerator.position < position)\n      if (!enumerator.next())\n\treturn null;\n\n    return enumerator.term();\n  }\n\n  /** Returns the position of a Term in the set or -1. */\n  final long getPosition(Term term) throws IOException {\n    if (size == 0) return -1;\n\n    int indexOffset = getIndexOffset(term);\n    seekEnum(indexOffset);\n\n    SegmentTermEnum enumerator = getEnum();\n    while(term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}\n\n    if (term.compareTo(enumerator.term()) == 0)\n      return enumerator.position;\n    else\n      return -1;\n  }\n\n  /** Returns an enumeration of all the Terms and TermInfos in the set. */\n  public SegmentTermEnum terms() {\n    return (SegmentTermEnum)origEnum.clone();\n  }\n\n  /** Returns an enumeration of terms starting at or after the named term. */\n  public SegmentTermEnum terms(Term term) throws IOException {\n    get(term);\n    return (SegmentTermEnum)getEnum().clone();\n  }\n\n  /* some jvms might have trouble gc-ing enumeratorsByThread */ \n  protected void finalize() throws Throwable {\n    try {\n        // make sure gc can clear up.\n        enumeratorsByThread.clear();\n    } finally {\n        super.finalize();\n    }\n  }\n}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1918",
        "summary": "Adding empty ParallelReader indexes to an IndexWriter may cause ArrayIndexOutOfBoundsException or NoSuchElementException",
        "description": "Hi,\nI recently stumbled upon this:\n\nIt is possible (and perfectly legal) to add empty indexes (IndexReaders) to an IndexWriter. However, when using ParallelReaders in this context, in two situations RuntimeExceptions may occur for no good reason.\n\nCondition 1:\nThe indexes within the ParallelReader are just empty.\n\nWhen adding them to the IndexWriter, we get a java.util.NoSuchElementException triggered by ParallelTermEnum's constructor. The reason for that is the TreeMap#firstKey() method which was assumed to return null if there is no entry (which is not true, apparently -- it only returns null if the first key in the Map is null).\n\n\nCondition 2 (Assuming the aforementioned bug is fixed):\nThe indexes within the ParallelReader originally contained one or more fields with TermVectors, but all documents have been marked as deleted.\n\nWhen adding the indexes to the IndexWriter, we get a java.lang.ArrayIndexOutOfBoundsException triggered by TermVectorsWriter#addAllDocVectors. The reason here is that TermVectorsWriter assumes that if the index is marked to have TermVectors, at least one field actually exists for that. This unfortunately is not true, either.\n\nPatches and a testcase demonstrating the two bugs are provided.\n\nCheers,\nChristian",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3495",
        "summary": "BlockJoinQuery doesn't implement boost",
        "description": "After reviewing LUCENE-3494, i checked other queries and noticed that BlockJoinQuery currently throws UOE for getBoost and setBoost:\n{noformat}\nthrow new UnsupportedOperationException(\"this query cannot support boosting; please use childQuery.setBoost instead\");\n{noformat}\n\nI don't think we can safely do that in queries, because other parts of lucene rely upon this working... for example BQs rewrite when\nit has a single clause and erases itself.\n\nSo I think we should just pass down the boost to the inner weight.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1763",
        "summary": "MergePolicy should require an IndexWriter upon construction",
        "description": "MergePolicy does not require an IW upon construction, but requires one to be passed as method arg to various methods. This gives the impression as if a single MP instance can be shared across various IW instances, which is not true for all MPs (if at all). In addition, LogMergePolicy uses the IW instance passed to these methods incosistently, and is currently exposed to potential NPEs.\n\nThis issue will change MP to require an IW instance, however for back-compat reasons the following changes will be made:\n# A new MP ctor w/ IW as arg will be introduced. Additionally, for back-compat a default ctor will also be declared which will assign null to the member IW.\n# Methods that require IW will be deprecated, and new ones will be declared.\n#* For back-compat, the new ones will not be made abstract, but will throw UOE, with a comment that they will become abstract in 3.0.\n# All current MP impls will move to use the member instance.\n# The code which calls MP methods will continue to use the deprecated methods, passing an IW even that it won't be necessary --> this is strictly for back-compat.\n\nIn 3.0, we'll remove the deprecated default ctor and methods, and change the code to not call the IW method variants anymore.\n\nI hope that I didn't leave anything out. I'm sure I'll find out when I work on the patch :).",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1766",
        "summary": "Add Thread-Safety note to IndexWriter JavaDoc",
        "description": "IndexWriter Javadocs should contain a note about thread-safety. This is already mentioned on the wiki FAQ page but such an essential information should be part of the module documentation too.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3863",
        "summary": "DocValues.type() -> DocValues.getType()",
        "description": "This makes the method easier to find and more clear that it has no side effects... on a\nfew occasions I've looked for this getter and missed it because of the name.\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-212",
        "summary": "FileInputStream never closed in HTMLParser",
        "description": "HTMLParser.java contains this code: \n \n  public HTMLParser(File file) throws FileNotFoundException { \n    this(new FileInputStream(file)); \n  } \n \nThis FileInputStream should be closed with the close() method, as there's no \nguarantee that the garbage collection will run and do this for you. I don't \nknow how to fix this without changing the API to take a FileInputStream \ninstead of a File, as the call to this() must be the first thing in the \nconstructor, i.e. you cannot create the stream, call this(...), and then close \nthe stream.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2978",
        "summary": "Upgrade benchmark from commons-compress-1.0 to commons-compress-1.1 for 15 times faster gzip decompression",
        "description": "In LUCENE-1540 TrecContentSource moved from Java's GZipInputStream to common-compress 1.0. \nThis slowed down gzip decompression by a factor of 15. \nUpgrading to 1.1 solves this problem.\nI verified that the problem is only in GZIP, not in BZIP.\nOn the way, as 1.1 introduced constants for the compression methods, the code can be made a bit nicer.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3776",
        "summary": "NRTManager shouldn't expose its private SearcherManager",
        "description": "Spinoff from LUCENE-3769.\n\nTo actually obtain an IndexSearcher from NRTManager, it's a 2-step process now.\n\nYou must .getSearcherManager(), then .acquire() from the returned SearcherManager.\n\nThis is very trappy... because if the app incorrectly calls maybeReopen on that private SearcherManager (instead of NRTManager.maybeReopen) then it can unexpectedly cause threads to block forever, waiting for the necessary gen to become visible.  This will be hard to debug... I don't like creating trappy APIs.\n\nHopefully once LUCENE-3761 is in, we can fix NRTManager to no longer expose its private SM, instead subclassing ReferenceManaager.\n\nOr alternatively, or in addition, maybe we factor out a new interface (SearcherProvider or something...) that only has acquire and release methods, and both NRTManager and ReferenceManager/SM impl that, and we keep NRTManager's SM private.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3428",
        "summary": "trunk tests hang/deadlock TestIndexWriterWithThreads",
        "description": "trunk tests have been hanging often lately in hudson, this time i was careful to kill and get a good stacktrace:",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2480",
        "summary": "Remove support for pre-3.0 indexes",
        "description": "We should remove support for 2.x (and 1.9) indexes in 4.0. It seems that nothing can be done in 3x because there is no special code which handles 1.9, so we'll leave it there. This issue should cover:\n# Remove the .zip indexes\n# Remove the unnecessary code from SegmentInfo and SegmentInfos. Mike suggests we compare the version headers at the top of SegmentInfos, in 2.9.x vs 3.0.x, to see which ones can go.\n# remove FORMAT_PRE from FieldInfos\n# Remove old format from TermVectorsReader\n\nIf you know of other places where code can be removed, then please post a comment here.\n\nI don't know when I'll have time to handle it, definitely not in the next few days. So if someone wants to take a stab at it, be my guest.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3227",
        "summary": "Add Rewriteable Support to SortField.toString",
        "description": "I missed adding support for the new Rewriteable SortField type to toString().",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-276",
        "summary": "Memory leak when sorting",
        "description": "This is the same post I sended two days before to the Lucene user's list. This \nbug seems to have something in common with bug no. 30628 but that bug is closed \nas invalid.\n\nI'm sending test code that everyone can try. The code is singular, don't say \nthere is no sense in reopening the same index. I can only show, that reopening \nleaks memory. The index is filled by pseudo-real data, they aren't significant \nand the process of index creation as well. \n\nThe problem must be in field caching code used by sort.\n\nAffected versions of Lucene:\n1.4.1\nCVS 1.5-rc1-dev\n\nThis code survives only few first iterations if you run java with -Xmx5m. With \nLucene 1.4-final ends regulary.\n\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Hits;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.Searcher;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.TermQuery;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.RAMDirectory;\n\nimport java.io.IOException;\nimport java.text.SimpleDateFormat;\nimport java.util.Calendar;\nimport java.util.Date;\n\n/**\n * Run this test with Lucene 1.4.1 and -Xmx5m\n */\npublic class ReopenTest\n{\n    private static long mem_last = 0;\n\n    public static void main(String[] args) throws IOException\n    {\n        Directory directory = create_index();\n\n        for (int i = 1; i < 100; i++) {\n            System.err.println(\"loop \" + i + \", index version: \" + IndexReader.\ngetCurrentVersion(directory));\n            search_index(directory);\n            add_to_index(directory, i);\n        }\n    }\n\n    private static void add_to_index(Directory directory, int i) throws \nIOException\n    {\n        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), \nfalse);\n\n        SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\");\n        Document doc = new Document();\n\n        doc.add(Field.Keyword(\"date\", \n          df.format(new Date(System.currentTimeMillis()))));\n        doc.add(Field.Keyword(\"id\", \"CD\" + String.valueOf(i)));\n        doc.add(Field.Text(\"text\", \"Tohle neni text \" + i));\n        writer.addDocument(doc);\n\n        System.err.println(\"index size: \" + writer.docCount());\n        writer.close();\n    }\n\n    private static void search_index(Directory directory) throws IOException\n    {\n        IndexReader reader = IndexReader.open(directory);\n        Searcher searcher = new IndexSearcher(reader);\n\n        print_mem(\"search 1\");\n        SortField[] fields = new SortField[2];\n        fields[0] = new SortField(\"date\", SortField.STRING, true);\n        fields[1] = new SortField(\"id\", SortField.STRING, false);\n        Sort sort = new Sort(fields);\n        TermQuery query = new TermQuery(new Term(\"text\", \"\\\"text 5\\\"\"));\n\n        print_mem(\"search 2\");\n        Hits hits = searcher.search(query, sort);\n        print_mem(\"search 3\");\n\n        for (int i = 0; i < hits.length(); i++) {\n            Document doc = hits.doc(i);\n            System.out.println(\"doc \" + i + \": \" + doc.toString());\n        }\n        print_mem(\"search 4\");\n        searcher.close();\n        reader.close();\n    }\n\n    private static void print_mem(String log)\n    {\n        long mem_free = Runtime.getRuntime().freeMemory();\n        long mem_total = Runtime.getRuntime().totalMemory();\n        long mem_max = Runtime.getRuntime().maxMemory();\n\n        long delta = (mem_last - mem_free) * -1;\n\n        System.out.println(log + \"= delta: \" + delta + \", free: \" + mem_free + \n\", used: \" + (mem_total-mem_free) + \", total: \" + mem_total + \", max: \" + \nmem_max);\n\n        mem_last = mem_free;\n    }\n\n    private static Directory create_index() throws IOException\n    {\n        print_mem(\"create 1\");\n        Directory directory = new RAMDirectory();\n\n        Calendar c = Calendar.getInstance();\n        SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\");\n        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), \ntrue);\n        for (int i = 0; i < 365 * 15; i++) {\n            Document doc = new Document();\n\n            doc.add(Field.Keyword(\"date\", \n               df.format(new Date(c.getTimeInMillis()))));\n            doc.add(Field.Keyword(\"id\", \"AB\" + String.valueOf(i)));\n            doc.add(Field.Text(\"text\", \"Tohle je text \" + i));\n            writer.addDocument(doc);\n\n            doc = new Document();\n\n            doc.add(Field.Keyword(\"date\", \n               df.format(new Date(c.getTimeInMillis()))));\n            doc.add(Field.Keyword(\"id\", \"ef\" + String.valueOf(i)));\n            doc.add(Field.Text(\"text\", \"Je tohle text \" + i));\n            writer.addDocument(doc);\n\n            c.add(Calendar.DAY_OF_YEAR, 1);\n        }\n        writer.optimize();\n        System.err.println(\"index size: \" + writer.docCount());\n        writer.close();\n\n        print_mem(\"create 2\");\n        return directory;\n    }\n}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2568",
        "summary": "TestUTF32ToUTF8 fails on IBM's JRE",
        "description": "This is because AutomatonTestUtil.RandomAcceptedString is returning an invalid UTF32 int[] -- it has an unpaired surrogate, and IBM's JRE handles this differently than Oracle's.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2956",
        "summary": "Support updateDocument() with DWPTs",
        "description": "With separate DocumentsWriterPerThreads (DWPT) it can currently happen that the delete part of an updateDocument() is flushed and committed separately from the corresponding new document.\n\nWe need to make sure that updateDocument() is always an atomic operation from a IW.commit() and IW.getReader() perspective.  See LUCENE-2324 for more details.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3548",
        "summary": "CharsRef#append broken on trunk & 3.x",
        "description": "Current impl. for append on CharsRef is broken - it overrides the actual content rather than append. its used in many places especially in solr so we might have some broken ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1140",
        "summary": "NPE in StopFilter caused by StandardAnalyzer(boolean replaceInvalidAcronym) constructor",
        "description": "I think that I found a problem with the new code (https://issues.apache.org/jira/browse/LUCENE-1068).\nUsage of the new constructor StandardAnalyzer(boolean replaceInvalidAcronym) causes NPE in\nStopFilter:\n\njava.lang.NullPointerException\n        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:74)\n        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:86)\n        at\norg.apache.lucene.analysis.standard.StandardAnalyzer.tokenStream(StandardAnalyzer.java:151)\n        at\norg.apache.lucene.queryParser.QueryParser.getFieldQuery(QueryParser.java:452)\n        at\norg.apache.lucene.queryParser.QueryParser.Term(QueryParser.java:1133)\n        at\norg.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1020)\n        at\norg.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)\n        at\norg.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1024)\n        at\norg.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)\n        at\norg.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:937)\n        at\norg.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:147)\n\nThe reason is that new constructor forgets to initialize the stopSet field:\n  public StandardAnalyzer(boolean replaceInvalidAcronym) {\n    this.replaceInvalidAcronym = replaceInvalidAcronym;\n  }\n\nI guess this should be changed to something like this:\n  public StandardAnalyzer(boolean replaceInvalidAcronym) {\n    this(STOP_WORDS);\n    this.replaceInvalidAcronym = replaceInvalidAcronym;\n  }\n\nThe bug is present in RC3. Fix is one line, it'll be great to have it in 2.3\nrelease.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1975",
        "summary": "Remove deprecated SpanQuery.getTerms() and generify Query.extractTerms(Set<Term>)",
        "description": "",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1412",
        "summary": "FileNotFoundException thrown by Directory.copy()",
        "description": "java.io.FileNotFoundException: segments_bu\n        at org.apache.lucene.store.RAMDirectory.openInput(RAMDirectory.java:234)\n        at org.apache.lucene.store.Directory.copy(Directory.java:190)\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3191",
        "summary": "Add TopDocs.merge to merge multiple TopDocs",
        "description": "It's not easy today to merge TopDocs, eg produced by multiple shards,\nsupporting arbitrary Sort.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-318",
        "summary": "\"System Properties\" doc lists \"lockDir\" instead of \"lockdir\"",
        "description": "The \"System Properties\" documentation page states that the lock file directory\ncan be set with the system property \"org.apache.lucene.lockDir\".  However, as\nimplemented in org.apache.lucene.store.FSDirectory, line 56, the property name\nis actually \"org.apache.lucene.lockdir\" (lower case \"d\" in \"lockdir\"). \nRecommend changing documentation to match code.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-886",
        "summary": "spellchecker cleanup",
        "description": "Some cleanup, attached here so it can be tracked if necessary: javadoc improvements; don't print exceptions to stderr but re-throw them; new constructor for a new test case. I will commit this soon.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1078",
        "summary": "Cleanup some unused and unnecessary code",
        "description": "Several classes in trunk have some unused and unnecessary code. This includes unused fields, unused automatic variables, unused imports and unnecessary assignments. Attached it a patch to clean these up.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1394",
        "summary": "Include all CHANGES.txt under contrib",
        "description": "We already include to root CHANGES.txt but fail to include the ones under contrib.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-726",
        "summary": "Remove usage of deprecated method Document.fields()",
        "description": "The classes DocumentWriter, FieldsWriter, and ParallelReader use the deprecated method Document.fields(). This simple patch changes these three classes to use Document.getFields() instead.\n\nAll unit tests pass.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2110",
        "summary": "Refactoring of FilteredTermsEnum and MultiTermQuery",
        "description": "FilteredTermsEnum is confusing as it is initially positioned to the first term. It should instead work like an uninitialized TermsEnum for a field before the first call to next() or seek().\nFilteredTermsEnums cannot implement seek() as eg. NRQ or Automaton are not able to support this. Seeking is also not needed for MTQ at all, so seek can just throw UOE.\nThis issue changes some of the internal behaviour of MTQ and FilteredTermsEnum to allow also seeking in NRQ and Automaton (see comments below).",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-419",
        "summary": "[REFACTORING] FieldSortedHitQueue has too much duplicated code",
        "description": "There's 40LOC duplicated in FieldDocSortedHitQueue::lessThan just to handle \nthe reverse sort. It would be more readable to actually do something like \n(YMMV):\n\nif (field.getReverse()) {\n    c = -c;\n}",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2464",
        "summary": "FastVectorHighlighter: add a FragmentBuilder to return entire field contents",
        "description": "In Highlightrer, there is a Nullfragmenter. There is a requirement its counterpart in FastVectorhighlighter.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1935",
        "summary": "Generify PriorityQueue",
        "description": "Priority Queue should use generics like all other Java 5 Collection API classes. This very simple, but makes code more readable.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2845",
        "summary": "move contrib/benchmark to modules/benchmark",
        "description": "I think we should move lucene/contrib/benchmark to a shared modules/benchmark, so you can easily benchmark anything (lucene, solr, other modules like analysis or whatever).\n\nFor example, if you want to do some benchmarking of something in solr (LUCENE-2844) you should be able to do this.\nAnother example is simply being able to benchmark an analyzer definition from a schema.xml, its more convenient than writing the equivalent java analyzer just for benchmarking.\n\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-570",
        "summary": "Expose directory on IndexReader",
        "description": "It would be really useful to expose the index directory on the IndexReader class.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2288",
        "summary": "Create EMPTY_ARGS constsant in SnowballProgram instead of allocating new Object[0]",
        "description": "Instead of allocating new Object[0] create a proper constant in SnowballProgram. The same (for new Class[0]) is created in Among, although it's less critical because Among is called from static initializers ... Patch will follow shortly.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3625",
        "summary": "FieldValueFitler should expose the field it uses",
        "description": "FieldValueFitler should expose the field it uses. It currently hides this entirely.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1013",
        "summary": "IndexWriter.setMaxMergeDocs gives non-backwards-compatible exception \"out of the box\"",
        "description": "Yonik hit this (see details in LUCENE-994): because we have switched\nto LogByteSizeMergePolicy by default in IndexWriter, which uses MB to\nlimit max size of merges (setMaxMergeMB), when an existing app calls\nsetMaxMergeDocs (or getMaxMergeDocs) it will hit an\nIllegalArgumentException on dropping in the new JAR.\n\nI think the simplest solution is to fix LogByteSizeMergePolicy to\nallow setting of the max by either MB or by doc count, just like how\nin LUCENE-1007 allows flushing by either MB or docCount or both.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    }
]