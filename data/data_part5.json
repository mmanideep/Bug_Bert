[
  {
        "key": "LUCENE-645",
        "summary": "Highligter fails to include non-token at end of string to be highlighted",
        "description": "The following code extract show the problem\n\n\n\t\tTermQuery query= new TermQuery( new Term( \"data\", \"help\" )); \n\t\tHighlighter hg = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer( query ));\n\t\thg.setTextFragmenter( new NullFragmenter() );\n\t\t\n\t\tString match = null;\n\t\ttry {\n\t\t\tmatch = hg.getBestFragment( new StandardAnalyzer(), \"data\", \"help me [54-65]\" );\n\t\t} catch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t\tSystem.out.println( match );\n\n\nThe sytsem outputs \n\n<B>help</B> me [54-65\n\n\nwould expect \n\n<B>help</B> me [54-65]\n\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1852",
        "summary": "Fix remaining localization test failures in lucene",
        "description": "see also LUCENE-1836 and LUCENE-1846\n\nall tests should pass under different locales.\nthe fix is to run 'ant test' under different locales, look and fix problems, and use the LocalizedTestCase from LUCENE-1836 to keep them from coming back.\n\nthe same approach as LUCENE-1836 fixes the core queryparser, but I am running ant test under a few locales to look for more problems.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2590",
        "summary": "Enable access to the freq information in a Query's sub-scorers",
        "description": "The ability to gather more details than just the score, of how a given\ndoc matches the current query, has come up a number of times on the\nuser's lists.  (most recently in the thread \"Query Match Count\" by\nRyan McV on java-user).\n\nEG if you have a simple TermQuery \"foo\", on each hit you'd like to\nknow how many times \"foo\" occurred in that doc; or a BooleanQuery +foo\n+bar, being able to separately see the freq of foo and bar for the\ncurrent hit.\n\nLucene doesn't make this possible today, which is a shame because\nLucene in fact does compute exactly this information; it's just not\naccessible from the Collector.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2341",
        "summary": "explore morfologik integration",
        "description": "Dawid Weiss mentioned on LUCENE-2298 that there is another Polish stemmer available:\nhttp://sourceforge.net/projects/morfologik/\n\nThis works differently than LUCENE-2298, and ideally would be another option for users.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2220",
        "summary": "Stackoverflow when calling deprecated CharArraySet.copy()",
        "description": "Calling CharArraySet#copy(set) without the version argument (deprecated) with an instance of CharArraySet results in a stack overflow as this method checks if the given set is a CharArraySet and then calls itself again. This was accidentially introduced due to an overloaded alternative method during LUCENE-2169 which was not used in the final patch.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3101",
        "summary": "TestMinimize.testAgainstBrzozowski reproducible seed OOM",
        "description": "{code}\n    [junit] Testsuite: org.apache.lucene.util.automaton.TestMinimize\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 3.792 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestMinimize -Dtestmethod=testAgainstBrzozowski -Dtests.seed=-7429820995201119781:1013305000165135537\n    [junit] NOTE: test params are: codec=PreFlex, locale=ru, timezone=America/Pangnirtung\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestMinimize]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=294745976,total=310378496\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testAgainstBrzozowski(org.apache.lucene.util.automaton.TestMinimize):     Caused an ERROR\n    [junit] Java heap space\n    [junit] java.lang.OutOfMemoryError: Java heap space\n    [junit]     at java.util.BitSet.initWords(BitSet.java:144)\n    [junit]     at java.util.BitSet.<init>(BitSet.java:139)\n    [junit]     at org.apache.lucene.util.automaton.MinimizationOperations.minimizeHopcroft(MinimizationOperations.java:85)\n    [junit]     at org.apache.lucene.util.automaton.MinimizationOperations.minimize(MinimizationOperations.java:52)\n    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:502)\n    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomatonAllowMutate(RegExp.java:478)\n    [junit]     at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:428)\n    [junit]     at org.apache.lucene.util.automaton.AutomatonTestUtil.randomAutomaton(AutomatonTestUtil.java:256)\n    [junit]     at org.apache.lucene.util.automaton.TestMinimize.testAgainstBrzozowski(TestMinimize.java:43)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.util.automaton.TestMinimize FAILED\n\n\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3256",
        "summary": "Consolidate CustomScoreQuery, ValueSourceQuery and BoostedQuery ",
        "description": "Lucene's CustomScoreQuery and Solr's BoostedQuery do essentially the same thing: they boost the scores of Documents by the value from a ValueSource.  BoostedQuery does this in a direct fashion, by accepting a ValueSource. CustomScoreQuery on the other hand, accepts a series of ValueSourceQuerys.  ValueSourceQuery seems to do exactly the same thing as FunctionQuery.\n\nWith Lucene's ValueSource being deprecated / removed, we need to resolve these dependencies and simplify the code.\n\nTherefore I recommend we do the following things:\n\n- Move CustomScoreQuery (and CustomScoreProvider) to the new Queries module and change it over to use FunctionQuerys instead of ValueSourceQuerys.  \n- Deprecate Solr's BoostedQuery in favour of the new CustomScoreQuery.  CSQ provides a lot of support for customizing the scoring process.\n- Move and consolidate all tests of CSQ and BoostedQuery, to the Queries module and have them test CSQ instead.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-3466",
        "summary": "Rename Analyzer.reusableTokenStream() to tokenStream()",
        "description": "All Analysis consumers now use reusableTokenStream().  To finally make reuse mandatory, lets rename resuableTokenStream() to tokenStream() (removing the old tokenStream() method).",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-1348",
        "summary": "TestTimeLimitedCollector  shuold not fail if the testing machine happens to be slow",
        "description": "The test fails on Hudson about once a month, like this:\n\n{quote}\n   [junit] Testcase: testTimeoutNotGreedy(org.apache.lucene.search.TestTimeLimitedCollector):  FAILED\n   [junit] lastDoc=21 ,&& allowed=799 ,&& elapsed=900 >= 886 = ( 2*resolution +  TIME_ALLOWED + SLOW_DOWN = 2*20 + 799 + 47)\n   [junit] junit.framework.AssertionFailedError: lastDoc=21 ,&& allowed=799 ,&& elapsed=900 >= 886 = ( 2*resolution +  TIME_ALLOWED + SLOW_DOWN = 2*20 + 799 + 47)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:189)\n   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutNotGreedy(TestTimeLimitedCollector.java:150)\n   [junit]\n   [junit]\n   [junit] Test org.apache.lucene.search.TestTimeLimitedCollector FAILED\n{quote}\n\nModify the test to just print a warning in this case - but still verify that there was an early termination.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2722",
        "summary": "Sep codec should store less in terms dict",
        "description": "I'm working on improving Lucene's performance with int block codecs\n(FOR/PFOR), but in early perf testing I found that these codecs cause\na big perf hit to those MTQs that need to scan many terms but don't\nend up accepting many of those terms (eg fuzzy, wildcard, regexp).\n\nThis is because sep codec stores much more in the terms dict, since\neach file is separate, ie seek points for each of doc, frq, pos, pyl,\nskp files.\n\nSo I'd like to shift these seek points to instead be stored in the doc\nfile, except for the doc seek point itself.  Since a given query will\nalways need to seek to the doc file, this does not add an extra seek.\nBut it saves tons of vInt decodes for the next/seke intensive MTQs...\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-411",
        "summary": "[PATCH] BitSetQuery, FastPrefixQuery, FastWildcardQuery and FastQueryParser",
        "description": "FastPrefixQuery and FastWildcardQuery rewrites to BitSetQuery instead of OR'ed\nBooleanQuery's.  A BitSetQuery contains a BitSet that desginates which document\nshould be included in the search result.  BitSetQuery cannot be used by itself\nwith MultiSearcher as of now.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1685",
        "summary": "Make the Highlighter use SpanScorer by default",
        "description": "I've always thought this made sense, but frankly, it took me a year to get the SpanScorer included with Lucene at all, so I was pretty much ready to move on after I it got in, rather than push for it as a default.\n\nI think it makes sense as the default in Solr as well, and I mentioned that back when it was put in, but alas, its an option there as well.\n\nThe Highlighter package has no back compat req, but custom has been conservative - one reason I havn't pushed for this change before. Might be best to actually make the switch in 3? I could go either way - as is, I know a bunch of people use it, but I'm betting its the large minority. It has never been listed in a changes entry and its not in LIA 1, so you pretty much have to stumble upon it, and figure out what its for.\n\nI'll point out again that its just as fast as the standard scorer for any clause of a query that is not position sensitive. Position sensitive query clauses will obviously be somewhat slower to highlight, but that is because they will be highlighted correctly rather than ignoring position.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3457",
        "summary": "Upgrade to commons-compress 1.2",
        "description": "Commons Compress bug COMPRESS-127 was fixed in 1.2, so the workaround in benchmark's StreamUtils is no longer required. Compress is also used in solr. Replace with new jar in both benchmark and solr and get rid of that workaround.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2206",
        "summary": "integrate snowball stopword lists",
        "description": "The snowball project creates stopword lists as well as stemmers, example: http://svn.tartarus.org/snowball/trunk/website/algorithms/english/stop.txt?view=markup\n\nThis patch includes the following:\n* snowball stopword lists for 13 languages in contrib/snowball/resources\n* all stoplists are unmodified, only added license header and converted each one from whatever encoding it was in to UTF-8\n* added getSnowballWordSet  to WordListLoader, this is because the format of these files is very different, for example it supports multiple words per line and embedded comments.\n\nI did not add any changes to SnowballAnalyzer to actually automatically use these lists yet, i would like us to discuss this in a future issue proposing integrating snowball with contrib/analyzers.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3738",
        "summary": "Be consistent about negative vInt/vLong",
        "description": "Today, write/readVInt \"allows\" a negative int, in that it will encode and decode correctly, just horribly inefficiently (5 bytes).\n\nHowever, read/writeVLong fails (trips an assert).\n\nI'd prefer that both vInt/vLong trip an assert if you ever try to write a negative number... it's badly trappy today.  But, unfortunately, we sometimes rely on this... had we had this assert in 'since the beginning' we could have avoided that.\n\nSo, if we can't add that assert in today, I think we should at least fix readVLong to handle negative longs... but then you quietly spend 9 bytes (even more trappy!).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-494",
        "summary": "Analyzer for preventing overload of search service by queries with common terms in large indexes",
        "description": "An analyzer used primarily at query time to wrap another analyzer and provide a layer of protection\nwhich prevents very common words from being passed into queries. For very large indexes the cost\nof reading TermDocs for a very common word can be  high. This analyzer was created after experience with\na 38 million doc index which had a term in around 50% of docs and was causing TermQueries for \nthis term to take 2 seconds.\n\nUse the various \"addStopWords\" methods in this class to automate the identification and addition of \nstop words found in an already existing index.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1123",
        "summary": "Allow overriding the specification version in MANIFEST.MF",
        "description": "The specification version in MANIFEST.MF should only consist of\ndigits. When we e. g. build a release candidate with a version like\n2.3-rc1 then we have to specify a different specification version.\n\nSee related discussion:\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/56611\n\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-744",
        "summary": "TestFieldsReader - TestLazyPerformance problems w/ permissions in temp dir in multiuser environment",
        "description": "Was trying to setup some enhancements to the nightly builds and the testLazyPerformance test failed in TestFieldsReader since it couldn't delete the lazyDir directory from someone else's running of the test.  Will change it to append user.name System property to the directory name.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2086",
        "summary": "When resolving deletes, IW should resolve in term sort order",
        "description": "See java-dev thread \"IndexWriter.updateDocument performance improvement\".",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2883",
        "summary": "Consolidate Solr  & Lucene FunctionQuery into modules",
        "description": "Spin-off from the [dev list | http://www.mail-archive.com/dev@lucene.apache.org/msg13261.html]  ",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-590",
        "summary": "Demo HTML parser gives incorrect summaries when title is repeated as a heading",
        "description": "If you have an html document where the title is repeated as a heading at the top of the document, the HTMLParser will return the title as the summary, ignoring everything else that was added to the summary. Instead, it should keep the rest of the summary and chop off the title part at the beginning (essentially the opposite). I don't see any benefit to repeating the title in the summary for any case.\n\nIn HTMLParser.jj's getSummary():\n\n    String sum = summary.toString().trim();\n    String tit = getTitle();\n    if (sum.startsWith(tit) || sum.equals(\"\"))\n      return tit;\n    else\n      return sum;\n\nchange it to: (* denotes a line that has changed)\n\n    String sum = summary.toString().trim();\n    String tit = getTitle();\n*    if (sum.startsWith(tit))             // don't repeat title in summary\n*      return sum.substring(tit.length()).trim();\n    else\n      return sum;\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1839",
        "summary": "Scorer.explain is deprecated but abstract, should have impl that throws UnsupportedOperationException",
        "description": "Suggest having Scorer implement explain to throw UnsupportedOperationException\n\nright now, i have to implement this method (because its abstract), and javac yells at me for overriding a deprecated method\n\nif the following implementation is in Scorer, i can remove my \"empty\" implementations of explain from my Scorers\n{code}\n  /** Returns an explanation of the score for a document.\n   * <br>When this method is used, the {@link #next()}, {@link #skipTo(int)} and\n   * {@link #score(HitCollector)} methods should not be used.\n   * @param doc The document number for the explanation.\n   *\n   * @deprecated Please use {@link IndexSearcher#explain}\n   * or {@link Weight#explain} instead.\n   */\n  public Explanation explain(int doc) throws IOException {\n    throw new UnsupportedOperationException();\n  }\n{code}\n\nbest i figure, this shouldn't break back compat (people already have to recompile anyway) (2.9 definitely not binary compatible with 2.4)\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1101",
        "summary": "TokenStream.next(Token) reuse 'policy': calling Token.clear() should be responsibility of producer.",
        "description": "Tokenizers which implement the reuse form of the next method:\n    next(Token result) \nshould reset the postionIncrement of the returned token to 1.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3125",
        "summary": "TestDocValuesIndexing.testAddIndexes failures on docvalues branch",
        "description": "doc values branch r1124825, reproducible \n{code}\n    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.716 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=5939035003978436534:-6429764582682717131\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, BYTES_VAR_DEREF=MockRandom, INTS=Pulsing(freqCutoff=13)}, locale=da_DK, timezone=Asia/Macao\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestDocValuesIndexing]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88582432,total=125632512\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR\n    [junit] null\n    [junit] java.nio.channels.ClosedChannelException\n    [junit]     at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)\n    [junit]     at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:603)\n    [junit]     at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:161)\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)\n    [junit]     at org.apache.lucene.store.DataInput.readInt(DataInput.java:73)\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.readInt(BufferedIndexInput.java:162)\n    [junit]     at org.apache.lucene.store.DataInput.readLong(DataInput.java:115)\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.readLong(BufferedIndexInput.java:175)\n    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readLong(MockIndexInputWrapper.java:136)\n    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:263)\n    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:249)\n    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsReader.getEnum(PackedIntsImpl.java:239)\n    [junit]     at org.apache.lucene.index.values.DocValues.getEnum(DocValues.java:54)\n    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:484)\n    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:202)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED\n{code}\n\nand\n\n{code}\n\n    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.94 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3677966427932339626:-4746638811786223564\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_FIXED_DEREF=MockSep, FLOAT_64=SimpleText}, locale=ca, timezone=Asia/Novosibirsk\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestDocValuesIndexing]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88596152,total=125632512\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR\n    [junit] Bad file descriptor\n    [junit] java.io.IOException: Bad file descriptor\n    [junit]     at java.io.RandomAccessFile.seek(Native Method)\n    [junit]     at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.readInternal(SimpleFSDirectory.java:101)\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)\n    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:105)\n    [junit]     at org.apache.lucene.index.values.Floats$FloatsReader.load(Floats.java:281)\n    [junit]     at org.apache.lucene.index.values.SourceCache$DirectSourceCache.load(SourceCache.java:101)\n    [junit]     at org.apache.lucene.index.values.DocValues.getSource(DocValues.java:101)\n    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getSource(TestDocValuesIndexing.java:472)\n    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:482)\n    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:203)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1840",
        "summary": "QueryUtils should check that equals properly handles null",
        "description": "Its part of the equals contract, but many classes currently violate",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-892",
        "summary": "CompoundFileReader's openInput produces streams that may do an extra buffer copy",
        "description": "Spinoff of LUCENE-888.\n\nThe class for reading from a compound file (CompoundFileReader) has a\nprimary stream which is a BufferedIndexInput when that stream is from\nan FSDirectory (which is the norm).  That is one layer of buffering.\n\nThen, when its openInput is called, a CSIndexInput is created which\nalso subclasses from BufferedIndexInput.  That's a second layer of\nbuffering.\n\nWhen a consumer actually uses that CSIndexInput to read, and a call to\nreadByte or readBytes runs out of what's in the first buffer, it will\ngo to refill its buffer.  But that refill calls the first\nBufferedIndexInput which in turn may refill its buffer (a double\ncopy) by reading the underlying stream.\n\nNot sure how to fix it yet but we should change things to not do the\nextra buffer copy.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3472",
        "summary": "add back Document.getValues()",
        "description": "I'm porting some code to trunk's new Doc/Field apis, and i keep running into this pattern:\n{noformat}\nString[] values = doc.getValues(\"field\");\n{noformat}\n\nBut with the new apis, this becomes a little too verbose:\n\n{noformat}\nIndexableField[] fields = doc.getFields(\"field\");\nString[] values = new String[fields.length];\nfor (int i = 0; i < values.length; i++) {\n  values[i] = fields[i].stringValue();\n}\n{noformat}\n\nI think we should probably add back the sugar api (with the same name) ?\n",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1156",
        "summary": "Wikipedia Document Generation Changes",
        "description": "The EnwikiDocMaker currently produces a fair number of documents that are in the download, but are of dubious use in terms of both benchmarking and indexing.  \n\nThese issues are:\n\n# Redirect (it currently only handles REDIRECT and redirect, but there are documents as Redirect\n# Template files appear to be useless.  These are marked by the term Template: at the beginning of the body.  See for example: http://en.wikipedia.org/wiki/Template:=)\n# Image only pages, as in http://en.wikipedia.org/wiki/Image:Sciencefieldnewark.jpg.jpg  These are about as useful as the Redirects and Templates\n# Files pending deletion:  This one is a bit trickier to handle, but they are generally marked by \"Wikipedia:Votes for deletion\" or some variation of that depending where along it is in being deleted\n\nI think I can implement this such that it is backward compatible, if there is such a need when it comes to the contrib/benchmark suite.\n\n\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3241",
        "summary": "Remove Lucene core's FunctionQuery impls",
        "description": "As part of the consolidation of FunctionQuerys, we want to remove Lucene core's impls.  Included in this work, we will make sure that all the functionality provided by the core impls is also provided by the new module.  Any tests will be ported across too, to increase the test coverage.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": ""
    },
    {
        "key": "LUCENE-1009",
        "summary": "LogByteSizeMergePolicy over-merges with autoCommit=false and documents with term vectors and/or stored fields",
        "description": "Mark Miller noticed this slowdown (see details in LUCENE-994) in his\napp.\n\nThis happens because in SegmentInfo.sizeInBytes(), we just run through\nall files associated with that segment, summing up their byte sizes.\n\nBut in the case of shared doc stores (which happens when\nautoCommit=false), this is not quite correct because those files are\nshared across multiple segments.\n\nI plan to fix sizeInBytes() to not include the size of the doc stores\nwhen they are shared.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1174",
        "summary": "outdated information in Analyzer javadoc",
        "description": "I'm sure you find more ways to improve the javadoc, so feel free to change and extend my patch.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3621",
        "summary": "switch appendingcodec to use appending blocktree",
        "description": "currently it still uses block terms + fixed gap index",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1998",
        "summary": "Use Java 5 enums",
        "description": "Replace the use of o.a.l.util.Parameter with Java 5 enums, deprecating Parameter.\n\nReplace other custom enum patterns with Java 5 enums.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1076",
        "summary": "Allow MergePolicy to select non-contiguous merges",
        "description": "I started work on this but with LUCENE-1044 I won't make much progress\non it for a while, so I want to checkpoint my current state/patch.\n\nFor backwards compatibility we must leave the default MergePolicy as\nselecting contiguous merges.  This is necessary because some\napplications rely on \"temporal monotonicity\" of doc IDs, which means\neven though merges can re-number documents, the renumbering will\nalways reflect the order in which the documents were added to the\nindex.\n\nStill, for those apps that do not rely on this, we should offer a\nMergePolicy that is free to select the best merges regardless of\nwhether they are continuguous.  This requires fixing IndexWriter to\naccept such a merge, and, fixing LogMergePolicy to optionally allow\nit the freedom to do so.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3072",
        "summary": "3.1 fileformats out of date",
        "description": "The 3.1 fileformats is missing the change from LUCENE-2811",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-586",
        "summary": "Very inefficient implementation of MultiTermDocs.skipTo",
        "description": "In our application anytime the index was unoptimized/contained more than one segment there was a sharp drop in performance, which amounted to over 50ms per search on average.  We would consistently see this drop anytime an index went from an optimized state to an unoptimized state.\n\nI tracked down the issue to the implementation of MultiTermDocs.skipTo function (found in MultiReader.java).  Optimized indexes do not use this class during search but unoptimized indexes do.  The comment on this function even explicitly states 'As yet unoptimized implementation.'  It was implemented just by calling 'next' over and over so even if it knew it could skip ahead hundreds of thousands of hits it would not.\n\nSo I re-implemented the function very similar to how the MultiTermDocs.next function was implemented and tested it out on or application for correctness and performance and it passed all our tests and the performance penalty of having multiple segments vanished.  We have already put the new jar onto our production machines.\n\nHere is my implementation of skipTo, which closely mirrors the accepted implementation of 'next', please feel free to test it and commit it.\n\n  /** Much more optimized implementation. Could be\n   * optimized fairly easily to skip entire segments */\n  public boolean skipTo(int target) throws IOException {\n    if (current != null && current.skipTo(target-base)) {\n      return true;\n    } else if (pointer < readers.length) {\n      base = starts[pointer];\n      current = termDocs(pointer++);\n      return skipTo(target);\n    } else\n      return false;\n  }",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2796",
        "summary": "Tests need to clean up after themselves",
        "description": "I havent run 'ant clean' for a while.\n\nThe randomly generated temporarily file names just piled up from running the tests many times... so ant clean is still running after quite a long time.\n\nWe should take the logic in the base solr test cases, and push it into LuceneTestCase, so a test cleans up all its temporary stuff.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1441",
        "summary": "KeywordTokenizer does not set start/end offset of the Token it produces",
        "description": "I think just adding these two lines in the next(Token) method is the right fix:\n\n           reusableToken.setStartOffset(0);\n           reusableToken.setEndOffset(upto);\n\nI don't think this is a back compat issue because the start/end offset are now meaningless since they will inherit whatever the reusable token had previously been used for.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1819",
        "summary": "MatchAllDocsQuery.toString(String field) does not honor the javadoc contract",
        "description": "Should be \n\npublic String toString(String field){\n  return \"*:*\";\n}\n\nQueryParser needs to be able to parse the String form of this query.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2281",
        "summary": "Add doBeforeFlush to IndexWriter",
        "description": "IndexWriter has doAfterFlush which can be overridden by extensions in order to perform operations after flush has been called. Since flush is final, one can only override doAfterFlush. This issue will handle two things:\n# Make doAfterFlush protected, instead of package-private, to allow for easier extendability of IW.\n# Add doBeforeFlush which will be called by flush before it starts, to allow extensions to perform any operations before flush begings.\n\nWill post a patch shortly.\n\nBTW, any chance to get it out in 3.0.1?",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1474",
        "summary": "Incorrect SegmentInfo.delCount when IndexReader.flush() is used",
        "description": "When deleted documents are flushed using IndexReader.flush() the delCount in SegmentInfo is updated based on the current value and SegmentReader.pendingDeleteCount (introduced by LUCENE-1267). It seems that pendingDeleteCount is not reset after the commit, which means after a second flush() or close() of an index reader the delCount in SegmentInfo is incorrect. A subsequent IndexReader.open() call will fail with an error when assertions are enabled. E.g.:\n\njava.lang.AssertionError: delete count mismatch: info=3 vs BitVector=2\n\tat org.apache.lucene.index.SegmentReader.loadDeletedDocs(SegmentReader.java:405)\n[...]",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2955",
        "summary": "Add utitily class to manage NRT reopening",
        "description": "I created a simple class, NRTManager, that tries to abstract away some\nof the reopen logic when using NRT readers.\n\nYou give it your IW, tell it min and max nanoseconds staleness you can\ntolerate, and it privately runs a reopen thread to periodically reopen\nthe searcher.\n\nIt subsumes the SearcherManager from LIA2.  Besides running the reopen\nthread, it also adds the notion of a \"generation\" containing changes\nyou've made.  So eg it has addDocument, returning a long.  You can\nthen take that long value and pass it back to the getSearcher method\nand getSearcher will return a searcher that reflects the changes made\nin that generation.\n\nThis gives your app the freedom to force \"immediate\" consistency (ie\nwait for the reopen) only for those searches that require it, like a\nverifier that adds a doc and then immediately searches for it, but\nalso use \"eventual consistency\" for other searches.\n\nI want to also add support for the new \"applyDeletions\" option when\npulling an NRT reader.\n\nAlso, this is very new and I'm sure buggy -- the concurrency is either\nwrong over overly-locking.  But it's a start...\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3528",
        "summary": "TestNRTManager hang",
        "description": "didn't check 3.x yet, just encountered this one running the tests",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2013",
        "summary": "QueryScorer and SpanRegexQuery are incompatible.",
        "description": "Since the resolution of #LUCENE-1685, users are not supposed to rewrite their queries before submitting them to QueryScorer:\n\nbq.------------------------------------------------------------------------\nbq.r800796 | markrmiller | 2009-08-04 06:56:11 -0700 (Tue, 04 Aug 2009) | 1 line\nbq.\nbq.LUCENE-1685: The position aware SpanScorer has become the default scorer for Highlighting. The SpanScorer implementation has replaced QueryScorer and the old term highlighting QueryScorer has been renamed to QueryTermScorer. Multi-term queries are also now expanded by default. If you were previously rewritting the query for multi-term query highlighting, you should no longer do that (unless you switch to using QueryTermScorer). The SpanScorer API (now QueryScorer) has also been improved to more closely match the API of the previous QueryScorer implementation.\nbq.------------------------------------------------------------------------\n\nThis is a great convenience for the most part, but it's causing me difficulties with SpanRegexQuerys, as the WeightedSpanTermExtractor uses Query.extractTerms() to collect the fields used in the query, but SpanRegexQuery does not implement this method, so highlighting any query with a SpanRegexQuery throws an UnsupportedOpertationException.  If this issue is circumvented, there is still the issue of SpanRegexQuery throwing an exception when someone calls its getSpans() method.\n\nI can provide the patch that I am currently using, but I'm not sure that my solution is optimal.  It adds two methods to SpanQuery: extractFields(Set<String> fields) which is equivalent to fields.add(getField()) except when MaskedFieldQuerys get involved, and mustBeRewrittenToGetSpans() which returns true for SpanQuery, false for SpanTermQuery, and is overridden in each composite SpanQuery to return a value depending on its components.  In this way SpanRegexQuery (and any other custom SpanQuerys) do not need to be adjusted.\n\nCurrently the collection of fields and non-weighted terms are done in a single step.  In the proposed patch the WeightedSpanTerm extraction from a SpanQuery proceeds in two steps.  First, if the QueryScorer's field is null, then the fields are collected from the SpanQuery using the extractFields() method.  Second the terms are collected using extractTerms(), rewriting the query for each field if mustBeRewrittenToGetSpans() returns true.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3207",
        "summary": "CustomScoreQuery calls weight() where it should call createWeight()",
        "description": "Thanks to Uwe for helping me track down this bug after I pulled my hair out for hours on LUCENE-3174.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1147",
        "summary": "add option to CheckIndex to only check certain segments",
        "description": "Simple patch to add -segment option to CheckIndex tool, to have it only check the particular segment, instead of all segments, from your index.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2680",
        "summary": "Improve how IndexWriter flushes deletes against existing segments",
        "description": "IndexWriter buffers up all deletes (by Term and Query) and only\napplies them if 1) commit or NRT getReader() is called, or 2) a merge\nis about to kickoff.\n\nWe do this because, for a large index, it's very costly to open a\nSegmentReader for every segment in the index.  So we defer as long as\nwe can.  We do it just before merge so that the merge can eliminate\nthe deleted docs.\n\nBut, most merges are small, yet in a big index we apply deletes to all\nof the segments, which is really very wasteful.\n\nInstead, we should only apply the buffered deletes to the segments\nthat are about to be merged, and keep the buffer around for the\nremaining segments.\n\nI think it's not so hard to do; we'd have to have generations of\npending deletions, because the newly merged segment doesn't need the\nsame buffered deletions applied again.  So every time a merge kicks\noff, we pinch off the current set of buffered deletions, open a new\nset (the next generation), and record which segment was created as of\nwhich generation.\n\nThis should be a very sizable gain for large indices that mix\ndeletes, though, less so in flex since opening the terms index is much\nfaster.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1740",
        "summary": "Lucli: Command to change the Analyzer",
        "description": "Currently, Lucli is hardcoded to use StandardAnalyzer. The provided patch introduces a command \"analyzer\" to specify a different Analyzer class. \nIf something fails, StandardAnalyzer is the fall-back.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-800",
        "summary": "Incorrect parsing by QueryParser.parse() when it encounters backslashes (always eats one backslash.)",
        "description": "Test code and output follow. Tested  Lucene 1.9 version only. Affects hose who would index/search for Lucene's reserved characters.\n\nDescription: When an input search string has a sequence of N (java-escaped) backslashes, where N >= 2, the QueryParser will produce a query in which that sequence has N-1 backslashes.\n\nTEST CODE:\n    Analyzer analyzer = new WhitespaceAnalyzer();\n    String[] queryStrs = {\"item:\\\\\\\\\",\n                          \"item:\\\\\\\\*\",\n                          \"(item:\\\\\\\\ item:ABCD\\\\\\\\))\",\n                          \"(item:\\\\\\\\ item:ABCD\\\\\\\\)\"};\n    for (String queryStr : queryStrs) {\n      System.out.println(\"--------------------------------------\");\n      System.out.println(\"String queryStr = \" + queryStr);\n      Query luceneQuery = null;\n      try {\n        luceneQuery = new QueryParser(\"_default_\", analyzer).parse(queryStr);\n        System.out.println(\"luceneQuery.toString() = \" + luceneQuery.toString());\n      } catch (Exception e) {\n        System.out.println(e.getClass().toString());\n      }\n    }\n\nOUTPUT (with remarks in comment notation:) \n--------------------------------------\nString queryStr = item:\\\\\nluceneQuery.toString() = item:\\             //One backslash has disappeared. Searcher will fail on this query.\n--------------------------------------\nString queryStr = item:\\\\*\nluceneQuery.toString() = item:\\*           //One backslash has disappeared. This query will search for something unintended.\n--------------------------------------\nString queryStr = (item:\\\\ item:ABCD\\\\))\nluceneQuery.toString() = item:\\ item:ABCD\\)     //This should have thrown a ParseException because of an unescaped ')'. It did not.\n--------------------------------------\nString queryStr = (item:\\\\ item:ABCD\\\\)\nclass org.apache.lucene.queryParser.ParseException        //...and this one should not have, but it did.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2826",
        "summary": "LineDocSource should assign stable IDs; docdate field should be NumericField",
        "description": "Some small enhancements when indexing docs from a line doc source:\n\n  * Assign docid by line number instead of by number-of-docs-indexed;\n    this makes the resulting ID stable when using multiple threads\n\n  * The docdate field is now indexed as a String (possible created\n    through DateTools).  I added two numeric fields: one that indexes\n    .getTime() (= long msec) and another that indexes seconds since\n    the day started.  This gives us two numeric fields to play\n    with...\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-289",
        "summary": "[patch] better support gcj compilation",
        "description": "There are two methods in IndexReader.java called 'delete'. That is a reserved\nkeyword in C++ and these methods cause trouble for gcj which implements a clever\nworkaround in renaming them delete$ but the OS X dynamic linker doesn't pick-up\non it.\nThe attached patch renames delete(int) to deleteDocument(int) and delete(Term)\nto deleteDocuments(Term) and deprecates the delete methods (as requested by Doug\nCutting).",
        "label": "NUG",
        "classified": "OTHER",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3455",
        "summary": "All Analysis Consumers should use reusableTokenStream",
        "description": "With Analyzer now using TokenStreamComponents, theres no reason for Analysis consumers to use tokenStream() (it just gives bad performance).  Consequently all consumers will be moved over to using reusableTokenStream().  The only challenge here is that reusableTokenStream throws an IOException which many consumers are not rigged to deal with.\n\nOnce all consumers have been moved, we can rename reusableTokenStream() back to tokenStream().",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": ""
    },
    {
        "key": "LUCENE-1811",
        "summary": "TestIndexReaderReopen nightly build failure",
        "description": "An interesting failure in last night's build (http://hudson.zones.apache.org/hudson/job/Lucene-trunk/920).\n\nI think the root cause wast he AIOOB exception... all the \"lock obtain timed out\" exceptions look like they cascaded.\n\n{code}\n    [junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen\n    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock)\n    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 31.087 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 148\n    [junit] \tat org.apache.lucene.util.BitVector.getAndSet(BitVector.java:74)\n    [junit] \tat org.apache.lucene.index.SegmentReader.doDelete(SegmentReader.java:908)\n    [junit] \tat org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)\n    [junit] \tat org.apache.lucene.index.DirectoryReader.doDelete(DirectoryReader.java:521)\n    [junit] \tat org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:638)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)\n    [junit] org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@88d319: write.lock\n    [junit] \tat org.apache.lucene.store.Lock.obtain(Lock.java:85)\n    [junit] \tat org.apache.lucene.index.DirectoryReader.acquireWriteLock(DirectoryReader.java:666)\n    [junit] \tat org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:994)\n    [junit] \tat org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:1020)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:634)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)\n    ...\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):\tFAILED\n    [junit] Error occurred in thread Thread-36:\n    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock\n    [junit] junit.framework.AssertionFailedError: Error occurred in thread Thread-36:\n    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:764)\n    [junit] \n    [junit] \n{code}",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-285",
        "summary": "David Spencer Spell Checker improved",
        "description": "hy,\ni developed a SpellChecker based on the David Spencer code (DSc) but more flexible.\nthe structure of the index is inspired of the DSc (for a 3-4 gram):\nword:\ngram3:\ngram4:\n \n3start:\n4start:\n..\n3end:\n4end:\n..\ntransposition:\n \nThis index is a dictonary so there isn't the \"freq\" field like with DSc version.\nit's independant of the user index. So we can add words becoming to several\nfields of several index for example or, why not, to a file with a list of words.\nThe suggestSimilar method return a list of suggests word sorted by the\nLevenshtein distance and optionaly to the popularity of the word for a specific\nfield in a user index. More of that, this list can be restricted only to words\npresent in a specific field of a user index.\n \nSee the test case.\n \ni hope this code will be put in the lucene sandbox. \n \nNicolas Maisonneuve",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2591",
        "summary": "TestNLS fails with ja locale",
        "description": "set ANT_ARGS=\"-Dargs=-Duser.language=ja -Duser.country=JP\"\nant test-core -Dtestcase=TestNLS\n\nThe test has 2 sets of message, one fallback, and one ja.\nThe tests assume if it asks for a non-ja locale, that it will get the fallback message,\nbut this is not how ResourceBundle.getBundle works:\n{noformat}\nOtherwise, the following sequence is generated from the attribute values of the specified locale \n(language1, country1, and variant1) and of the default locale (language2, country2, and variant2):\n\nbaseName + \"_\" + language1 + \"_\" + country1 + \"_\" + variant1\nbaseName + \"_\" + language1 + \"_\" + country1\nbaseName + \"_\" + language1\nbaseName + \"_\" + language2 + \"_\" + country2 + \"_\" + variant2\nbaseName + \"_\" + language2 + \"_\" + country2\nbaseName + \"_\" + language2\nbaseName\n{noformat}\n\nSo in the case of ja default locale, you get a japanese message instead from the baseName + \"_\" + language2 match",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-486",
        "summary": "Core Test should not have dependencies on the Demo code",
        "description": "The TestDoc.java Test file has a dependency on the Demo FileDocument code.  Some of us don't keep the Demo code around after downloading, so this breaks the build.\n\nPatch will be along shortly",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3436",
        "summary": "Spellchecker \"Suggest Mode\" Support",
        "description": "This is a spin-off from SOLR-2585.\n\nCurrently o.a.l.s.s.SpellChecker and o.a.l.s.s.DirectSpellChecker support two \"Suggest Modes\":\n1. Suggest for terms that are not in the index.\n2. Suggest \"more popular\" terms.\n\nThis issue is to add a third Suggest Mode:\n3. Suggest always.\n\nThis will assist users in developing context-sensitive spell suggestions and/or did-you-mean suggestions.  See SOLR-2585 for a full discussion.\n\nNote that o.a.l.s.s.SpellChecker already can support this functionality, if the user passes in a NULL term & IndexReader.  This, however, is not intutive.  o.a.l.s.s.DirectSpellChecker currently has no support for this third Suggest Mode.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1960",
        "summary": "Remove deprecated Field.Store.COMPRESS",
        "description": "Also remove FieldForMerge and related code.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3872",
        "summary": "Index changes are lost if you call prepareCommit() then close()",
        "description": "You are supposed to call commit() after calling prepareCommit(), but... if you forget, and call close() after prepareCommit() without calling commit(), then any changes done after the prepareCommit() are silently lost (including adding/deleting docs, but also any completed merges).\n\nSpinoff from java-user thread \"lots of .cfs (compound files) in the index directory\" from Tim Bogaert.\n\nI think to fix this, IW.close should throw an IllegalStateException if prepareCommit() was called with no matching call to commit().",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2037",
        "summary": "Allow Junit4 tests in our environment.",
        "description": "Now that we're dropping Java 1.4 compatibility for 3.0, we can incorporate Junit4 in testing. Junit3 and junit4 tests can coexist, so no tests should have to be rewritten. We should start this for the 3.1 release so we can get a clean 3.0 out smoothly.\n\nIt's probably worthwhile to convert a small set of tests as an exemplar.\n\n\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3123",
        "summary": "TestIndexWriter.testBackgroundOptimize fails with too many open files",
        "description": "Recreate with this line:\n\nant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240\n\nMight be related to LUCENE-2873 ?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3856",
        "summary": "Create docvalues based grouped facet collector",
        "description": "Create docvalues based grouped facet collector. Currently only term based collectors have been implemented (LUCENE-3802).",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3599",
        "summary": "haversine() is broken / misdocumented",
        "description": "DistanceUtils.haversine() is coded in a way that is erroneous based on the documented order of the parameters.  The parameters are defined as (x1,y1,x2,y2,radius)  -- i.e. lon,lat order.  The code implementing the algorithm, however, is as if the meaning of x and y are transposed, which means that if you supply the arguments in lat,lon (y,x) order, you will get the correct behavior.  It turns out that all callers of this method do this!\n\nFYI I found out about this bug since it is inherited code in LSP (lucene-spatial-playground) and I have been supplying parameters according to its documented order.  Apparently I shouldn't do that ;-)",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1185",
        "summary": "[PATCH] Avoid checking for TermBuffer in SegmentTermEnum#scanTo",
        "description": "It seems that SegmentTermEnum#scanTo is a critical method which is called very often, especially whenever we iterate over a sequence of terms in an index.\n\nWhen that method is called, the first thing happens is that it checks whether a temporary TermBuffer \"scratch\" has already been initialized.\n\nIn fact, this is not necessary. We can simply declare and initialize the \"scratch\"-Buffer at the class-level (right now, the initial value is _null_). Java's lazy-loading capabilities allow this without adding any memory footprint for cases where we do not need that buffer.\n\nThe attached patch takes care of this. We now save one comparison per term.\nIn addition to that, the patch renames \"scratch\" to \"scanBuffer\", which aligns with the naming of the other two buffers that are declared in the class.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2833",
        "summary": "upgrade contrib/ant's tidy.jar",
        "description": "contrib/ant uses a Tidy.jar that also includes classes in org.w3c.dom, org.xml.sax, etc.\n\nThis is no problem if you are an ant user, but if you are an IDE user you need to carefully configure the order of your classpath or things will not compile, as these will override the ones in the Solr libs, for example.\n\nThe solution is to upgrade the tidy.jar to the newest one that only includes org.w3c.tidy and doesn't cause these problems.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-378",
        "summary": "GCJ makefile hardcodes compiler commands",
        "description": "src/gcj/Makefile hardcodes the command names for gcj, gcjh, and g++. This makes it difficult to \ncompile with a particular version of GCJ if multiple are installed with suffixes (eg, gcj-4.0)\n\nSteps to reproduce:\n1. Configure, compile, and install GCC/GCJ with something like --program-suffix=-4.0\n2. cd ~/src/lucene && ant gcj\n\nExpected results:\nSomehow be able to specify my compiler.\n\nActual results:\nCan't find 'gcj' executable, or worse runs wrong version. :)\n\nSuggested fix: as is common with variable names like CC to force a C compiler, allow the builder to \noverride the compiler commands used by setting optional environment variables GCJ etc.\nPatch to be attached.\n\nAdditional info:\nBuilding Lucene from SVN 2005-04-19.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-912",
        "summary": "DisjunctionMaxScorer.skipTo has bug that keeps it from skipping",
        "description": "as reported on the mailing list, DisjunctionMaxScorer.skipTo is broken if called before next in some situations...\n\nhttp://www.nabble.com/Potential-issue-with-DisjunctionMaxScorer-tf3846366.html#a10894987",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2219",
        "summary": "improve BaseTokenStreamTestCase to test end()",
        "description": "If offsetAtt/end() is not implemented correctly, then there can be problems with highlighting: see LUCENE-2207 for an example with CJKTokenizer.\n\nIn my opinion you currently have to write too much code to test this.\n\nThis patch does the following:\n* adds optional Integer finalOffset (can be null for no checking) to assertTokenStreamContents\n* in assertAnalyzesTo, automatically fill this with the String length()\n\nIn my opinion this is correct, for assertTokenStreamContents the behavior should be optional, it may not even have a Tokenizer. If you are using assertTokenStreamContents with a Tokenizer then simply provide the extra expected value to check it.\n\nfor assertAnalyzesTo then it is implied there is a tokenizer so it should be checked.\n\nthe tests pass for core but there are failures in contrib even besides CJKTokenizer (apply Koji's patch from LUCENE-2207, it is correct). Specifically ChineseTokenizer has a similar problem.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3190",
        "summary": "TestStressIndexing2 testMultiConfig failure",
        "description": "trunk: r1134311\n\nreproducible\n\n{code}\n    [junit] Testsuite: org.apache.lucene.index.TestStressIndexing2\n    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 0.882 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808\n    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:102)\n    [junit]     at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:164)\n    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:380)\n    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1473)\n    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1445)\n    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.indexDoc(TestStressIndexing2.java:723)\n    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:757)\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressIndexing2 -Dtestmethod=testMultiConfig -Dtests.seed=2571834029692482827:-8116419692655152763\n    [junit] The following exceptions were thrown by threads:\n    [junit] *** Thread: Thread-0 ***\n    [junit] junit.framework.AssertionFailedError: java.lang.AssertionError: ram was 460908 expected: 408216 flush mem: 395100 active: 65808\n    [junit]     at junit.framework.Assert.fail(Assert.java:47)\n    [junit]     at org.apache.lucene.index.TestStressIndexing2$IndexingThread.run(TestStressIndexing2.java:762)\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {f33=Standard, f57=MockFixedIntBlock(blockSize=649), f11=Standard, f41=MockRandom, f40=Standard, f62=MockRandom, f75=Standard, f73=MockSep, f29=MockFixedIntBlock(blockSize=649), f83=MockRandom, f66=MockSep, f49=MockVariableIntBlock(baseBlockSize=9), f72=Pulsing(freqCutoff=7), f54=Standard, id=MockFixedIntBlock(blockSize=649), f80=MockRandom, f94=MockSep, f93=Pulsing(freqCutoff=7), f95=Standard}, locale=en_SG, timezone=Pacific/Palau\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestStressIndexing2]\n    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=133324528,total=158400512\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED\n    [junit] r1.numDocs()=17 vs r2.numDocs()=16\n    [junit] junit.framework.AssertionFailedError: r1.numDocs()=17 vs r2.numDocs()=16\n    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:308)\n    [junit]     at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:278)\n    [junit]     at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:124)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)\n    [junit] \n    [junit] \n    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):     FAILED\n    [junit] Some threads threw uncaught exceptions!\n    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!\n    [junit]     at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:603)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.index.TestStressIndexing2 FAILED\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3463",
        "summary": "Jenkins trunk tests (nightly only) fail quite often with OOM in Automaton/FST tests",
        "description": "The nightly Job Lucene-trunk quite often fails with OOM (in several methods, not always in the same test):\n\nExample from last night (this time a huge Automaton):\n\n{noformat}\n[junit] java.lang.OutOfMemoryError: Java heap space\n[junit] Dumping heap to /home/hudson/hudson-slave/workspace/Lucene-trunk/heapdumps/java_pid38855.hprof ...\n[junit] Heap dump file created [86965954 bytes in 1.186 secs]\n[junit] Testsuite: org.apache.lucene.index.TestTermsEnum\n[junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):\tCaused an ERROR\n[junit] Java heap space\n[junit] java.lang.OutOfMemoryError: Java heap space\n[junit] \tat org.apache.lucene.util.automaton.RunAutomaton.<init>(RunAutomaton.java:128)\n[junit] \tat org.apache.lucene.util.automaton.ByteRunAutomaton.<init>(ByteRunAutomaton.java:28)\n[junit] \tat org.apache.lucene.util.automaton.CompiledAutomaton.<init>(CompiledAutomaton.java:134)\n[junit] \tat org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:266)\n[junit] \tat org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)\n[junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)\n[junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)\n[junit] \n[junit] \n[junit] Tests run: 6, Failures: 0, Errors: 1, Time elapsed: 11.699 sec\n{noformat}\n\nOther traces:\n\n{noformat}\n[junit] Testsuite: org.apache.lucene.util.fst.TestFSTs\n[junit] Testcase: testRealTerms(org.apache.lucene.util.fst.TestFSTs):\tCaused an ERROR\n[junit] Java heap space\n[junit] java.lang.OutOfMemoryError: Java heap space\n[junit] \tat org.apache.lucene.util.ArrayUtil.grow(ArrayUtil.java:338)\n[junit] \tat org.apache.lucene.util.fst.FST$BytesWriter.writeBytes(FST.java:927)\n[junit] \tat org.apache.lucene.util.fst.ByteSequenceOutputs.write(ByteSequenceOutputs.java:113)\n[junit] \tat org.apache.lucene.util.fst.ByteSequenceOutputs.write(ByteSequenceOutputs.java:32)\n[junit] \tat org.apache.lucene.util.fst.FST.addNode(FST.java:451)\n[junit] \tat org.apache.lucene.util.fst.NodeHash.add(NodeHash.java:122)\n[junit] \tat org.apache.lucene.util.fst.Builder.compileNode(Builder.java:180)\n[junit] \tat org.apache.lucene.util.fst.Builder.finish(Builder.java:495)\n[junit] \tat org.apache.lucene.index.codecs.memory.MemoryCodec$TermsWriter.finish(MemoryCodec.java:232)\n[junit] \tat org.apache.lucene.index.FreqProxTermsWriterPerField.flush(FreqProxTermsWriterPerField.java:414)\n[junit] \tat org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:92)\n[junit] \tat org.apache.lucene.index.TermsHash.flush(TermsHash.java:117)\n[junit] \tat org.apache.lucene.index.DocInverter.flush(DocInverter.java:80)\n[junit] \tat org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:78)\n[junit] \tat org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:472)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:420)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:568)\n[junit] \tat org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:366)\n[junit] \tat org.apache.lucene.index.IndexReader.open(IndexReader.java:317)\n[junit] \tat org.apache.lucene.util.fst.TestFSTs.testRealTerms(TestFSTs.java:1034)\n[junit] \tat org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)\n{noformat}\n\nor:\n\n{noformat}\n[junit] Testsuite: org.apache.lucene.util.automaton.TestCompiledAutomaton\n[junit] Testcase: testRandom(org.apache.lucene.util.automaton.TestCompiledAutomaton):\tCaused an ERROR\n[junit] Java heap space\n[junit] java.lang.OutOfMemoryError: Java heap space\n[junit] \tat org.apache.lucene.util.automaton.RunAutomaton.<init>(RunAutomaton.java:128)\n[junit] \tat org.apache.lucene.util.automaton.ByteRunAutomaton.<init>(ByteRunAutomaton.java:28)\n[junit] \tat org.apache.lucene.util.automaton.CompiledAutomaton.<init>(CompiledAutomaton.java:134)\n[junit] \tat org.apache.lucene.util.automaton.TestCompiledAutomaton.build(TestCompiledAutomaton.java:39)\n[junit] \tat org.apache.lucene.util.automaton.TestCompiledAutomaton.testTerms(TestCompiledAutomaton.java:55)\n[junit] \tat org.apache.lucene.util.automaton.TestCompiledAutomaton.testRandom(TestCompiledAutomaton.java:101)\n[junit] \tat org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)\n[junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)\n[junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)\n{noformat}\n\nAlmost every nightly test fails, history: [https://builds.apache.org/job/Lucene-trunk]\n\nWe should maybe raise the max heap space or reduce doc counts/...",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1347",
        "summary": "IndexWriter.rollback can hang if a previous call hit an exception",
        "description": "IW.rollback has logic to make sure only one thread actually gets to do\nthe rollback whenever multiple threads are calling it at the same\ntime, by setting the \"boolean closing\" to true in the thread that got\nthere first.\n\nOther threads wait for that variable to become false again before\nreturning from abort.\n\nBut, we are not restoring closing to false in a try/finally in\nrollback(), which means on hitting an exception in rollback, a\nsubsequent call to rollback() will hang forever.\n\nclose() has the same logic, but there is already a try/finally there\nto restore closing to false on exception.\n\nThe fix is straightforward.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2730",
        "summary": "intermittent deadlock in TestAtomicUpdate,TestIndexWriterExceptions",
        "description": "While backporting issues for 2.9.x/3.0.x release I hit deadlocks in these two tests, under both test-core and test-tag.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1779",
        "summary": "Remove unused \"numSlotsFull\" from FieldComparator.setNextReader",
        "description": "This param is a relic from older optimizations that we've since turned off, and it's quite confusing.  I don't think we need it, and we haven't released the API yet so we're free to remove it now.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-443",
        "summary": "ConjunctionScorer tune-up",
        "description": "I just recently ran a load test on the latest code from lucene , which is using a new BooleanScore and noticed the ConjunctionScorer was crunching through objects , especially while sorting as part of the skipTo call. It turns a linked list into an array, sorts the array, then converts the array back to a linked list for further processing by the scoring engines below.\n\n'm not sure if anyone else is experiencing this as I have a very large index (> 4 million items) and I am issuing some heavily nested queries\n\nAnyway, I decide to change the link list into an array and use a first and last marker to \"simulate\" a linked list.\n\nThis scaled much better during my load test as the java gargbage collector was less - umm - virulent ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-265",
        "summary": "[PATCH] to remove synchronized code from TermVectorsReader",
        "description": "Otis,\n\nhere the latest and last patch to get rid of all synchronized code from\nTermVectorsReader. It should include at least 3 files, TermVectorsReader.diff,\nSegmentReader.diff and the new junit test case TestMultiThreadTermVectors.java.\nThe patch was generated against the current CVS version of TermVectorsReader and\nSegmentReader. All lucene related junit tests pass fine.\n\nbest regards\nBernhard",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2811",
        "summary": "SegmentInfo should explicitly track whether that segment wrote term vectors",
        "description": "Today SegmentInfo doesn't know if it has vectors, which means its files() method must check if the files exist.\n\nThis leads to subtle bugs, because Si.files() caches the files but then we fail to invalidate that later when the term vectors files are created.\n\nIt also leads to sloppy code, eg TermVectorsReader \"gracefully\" handles being opened when the files do not exist.  I don't like that; it should only be opened if they exist.\n\nThis also fixes these intermittent failures we've been seeing:\n\n{noformat}\njunit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx\n       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)\n       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)\n       at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)\n       at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)\n       at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)\n       at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)\n       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)\n       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)\n       at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2636",
        "summary": "Create ChainingCollector",
        "description": "ChainingCollector allows chaining a bunch of Collectors, w/o them needing to know or care about each other, and be passed into Lucene's search API, since it is a Collector on its own. It is a convenient, yet useful, class. Will post a patch w/ it shortly.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3385",
        "summary": "EasySimilarity to interpret document length as float",
        "description": "",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": ""
    },
    {
        "key": "LUCENE-2985",
        "summary": "Build SegmentCodecs incrementally for consistent codecIDs during indexing",
        "description": "currently we build the SegementCodecs during flush which is fine as long as no codec needs to know which fields it should handle. This will change with DocValues or when we expose StoredFields / TermVectors via Codec (see LUCENE-2621 or LUCENE-2935). The other downside it that we don't have a consistent view of which codec belongs to which field during indexing and all FieldInfo instances are unassigned (set to -1). Instead we should build the SegmentCodecs incrementally as fields come in so no matter when a codec needs to be selected to process a document / field we have the right codec ID assigned.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2108",
        "summary": "SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internally",
        "description": "I can't find any way to close the IndexSearcher (and IndexReader) that\nis being used by SpellChecker internally.\n\nI've worked around this issue by keeping a single SpellChecker open\nfor each index, but I'd really like to be able to close it and\nreopen it on demand without leaking file descriptors.\n\nCould we add a close() method to SpellChecker that will close the\nIndexSearcher and null the reference to it? And perhaps add some code\nthat reopens the searcher if the reference to it is null? Or would\nthat break thread safety of SpellChecker?\n\nThe attached patch adds a close method but leaves it to the user to\ncall setSpellIndex to reopen the searcher if desired.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-465",
        "summary": "surround test code is incompatible with *Test pattern in test target.",
        "description": "Attachments to follow:\nrenamed BooleanQueryTest to BooleanQueryTst,\nrenamed ExceptionQueryTest to ExceptionQueryTst,\npatch for the remainder of the test code to use the renamed classes.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-462",
        "summary": "bad normalization in sorted search returning TopDocs",
        "description": "FieldSortedHitQueue.maxscore is maintained in the lessThan method (which never gets called if a single document is added to the queue).\n\nI've checked in a test to TestSort.testTopDocsScores() with the final assertion commented out.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3734",
        "summary": "Allow customizing/subclassing of DirectoryReader",
        "description": "DirectoryReader is final and has only static factory methods. It is not possible to subclass it in any way.\n\nThe problem is mainly Solr, as Solr accesses directory(), IndexCommits,... and therefore cannot work on abstract IndexReader anymore. This should be changed, by e.g. handling reopening in the IRFactory, also versions, commits,... Currently its not possible to implement any other IRFactory that returns something else.\n\nOn the other hand, it should be possible to \"wrap\" a DirectoryReader / CompositeReader to handle filtering of collection based information (subreaders, reopening hooks,...). This can be done by making DirectoryReader abstract and let DirectoryReader.open return a internal hidden class \"StandardDirectoryReader\". This is similar to the relatinship between IndexReader and hidden DirectoryReader in the past.\n\nDirectoryReader will have final implementations of most methods like getting document stored fields, global docFreq and other statistics, but allows hooking into doOpenIfChanged. Also it should not be limited to SegmentReaders as childs - any AtomicReader is fine. This allows users to create e.g. a Directory-based ParallelReader (see LUCENE-3736) that supports reopen and (partially commits).",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": ""
    },
    {
        "key": "LUCENE-1022",
        "summary": "IndexWriter.setInfoStream should percolate down to mergePolicy & mergeScheduler",
        "description": "Right now *MergePolicy and *MergeScheduler have their own ad-hoc means\nof being verbose about their actions.  We should unify these with\nIndexWriter's infoStream.  Thanks to Hoss for suggesting this.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3915",
        "summary": "Add Japanese filter to replace term attribute with readings",
        "description": "Koji and Robert are working on LUCENE-3888 that allows spell-checkers to do their similarity matching using a different word than its surface form.\n\nThis approach is very useful for languages such as Japanese where the surface form and the form we'd like to use for similarity matching is very different.  For Japanese, it's useful to use readings for this -- probably with some normalization.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1279",
        "summary": "RangeQuery and RangeFilter should use collation to check for range inclusion",
        "description": "See [this java-user discussion|http://www.nabble.com/lucene-farsi-problem-td16977096.html] of problems caused by Unicode code-point comparison, instead of collation, in RangeQuery.\n\nRangeQuery could take in a Locale via a setter, which could be used with a java.text.Collator and/or CollationKey's, to handle ranges for languages which have alphabet orderings different from those in Unicode.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2261",
        "summary": "configurable MultiTermQuery TopTermsScoringBooleanRewrite pq size",
        "description": "MultiTermQuery has a TopTermsScoringBooleanRewrite, that uses a priority queue to expand the query to the top-N terms.\n\ncurrently N is hardcoded at BooleanQuery.getMaxClauseCount(), but it would be nice to be able to set this for top-N MultiTermQueries: e.g. expand a fuzzy query to at most only the 50 closest terms.\n\nat a glance it seems one way would be to expose TopTermsScoringBooleanRewrite (it is private right now) and add a ctor to it, so a MultiTermQuery can instantiate one with its own limit.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1118",
        "summary": "core analyzers should not produce tokens > N (100?) characters in length",
        "description": "Discussion that led to this:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/56103\n\nI believe nearly any time a token > 100 characters in length is\nproduced, it's a bug in the analysis that the user is not aware of.\n\nThese long tokens cause all sorts of problems, downstream, so it's\nbest to catch them early at the source.\n\nWe can accomplish this by tacking on a LengthFilter onto the chains\nfor StandardAnalyzer, SimpleAnalyzer, WhitespaceAnalyzer, etc.\n\nShould we do this in 2.3?  I realize this is technically a break in\nbackwards compatibility, however, I think it must be incredibly rare\nthat this break would in fact break something real in the application?",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-660",
        "summary": "GData html render preview",
        "description": "GData output is usually ATOM / RSS e.g plain xml. This feature enables users or admins to preview the server output as html transformed by user defined xsl stylesheet. Stylesheet is configurable per service.\n\nThat's just a cool feature for developing and for users wanna use the server for simple blog or feed server.\n\nregards simon",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3225",
        "summary": "Optimize TermsEnum.seek when caller doesn't need next term",
        "description": "Some codecs are able to save CPU if the caller is only interested in\nexact matches.  EG, Memory codec and SimpleText can do more efficient\nFSTEnum lookup if they know the caller doesn't need to know the term\nfollowing the seek term.\n\nWe have cases like this in Lucene, eg when IW deletes documents by\nTerm, if the term is not found in a given segment then it doesn't need\nto know the ceiling term.  Likewise when TermQuery looks up the term\nin each segment.\n\nI had done this change as part of LUCENE-3030, which is a new terms\nindex that's able to save seeking for exact-only lookups, but now that\nwe have Memory codec that can also save CPU I think we should commit\nthis today.\n\nThe change adds a \"boolean onlyExact\" param to seek(BytesRef).\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1722",
        "summary": "SmartChineseAnalyzer javadoc improvement",
        "description": "Chinese -> English, and corrections to match reality (removes several javadoc warnings)",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3171",
        "summary": "BlockJoinQuery/Collector",
        "description": "I created a single-pass Query + Collector to implement nested docs.\nThe approach is similar to LUCENE-2454, in that the app must index\ndocuments in \"join order\", as a block (IW.add/updateDocuments), with\nthe parent doc at the end of the block, except that this impl is one\npass.\n\nOnce you join at indexing time, you can take any query that matches\nchild docs and join it up to the parent docID space, using\nBlockJoinQuery.  You then use BlockJoinCollector, which sorts parent\ndocs by provided Sort, to gather results, grouped by parent; this\ncollector finds any BlockJoinQuerys (using Scorer.visitScorers) and\nretains the child docs corresponding to each collected parent doc.\n\nAfter searching is done, you retrieve the TopGroups from a provided\nBlockJoinQuery.\n\nLike LUCENE-2454, this is less general than the arbitrary joins in\nSolr (SOLR-2272) or parent/child from ElasticSearch\n(https://github.com/elasticsearch/elasticsearch/issues/553), since you\nmust do the join at indexing time as a doc block, but it should be\nable to handle nested joins as well as joins to multiple tables,\nthough I don't yet have test cases for these.\n\nI put this in a new Join module (modules/join); I think as we\nrefactor join impls we should put them here.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2124",
        "summary": "move JDK collation to core, ICU collation to ICU contrib",
        "description": "As mentioned on the list, I propose we move the JDK-based CollationKeyFilter/CollationKeyAnalyzer, currently located in contrib/collation into core for collation support (language-sensitive sorting)\n\nThese are not much code (the heavy duty stuff is already in core, IndexableBinaryString). \n\nAnd I would also like to move the ICUCollationKeyFilter/ICUCollationKeyAnalyzer (along with the jar file they depend on) also currently located in contrib/collation into a contrib/icu.\n\nThis way, we can start looking at integrating other functionality from ICU into a fully-fleshed out icu contrib.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-538",
        "summary": "Using WildcardQuery with MultiSearcher, and Boolean MUST_NOT clause",
        "description": "We are searching across multiple indices using a MultiSearcher. There seems to be a problem when we use a WildcardQuery to exclude documents from the result set. I attach a set of unit tests illustrating the problem.\n\nIn these tests, we have two indices. Each index contains a set of documents with fields for 'title',  'section' and 'index'. The final aim is to do a keyword search, across both indices, on the title field and be able to exclude documents from certain sections (and their subsections) using a\nWildcardQuery on the section field.\n \n e.g. return documents from both indices which have the string 'xyzpqr' in their title but which do not lie\n in the news section or its subsections (section = /news/*).\n \nThe first unit test (testExcludeSectionsWildCard) fails trying to do this.\n If we relax any of the constraints made above, tests pass:\n \n* Don't use WildcardQuery, but pass in the news section and it's child section to exclude explicitly (testExcludeSectionsExplicit)</li>\n* Exclude results from just one section, not it's children too i.e. don't use WildcardQuery(testExcludeSingleSection)</li>\n* Do use WildcardQuery, and exclude a section and its children, but just use one index thereby using the simple\n   IndexReader and IndexSearcher objects (testExcludeSectionsOneIndex).\n* Try the boolean MUST clause rather than MUST_NOT using the WildcardQuery i.e. only include results from the /news/ section\n   and its children.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-896",
        "summary": "Let users set Similarity for MoreLikeThis",
        "description": "Let users set Similarity used for MoreLikeThis\n\nFor discussion, see:\nhttp://www.nabble.com/MoreLikeThis-API-changes--tf3838535.html",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-713",
        "summary": "File Formats Documentation is not correct for Term Vectors",
        "description": "From Samir Abdou on the dev mailing list:\n\nHi, \n\nThere is an inconsistency between the files format page (from Lucene\nwebsite) and the source code. It concerns the positions and offsets of term\nvectors. It seems that documentation (website) is not up to date. According\nto the file format page, offsets and positions are not stored! Is that\ncorrect?\n\nMany thanks,\n\nSamir\n-----\nIndeed, in the file formats term vectors section it doesn't talk about the storing of position and offset info.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1075",
        "summary": "Possible thread hazard in IndexWriter.close(false)",
        "description": "Spinoff from this thread:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/55391\n\nOn reviewing the code I found one case where an aborted merge (from\ncalling close(false)) could write to files that a newly opened\nIndexWriter would also try to write to.\n\nI strengthened an existing test case in TestConcurrentMergeScheduler\nto tickle this case, and also modified MockRAMDirectory to throw an\nIOException if ever a file besides segments.gen is overwritten.\n\nHowever, strangely, I can't get an unhandled exception to occur during\nthe test and I'm not sure why.  Still I think this is a good defensive\ncheck so we should commit it.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1925",
        "summary": "In IndexSearcher class, make subReader and docCount arrays protected so sub classes can access them",
        "description": "Please make these two member variables protected so subclasses can access them, e.g.:\n\n  protected IndexReader[] subReaders;\n  protected int[] docStarts;\n\nThanks",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3293",
        "summary": "Use IOContext.READONCE in VarGapTermsIndexReader to load FST",
        "description": "VarGapTermsIndexReader should pass READONCE context down when it\nopens/reads the FST. Yet, it should just replace the ctx passed in, ie if we are merging vs reading we want to differentiate.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-329",
        "summary": "Fuzzy query scoring issues",
        "description": "Queries which automatically produce multiple terms (wildcard, range, prefix, \nfuzzy etc)currently suffer from two problems:\n\n1) Scores for matching documents are significantly smaller than term queries \nbecause of the volume of terms introduced (A match on query Foo~ is 0.1 \nwhereas a match on query Foo is 1).\n2) The rarer forms of expanded terms are favoured over those of more common \nforms because of the IDF. When using Fuzzy queries for example, rare mis-\nspellings typically appear in results before the more common correct spellings.\n\n\nI will attach a patch that corrects the issues identified above by \n1) Overriding Similarity.coord to counteract the downplaying of scores \nintroduced by expanding terms.\n2) Taking the IDF factor of the most common form of expanded terms as the \nbasis of scoring all other expanded terms.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1809",
        "summary": "highlight-vs-vector-highlight.alg is unfair",
        "description": "highlight-vs-vector-highlight.alg uses EnwikiQueryMaker which makes SpanQueries, but FastVectorHighlighter simply ignores SpanQueries.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1756",
        "summary": "contrib/memory: PatternAnalyzerTest is a very, very, VERY, bad unit test",
        "description": "while working on something else i was started getting consistent IllegalStateExceptions from PatternAnalyzerTest -- but only when running the test from the top level.\n\nDigging into the test, i've found numerous things that are very scary...\n* instead of using assertions to test that tokens streams match, it throws an IllegalStateExceptions when they don't, and then logs a bunch of info about the token streams to System.out -- having assertion messages that tell you *exactly* what doens't match would make a lot more sense.\n* it builds up a list of files to analyze using patsh thta it evaluates relative to the current working directory -- which means you get different files depending on wether you run the tests fro mthe contrib level, or from the top level build file\n* the list of files it looks for include: \"../../*.txt\", \"../../*.html\", \"../../*.xml\" ... so not only do you get different results when you run the tests in the contrib vs at the top level, but different people runing the tests via the top level build file will get different results depending on what types of text, html, and xml files they happen to have two directories above where they checked out lucene.\n* the test comments indicates that it's purpose is to show that PatternAnalyzer produces the same tokens as other analyzers - but points out this will fail for WhitespaceAnalyzer because of the 255 character token limit WhitespaceTokenizer imposes -- the test then proceeds to compare PaternAnalyzer to WhitespaceTokenizer, garunteeing a test failure for anyone who happens to have a text file containing more then 255 characters of non-whitespace in a row somewhere in \"../../\" (in my case: my bookmarks.html file, and the hex encoded favicon.gif images)\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1177",
        "summary": "IW.optimize() can do too many merges at the very end",
        "description": "This was fixed on trunk in LUCENE-1044 but I'd like to separately\nbackport it to 2.3.\n\nWith ConcurrentMergeScheduler there is a bug, only when CFS is on,\nwhereby after the final merge of an optimize has finished and while\nit's building its CFS, the merge policy may incorrectly ask for\nanother merge to collapse that segment into a compound file.  The net\neffect is optimize can spend many extra iterations unecessarily\nmerging a single segment to collapse it to compound file.\n\nI believe the case is rare (hard to hit), and maybe only if you have\nmultiple threads calling optimize at once (the TestThreadedOptimize\ntest can hit it), but it's a low-risk fix so I plan to commit to 2.3\nshortly.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2467",
        "summary": "IndexWriter memory leak when large docs are indexed",
        "description": "Spinoff from the java-user thread \"IndexWriter and memory usage\"...\n\nIndexWriter has had a long standing memory leak, since LUCENE-843.\n\nWhen the byte/char/int blocks are recycled to the common pool, the\nper-thread DW classes incorrectly still hold a reference to them.\n\nThis normally is not a problem, since these buffers will be re-used\nagain.\n\nBut, if you index a massive document, causing IW to allocate more than\nthe RAM buffer allocated to it, then the leak happens.  So you could\nhave a 16 MB RAM buffer set, but if a huge doc required allocation of\n200 MB worth of arrays, those 200 MB are never freed (well, until you\nclose the IW and deref it from the app).\n\nIt's even worse if you use multiple threads: if each thread has ever\nhad to index a massive document, then that thread incorrectly holds\nonto the extra arrays.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3859",
        "summary": "nuke/clean up AtomicReader.hasNorms",
        "description": "implementations already have to return fieldInfos() [which can tell you this], and normValues() [which can also tell you this].\n\nSo if we want to keep it, I think it should just have a final implementation and not be required for FilterReaders, etc.\n\nOr we can just nuke it... do we really need 3 ways to do the same thing?",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3886",
        "summary": "MemoryIndex memory estimation in toString inconsistent with getMemorySize()",
        "description": "After LUCENE-3867 was committed, there are some more minor problems with MemoryIndex's estimates. This patch will fix those and also add verbose test output of RAM needed for MemoryIndex vs. RAMDirectory.\n\nInterestingly, the RAMDirectory always takes (according to estimates, so even with buffer overheads) only 2/3 of the MemoryIndex (excluding IndexReaders).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-204",
        "summary": "[PATCH] usage feedback for IndexFiles demo",
        "description": "Just a small patch that adds \"usage\" output if the demo is called without a \nparameter, makes it a little bit friendlier to beginners.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3179",
        "summary": "OpenBitSet.prevSetBit()",
        "description": "Find a previous set bit in an OpenBitSet.\nUseful for parent testing in nested document query execution LUCENE-2454 .",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-931",
        "summary": "Some files are missing the license headers",
        "description": "Jukka provided the following list of files that are missing the license headers.\nIn addition there might be other files (like build scripts) that don't have the headers.\n\nsrc/java/org/apache/lucene/document/MapFieldSelector.java\nsrc/java/org/apache/lucene/search/PrefixFilter.java\nsrc/test/org/apache/lucene/TestHitIterator.java\nsrc/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java\nsrc/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java\nsrc/test/org/apache/lucene/index/TestBackwardsCompatibility.java\nsrc/test/org/apache/lucene/index/TestFieldInfos.java\nsrc/test/org/apache/lucene/index/TestIndexFileDeleter.java\nsrc/test/org/apache/lucene/index/TestIndexWriter.java\nsrc/test/org/apache/lucene/index/TestIndexWriterDelete.java\nsrc/test/org/apache/lucene/index/TestIndexWriterLockRelease.java\nsrc/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java\nsrc/test/org/apache/lucene/index/TestNorms.java\nsrc/test/org/apache/lucene/index/TestParallelTermEnum.java\nsrc/test/org/apache/lucene/index/TestSegmentTermEnum.java\nsrc/test/org/apache/lucene/index/TestTerm.java\nsrc/test/org/apache/lucene/index/TestTermVectorsReader.java\nsrc/test/org/apache/lucene/search/TestRangeQuery.java\nsrc/test/org/apache/lucene/search/TestTermScorer.java\nsrc/test/org/apache/lucene/store/TestBufferedIndexInput.java\nsrc/test/org/apache/lucene/store/TestWindowsMMap.java\nsrc/test/org/apache/lucene/store/_TestHelper.java\nsrc/test/org/apache/lucene/util/_TestUtil.java\ncontrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java\ncontrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/FeedNotFoundException.java\ncontrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/ComponentType.java\ncontrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/RegistryException.java\ncontrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/storage/lucenestorage/StorageAccountWrapper.java\ncontrib/gdata-server/src/core/src/test/org/apache/lucene/gdata/storage/lucenestorage/TestModifiedEntryFilter.java\ncontrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/AtomUriElementTest.java\ncontrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMEntryImplTest.java\ncontrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMFeedImplTest.java\ncontrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMGenereatorImplTest.java\ncontrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMSourceImplTest.java\ncontrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java\ncontrib/javascript/queryConstructor/luceneQueryConstructor.js\ncontrib/javascript/queryEscaper/luceneQueryEscaper.js\ncontrib/javascript/queryValidator/luceneQueryValidator.js\ncontrib/queries/src/java/org/apache/lucene/search/BooleanFilter.java\ncontrib/queries/src/java/org/apache/lucene/search/BoostingQuery.java\ncontrib/queries/src/java/org/apache/lucene/search/FilterClause.java\ncontrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java\ncontrib/queries/src/java/org/apache/lucene/search/TermsFilter.java\ncontrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThisQuery.java\ncontrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java\ncontrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java\ncontrib/snowball/src/java/net/sf/snowball/Among.java\ncontrib/snowball/src/java/net/sf/snowball/SnowballProgram.java\ncontrib/snowball/src/java/net/sf/snowball/TestApp.java\ncontrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java\ncontrib/surround/src/test/org/apache/lucene/queryParser/surround/query/BooleanQueryTst.java\ncontrib/surround/src/test/org/apache/lucene/queryParser/surround/query/ExceptionQueryTst.java\ncontrib/surround/src/test/org/apache/lucene/queryParser/surround/query/SingleFieldTestDb.java\ncontrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test01Exceptions.java\ncontrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test02Boolean.java\ncontrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test03Distance.java\ncontrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java\ncontrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java\ncontrib/wordnet/src/java/org/apache/lucene/wordnet/Syns2Index.java\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-3626",
        "summary": "Make PKIndexSplitter and MultiPassIndexSplitter work per segment",
        "description": "Spinoff from LUCENE-3624: DocValuesw merger throws exception on IW.addIndexes(SlowMultiReaderWrapper) as string-index like docvalues cannot provide asSortedSource.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1899",
        "summary": "Inefficient growth of OpenBitSet",
        "description": "Hi, I found a potentially serious efficiency problem with OpenBitSet.\n\nOne typical (I think) way to build a bit set is to set() the bits one by one -\ne.g., have a HitCollector set() the bit for each matching document.\nThe underlying array of longs needs to grow as more as more bits are set, of\ncourse.\n\nBut looking at the code, it appears to me that the array grows very\nineefficiently - in the worst case (when doc ids are sorted, as they would\nnormally be in the HitCollector case for example), copying the array again\nand again for every added bit... The relevant code in OpenBitSet.java is:\n\n  public void set(long index) {\n    int wordNum = expandingWordNum(index);\n    ...\n  }\n\n  protected int expandingWordNum(long index) {\n    int wordNum = (int)(index >> 6);\n    if (wordNum>=wlen) {\n      ensureCapacity(index+1);\n    ...\n  }\n  public void ensureCapacityWords(int numWords) {\n    if (bits.length < numWords) {\n      long[] newBits = new long[numWords];\n      System.arraycopy(bits,0,newBits,0,wlen);\n      bits = newBits;\n    }\n  }\n\nAs you can see, if the bits array is not long enough, a new one is\nallocated at exactly the right size - and in the worst case it can grow\njust one word every time...\n\nShouldn't the growth be more exponential in nature, e.g., grow to the maximum\nof index+1 and twice the existing size?\n\nAlternatively, if the growth is so inefficient, this should be documented,\nand it should be recommended to use the variant of the constructor with the\ncorrect initial size (e.g., in the HitCollector case, the number of documents\nin the index). and the fastSet() method instead of set().\n\nThanks,\nNadav.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2503",
        "summary": "light/minimal stemming for euro languages",
        "description": "The snowball stemmers are very aggressive and it would be nice if there were lighter alternatives.\n\nSome applications may want to perform less aggressive stemming, for example:\nhttp://www.lucidimagination.com/search/document/5d16391e21ca6faf/plural_only_stemmer\n\nGood, relevance tested algorithms exist and I think we should provide these alternatives.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2008",
        "summary": "TokenStream/Tokenizer/TokenFilter/Token javadoc improvements",
        "description": "Some of the javadoc for the new TokenStream/Tokenizer/TokenFilter/Token APIs had javadoc errors.  To the best of my knowledge, I corrected these and refined the copy a bit.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3656",
        "summary": "IndexReader's add/removeCloseListener should not use ConcurrentHashMap, just a synchronized set",
        "description": "The use-case for ConcurrentHashMap is when many threads are reading and less writing to the structure. Here this is just funny: The only reader is close(). Here you can just use a synchronized HashSet. The complexity of CHM is making this just a joke :-)",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3778",
        "summary": "Create a grouping convenience class",
        "description": "Currently the grouping module has many collector classes with a lot of different options per class. I think it would be a good idea to have a GroupUtil (Or another name?) convenience class. I think this could be a builder, because of the many options (sort,sortWithinGroup,groupOffset,groupCount and more) and implementations (term/dv/function) grouping has.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2708",
        "summary": "when a test Assume fails, display information",
        "description": "Currently if a test uses Assume.assumeTrue, it silently passes.\n\nI think we should output something, at *least* if you have VERBOSE set, maybe always.\n\nHere's an example of what the output might look like:\n{noformat}\njunit-sequential:\n    [junit] Testsuite: org.apache.solr.servlet.SolrRequestParserTest\n    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 1.582 sec\n    [junit]\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: testStreamURL Assume failed (ignored):\n    [junit] org.junit.internal.AssumptionViolatedException: got: <java.io.FileNotFoundException: http://www.apdfgdfgache\n.org/dist/lucene/solr/>, expected: null\n    [junit]     at org.junit.Assume.assumeThat(Assume.java:70)\n    [junit]     at org.junit.Assume.assumeNoException(Assume.java:92)\n    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:123)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:802)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:775)\n    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)\n    [junit] Caused by: java.io.FileNotFoundException: http://www.apdfgdfgache.org/dist/lucene/solr/\n    [junit]     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1311)\n    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:120)\n    [junit]     ... 26 more\n    [junit] ------------- ---------------- ---------------\n{noformat}",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1176",
        "summary": "TermVectors corruption case when autoCommit=false",
        "description": "I took Yonik's awesome test case (TestStressIndexing2) and extended it to also compare term vectors, and, it's failing.\n\nI still need to track down why, but it seems likely a separate issue.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2190",
        "summary": "CustomScoreQuery (function query) is broken (due to per-segment searching)",
        "description": "Spinoff from here:\n\n  http://lucene.markmail.org/message/psw2m3adzibaixbq\n\nWith the cutover to per-segment searching, CustomScoreQuery is not really usable anymore, because the per-doc custom scoring method (customScore) receives a per-segment docID, yet there is no way to figure out which segment you are currently searching.\n\nI think to fix this we must also notify the subclass whenever a new segment is switched to.  I think if we copy Collector.setNextReader, that would be sufficient.  It would by default do nothing in CustomScoreQuery, but a subclass could override.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1911",
        "summary": "When using QueryWrapperFilter with CachingWrapperFilter, QueryWrapperFilter returns a DocIdSet that creates a Scorer, which gets cached rather than a bit set",
        "description": "there is a large performance cost to this.\n\nThe old impl for this type of thing, QueryFilter, recommends :\n\n@deprecated use a CachingWrapperFilter with QueryWrapperFilter\n\nThe deprecated QueryFilter itself also suffers from the problem because its now implemented using a CachingWrapperFilter and QueryWrapperFilter.\n\nsee http://search.lucidimagination.com/search/document/7f54715f14b8b7a/lucene_2_9_0rc4_slower_than_2_4_1",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1055",
        "summary": "Remove GData from trunk ",
        "description": "GData doesn't seem to be maintained anymore. We're going to remove it before we cut the 2.3 release unless there are negative votes.\n\nIn case someones jumps in in the future and starts to maintain it, we can re-add it to the trunk.\n\nIf anyone is using GData and needs it to be in 2.3 please let us know soon!",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3824",
        "summary": "TermOrdVal/DocValuesComparator does too much work in compareBottom",
        "description": "We now have logic to fall back to by-value comparison, when the bottom\nslot is not from the current reader.\n\nBut this is silly, because if the bottom slot is from a different\nreader, it means the tie-break case is not possible (since the current\nreader didn't have the bottom value), so when the incoming ord equals\nthe bottom ord we should always return x > 0.\n\nI added a new random string sort test case to TestSort...\n\nI also renamed DocValues.SortedSource.getByValue -> getOrdByValue and\ncleaned up some whitespace.\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-611",
        "summary": "TestConstantScoreRangeQuery does not compile with ecj",
        "description": "TestConstantScoreRangeQuery has an assertEquals(String, Float, Float)\nbut most of the calls to assertEquals are (String, int, int).\n\necj complains with the following error:\nThe method assertEquals(String, float, float) is ambiguous for the type TestConstantScoreRangeQuery\n\nThe simple solution is to supply an assertEquals(String, int, int) which calls Assert.assertEquals(String, int, int)\n\nPatch to follow.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2611",
        "summary": "IntelliJ IDEA and Eclipse setup",
        "description": "Setting up Lucene/Solr in IntelliJ IDEA or Eclipse can be time-consuming.\n\nThe attached patches add a new top level directory {{dev-tools/}} with sub-dirs {{idea/}} and {{eclipse/}} containing basic setup files for trunk, as well as top-level ant targets named \"idea\" and \"eclipse\" that copy these files into the proper locations.  This arrangement avoids the messiness attendant to in-place project configuration files directly checked into source control.\n\nThe IDEA configuration includes modules for Lucene and Solr, each Lucene and Solr contrib, and each analysis module.  A JUnit run configuration per module is included.\n\nThe Eclipse configuration includes a source entry for each source/test/resource location and classpath setup: a library entry for each jar.\n\nFor IDEA, once {{ant idea}} has been run, the only configuration that must be performed manually is configuring the project-level JDK.  For Eclipse, once {{ant eclipse}} has been run, the user has to refresh the project (right-click on the project and choose Refresh).\n\nIf these patches is committed, Subversion svn:ignore properties should be added/modified to ignore the destination IDEA and Eclipse configuration locations.\n\nIam Jambour has written up on the Lucene wiki a detailed set of instructions for applying the 3.X branch patch for IDEA: http://wiki.apache.org/lucene-java/HowtoConfigureIntelliJ",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2894",
        "summary": "Use of google-code-prettify for Lucene/Solr Javadoc",
        "description": "My company, RONDHUIT uses google-code-prettify (Apache License 2.0) in Javadoc for syntax highlighting:\n\nhttp://www.rondhuit-demo.com/RCSS/api/com/rondhuit/solr/analysis/JaReadingSynonymFilterFactory.html\n\nI think we can use it for Lucene javadoc (java sample code in overview.html etc) and Solr javadoc (Analyzer Factories etc) to improve or simplify our life.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2769",
        "summary": "FilterIndexReader in trunk does not implement getSequentialSubReaders() correctly",
        "description": "Since LUCENE-2459, getSequentialSubReaders() in FilterIndexReader returns null, so it returns an atomic reader. But If you call then any of the enum methods, it throws Exception because the underlying reader is not atomic.\n\nWe should move the null-returning method to SlowMultiReaderWrapper and fix FilterIndexReader's default to return in.getSequentialSubReaders(). Ideally an implementation must of course also wrap the sub-readers.\n\nIf we change this we have to look into other Impls like the MultiPassIndexSplitter if we need to add atomicity.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-616",
        "summary": "CLONE -Merge error during add to index (IndexOutOfBoundsException)",
        "description": "I've been batch-building indexes, and I've build a couple hundred indexes with \na total of around 150 million records.  This only happened once, so it's \nprobably impossible to reproduce, but anyway... I was building an index with \naround 9.6 million records, and towards the end I got this:\n\njava.lang.IndexOutOfBoundsException: Index: 54, Size: 24\n        at java.util.ArrayList.RangeCheck(ArrayList.java:547)\n        at java.util.ArrayList.get(ArrayList.java:322)\n        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)\n        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)\n        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java\n:149)\n        at org.apache.lucene.index.SegmentTermEnum.next\n(SegmentTermEnum.java:115)\n        at org.apache.lucene.index.SegmentMergeInfo.next\n(SegmentMergeInfo.java:52)\n        at org.apache.lucene.index.SegmentMerger.mergeTermInfos\n(SegmentMerger.java:294)\n        at org.apache.lucene.index.SegmentMerger.mergeTerms\n(SegmentMerger.java:254)\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)\n        at org.apache.lucene.index.IndexWriter.mergeSegments\n(IndexWriter.java:487)\n        at org.apache.lucene.index.IndexWriter.maybeMergeSegments\n(IndexWriter.java:458)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3404",
        "summary": "testIWondiskfull checkindex failure",
        "description": "looks like charlie cron created a corrupt index on disk full.. can't reproduce with the seed on this machine, i can try on that VM with the same environment and see if i have better luck.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1573",
        "summary": "IndexWriter does not do the right thing when a Thread is interrupt()'d",
        "description": "Spinoff from here:\n\n    http://www.nabble.com/Deadlock-with-concurrent-merges-and-IndexWriter--Lucene-2.4--to22714290.html\n\nWhen a Thread is interrupt()'d while inside Lucene, there is a risk currently that it will cause a spinloop and starve BG merges from completing.\n\nInstead, when possible, we should allow interruption.  But unfortunately for back-compat, we will need to wrap the exception in an unchecked version.  In 3.0 we can change that to simply throw InterruptedException.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3433",
        "summary": "Random access non RAM resident IndexDocValues (CSF)",
        "description": "There should be a way to get specific IndexDocValues by going through the Directory rather than loading all of the values into memory.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3874",
        "summary": "bogus positions create a corrumpt index",
        "description": "Its pretty common that positionIncrement can overflow, this happens really easily \nif people write analyzers that don't clearAttributes().\n\nIt used to be the case that if this happened (and perhaps still is in 3.x, i didnt check),\nthat IW would throw an exception.\n\nBut i couldnt find the code checking this, I wrote a test and it makes a corrumpt index...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-722",
        "summary": "DEFAULT spelled DEFALT in MoreLikeThis.java",
        "description": "DEFAULT is spelled DEFALT in contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1694",
        "summary": "Query#mergeBooleanQueries argument should be of type BooleanQuery[] instead of Query[]",
        "description": "The method #mergeBooleanQueries accepts Query[] and casts elements to BooleanQuery without checking. This will guarantee a ClassCastException if it is not a boolean query. We should enforce this by changing the signature. This won't really break back compat. as it only works with instances of BooleanQuery.\n\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3746",
        "summary": "suggest.fst.Sort.BufferSize should not automatically fail just because of freeMemory()",
        "description": "Follow up op dev thread: [FSTCompletionTest failure \"At least 0.5MB RAM buffer is needed\" | http://markmail.org/message/d7ugfo5xof4h5jeh]",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1864",
        "summary": "bogus javadocs for FieldValueHitQuery.fillFields",
        "description": "FieldValueHitQuery.fillFields has javadocs that seem to be left over from a completely different method...\n\n{code}\n  /**\n   * Given a FieldDoc object, stores the values used to sort the given document.\n   * These values are not the raw values out of the index, but the internal\n   * representation of them. This is so the given search hit can be collated by\n   * a MultiSearcher with other search hits.\n   * \n   * @param doc\n   *          The FieldDoc to store sort values into.\n   * @return The same FieldDoc passed in.\n   * @see Searchable#search(Weight,Filter,int,Sort)\n   */\n  FieldDoc fillFields(final Entry entry) {\n    final int n = comparators.length;\n    final Comparable[] fields = new Comparable[n];\n    for (int i = 0; i < n; ++i) {\n      fields[i] = comparators[i].value(entry.slot);\n    }\n    //if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores\n    return new FieldDoc(entry.docID, entry.score, fields);\n  }\n\n{code}",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1402",
        "summary": "CheckIndex API changed without backwards compaitibility",
        "description": "The API of CheckIndex changed. The Check function returns a CheckIndexStatus and not boolean. And JavaDocs notes the boolean return value.\n\nI am not sure if it works, but it would be good to have the check method that returns boolean available @Deprecated, i.e.\n@Deprecated public static CheckIndexStatus check(Directory dir, boolean doFix) throws IOException {\n final CheckIndexStatus stat=this.check(dir,doFix);\n return stat.clean;\n}\n\nI am not sure, if it can be done with the same method name, but it prevents drop-in-replacements of Lucene to work.",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1614",
        "summary": "Add next() and skipTo() variants to DocIdSetIterator that return the current doc, instead of boolean",
        "description": "See http://www.nabble.com/Another-possible-optimization---now-in-DocIdSetIterator-p23223319.html for the full discussion. The basic idea is to add variants to those two methods that return the current doc they are at, to save successive calls to doc(). If there are no more docs, return -1. A summary of what was discussed so far:\n# Deprecate those two methods.\n# Add nextDoc() and skipToDoc(int) that return doc, with default impl in DISI (calls next() and skipTo() respectively, and will be changed to abstract in 3.0).\n#* I actually would like to propose an alternative to the names: advance() and advance(int) - the first advances by one, the second advances to target.\n# Wherever these are used, do something like '(doc = advance()) >= 0' instead of comparing to -1 for improved performance.\n\nI will post a patch shortly",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3417",
        "summary": "DictionaryCompoundWordTokenFilter does not properly add tokens from the end compound word.",
        "description": "Due to an off-by-one error, a subword placed at the end of a compound word will not get a token added to the token stream.\n\n\nFor example (from the unit test in the attached patch):\nDictionary: {\"ab\", \"cd\", \"ef\"}\nInput: \"abcdef\"\nCreated tokens: {\"abcdef\", \"ab\", \"cd\"}\nExpected tokens: {\"abcdef\", \"ab\", \"cd\", \"ef\"}\n\n\nAdditionally, it could produce tokens that were shorter than the minSubwordSize due to another off-by-one error. For example (again, from the attached patch):\n\n\nDictionary: {\"abc\", \"d\", \"efg\"}\nMinimum subword length: 2\nInput: \"abcdefg\"\nCreated tokens: {\"abcdef\", \"abc\", \"d\", \"efg\"}\nExpected tokens: {\"abcdef\", \"abc\", \"efg\"}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-891",
        "summary": "A number of documentation fixes for the search package summary",
        "description": "Improves readability and clarity, corrects some basic English, makes some example text even more clear, and repairs typos.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2513",
        "summary": "IndexReader overwrites future commits when you open it on a past commit",
        "description": "Hit this on trying to build up a test index for perf testing...\n\nIndexReader (and Writer) accept an IndexCommit on open.\n\nThis is quite powerful, because, if you use a deletion policy that keeps multiple commits around, you can open a not-current commit, make some changes, write a new commit, all without altering the \"future\" commits.\n\nI use this to first build up a big wikipedia index, including one commit w/ multiple segments, then another commit after optimize(), and then I open an writable IR to perform deletions off of both those commits.  This gives me a single test index that has all four combinations (single vs multi segment; deletions vs no deletions).\n\nBut IndexReader has a bug whereby it overwrites the segments_N file.  (IndexWriter works correctly).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-618",
        "summary": "GData - Server wrong commit does not build",
        "description": "The last GData - Server commit  does not build due to a wrong commit.\nYonik did not commit all the files in the diff file. There are several sources and packages missing.\n  \nThe diff - file with the date of 26.06.06 should be applied.\n--> http://issues.apache.org/jira/browse/LUCENE-598\n26.06.06.diff (644 kb)\n\ncould any of the lucene committers apply this patch. Yonik is on the way to Dublin.\n\nThanks Simon\n",
        "label": "NUG",
        "classified": "OTHER",
        "type": ""
    },
    {
        "key": "LUCENE-879",
        "summary": "Document number integrity merge policy",
        "description": "This patch allows for document numbers stays the same even after merge of segments with deletions.\n\nConsumer needs to do this:\nindexWriter.setSkipMergingDeletedDocuments(false);\n\nThe effect will be that deleted documents are replaced by a new Document() in the merged segment, but not marked as deleted. This should probably be some policy thingy that allows for different solutions such as keeping the old document, et c.\n\nAlso see http://www.nabble.com/optimization-behaviour-tf3723327.html#a10418880\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1287",
        "summary": "Allow usage of HyphenationCompoundWordTokenFilter without dictionary",
        "description": "We should allow to use the HyphenationCompoundWordTokenFilter without a dictionary. This produces a lot of \"nonword\" tokens but might be useful sometimes.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1628",
        "summary": "Persian Analyzer",
        "description": "A simple persian analyzer.\n\ni measured trec scores with the benchmark package below against http://ece.ut.ac.ir/DBRG/Hamshahri/ :\n\nSimpleAnalyzer:\nSUMMARY\n  Search Seconds:         0.012\n  DocName Seconds:        0.020\n  Num Points:           981.015\n  Num Good Points:       33.738\n  Max Good Points:       36.185\n  Average Precision:      0.374\n  MRR:                    0.667\n  Recall:                 0.905\n  Precision At 1:         0.585\n  Precision At 2:         0.531\n  Precision At 3:         0.513\n  Precision At 4:         0.496\n  Precision At 5:         0.486\n  Precision At 6:         0.487\n  Precision At 7:         0.479\n  Precision At 8:         0.465\n  Precision At 9:         0.458\n  Precision At 10:        0.460\n  Precision At 11:        0.453\n  Precision At 12:        0.453\n  Precision At 13:        0.445\n  Precision At 14:        0.438\n  Precision At 15:        0.438\n  Precision At 16:        0.438\n  Precision At 17:        0.429\n  Precision At 18:        0.429\n  Precision At 19:        0.419\n  Precision At 20:        0.415\n\nPersianAnalyzer:\nSUMMARY\n  Search Seconds:         0.004\n  DocName Seconds:        0.011\n  Num Points:           987.692\n  Num Good Points:       36.123\n  Max Good Points:       36.185\n  Average Precision:      0.481\n  MRR:                    0.833\n  Recall:                 0.998\n  Precision At 1:         0.754\n  Precision At 2:         0.715\n  Precision At 3:         0.646\n  Precision At 4:         0.646\n  Precision At 5:         0.631\n  Precision At 6:         0.621\n  Precision At 7:         0.593\n  Precision At 8:         0.577\n  Precision At 9:         0.573\n  Precision At 10:        0.566\n  Precision At 11:        0.572\n  Precision At 12:        0.562\n  Precision At 13:        0.554\n  Precision At 14:        0.549\n  Precision At 15:        0.542\n  Precision At 16:        0.538\n  Precision At 17:        0.533\n  Precision At 18:        0.527\n  Precision At 19:        0.525\n  Precision At 20:        0.518\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-852",
        "summary": "spellchecker: make hard-coded values configurable",
        "description": "the class org.apache.lucene.search.spell.SpellChecker uses the following hard-coded values in its method\nindexDictionary:\n        writer.setMergeFactor(300);\n        writer.setMaxBufferedDocs(150);\nthis poses problems when the spellcheck index is created on systems with certain limits, i.e. in unix\nenvironments where the ulimit settings are restricted for the user (http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428).\n\nthere are several ways to circumvent this:\n1. add another indexDictionary method with additional parameters:\n    public void indexDictionary (Dictionary dict, int mergeFactor, int maxBufferedDocs) throws IOException\n    \n2. add setter methods for mergeFactor and maxBufferedDocs \n    (see code in http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428 )\n\n3. Make SpellChecker subclassing easier as suggested by Chris Hostetter \n   (see reply  http://www.gossamer-threads.com/lists/lucene/java-dev/47463#47463)\n\nthanx,\nkarin\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1945",
        "summary": "Make all classes that have a close() methods instanceof Closeable (Java 1.5)",
        "description": "This should be simple.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2416",
        "summary": "Some improvements to Benchmark",
        "description": "I've noticed that WriteLineDocTask declares it does not support multi-threading, but taking a closer look I think this is really for no good reason. Most of the work is done by reading from the ContentSource and constructing the document. If those two are mult-threaded (and I think all ContentSources are), then we can synchronize only around writing the actual document to the line file.\n\nWhile investigating that, I've noticed some 1.5 TODOs and some other minor improvements that can be made. If you've wanted to make some minor improvements to benchmark, let me know :). I intend to include only minor and trivial ones.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2963",
        "summary": "Easier way to run benchmark",
        "description": "Move Benchmark.main to a more easily accessible method exec() that can be easily invoked by external programs.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1186",
        "summary": "[PATCH] Clear ThreadLocal instances in close()",
        "description": "As already found out in LUCENE-436, there seems to be a garbage collection problem with ThreadLocals at certain constellations, resulting in an OutOfMemoryError.\nThe resolution there was to remove the reference to the ThreadLocal value when calling the close() method of the affected classes (see FieldsReader and TermInfosReader).\nFor Java < 5.0, this can effectively be done by calling threadLocal.set(null); for Java >= 5.0, we would call threadLocal.remove()\n\nAnalogously, this should be done in *any* class which creates ThreadLocal values\n\nRight now, two classes of the core API make use of ThreadLocals, but do not properly remove their references to the ThreadLocal value\n1. org.apache.lucene.index.SegmentReader\n2. org.apache.lucene.analysis.Analyzer\n\nFor SegmentReader, I have attached a simple patch.\nFor Analyzer, there currently is no patch because Analyzer does not provide a close() method (future to-do?)\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-448",
        "summary": "optional norms",
        "description": "For applications with many indexed fields, the norms cause memory problems both during indexing and querying.\nThis patch makes norms optional on a per-field basis, in the same way that term vectors are optional per-field.\n\nOverview of changes:\n - Field.omitNorms that defaults to false\n - backward compatible lucene file format change: FieldInfos.FieldBits has a bit for omitNorms\n - IndexReader.hasNorms() method\n - During merging, if any segment includes norms, then norms are included.\n - methods to get norms return the equivalent 1.0f array for backward compatibility\n\nThe patch was designed for backward compatibility:\n - all current unit tests pass w/o any modifications required\n - compatible with old indexes since the default is omitNorms=false\n - compatible with older/custom subclasses of IndexReader since a default hasNorms() is provided\n - compatible with older/custom users of IndexReader such as Weight/Scorer/explain since a norm array is produced on demand, even if norms were not stored\n\nIf this patch is accepted (or if the direction is acceptable), performance for scoring  could be improved by assuming 1.0f when hasNorms(field)==false.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1569",
        "summary": "IndexReader.clone can leave files open",
        "description": "I hit this in working on LUCENE-1516.\n\nWhen not using compound file format, if you clone an IndexReader, then close the original, then close the clone, the stored fields files (_X.fdt, _X.fdx) remain incorrectly open.\n\nI have a test showing it; fix is trivial.  Will post patch & commit shortly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1703",
        "summary": "Add a waitForMerges() method to IndexWriter",
        "description": "It would be very useful to have a waitForMerges() method on the IndexWriter.\n\nRight now, the only way i can see to achieve this is to call IndexWriter.close()\n\nideally, there would be a method on the IndexWriter to wait for merges without actually closing the index.\nThis would make it so that background merges (or optimize) can be waited for without closing the IndexWriter, and then reopening a new IndexWriter\n\nthe close() reopen IndexWriter method can be problematic if the close() fails as the write lock won't be released\nthis could then result in the following sequence:\n* close() - fails\n* force unlock the write lock (per close() documentation)\n* new IndexWriter() (acquires write lock)\n* finalize() on old IndexWriter releases the write lock\n* Index is now not locked, and another IndexWriter pointing to the same directory could be opened\n\nIf you don't force unlock the write lock, opening a new IndexWriter will fail until garbage collection calls finalize() the old IndexWriter\n\nIf the waitForMerges() method is available, i would likely never need to close() the IndexWriter until right before the process being shutdown, so this issue would not occur (worst case scenario, the waitForMerges() fails)\n\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2701",
        "summary": "Factor maxMergeSize into findMergesForOptimize in LogMergePolicy",
        "description": "LogMergePolicy allows you to specify a maxMergeSize in MB, which is taken into consideration in regular merges, yet ignored by findMergesForOptimze. I think it'd be good if we take that into consideration even when optimizing. This will allow the caller to specify two constraints: maxNumSegments and maxMergeMB. Obviously both may not be satisfied, and therefore we will guarantee that if there is any segment above the threshold, the threshold constraint takes precedence and therefore you may end up w/ <maxNumSegments (if it's not 1) after optimize. Otherwise, maxNumSegments is taken into consideration.\n\nAs part of this change, I plan to change some methods to protected (from private) and members as well. I realized that if one wishes to implement his own LMP extension, he needs to either put it under o.a.l.index or copy some code over to his impl.\n\nI'll attach a patch shortly.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2550",
        "summary": "3.x backwards tests are using Version.LUCENE_CURRENT: aren't testing backwards!",
        "description": "The 3.x backwards tests are mostly all using Version.LUCENE_CURRENT, therefore they don't always test the behavior as they should.\n\nI added TEST_VERSION_CURRENT = 3.0 to the backwards/LuceneTestCase, and I think we should fix all backwards tests to use TEST_VERSION_CURRENT instead.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2484",
        "summary": "Remove deprecated TermAttribute from tokenattributes and legacy support in indexer",
        "description": "The title says it:\n- Remove interface TermAttribute\n- Remove empty fake implementation TermAttributeImpl extends CharTermAttributeImpl\n- Remove methods from CharTermAttributeImpl (and indirect from Token)\n- Remove sophisticated\u00ae backwards\u2122 Layer in TermsHash*\n- Remove IAE from NumericTokenStream, if TA is available in AS\n- Fix rest of core tests (TestToken)",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2118",
        "summary": "Intermittent failure in TestIndexWriterMergePolicy.testMaxBufferedDocsChange",
        "description": "Last night's build failed from it: http://hudson.zones.apache.org/hudson/job/Lucene-trunk/1019/changes\n\nHere's the exc:\n\n{code}\n    [junit] Testcase: testMaxBufferedDocsChange(org.apache.lucene.index.TestIndexWriterMergePolicy):\tFAILED\n    [junit] maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10\n    [junit] junit.framework.AssertionFailedError: maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10\n    [junit] \tat org.apache.lucene.index.TestIndexWriterMergePolicy.checkInvariants(TestIndexWriterMergePolicy.java:234)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterMergePolicy.testMaxBufferedDocsChange(TestIndexWriterMergePolicy.java:164)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:208)\n{code}\n\nTest doesn't fail if I run on opensolaris nor os X machines...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3031",
        "summary": "setFlushPending fails if we concurrently hit a aborting exception",
        "description": "If we select a DWPT for flushing but that DWPT is currently in flight and hits an exception after we selected them for flushing the num of docs is reset to 0 and we trip that exception. So we rather check if it is > 0 than assert on it here.\n{noformat}\n[junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions\n    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):\tFAILED\n    [junit] thread Indexer 3: hit unexpected failure\n    [junit] junit.framework.AssertionFailedError: thread Indexer 3: hit unexpected failure\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:227)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)\n    [junit] \n    [junit] \n    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 30.287 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] Indexer 3: unexpected exception2\n    [junit] java.lang.AssertionError\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushControl.setFlushPending(DocumentsWriterFlushControl.java:170)\n    [junit] \tat org.apache.lucene.index.FlushPolicy.markLargestWriterPending(FlushPolicy.java:108)\n    [junit] \tat org.apache.lucene.index.FlushByRamOrCountsPolicy.onInsert(FlushByRamOrCountsPolicy.java:61)\n    [junit] \tat org.apache.lucene.index.FlushPolicy.onUpdate(FlushPolicy.java:77)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:115)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:341)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1367)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1339)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:92)\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=3493970007652348212:2010109588873167237\n    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _1v(4.0):Cv2 _27(4.0):cv1 into _2h\n    [junit] WARNING: test method: 'testRandomExceptionsThreads' left thread running: merge thread: _2c(4.0):cv1 into _2m\n    [junit] RESOURCE LEAK: test method: 'testRandomExceptionsThreads' left 2 thread(s) running\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockFixedIntBlock(blockSize=421), field=MockSep, id=SimpleText, other=MockSep, contents=MockRandom, content1=Pulsing(freqCutoff=11), content2=MockSep, content4=SimpleText, content5=SimpleText, content6=MockRandom, crash=MockRandom, content7=MockVariableIntBlock(baseBlockSize=109)}, locale=mk_MK, timezone=Europe/Malta\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes, TestFilterIndexReader, TestIndexWriterExceptions]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=78897400,total=195821568\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1027",
        "summary": "contrib/benchmark config does not play nice with doubles with the flush.by.ram value",
        "description": "In the o.a.l.benchmark.byTask.utils.Config.java file, the nextRound and various other methods do not handle doubles in the \"round\" property configuration syntax.\n\nTo replicate this, copy the micro-standard.alg and replace \nmerge.factor=mrg:10:100:10:100\nmax.buffered=buf:10:10:100:100\n\nwith\n\nram.flush.mb=ram:32:40:48:56\n\nand you will get various ClassCastExceptions in Config (one in newRound() and, when that is fixed, in getColsValuesForValsByRound.\n\nThe fix seems to be to just to mirror the handling of int[].\n\nThe fix seems relatively minor.  Patch shortly and will plan to commit tomorrow evening.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2902",
        "summary": "tests should run checkIndex on indexes they create",
        "description": "I think we should add a boolean checkIndexesOnClose (default=true) to MockDirectoryWrapper.\n\nOnly a very few tests need to disable this.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2115",
        "summary": "Port to Generics - test cases in contrib ",
        "description": "LUCENE-1257 in Lucene 3.0 addressed porting to generics across public api-s . LUCENE-2065 addressed across src/test . \n\nThis would be a placeholder JIRA for any remaining pending generic conversions across the code base. \n\nPlease keep it open after commiting and we can close it when we are near a 3.1 release , so that this could be a placeholder ticket. \n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2064",
        "summary": "Highlighter should support all MultiTermQuery subclasses without casts",
        "description": "In order to support MultiTermQuery subclasses the Highlighter component applies instanceof checks for concrete classes from the lucene core. This prevents classes like RegexQuery in contrib from being supported. Introducing dependencies on other contribs is not feasible just for being supported by the highlighter.\n\nWhile the instanceof checks and subsequent casts might hopefully go somehow away  in the future but for supporting more multterm queries I have a alternative approach using a fake IndexReader that uses a RewriteMethod to force the MTQ to pass the field name to the given reader without doing any real work. It is easier to explain once you see the patch - I will upload shortly.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2533",
        "summary": "FileSwitchDirectory should uniqueify the String file names returned by listAll",
        "description": "Right now we blindly concatenate what's returned from primary & secondary.\n\nBut a legit use of FSD is pointing to the same underlying FSDir but w/ different impls for opening the inputs/outputs.\n\nI have simple patch that just uniqueifies using Set<String>.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3711",
        "summary": "small SentinelIntSet can cause infinite loop on resize",
        "description": "A small initial size of <=4 can cause the set to not rehash soon enough and thus go into an infinite loop searching the table for an open space.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2816",
        "summary": "MMapDirectory speedups",
        "description": "MMapDirectory has some performance problems:\n# When the file is larger than Integer.MAX_VALUE, we use MultiMMapIndexInput, \nwhich does a lot of unnecessary bounds-checks for its buffer-switching etc. \nInstead, like MMapIndexInput, it should rely upon the contract of these operations\nin ByteBuffer (which will do a bounds check always and throw BufferUnderflowException).\nOur 'buffer' is so large (Integer.MAX_VALUE) that its rare this happens and doing\nour own bounds checks just slows things down.\n# the readInt()/readLong()/readShort() are slow and should just defer to ByteBuffer.readInt(), etc\nThis isn't very important since we don't much use these, but I think there's no reason\nusers (e.g. codec writers) should have to readBytes() + wrap as bytebuffer + get an \nIntBuffer view when readInt() can be almost as fast...",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-687",
        "summary": "Performance improvement: Lazy skipping on proximity file",
        "description": "Hello,\n\nI'm proposing a patch here that changes org.apache.lucene.index.SegmentTermPositions to avoid unnecessary skips and reads on the proximity stream. Currently a call of next() or seek(), which causes a movement to a document in the freq file also moves the prox pointer to the posting list of that document.  But this is only necessary if actual positions have to be retrieved for that particular document. \n\nConsider for example a phrase query with two terms: the freq pointer for term 1 has to move to document x to answer the question if the term occurs in that document. But *only* if term 2 also matches document x, the positions have to be read to figure out if term 1 and term 2 appear next to each other in document x and thus satisfy the query. \n\nA move to the posting list of a document can be quite expensive. It has to be skipped to the last skip point before that document and then the documents between the skip point and the desired document have to be scanned, which means that the VInts of all positions of those documents have to be read and decoded. \n\nAn improvement is to move the prox pointer lazily to a document only if nextPosition() is called. This will become even more important in the future when the size of the proximity file increases (e. g. by adding payloads to the posting lists).\n\nMy patch implements this lazy skipping. All unit tests pass. \n\n\nI also attach a new unit test that works as follows:\nUsing a RamDirectory an index is created and test docs are added. Then the index is optimized to make sure it only has a single segment. This is important, because IndexReader.open() returns an instance of SegmentReader if there is only one segment in the index. The proxStream instance of SegmentReader is package protected, so it is possible to set proxStream to a different object. I am using a class called SeeksCountingStream that extends IndexInput in a way that it is able to count the number of invocations of seek(). \n\nThen the testcase searches the index using a PhraseQuery \"term1 term2\". It is known how many documents match that query and the testcase can verify that seek() on the proxStream is not called more often than number of search hits.\n\nExample:\nNumber of docs in the index: 500\nNumber of docs that match the query \"term1 term2\": 5\n\nInvocations of seek on prox stream (old code): 29\nInvocations of seek on prox stream (patched version): 5\n\n- Michael\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1080",
        "summary": "Make Token.DEFAULT_TYPE public",
        "description": "Make Token.DEFAULT_TYPE public so that TokenFilters using the reusable Token model have a way of setting the type back to the default.\n\nNo patch necessary.  I will commit soon.",
        "label": "NUG",
        "classified": "OTHER",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1389",
        "summary": "SimpleSpanFragmenter can create very short fragments",
        "description": "Line 74 of SimpleSpanFragmenter returns true when the current token is the start of a hit on a span or phrase, thus starting a new fragment. Two problems occur:\n\n- The previous fragment may be very short, but if it contains a hit it will be combined with the new fragment later so this disappears.\n- If the token is close to a natural fragment boundary the new fragment will end up very short; possibly even as short as just the span or phrase itself. This is the result of creating a new fragment without incrementing currentNumFrags.\n\nTo fix, remove or comment out line 74. The result is that fragments average to the fragment size unless a span or phrase hit is towards the end of the fragment - that fragment is made larger and the following fragment shorter to accommodate the hit.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1349",
        "summary": "Mark Fieldable as allowing some changes in 2.x future releases",
        "description": "See http://lucene.markmail.org/message/4k2gqs3n7coh4lmd?q=Fieldable\n\n1. We mark Fieldable as being subject to change. We heavily advertise (on java-dev and java-user and maybe general) that in the next minor release of Lucene (2.4), Fieldable will be changing. It is also marked at the top of CHANGES.txt very clearly for all the world to see. Since 2.4 is probably at least a month away, I think this gives anyone with a pulse enough time to react. ",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3098",
        "summary": "Grouped total count",
        "description": "When grouping currently you can get two counts:\n* Total hit count. Which counts all documents that matched the query.\n* Total grouped hit count. Which counts all documents that have been grouped in the top N groups.\n\nSince the end user gets groups in his search result instead of plain documents with grouping. The total number of groups as total count makes more sense in many situations. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-790",
        "summary": "contrib/benchmark - few improvements and a bug fix",
        "description": "Benchmark byTask was slightly improved:\n\n1. fixed a bug in the \"child-should-not-report\" mechanism. If a task sequence contained only simple tasks it worked as expected (i.e. child tasks did not report times/memory) but if a child was a task sequence, then its children would report - they should not - this was fixed, so this property is now \"penetrating/inherited\" all the way down.\n\n2. doc size control now possible also for the Reuters doc maker. (allowing to index N docs of size C characters each.)\n\n3. TrecDocMaker was added - it reads as input the .gz files used in Trec - e.g. .gov data - this can be handy to benchmark Lucene on these large collections.  Similar to the Reuters collection, the doc-maker scans the input directory for all the files and extracts documents from the files.  Here there are multiple documents in each input file. Unlike the Reuters collection, we cannot provide a 'loader' for these collections - they are available from http://trec.nist.gov - for research purposes.\n\n4. a new BasicDocMaker abstract class handles most of doc-maker tasks, including creating docs with specific size, so adding new doc-makers for other data is now much simpler.",
        "label": "NUG",
        "classified": "OTHER",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2837",
        "summary": "Collapse Searcher/Searchable/IndexSearcher; remove contrib/remote; merge PMS into IndexSearcher",
        "description": "We've discussed cleaning up our *Searcher stack for some time... I\nthink we should try to do this before releasing 4.0.\n\nSo I'm attaching an initial patch which:\n\n  * Removes Searcher, Searchable, absorbing all their methods into IndexSearcher\n\n  * Removes contrib/remote\n\n  * Removes MultiSearcher\n\n  * Absorbs ParallelMultiSearcher into IndexSearcher (ie you can now\n    pass useThreads=true, or a custom ES to the ctor)\n\nThe patch is rough -- I just ripped stuff out, did search/replace to\nIndexSearcher, etc.  EG nothing is directly testing using threads with\nIndexSearcher, but before committing I think we should add a\nnewSearcher to LuceneTestCase, which randomly chooses whether the\nsearcher uses threads, and cutover tests to use this instead of making\ntheir own IndexSearcher.\n\nI think MultiSearcher has a useful purpose, but as it is today it's\ntoo low-level, eg it shouldn't be involved in rewriting queries: the\nQuery.combine method is scary.  Maybe in its place we make a higher\nlevel class, with limited API, that's able to federate search across\nmultiple IndexSearchers?  It'd also be able to optionally use thread\nper IndexSearcher.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2046",
        "summary": "IndexReader.isCurrent incorrectly returns false after writer.prepareCommit has been called",
        "description": "Spinoff from thread \"2 phase commit with external data\" on java-user.\n\nThe IndexReader should not see the index as changed, after a prepareCommit has been called but before commit is called.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-460",
        "summary": "hashCode improvements",
        "description": "It would be nice for all Query classes to implement hashCode and equals to enable them to be used as keys when caching.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1396",
        "summary": "Improve PhraseQuery.toString()",
        "description": "PhraseQuery.toString() is overly simplistic, in that it doesn't correctly show phrases with gaps or overlapping terms. This may be misleading when presenting phrase queries built using complex analyzers and filters.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1715",
        "summary": "DirectoryIndexReader finalize() holding TermInfosReader longer than necessary",
        "description": "DirectoryIndexReader has a finalize method, which causes the JDK to keep a reference to the object until it can be finalized.  SegmentReader and MultiSegmentReader are subclasses that contain references to, potentially, hundreds of megabytes of cached data in a TermInfosReader.\n\nSome options would be removing finalize() from DirectoryIndexReader (it releases a write lock at the moment) or possibly nulling out references in various close() and doClose() methods throughout the class hierarchy so that the finalizable object doesn't references the Term arrays.\n\nOriginal mailing list message:\nhttp://mail-archives.apache.org/mod_mbox/lucene-java-user/200906.mbox/%3C7A5CB4A7BBCE0C40B81C5145C326C31301A62971@NUMEVP06.na.imtn.com%3E",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2142",
        "summary": "FieldCache.getStringIndex should not throw exception if term count exceeds doc count",
        "description": "Spinoff of LUCENE-2133/LUCENE-831.\n\nCurrently FieldCache cannot handle more than one value per field.\nWe may someday want to fix that... but until that day:\n\nFieldCache.getStringIndex currently does a simplistic check to try to\ncatch when you've accidentally allowed more than one term per field,\nby testing if the number of unique terms exceeds the number of\ndocuments.\n\nThe problem is, this is not a perfect check, in that it allows false\nnegatives (you could have more than one term per field for some docs\nand the check won't catch you).\n\nFurther, the exception thrown is the unchecked RuntimeException.\n\nSo this means... you could happily think all is good, until some day,\nwell into production, once you've updated enough docs, suddenly the\ncheck will catch you and throw an unhandled exception, stopping all\nsearches [that need to sort by this string field] in their tracks.\nIt's not gracefully degrading.\n\nI think we should simply remove the test, ie, if you have more terms\nthan docs then the terms simply overwrite one another.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2352",
        "summary": "Add a new TestBackwardsCompatibility index for flex backwards (a 3.0 one with also numeric fields)",
        "description": "In flex we change also the encode/decoder for numeric fields (NumericTokenSteam) using BytesRef and also the collation filters. We should add a test index from 3.0 that contains these fields and do some validation, that field contents did not change when read with flex.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1415",
        "summary": "MultiPhraseQuery has incorrect hashCode() implementation - Leads to Solr Cache misses",
        "description": "I found this while hunting for the cause of Solr Cache misses.\n\nThe MultiPhraseQuery class hashCode() implementation is non-deterministic. It uses termArrays.hashCode() in the computation. The contents of that ArrayList are actually arrays themselves, which return there reference ID as a hashCode instead of returning a hashCode which is based on the contents of the array. I would suggest an implementation involving the Arrays.hashCode() method.\n\nI will try to submit a patch soon, off for today.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-594",
        "summary": "Javadoc - Field constructor with Reader needs comment about retained reference",
        "description": "If you don't dig into the Lucene internals, it isn't obvious the Field constructor http://lucene.apache.org/java/docs/api/org/apache/lucene/document/Field.html#Field%28java.lang.String,%20java.io.Reader%29 retains a reference to the reader for use later on. It would be useful to have a comment added to the Javadoc saying something like:\n\nNote: A reference to java.io.Reader is retained by the field. Reader is read from when the Document which this field is added to is itself added to the index.\n\nWithout this, the caller is liable to do silly things like closing the stream after constructing the org.apache.lucene.document.Field.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1542",
        "summary": "Lucene can incorrectly set the position of tokens that start a field with positonInc 0.",
        "description": "More info in LUCENE-1465",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-224",
        "summary": "[PATCH] FilteredTermEnum code cleanup",
        "description": "FilteredTermEnum's constructor takes two parameters but doesn't use them. This \npatch changes that and thus makes the code easier readable. Maybe the old \nconstructor should be kept (as deprecated)? I'm not sure, this version seems \ncleaner to me.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2267",
        "summary": "Add solr's artifact signing scripts into lucene's build.xml/common-build.xml",
        "description": "Solr has nice artifact signing scripts in its common-build.xml and build.xml.\n\nFor me as release manager of 3.0 it would have be good to have them also when building lucene artifacts. I will investigate how to add them to src artifacts and maven artifacts",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1961",
        "summary": "Remove remaining deprecations in document package",
        "description": "Remove different deprecated APIs:\n- Field.Index.NO_NORMS, etc.\n- Field.binaryValue()\n- getOmitTf()/setOmitTf()\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1150",
        "summary": "The token types of the standard tokenizer is not accessible",
        "description": "The StandardTokenizerImpl not being public, these token types are not accessible :\n\n{code:java}\npublic static final int ALPHANUM          = 0;\npublic static final int APOSTROPHE        = 1;\npublic static final int ACRONYM           = 2;\npublic static final int COMPANY           = 3;\npublic static final int EMAIL             = 4;\npublic static final int HOST              = 5;\npublic static final int NUM               = 6;\npublic static final int CJ                = 7;\n/**\n * @deprecated this solves a bug where HOSTs that end with '.' are identified\n *             as ACRONYMs. It is deprecated and will be removed in the next\n *             release.\n */\npublic static final int ACRONYM_DEP       = 8;\n\npublic static final String [] TOKEN_TYPES = new String [] {\n    \"<ALPHANUM>\",\n    \"<APOSTROPHE>\",\n    \"<ACRONYM>\",\n    \"<COMPANY>\",\n    \"<EMAIL>\",\n    \"<HOST>\",\n    \"<NUM>\",\n    \"<CJ>\",\n    \"<ACRONYM_DEP>\"\n};\n{code}\n\nSo no custom TokenFilter can be based of the token type. Actually even the StandardFilter cannot be writen outside the org.apache.lucene.analysis.standard package.\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-290",
        "summary": "[PATCH] public static members in class TermVectorsWriter",
        "description": "hi all,\n\nlooking at the implementation of TermVectorsWriter, you'll find a bunch of\npublic static final members where the visibility could be reduced to be\nprotected. I don't see a reason for having them public if the class itself is\nprotected and all members are final values. May be somebody could check and\neither commit or enlighten me ;-)\n\nthx\nBernhard",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2979",
        "summary": "Simplify configuration API of contrib Query Parser",
        "description": "The current configuration API is very complicated and inherit the concept used by Attribute API to store token information in token streams. However, the requirements for both (QP config and token stream) are not the same, so they shouldn't be using the same thing.\n\nI propose to simplify QP config and make it less scary for people intending to use contrib QP. The task is not difficult, it will just require a lot of code change and figure out the best way to do it. That's why it's a good candidate for a GSoC project.\n\nI would like to hear good proposals about how to make the API more friendly and less scaring :)",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2627",
        "summary": "MMapDirectory chunking is buggy",
        "description": "MMapDirectory uses chunking with MultiMMapIndexInput.\n \nBecause Java's ByteBuffer uses an int to address the\nvalues, it's necessary to access a file >\nInteger.MAX_VALUE in size using multiple byte buffers.\n\nBut i noticed from the clover report the entire MultiMMapIndexInput class is completely untested: no surprise since all tests make tiny indexes.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3762",
        "summary": "Upgrade JUnit to 4.10, refactor state-machine of detecting setUp/tearDown call chaining.",
        "description": "Both Lucene and Solr use JUnit 4.7. I suggest we move forward and upgrade to JUnit 4.10 which provides several infrastructural changes (serializable Description objects, class-level rules, various tweaks). JUnit 4.10 also changes (or fixes, depends how you look at it) the order in which @Before/@After hooks and @Rules are applied. This makes the old state-machine in LuceneTestCase fail (because the order is changed).\n\nI rewrote the state machine and used a different, I think simpler, although Uwe may disagree :), mechanism in which the hook methods setUp/ tearDown are still there, but they are empty at the top level and serve only to detect whether subclasses chain super.setUp/tearDown properly (if they override anything).\n\nIn the long term, I would love to just get rid of public setup/teardown methods and make them private (so that they cannot be overriden or even seen by subclasses) but this will require changes to the runner itself.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3761",
        "summary": "Generalize SearcherManager",
        "description": "I'd like to generalize SearcherManager to a class which can manage instances of a certain type of interfaces. The reason is that today SearcherManager knows how to handle IndexSearcher instances. I have a SearcherManager which manages a pair of IndexSearcher and TaxonomyReader pair.\n\nRecently, few concurrency bugs were fixed in SearcherManager, and I realized that I need to apply them to my version as well. Which led me to think why can't we have an SM version which is generic enough so that both my version and Lucene's can benefit from?\n\nThe way I see SearcherManager, it can be divided into two parts: (1) the part that manages the logic of acquire/release/maybeReopen (i.e., ensureOpen, protect from concurrency stuff etc.), and (2) the part which handles IndexSearcher, or my SearcherTaxoPair. I'm thinking that if we'll have an interface with incRef/decRef/tryIncRef/maybeRefresh, we can make SearcherManager a generic class which handles this interface.\n\nI will post a patch with the initial idea, and we can continue from there.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-565",
        "summary": "Supporting deleteDocuments in IndexWriter (Code and Performance Results Provided)",
        "description": "Today, applications have to open/close an IndexWriter and open/close an\nIndexReader directly or indirectly (via IndexModifier) in order to handle a\nmix of inserts and deletes. This performs well when inserts and deletes\ncome in fairly large batches. However, the performance can degrade\ndramatically when inserts and deletes are interleaved in small batches.\nThis is because the ramDirectory is flushed to disk whenever an IndexWriter\nis closed, causing a lot of small segments to be created on disk, which\neventually need to be merged.\n\nWe would like to propose a small API change to eliminate this problem. We\nare aware that this kind change has come up in discusions before. See\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/23049?search_string=indexwriter%20delete;#23049\n. The difference this time is that we have implemented the change and\ntested its performance, as described below.\n\nAPI Changes\n-----------\nWe propose adding a \"deleteDocuments(Term term)\" method to IndexWriter.\nUsing this method, inserts and deletes can be interleaved using the same\nIndexWriter.\n\nNote that, with this change it would be very easy to add another method to\nIndexWriter for updating documents, allowing applications to avoid a\nseparate delete and insert to update a document.\n\nAlso note that this change can co-exist with the existing APIs for deleting\ndocuments using an IndexReader. But if our proposal is accepted, we think\nthose APIs should probably be deprecated.\n\nCoding Changes\n--------------\nCoding changes are localized to IndexWriter. Internally, the new\ndeleteDocuments() method works by buffering the terms to be deleted.\nDeletes are deferred until the ramDirectory is flushed to disk, either\nbecause it becomes full or because the IndexWriter is closed. Using Java\nsynchronization, care is taken to ensure that an interleaved sequence of\ninserts and deletes for the same document are properly serialized.\n\nWe have attached a modified version of IndexWriter in Release 1.9.1 with\nthese changes. Only a few hundred lines of coding changes are needed. All\nchanges are commented by \"CHANGE\". We have also attached a modified version\nof an example from Chapter 2.2 of Lucene in Action.\n\nPerformance Results\n-------------------\nTo test the performance our proposed changes, we ran some experiments using\nthe TREC WT 10G dataset. The experiments were run on a dual 2.4 Ghz Intel\nXeon server running Linux. The disk storage was configured as RAID0 array\nwith 5 drives. Before indexes were built, the input documents were parsed\nto remove the HTML from them (i.e., only the text was indexed). This was\ndone to minimize the impact of parsing on performance. A simple\nWhitespaceAnalyzer was used during index build.\n\nWe experimented with three workloads:\n  - Insert only. 1.6M documents were inserted and the final\n    index size was 2.3GB.\n  - Insert/delete (big batches). The same documents were\n    inserted, but 25% were deleted. 1000 documents were\n    deleted for every 4000 inserted.\n  - Insert/delete (small batches). In this case, 5 documents\n    were deleted for every 20 inserted.\n\n                                current       current          new\nWorkload                      IndexWriter  IndexModifier   IndexWriter\n-----------------------------------------------------------------------\nInsert only                     116 min       119 min        116 min\nInsert/delete (big batches)       --          135 min        125 min\nInsert/delete (small batches)     --          338 min        134 min\n\nAs the experiments show, with the proposed changes, the performance\nimproved by 60% when inserts and deletes were interleaved in small batches.\n\n\nRegards,\nNing\n\n\nNing Li\nSearch Technologies\nIBM Almaden Research Center\n650 Harry Road\nSan Jose, CA 95120",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1128",
        "summary": "Add Highlighting benchmark support to contrib/benchmark",
        "description": "I would like to be able to test the performance (speed, initially) of the Highlighter in a standard way.  Patch to follow that adds the Highlighter as a dependency benchmark and adds in tasks extending the ReadTask to perform highlighting on retrieved documents.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-309",
        "summary": "[PATCH] IndexSearcher.search(query,filter,nDocs) accepts zero nDocs",
        "description": "This caused an npe from the ht.top().score lateron. \nThe root cause was a bug in a test case, which took \nmore time to track down than would have been necessary \nwith the attached patch. \nThe patch throws an IllegalArgumentException for non positive nDocs. \nAll current tests pass with the patch applied. \n \nRegards, \nPaul",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1617",
        "summary": "Add \"testpackage\" to common-build.xml",
        "description": "One can define \"testcase\" to execute just one test class, which is convenient. However, I didn't notice any equivalent for testing a whole package. I find it convenient to be able to test packages rather than test cases because often it is not so clear which test class to run.\n\nFollowing patch allows one to \"ant test -Dtestpackage=search\" (for example) and run all tests under the \\*/search/\\* packages in core, contrib and tags, or do \"ant test-core -Dtestpackage=search\" and execute similarly just for core, or do \"ant test-core -Dtestpacakge=lucene/search/function\" and run all the tests under \\*/lucene/search/function/\\* (just in case there is another o.a.l.something.search.function package out there which we want to exclude.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1769",
        "summary": "Fix wrong clover analysis because of backwards-tests, upgrade clover to 2.6.3 or better",
        "description": "This is a followup for [http://www.lucidimagination.com/search/document/6248d6eafbe10ef4/build_failed_in_hudson_lucene_trunk_902]\n\nThe problem with clover running on hudson is, that it does not instrument all tests ran. The autodetection of clover 1.x is not able to find out which files are the correct tests and only instruments the backwards test. Because of this, the current coverage report is only from the backwards tests running against the current Lucene JAR.\n\nYou can see this, if you install clover and start the tests. During test-core no clover data is added to the db, only when backwards-tests begin, new files are created in the clover db folder.\n\nClover 2.x supports a new ant task, <testsources> that can be used to specify the files, that are the tests. It works here locally with clover 2.4.3 and produces a really nice coverage report, also linking with test files work, it tells which tests failed and so on.\n\nI will attach a patch, that changes common-build.xml to the new clover version (other initialization resource) and tells clover where to find the tests (using the test folder include/exclude properties).\n\nOne problem with the current patch: It does *not* instrument the backwards branch, so you see only coverage of the core/contrib tests. Getting the coverage also from the backwards tests is not easy possible because of two things:\n- the tag test dir is not easy to find out and add to <testsources> element (there may be only one of them)\n- the test names in BW branch are identical to the trunk tests. This completely corrupts the linkage between tests and code in the coverage report.\n\nIn principle the best would be to generate a second coverage report for the backwards branch with a separate clover DB. The attached patch does not instrument the bw branch, it only does trunk tests.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3524",
        "summary": "Add \"direct\" PackedInts.Reader impl, that reads directly from disk on each get",
        "description": "Spinoff from LUCENE-3518.\n\nIf we had a direct PackedInts.Reader impl we could use that instead of\nthe RandomAccessReaderIterator.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2779",
        "summary": "Use ConcurrentHashMap in RAMDirectory",
        "description": "RAMDirectory synchronizes on its instance in many places to protect access to map of RAMFiles, in addition to updating the sizeInBytes member. In many places the sync is done for 'read' purposes, while only in few places we need 'write' access. This looks like a perfect use case for ConcurrentHashMap\n\nAlso, syncing around sizeInBytes is unnecessary IMO, since it's an AtomicLong ...\n\nI'll post a patch shortly.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1775",
        "summary": "Change remaining contrib streams/filters to use new TokenStream API",
        "description": "All other contrib streams/filters have already been converted with LUCENE-1460.\n\nThe two shingle filters are the last ones we need to convert.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2107",
        "summary": "Add contrib/fast-vector-highlighter to Maven central repo",
        "description": "I'm not at all familiar with the Lucene build/deployment process, but it would be very nice if releases of the fast vector highlighter were pushed to the maven central repository, as is done with other contrib modules.\n\n(Issue filed at the request of Grant Ingersoll.)",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1699",
        "summary": "Field tokenStream should be usable with stored fields.",
        "description": "Field.tokenStream should be usable for indexing even for stored values.  Useful for many types of pre-analyzed values (text/numbers, etc)\nhttp://search.lucidimagination.com/search/document/902bad4eae20bdb8/field_tokenstreamvalue",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1187",
        "summary": "Things to be done now that Filter is independent from BitSet",
        "description": "(Aside: where is the documentation on how to mark up text in jira comments?)\n\nThe following things are left over after LUCENE-584 :\n\nFor Lucene 3.0  Filter.bits() will have to be removed.\n\nThere is a CHECKME in IndexSearcher about using ConjunctionScorer to have the boolean behaviour of a Filter.\n\nI have not looked into Filter caching yet, but I suppose there will be some room for improvement there.\nIirc the current core has moved to use OpenBitSetFilter and that is probably what is being cached.\nIn some cases it might be better to cache a SortedVIntList instead.\n\nBoolean logic on DocIdSetIterator is already available for Scorers (that inherit from DocIdSetIterator) in the search package. This is currently implemented by ConjunctionScorer, DisjunctionSumScorer,\nReqOptSumScorer and ReqExclScorer.\nBoolean logic on BitSets is available in contrib/misc and contrib/queries\n\nDisjunctionSumScorer calls score() on its subscorers before the score value actually needed.\nThis could be a reason to introduce a DisjunctionDocIdSetIterator, perhaps as a superclass of DisjunctionSumScorer.\n\nTo fully implement non scoring queries a TermDocIdSetIterator will be needed, perhaps as a superclass of TermScorer.\n\nThe javadocs in org.apache.lucene.search using matching vs non-zero score:\nI'll investigate this soon, and provide a patch when necessary.\n\nAn early version of the patches of LUCENE-584 contained a class Matcher,\nthat differs from the current DocIdSet in that Matcher has an explain() method.\nIt remains to be seen whether such a Matcher could be useful between\nDocIdSet and Scorer.\n\nThe semantics of scorer.skipTo(scorer.doc()) was discussed briefly.\nThis was also discussed at another issue recently, so perhaps it is wortwhile to open a separate issue for this.\n\nSkipping on a SortedVIntList is done using linear search, this could be improved by adding multilevel skiplist info much like in the Lucene index for documents containing a term.\n\nOne comment by me of 3 Dec 2008:\n\nA few complete (test) classes are deprecated, it might be good to add the target release for removal there.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1539",
        "summary": "Improve Benchmark",
        "description": "Benchmark can be improved by incorporating recent suggestions posted\non java-dev. M. McCandless' Python scripts that execute multiple\nrounds of tests can either be incorporated into the codebase or\nconverted to Java.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2669",
        "summary": "NumericRangeQuery.NumericRangeTermsEnum sometimes seeks backwards",
        "description": "Subclasses of FilteredTermsEnum are \"supposed to\" seek forwards only (this gives better performance, typically).\n\nHowever, we don't check for this, so I added an assert to do that (while digging into testing the SimpleText codec) and NumericRangeQuery trips the assert!\n\nOther MTQs seem not to trip it.\n\nI think I know what's happening -- say NRQ has term ranges a-c, e-f to seek to, but then while it's .next()'ing through the first range, the first term after c is f.  At this point NRQ sees the range a-c is done, and then tries to seek to term e which is before f.  Maybe NRQ's accept method should detect this case (where you've accidentally .next()'d into or possibly beyond the next one or more seek ranges)?",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1087",
        "summary": "MultiSearcher.explain returns incorrect score/explanation relating to docFreq",
        "description": "Creating 2 different indexes, searching  each individually and print score details and compare to searching both indexes with MulitSearcher and printing score details.  \n \nThe \"docFreq\" value printed isn't correct - the values it prints are as if each index was searched individually.\n\nCode is like:\n{code}\nMultiSearcher multi = new MultiSearcher(searchables);\nHits hits = multi.search(query);\nfor(int i=0; i<hits.length(); i++)\n{\n  Explanation expl = multi.explain(query, hits.id(i));\n  System.out.println(expl.toString());\n}\n{code}\n\nI raised this in the Lucene user mailing list and was advised to log a bug, email thread given below.\n\n{noformat} \n-----Original Message-----\nFrom: Chris Hostetter  \nSent: Friday, December 07, 2007 10:30 PM\nTo: java-user\nSubject: Re: does the MultiSearcher class calculate IDF properly?\n\n\na quick glance at the code seems to indicate that MultiSearcher has code \nfor calcuating the docFreq accross all of the Searchables when searching \n(or when the docFreq method is explicitly called) but that explain method \njust delegates to Searchable that the specific docid came from.\n\nif you compare that Explanation score you got with the score returned by \na HitCollector (or TopDocs) they probably won't match.\n\nSo i would say \"yes MultiSearcher calculates IDF properly, but \nMultiSeracher.explain is broken.  Please file a bug about this, i can't \nthink of an easy way to fix it, but it certianly seems broken to me.\n\n\n: Subject: does the MultiSearcher class calculate IDF properly?\n: \n: I tried the following.  Creating 2 different indexes, search each\n: individually and print score details and compare to searching both\n: indexes with MulitSearcher and printing score details.  \n: \n: The \"docFreq\" value printed don't seem right - is this just a problem\n: with using Explain together with the MultiSearcher?\n: \n: \n: Code is like:\n: MultiSearcher multi = new MultiSearcher(searchables);\n: Hits hits = multi.search(query);\n: for(int i=0; i<hits.length(); i++)\n: {\n:   Explanation expl = multi.explain(query, hits.id(i));\n:   System.out.println(expl.toString());\n: }\n: \n: \n: Output:\n: id = 14 score = 0.071\n: 0.07073946 = (MATCH) fieldWeight(contents:climate in 2), product of:\n:   1.0 = tf(termFreq(contents:climate)=1)\n:   1.8109303 = idf(docFreq=1)\n:   0.0390625 = fieldNorm(field=contents, doc=2)\n{noformat} ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2923",
        "summary": "cleanup contrib/demo",
        "description": "I don't think we should include optimize in the demo; many people start from the demo and may think you must optimize to do searching, and that's clearly not the case.\n\nI think we should also use a buffered reader in FileDocument?\n\nAnd... I'm tempted to remove IndexHTML (and the html parser) entirely.  It's ancient, and we now have Tika to extract text from many doc formats.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2598",
        "summary": "allow tests to use different Directory impls",
        "description": "Now that all tests use MockRAMDirectory instead of RAMDirectory, they are all picky like windows and force our tests to\nclose readers etc before closing the directory.\n\nI think we should do the following:\n# change new MockRAMDIrectory() in tests to .newDirectory(random)\n# LuceneTestCase[J4] tracks if all dirs are closed at tearDown and also cleans up temp dirs like solr.\n# factor out the Mockish stuff from MockRAMDirectory into MockDirectoryWrapper\n# allow a -Dtests.directoryImpl or simpler to specify the default Directory to use for tests: default being \"random\"\n\ni think theres a chance we might find some bugs that havent yet surfaced because they are easier to trigger with FSDir\nFurthermore, this would be beneficial to Directory-implementors as they could run the entire testsuite against their Directory impl, just like codec-implementors can do now.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1644",
        "summary": "Enable MultiTermQuery's constant score mode to also use BooleanQuery under the hood",
        "description": "When MultiTermQuery is used (via one of its subclasses, eg\nWildcardQuery, PrefixQuery, FuzzyQuery, etc.), you can ask it to use\n\"constant score mode\", which pre-builds a filter and then wraps that\nfilter as a ConstantScoreQuery.\n\nIf you don't set that, it instead builds a [potentially massive]\nBooleanQuery with one SHOULD clause per term.\n\nThere are some limitations of this approach:\n\n  * The scores returned by the BooleanQuery are often quite\n    meaningless to the app, so, one should be able to use a\n    BooleanQuery yet get constant scores back.  (Though I vaguely\n    remember at least one example someone raised where the scores were\n    useful...).\n\n  * The resulting BooleanQuery can easily have too many clauses,\n    throwing an extremely confusing exception to newish users.\n\n  * It'd be better to have the freedom to pick \"build filter up front\"\n    vs \"build massive BooleanQuery\", when constant scoring is enabled,\n    because they have different performance tradeoffs.\n\n  * In constant score mode, an OpenBitSet is always used, yet for\n    sparse bit sets this does not give good performance.\n\nI think we could address these issues by giving BooleanQuery a\nconstant score mode, then empower MultiTermQuery (when in constant\nscore mode) to pick & choose whether to use BooleanQuery vs up-front\nfilter, and finally empower MultiTermQuery to pick the best (sparse vs\ndense) bit set impl.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2060",
        "summary": "CMS should default its maxThreadCount to 1 (not 3)",
        "description": "From rough experience, I think the current default of 3 is too large.  I think we get the most bang for the buck going from 0 to 1.\n\nI think this will especially impact optimize on an index with many segments -- in this case the MergePolicy happily exposes concurrency (multiple pending merges), and CMS will happily launch 3 threads to carry that out.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1126",
        "summary": "Simplify StandardTokenizer JFlex grammar",
        "description": "Summary of thread entitled \"Fullwidth alphanumeric characters, plus a question on Korean ranges\" begun by Daniel Noll on java-user, and carried over to java-dev:\n\nOn 01/07/2008 at 5:06 PM, Daniel Noll wrote:\n> I wish the tokeniser could just use Character.isLetter and\n> Character.isDigit instead of having to know all the ranges itself, since\n> the JRE already has all this information.  Character.isLetter does\n> return true for CJK characters though, so the ranges would still come in\n> handy for determining what kind of letter they are.  I don't support\n> JFlex has a way to do this...\n\nThe DIGIT macro could be replaced by JFlex's predefined character class [:digit:], which has the same semantics as java.lang.Character.isDigit().\n\nAlthough JFlex's predefined character class [:letter:] (same semantics as java.lang.Character.isLetter()) includes CJK characters, there is a way to handle this using JFlex's regex negation syntax {{!}}.  From [the JFlex documentation|http://jflex.de/manual.html]:\n\nbq. [T]he expression that matches everything of {{a}} not matched by {{b}} is !(!{{a}}|{{b}}) \n\nSo to exclude CJ characters from the LETTER macro:\n\n{code}\n    LETTER = ! ( ! [:letter:] | {CJ} )\n{code}\n \nSince [:letter:] includes all of the Korean ranges, there's no reason (AFAICT) to treat them separately; unlike Chinese and Japanese characters, which are individually tokenized, the Korean characters should participate in the same token boundary rules as all of the other letters.\n\nI looked at some of the differences between Unicode 3.0.0, which Java 1.4.2 supports, and Unicode 5.0, the latest version, and there are lots of new and modified letter and digit ranges.  This stuff gets tweaked all the time, and I don't think Lucene should be in the business of trying to track it, or take a position on which Unicode version users' data should conform to.  \n\nSwitching to using JFlex's [:letter:] and [:digit:] predefined character classes ties (most of) these decisions to the user's choice of JVM version, and this seems much more reasonable to me than the current status quo.\n\nI will attach a patch shortly.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3630",
        "summary": "MultiReader and ParallelReader accidently override doOpenIfChanged(boolean readOnly) with doOpenIfChanged(boolean doClone)",
        "description": "I found this during adding deprecations for RW access in LUCENE-3606:\n\nthe base class defines doOpenIfChanged(boolean readOnly), but MultiReader and ParallelReader \"override\" this method with a signature doOpenIfChanged(doClone) and missing @Override. This makes consumers calling IR.openIfChanged(boolean readOnly) do the wrong thing. Instead they should get UOE like for the other unimplemented doOpenIfChanged methods in MR and PR.\n\nEasy fix is to rename and hide this internal \"reopen\" method, like DirectoryReader,...",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3386",
        "summary": "Integrate MockBM25Similarity and MockLMSimilarity into the framework",
        "description": "Steps:\n1. Decide if {{MockLMSimilarity}} is needed at all (we have {{LMDirichletSimilarity}})\n2. Move the classes to the similarities package\n3. Move the similarities package to src/\n4. Move all sims (inc. Similarity) to similarities\n5. Make MockBM25Similarity a subclass of EasySimilarity?",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-3557",
        "summary": "Spellchecker should take IndexWriterConfig... deprecate old methods?",
        "description": "When looking at LUCENE-3490, i realized there was no way to specify the codec for the spellchecker to use.\n\nIt has the following current methods:\n* indexDictionary(Dictionary dict): this causes optimize!\n* indexDictionary(Dictionary dict, int mergeFactory, int ramMB): this causes optimize!\n* indexDictionary(Dictionary dict, int mergeFactor, int ramMB, boolean optimize)\n\nBut no way to specify an IndexwriterConfig. Additionally, I don't like that several of these ctors force an optimize in a tricky way,\neven though it was like this all along.\n\nSo I think we should add indexDictionary(Dictionary dict, IndexWriterConfig config, boolean optimize).\n\nWe should either deprecate all the other ctors in 3.x and nuke in trunk, or at least add warnings to the ones that optimize.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2017",
        "summary": "CloseableThreadLocal is now obsolete",
        "description": "Since Lucene 3 depends on Java 5, we can use ThreadLocal#remove() to take care or resource management.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1196",
        "summary": "RAMDirectory reports incorrect EOF on seek",
        "description": "If you create a file whose length is a multiple of 1024 (BUFFER_SIZE),\nand then try to seek to the very end of the file, you hit\nEOFException.\n\nBut this is actually \"legal\" as long as you don't try to read any\nbytes at that point.\n\nI'm hitting this (rarely) with the bulk-merging logic for term vectors\n(LUCENE-1120), which can seek to the very end of the file but not read\nany bytes if conditions are right.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2383",
        "summary": "Some small fixes after the flex merge...",
        "description": "Changes:\n\n  * Re-introduced specialization optimization to FieldCacheRangeQuery;\n    also fixed bug (was failing to check deletions in advance)\n\n  * Changes 2 checkIndex methods from protected -> public\n\n  * Add some missing null checks when calling MultiFields.getFields or\n    IndexReader.fields()\n\n  * Tweak'd CHANGES a bit\n\n  * Removed some small dead code\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2323",
        "summary": "reorganize contrib modules",
        "description": "it would be nice to reorganize contrib modules, so that they are bundled together by functionality.\n\nFor example:\n* the wikipedia contrib is a tokenizer, i think really belongs in contrib/analyzers\n* there are two highlighters, i think could be one highlighters package.\n* there are many queryparsers and queries in different places in contrib\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-930",
        "summary": "fail build if contrib tests fail to compile",
        "description": "spinoff of LUCENE-885, from Steven's comments...\n\nLooking at the current build (r545324) it looks like the some contrib failures are getting swallowed. Things like lucli are throwing errors along the lines of\n\n [subant] /home/barronpark/smparkes/work/lucene/trunk/common-build.xml:366: srcdir \"/home/barronpark/smparkes/work/lucene/trunk/contrib/lucli/src/test\" does not exist!\n\nbut these don't make it back up to the top level status.\n\nIt looks like the current state will bubble up junit failures, but maybe not build failures?\n\n...\n\nIt's \"test-compile-contrib\" (if you will) that fails and rather being contrib-crawled, that's only done as the target of \"test\" in each contrib directory, at which point, it's running in the protected contrib-crawl.\n\nEasy enough to lift this loop into another target, e.g., build-contrib-test. And that will start surfacing errors, which I can work through.\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1651",
        "summary": "Make IndexReader.open() always return MSR to simplify (re-)opens.",
        "description": "As per discussion in mailing list, I'm making DirectoryIndexReader.open() always return MSR, even for single-segment indexes.\nWhile theoretically valid in the past (if you make sure to keep your index constantly optimized) this feature is made practically obsolete by per-segment collection.\n\nThe patch somewhat de-hairies (re-)open logic for MSR/SR.\nSR no longer needs an ability to pose as toplevel directory-owning IR.\nAll related logic is moved from DIR to MSR.\nDIR becomes almost empty, and copying two or three remaining fields over to MSR/SR, I remove it.\nLots of tests fail, as they rely on SR returned from IR.open(), I fix by introducing SR.getOnlySegmentReader static package-private method.\nSome previous bugs are uncovered, one is fixed in LUCENE-1645, another (partially fixed in LUCENE-1648) is fixed in this patch. ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2373",
        "summary": "Create a Codec to work with streaming and append-only filesystems",
        "description": "Since early 2.x times Lucene used a skip/seek/write trick to patch the length of the terms dict into a place near the start of the output data file. This however made it impossible to use Lucene with append-only filesystems such as HDFS.\n\nIn the post-flex trunk the following code in StandardTermsDictWriter initiates this:\n{code}\n    // Count indexed fields up front\n    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); \n\n    out.writeLong(0);                             // leave space for end index pointer\n{code}\nand completes this in close():\n{code}\n      out.seek(CodecUtil.headerLength(CODEC_NAME));\n      out.writeLong(dirStart);\n{code}\n\nI propose to change this layout so that this pointer is stored simply at the end of the file. It's always 8 bytes long, and we known the final length of the file from Directory, so it's a single additional seek(length - 8) to read it, which is not much considering the benefits.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-806",
        "summary": "Synchronization bottleneck in FieldSortedHitQueue with many concurrent readers",
        "description": "The below is from a post by (my colleague) Paul Smith to the java-users list:\n\n---\n\nHi ho peoples.\n\nWe have an application that is internationalized, and stores data from many languages (each project has it's own index, mostly aligned with a single language, maybe 2).\n\nAnyway, I've noticed during some thread dumps diagnosing some performance issues, that there appears to be a _potential_ synchronization bottleneck using Locale-based sorting of Strings.  I don't think this problem is the root cause of our performance problem, but I thought I'd mention it here.  Here's the stack dump of a thread waiting:\n\n\"http-1001-Processor245\" daemon prio=1 tid=0x31434da0 nid=0x3744 waiting for monitor entry [0x2cd44000..0x2cd45f30]\n        at java.text.RuleBasedCollator.compare(RuleBasedCollator.java)\n        - waiting to lock <0x6b1e8c68> (a java.text.RuleBasedCollator)\n        at org.apache.lucene.search.FieldSortedHitQueue$4.compare(FieldSortedHitQueue.java:320)\n        at org.apache.lucene.search.FieldSortedHitQueue.lessThan(FieldSortedHitQueue.java:114)\n        at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:120)\n        at org.apache.lucene.util.PriorityQueue.put(PriorityQueue.java:47)\n        at org.apache.lucene.util.PriorityQueue.insert(PriorityQueue.java:58)\n        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:90)\n        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:97)\n        at org.apache.lucene.search.TopFieldDocCollector.collect(TopFieldDocCollector.java:47)\n        at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:291)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:132)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:110)\n        at com.aconex.index.search.FastLocaleSortIndexSearcher.search(FastLocaleSortIndexSearcher.java:90)\n.....\n\nIn our case we had 12 threads waiting like this, while one thread had the lock on the RuleBasedCollator.  Turns out RuleBasedCollator's.compare(...) method is synchronized.  I wonder if a ThreadLocal based collator would be better here... ?  There doesn't appear to be a reason for other threads searching the same index to wait on this sort.  Be just as easy to use their own.  (Is RuleBasedCollator a \"heavy\" object memory wise?  Wouldn't have thought so, per thread)\n\nThoughts?\n\n---\n\nI've investigated this somewhat, and agree that this is a potential problem with a series of possible workarounds. Further discussion (including proof-of-concept patch) to follow.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1752",
        "summary": "incorrect snippet returned with SpanScorer",
        "description": "This problem was reported by my customer. They are using Solr 1.3 and uni-gram, but it can be reproduced with Lucene 2.9 and WhitespaceAnalyzer.\n\n{panel:title=Query}\n(f1:\"a b c d\" OR f2:\"a b c d\") AND (f1:\"b c g\" OR f2:\"b c g\")\n{panel}\n\nThe snippet we expected is:\n{panel}\nx y z <B>a</B> <B>b</B> <B>c</B> <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>\n{panel}\n\nbut we got:\n{panel}\nx y z <B>a</B> b c <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>\n{panel}\n\nProgram to reproduce the problem:\n{code}\npublic class TestHighlighter {\n\n  static final String CONTENT = \"x y z a b c d e f g b c g\";\n  static final String PH1 = \"\\\"a b c d\\\"\";\n  static final String PH2 = \"\\\"b c g\\\"\";\n  static final String F1 = \"f1\";\n  static final String F2 = \"f2\";\n  static final String F1C = F1 + \":\";\n  static final String F2C = F2 + \":\";\n  static final String QUERY_STRING =\n    \"(\" + F1C + PH1 + \" OR \" + F2C + PH1 + \") AND (\"\n    + F1C + PH2 + \" OR \" + F2C + PH2 + \")\";\n  static Analyzer analyzer = new WhitespaceAnalyzer();\n  \n  public static void main(String[] args) throws Exception {\n    QueryParser qp = new QueryParser( F1, analyzer );\n    Query query = qp.parse( QUERY_STRING );\n    CachingTokenFilter stream = new CachingTokenFilter( analyzer.tokenStream( F1, new StringReader( CONTENT ) ) );\n    Scorer scorer = new SpanScorer( query, F1, stream, false );\n    Highlighter h = new Highlighter( scorer );\n    System.out.println( \"query : \" + QUERY_STRING );\n    System.out.println( h.getBestFragment( analyzer, F1,  CONTENT ) );\n  }\n}\n{code}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3174",
        "summary": "Similarity.Stats class for term & collection statistics",
        "description": "In order to support ranking methods besides TF-IDF, we need to make the statistics they need available. These statistics could be computed in computeWeight (soon to become computeStats) and stored in a separate object for easy access. Since this object will be used solely by subclasses of Similarity, it should be implented as a static inner class, i.e. Similarity.Stats.\n\nThere are two ways this could be implemented:\n- as a single Similarity.Stats class, reused by all ranking algorithms. In this case, this class would have a member field for all statistics;\n- as a hierarchy of Stats classes, one for each ranking algorithm. Each subclass would define only the statistics needed for the ranking algorithm.\n\nIn the second case, the Stats class in DefaultSimilarity would have a single field, idf, while the one in e.g. BM25Similarity would have idf and average field/document length.",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-2011",
        "summary": "Remove deprecated Scorer.explain(int) method",
        "description": "This is the only remaining deprecation in core, but is not so easy to handle, because lot's of code in core still uses the explain() method in Scorer. So e.g. in PhraseQuery, the explain method has to be moved from Scorer to the Weight.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1862",
        "summary": "duplicate package.html files in queryParser and analsysis.cn packages",
        "description": "These files conflict with eachother when building the javadocs. there can be only one (of each) ...\n\n{code}\nhossman@brunner:~/lucene/java$ find src contrib -name package.html | perl -ple 's{.*src/java/}{}' | sort | uniq -c | grep -v \" 1 \"\n   2 org/apache/lucene/analysis/cn/package.html\n   2 org/apache/lucene/queryParser/package.html\nhossman@brunner:~/lucene/java$ find src contrib -path \\*queryParser/package.html\nsrc/java/org/apache/lucene/queryParser/package.html\ncontrib/queryparser/src/java/org/apache/lucene/queryParser/package.html\nhossman@brunner:~/lucene/java$ find src contrib -path \\*cn/package.html\ncontrib/analyzers/common/src/java/org/apache/lucene/analysis/cn/package.html\ncontrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/package.html\n{code}\n\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3444",
        "summary": "Distinct field value count per group",
        "description": "Support a second pass collector that counts unique field values of a field per group.\nThis is just one example of group statistics that one might want.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2656",
        "summary": "If tests fail, don't report about unclosed resources",
        "description": "LuceneTestCase ensures in afterClass() if you closed all your directories, which in turn will check if you have closed any open files.\n\nThis is good, as a test will fail if we have resource leaks.\n\nBut if a test truly fails, this is just confusing, because its usually not going to make it to the part of its code where it would call .close()\n\nSo, if any tests fail, I think we should omit this check in afterClass()",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1905",
        "summary": "Instantiating SimpleFSLockFactory by its String param constructor throws an IllegalStateException",
        "description": "",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1906",
        "summary": "Backwards problems with CharStream and Tokenizers with custom reset(Reader) method",
        "description": "When reviewing the new CharStream code added to Tokenizers, I found a\nserious problem with backwards compatibility and other Tokenizers, that do\nnot override reset(CharStream).\n\nThe problem is, that e.g. CharTokenizer only overrides reset(Reader):\n\n{code}\n  public void reset(Reader input) throws IOException {\n    super.reset(input);\n    bufferIndex = 0;\n    offset = 0;\n    dataLen = 0;\n  }\n{code}\n\nIf you reset such a Tokenizer with another CharStream (not a Reader), this\nmethod will never be called and breaking the whole Tokenizer.\n\nAs CharStream extends Reader, I propose to remove this reset(CharStream\nmethod) and simply do an instanceof check to detect if the supplied Reader\nis no CharStream and wrap it. We could also remove the extra ctor (because\nmost Tokenizers have no support for passing CharStreams). If the ctor also\nchecks with instanceof and warps as needed the code is backwards compatible\nand we do not need to add additional ctors in subclasses.\n\nAs this instanceof check is always done in CharReader.get() why not remove\nctor(CharStream) and reset(CharStream) completely?\n\nAny thoughts?\n\nI would like to fix this somehow before RC4, I'm, sorry :(\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2089",
        "summary": "explore using automaton for fuzzyquery",
        "description": "we can optimize fuzzyquery by using AutomatonTermsEnum. The idea is to speed up the core FuzzyQuery in similar fashion to Wildcard and Regex speedups, maintaining all backwards compatibility.\n\nThe advantages are:\n* we can seek to terms that are useful, instead of brute-forcing the entire terms dict\n* we can determine matches faster, as true/false from a DFA is array lookup, don't even need to run levenshtein.\n\nWe build Levenshtein DFAs in linear time with respect to the length of the word: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652\n\nTo implement support for 'prefix' length, we simply concatenate two DFAs, which doesn't require us to do NFA->DFA conversion, as the prefix portion is a singleton. the concatenation is also constant time with respect to the size of the fuzzy DFA, it only need examine its start state.\n\nwith this algorithm, parametric tables are precomputed so that DFAs can be constructed very quickly.\nif the required number of edits is too large (we don't have a table for it), we use \"dumb mode\" at first (no seeking, no DFA, just brute force like now).\n\nAs the priority queue fills up during enumeration, the similarity score required to be a competitive term increases, so, the enum gets faster and faster as this happens. This is because terms in core FuzzyQuery are sorted by boost value, then by term (in lexicographic order).\n\nFor a large term dictionary with a low minimal similarity, you will fill the pq very quickly since you will match many terms. \nThis not only provides a mechanism to switch to more efficient DFAs (edit distance of 2 -> edit distance of 1 -> edit distance of 0) during enumeration, but also to switch from \"dumb mode\" to \"smart mode\".\n\nWith this design, we can add more DFAs at any time by adding additional tables. The tradeoff is the tables get rather large, so for very high K, we would start to increase the size of Lucene's jar file. The idea is we don't have include large tables for very high K, by using the 'competitive boost' attribute of the priority queue.\n\nFor more information, see http://en.wikipedia.org/wiki/Levenshtein_automaton",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2931",
        "summary": "Improved javadocs for PriorityQueue#lessThan",
        "description": "It kills me that I have to inspect the code every time I implement a PriorityQueue. :)",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2012",
        "summary": "Add @Override annotations",
        "description": "During removal of deprecated APIs, mostly the problem was, to not only remove the method in the (abstract) base class (e.g. Scorer.explain()), but also remove it in sub classes that override it. You can easily forget that (especially, if the method was not marked deprecated in the subclass). By adding @Override annotations everywhere in Lucene, such removals are simple, because the compiler throws out an error message in all subclasses which then no longer override the method.\n\nAlso it helps preventing the well-known traps like overriding hashcode() instead of hashCode().\n\nThe patch was generated automatically, and is rather large. Should I apply it, or would it break too many patches (but I think, trunk has changed so much, that this is only a minimum of additional work to merge)?",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2153",
        "summary": "IndexReader.open should take Codecs",
        "description": "Need to make this public... it's private now.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3705",
        "summary": "deadlock in TestIndexWriterExceptions",
        "description": "    [junit] 2012-01-18 18:18:16\n    [junit] Full thread dump Java HotSpot(TM) 64-Bit Server VM (19.1-b02 mixed mode):\n    [junit] \n    [junit] \"Indexer 3\" prio=10 tid=0x0000000041b9b800 nid=0x6291 waiting for monitor entry [0x00007f7e8868f000]\n    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)\n    [junit] \t- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)\n    [junit] \n    [junit] \"Indexer 2\" prio=10 tid=0x0000000041b9b000 nid=0x6290 waiting on condition [0x00007f7e8838c000]\n    [junit]    java.lang.Thread.State: WAITING (parking)\n    [junit] \tat sun.misc.Unsafe.park(Native Method)\n    [junit] \t- parking to wait for  <0x00000000e4103100> (a org.apache.lucene.index.DocumentsWriterStallControl$Sync)\n    [junit] \tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)\n    [junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)\n    [junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:941)\n    [junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1261)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterStallControl.waitIfStalled(DocumentsWriterStallControl.java:115)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushControl.waitIfStalled(DocumentsWriterFlushControl.java:591)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.preUpdate(DocumentsWriter.java:302)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:362)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)\n    [junit] \n    [junit] \"Indexer 1\" prio=10 tid=0x0000000042500000 nid=0x628f waiting for monitor entry [0x00007f7e8858e000]\n    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushQueue.addSegment(DocumentsWriterFlushQueue.java:84)\n    [junit] \t- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)\n    [junit] \n    [junit] \"Indexer 0\" prio=10 tid=0x0000000041508000 nid=0x628d waiting on condition [0x00007f7e8848d000]\n    [junit]    java.lang.Thread.State: WAITING (parking)\n    [junit] \tat sun.misc.Unsafe.park(Native Method)\n    [junit] \t- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\n    [junit] \tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)\n    [junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)\n    [junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)\n    [junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)\n    [junit] \tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)\n    [junit] \tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)\n    [junit] \t- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)\n    [junit] \n    [junit] \"Low Memory Detector\" daemon prio=10 tid=0x00007f7e84025800 nid=0x6003 runnable [0x0000000000000000]\n    [junit]    java.lang.Thread.State: RUNNABLE\n    [junit] \n    [junit] \"CompilerThread1\" daemon prio=10 tid=0x00007f7e84022800 nid=0x6002 waiting on condition [0x0000000000000000]\n    [junit]    java.lang.Thread.State: RUNNABLE\n    [junit] \n    [junit] \"CompilerThread0\" daemon prio=10 tid=0x00007f7e8401f800 nid=0x6001 waiting on condition [0x0000000000000000]\n    [junit]    java.lang.Thread.State: RUNNABLE\n    [junit] \n    [junit] \"Signal Dispatcher\" daemon prio=10 tid=0x00007f7e8401d800 nid=0x6000 waiting on condition [0x0000000000000000]\n    [junit]    java.lang.Thread.State: RUNNABLE\n    [junit] \n    [junit] \"Finalizer\" daemon prio=10 tid=0x00007f7e84001000 nid=0x5ffa in Object.wait() [0x00007f7e8961b000]\n    [junit]    java.lang.Thread.State: WAITING (on object monitor)\n    [junit] \tat java.lang.Object.wait(Native Method)\n    [junit] \t- waiting on <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)\n    [junit] \tat java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)\n    [junit] \t- locked <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)\n    [junit] \tat java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)\n    [junit] \tat java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)\n    [junit] \n    [junit] \"Reference Handler\" daemon prio=10 tid=0x000000004101e000 nid=0x5ff9 in Object.wait() [0x00007f7e8971c000]\n    [junit]    java.lang.Thread.State: WAITING (on object monitor)\n    [junit] \tat java.lang.Object.wait(Native Method)\n    [junit] \t- waiting on <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)\n    [junit] \tat java.lang.Object.wait(Object.java:485)\n    [junit] \tat java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)\n    [junit] \t- locked <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)\n    [junit] \n    [junit] \"main\" prio=10 tid=0x0000000040fb2000 nid=0x5fe2 in Object.wait() [0x00007f7e8ecc1000]\n    [junit]    java.lang.Thread.State: WAITING (on object monitor)\n    [junit] \tat java.lang.Object.wait(Native Method)\n    [junit] \t- waiting on <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)\n    [junit] \tat java.lang.Thread.join(Thread.java:1186)\n    [junit] \t- locked <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)\n    [junit] \tat java.lang.Thread.join(Thread.java:1239)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:286)\n    [junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit] \tat java.lang.reflect.Method.invoke(Method.java:597)\n    [junit] \tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit] \tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit] \tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit] \tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit] \tat org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:528)\n    [junit] \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit] \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit] \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n    [junit] \tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit] \tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit] \tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit] \tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit] \tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit] \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit] \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit] \tat org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit] \tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)\n    [junit] \n    [junit] \"VM Thread\" prio=10 tid=0x0000000041017800 nid=0x5fef runnable \n    [junit] \n    [junit] \"GC task thread#0 (ParallelGC)\" prio=10 tid=0x0000000040fc5000 nid=0x5fe3 runnable \n    [junit] \n    [junit] \"GC task thread#1 (ParallelGC)\" prio=10 tid=0x0000000040fc7000 nid=0x5fe4 runnable \n    [junit] \n    [junit] \"GC task thread#2 (ParallelGC)\" prio=10 tid=0x0000000040fc9000 nid=0x5fe5 runnable \n    [junit] \n    [junit] \"GC task thread#3 (ParallelGC)\" prio=10 tid=0x0000000040fca800 nid=0x5fe6 runnable \n    [junit] \n    [junit] \"GC task thread#4 (ParallelGC)\" prio=10 tid=0x0000000040fcc800 nid=0x5fe7 runnable \n    [junit] \n    [junit] \"GC task thread#5 (ParallelGC)\" prio=10 tid=0x0000000040fce800 nid=0x5fe8 runnable \n    [junit] \n    [junit] \"GC task thread#6 (ParallelGC)\" prio=10 tid=0x0000000040fd0000 nid=0x5fe9 runnable \n    [junit] \n    [junit] \"GC task thread#7 (ParallelGC)\" prio=10 tid=0x0000000040fd2000 nid=0x5fea runnable \n    [junit] \n    [junit] \"VM Periodic Task Thread\" prio=10 tid=0x00007f7e84030000 nid=0x6004 waiting on condition \n    [junit] \n    [junit] JNI global references: 1578\n    [junit] \n    [junit] \n    [junit] Found one Java-level deadlock:\n    [junit] =============================\n    [junit] \"Indexer 3\":\n    [junit]   waiting to lock monitor 0x0000000041477498 (object 0x00000000e40ff2a8, a org.apache.lucene.index.DocumentsWriterFlushQueue),\n    [junit]   which is held by \"Indexer 0\"\n    [junit] \"Indexer 0\":\n    [junit]   waiting for ownable synchronizer 0x00000000e414b408, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),\n    [junit]   which is held by \"Indexer 3\"\n    [junit] \n    [junit] Java stack information for the threads listed above:\n    [junit] ===================================================\n    [junit] \"Indexer 3\":\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)\n    [junit] \t- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)\n    [junit] \"Indexer 0\":\n    [junit] \tat sun.misc.Unsafe.park(Native Method)\n    [junit] \t- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\n    [junit] \tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)\n    [junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)\n    [junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)\n    [junit] \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)\n    [junit] \tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)\n    [junit] \tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)\n    [junit] \t- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)\n    [junit] \n    [junit] Found 1 deadlock.\n    [junit] \n    [junit] Heap\n    [junit]  PSYoungGen      total 67136K, used 4647K [0x00000000f5560000, 0x00000000fbc60000, 0x0000000100000000)\n    [junit]   eden space 65792K, 5% used [0x00000000f5560000,0x00000000f58a5e10,0x00000000f95a0000)\n    [junit]   from space 1344K, 96% used [0x00000000f9740000,0x00000000f98840a0,0x00000000f9890000)\n    [junit]   to   space 19840K, 0% used [0x00000000fa900000,0x00000000fa900000,0x00000000fbc60000)\n    [junit]  PSOldGen        total 171392K, used 66868K [0x00000000e0000000, 0x00000000ea760000, 0x00000000f5560000)\n    [junit]   object space 171392K, 39% used [0x00000000e0000000,0x00000000e414d080,0x00000000ea760000)\n    [junit]  PSPermGen       total 21248K, used 14733K [0x00000000dae00000, 0x00000000dc2c0000, 0x00000000e0000000)\n    [junit]   object space 21248K, 69% used [0x00000000dae00000,0x00000000dbc635a8,0x00000000dc2c0000)\n    [junit] \n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2696",
        "summary": "TestIndexWriterDelete makes broken segments with payloads on",
        "description": "This could just be a SimpleText problem.... but just in case\n\nGrant added payloads to MockAnalyzer in LUCENE-2692\n\nI wondered what would happen if i turned on his payload filter by default for all tests.\n\n{noformat}\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete\n    [junit] Testcase: testDeletesOnDiskFull(org.apache.lucene.index.TestIndexWriterDelete):     Caused an ERROR\n    [junit] CheckIndex failed\n    [junit] java.lang.RuntimeException: CheckIndex failed\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:80)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5\n38)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit]\n    [junit]\n    [junit] Testcase: testUpdatesOnDiskFull(org.apache.lucene.index.TestIndexWriterDelete):     Caused an ERROR\n    [junit] CheckIndex failed\n    [junit] java.lang.RuntimeException: CheckIndex failed\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:80)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5\n38)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit]\n    [junit]\n    [junit] Tests run: 14, Failures: 0, Errors: 2, Time elapsed: 0.322 sec\n    [junit]\n    [junit] ------------- Standard Output ---------------\n    [junit] CheckIndex failed\n    [junit] Segments file=segments_2 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]\n    [junit]   1 of 1: name=_0 docCount=157\n    [junit]     codec=MockFixedIntBlock\n    [junit]     compound=true\n    [junit]     hasProx=true\n    [junit]     numFiles=2\n    [junit]     size (MB)=0,017\n    [junit]     diagnostics = {os.version=6.0, os=Windows Vista, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=x86,\n java.version=1.6.0_21, java.vendor=Sun Microsystems Inc.}\n    [junit]     has deletions [delFileName=_0_1.del]\n    [junit]     test: open reader.........OK [13 deleted docs]\n    [junit]     test: fields..............OK [2 fields]\n    [junit]     test: field norms.........OK [2 fields]\n    [junit]     test: terms, freq, prox...ERROR [read past EOF]\n    [junit] java.io.IOException: read past EOF\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:119)\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:94)\n    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsAndPositionsEnum.getPayload(SepPostin\ngsReaderImpl.java:689)\n    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:693)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:489)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5\n38)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)\n    [junit]     test: stored fields.......OK [223 total field count; avg 1,549 fields per doc]\n    [junit]     test: term vectors........OK [242 total vector count; avg 1,681 term/freq vector fields per doc]\n    [junit] FAILED\n    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:\n    [junit] java.lang.RuntimeException: Term Index test failed\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:502)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5\n38)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)\n    [junit]\n    [junit] WARNING: 1 broken segments (containing 144 documents) detected\n    [junit]\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testDeletesOnDiskFull -Dtests.s\need=-8128829179004133416:-7192468460505114475\n    [junit] CheckIndex failed\n    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]\n    [junit]   1 of 1: name=_0 docCount=157\n    [junit]     codec=MockFixedIntBlock\n    [junit]     compound=false\n    [junit]     hasProx=true\n    [junit]     numFiles=14\n    [junit]     size (MB)=0,017\n    [junit]     diagnostics = {os.version=6.0, os=Windows Vista, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=x86,\n java.version=1.6.0_21, java.vendor=Sun Microsystems Inc.}\n    [junit]     no deletions\n    [junit]     test: open reader.........OK\n    [junit]     test: fields..............OK [2 fields]\n    [junit]     test: field norms.........OK [2 fields]\n    [junit]     test: terms, freq, prox...ERROR [Read past EOF]\n    [junit] java.io.IOException: Read past EOF\n    [junit]     at org.apache.lucene.store.RAMInputStream.switchCurrentBuffer(RAMInputStream.java:89)\n    [junit]     at org.apache.lucene.store.RAMInputStream.readBytes(RAMInputStream.java:73)\n    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readBytes(MockIndexInputWrapper.java:109)\n    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsAndPositionsEnum.getPayload(SepPostin\ngsReaderImpl.java:689)\n    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:693)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:489)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5\n38)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)\n    [junit]     test: stored fields.......OK [237 total field count; avg 1,51 fields per doc]\n    [junit]     test: term vectors........OK [254 total vector count; avg 1,618 term/freq vector fields per doc]\n    [junit] FAILED\n    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:\n    [junit] java.lang.RuntimeException: Term Index test failed\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:502)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5\n38)\n    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)\n    [junit]\n    [junit] WARNING: 1 broken segments (containing 157 documents) detected\n    [junit]\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testUpdatesOnDiskFull -Dtests.s\need=-8128829179004133416:-5617162412680232634\n    [junit] NOTE: test params are: codec=MockFixedIntBlock(blockSize=266), locale=tr_TR, timezone=Africa/Maseru\n    [junit] ------------- ---------------- ---------------\n    [junit] TEST org.apache.lucene.index.TestIndexWriterDelete FAILED\n    [junit] Testsuite: org.apache.lucene.index.TestLazyProxSkipping\n    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.034 sec\n    [junit]\n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestLazyProxSkipping -Dtestmethod=testLazySkipping -Dtests.seed=-7\n119541877291237950:-8499388603775233752\n    [junit] NOTE: test params are: codec=SimpleText, locale=da_DK, timezone=Pacific/Guam\n    [junit] ------------- ---------------- ---------------\n{noformat}",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-292",
        "summary": "[PATCH] Comment corrections in MMapDirectory.java",
        "description": "These comments ended up on the wrong lines after the last changes",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-995",
        "summary": "Add open ended range query syntax to QueryParser",
        "description": "The QueryParser fails to generate open ended range queries.\nParsing e.g. \"date:[1990 TO *]\"  gives zero results,\nbut\nConstantRangeQuery(\"date\",\"1990\",null,true,true)\ndoes produce the expected results.\n\n\"date:[* TO 1990]\" gives the same results as ConstantRangeQuery(\"date\",null,\"1990\",true,true).",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3867",
        "summary": "RamUsageEstimator.NUM_BYTES_ARRAY_HEADER and other constants are incorrect",
        "description": "RamUsageEstimator.NUM_BYTES_ARRAY_HEADER is computed like that: NUM_BYTES_OBJECT_HEADER + NUM_BYTES_INT + NUM_BYTES_OBJECT_REF. The NUM_BYTES_OBJECT_REF part should not be included, at least not according to this page: http://www.javamex.com/tutorials/memory/array_memory_usage.shtml\n\n{quote}\nA single-dimension array is a single object. As expected, the array has the usual object header. However, this object head is 12 bytes to accommodate a four-byte array length. Then comes the actual array data which, as you might expect, consists of the number of elements multiplied by the number of bytes required for one element, depending on its type. The memory usage for one element is 4 bytes for an object reference ...\n{quote}\n\nWhile on it, I wrote a sizeOf(String) impl, and I wonder how do people feel about including such helper methods in RUE, as static, stateless, methods? It's not perfect, there's some room for improvement I'm sure, here it is:\n\n{code}\n\t/**\n\t * Computes the approximate size of a String object. Note that if this object\n\t * is also referenced by another object, you should add\n\t * {@link RamUsageEstimator#NUM_BYTES_OBJECT_REF} to the result of this\n\t * method.\n\t */\n\tpublic static int sizeOf(String str) {\n\t\treturn 2 * str.length() + 6 // chars + additional safeness for arrays alignment\n\t\t\t\t+ 3 * RamUsageEstimator.NUM_BYTES_INT // String maintains 3 integers\n\t\t\t\t+ RamUsageEstimator.NUM_BYTES_ARRAY_HEADER // char[] array\n\t\t\t\t+ RamUsageEstimator.NUM_BYTES_OBJECT_HEADER; // String object\n\t}\n{code}\n\nIf people are not against it, I'd like to also add sizeOf(int[] / byte[] / long[] / double[] ... and String[]).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3260",
        "summary": "need a test that uses termsenum.seekExact() (which returns true), then calls next()",
        "description": "i tried to do some seekExact (where the result must exist) then next()ing in the faceting module,\nand it seems like there could be a bug here.\n\nI think we should add a test that mixes seekExact/seekCeil/next like this, to ensure that\nif seekExact returns true, that the enum is properly positioned.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1183",
        "summary": "TRStringDistance uses way too much memory (with patch)",
        "description": "The implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory). Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information.\n\nThe commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). I have reported the new implementation to TRStringDistance.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2302",
        "summary": "Replacement for TermAttribute+Impl with extended capabilities (byte[] support, CharSequence, Appendable)",
        "description": "For flexible indexing terms can be simple byte[] arrays, while the current TermAttribute only supports char[]. This is fine for plain text, but e.g NumericTokenStream should directly work on the byte[] array.\nAlso TermAttribute lacks of some interfaces that would make it simplier for users to work with them: Appendable and CharSequence\n\nI propose to create a new interface \"CharTermAttribute\" with a clean new API that concentrates on CharSequence and Appendable.\nThe implementation class will simply support the old and new interface working on the same term buffer. DEFAULT_ATTRIBUTE_FACTORY will take care of this. So if somebody adds a TermAttribute, he will get an implementation class that can be also used as CharTermAttribute. As both attributes create the same impl instance both calls to addAttribute are equal. So a TokenFilter that adds CharTermAttribute to the source will work with the same instance as the Tokenizer that requested the (deprecated) TermAttribute.\n\nTo also support byte[] only terms like Collation or NumericField needs, a separate getter-only interface will be added, that returns a reusable BytesRef, e.g. BytesRefGetterAttribute. The default implementation class will also support this interface. For backwards compatibility with old self-made-TermAttribute implementations, the indexer will check with hasAttribute(), if the BytesRef getter interface is there and if not will wrap a old-style TermAttribute (a deprecated wrapper class will be provided): new BytesRefGetterAttributeWrapper(TermAttribute), that is used by the indexer then.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-641",
        "summary": "maxFieldLength actual limit is 1 greater than expected value.",
        "description": "\n// Prepare document.\nDocument document = new Document();\ndocument.add(new Field(\"name\",\n            \"pattern oriented software architecture\", Store.NO,\n            Index.TOKENIZED, TermVector.WITH_POSITIONS_OFFSETS));\n\n// Set max field length to 2.\nindexWriter.setMaxFieldLength(2);\n\n// Add document into index.\nindexWriter.addDocument(document, new StandardAnalyzer());\n\n// Create a query.\nQueryParser queryParser = new QueryParser(\"name\", new StandardAnalyzer());\nQuery query = queryParser.parse(\"software\");\n\n// Search the 3rd term.\nHits hits = indexSearcher.search(query);\n\nAssert.assertEquals(0, hits.length());\n// failed. Actual hits.length() == 1, but expect 0.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2721",
        "summary": "Random Failure TestSizeBoundedOptimize#testFirstSegmentTooLarge",
        "description": "I am seeing this on trunk  \n\n{noformat}\n\n[junit] Testsuite: org.apache.lucene.index.TestSizeBoundedOptimize\n    [junit] Testcase: testFirstSegmentTooLarge(org.apache.lucene.index.TestSizeBoundedOptimize):\tFAILED\n    [junit] expected:<2> but was:<1>\n    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<1>\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:882)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:848)\n    [junit] \tat org.apache.lucene.index.TestSizeBoundedOptimize.testFirstSegmentTooLarge(TestSizeBoundedOptimize.java:160)\n    [junit] \n    [junit] \n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.658 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3\n    [junit] NOTE: test params are: codec=Standard, locale=sv_SE, timezone=Mexico/BajaNorte\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestSizeBoundedOptimize]\n    [junit] ------------- ---------------- ---------------\n    [junit] Test org.apache.lucene.index.TestSizeBoundedOptimize FAILED\n{noformat}\n\nwhen running with this seed\nant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1164",
        "summary": "Improve how ConcurrentMergeScheduler handles too-many-merges case",
        "description": "CMS now lets you set \"maxMergeThreads\" to control max # simultaneous\nmerges.\n\nHowever, when CMS hits that max, it still allows further merges to\nrun, by running them in the foreground thread.  So if you set this max\nto 1, and use 1 thread to add docs, you can get 2 merges running at\nonce (which I think is broken).\n\nI think, instead, CMS should pause the foreground thread, waiting\nuntil the number of merge threads drops below the limit.  Then, kick\noff the backlog merge in a thread and return control back to primary\nthread.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1826",
        "summary": "All Tokenizer implementations should have constructors that take AttributeSource and AttributeFactory",
        "description": "I have a TokenStream implementation that joins together multiple sub TokenStreams (i then do additional filtering on top of this, so i can't just have the indexer do the merging)\n\nin 2.4, this worked fine.\nonce one sub stream was exhausted, i just started using the next stream \n\nhowever, in 2.9, this is very difficult, and requires copying Term buffers for every token being aggregated\n\nhowever, if all the sub TokenStreams share the same AttributeSource, and my \"concat\" TokenStream shares the same AttributeSource, this goes back to being very simple (and very efficient)\n\n\nSo for example, i would like to see the following constructor added to StandardTokenizer:\n{code}\n  public StandardTokenizer(AttributeSource source, Reader input, boolean replaceInvalidAcronym) {\n    super(source);\n    ...\n  }\n{code}\n\nwould likewise want similar constructors added to all Tokenizer sub classes provided by lucene\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3084",
        "summary": "MergePolicy.OneMerge.segments should be List<SegmentInfo> not SegmentInfos, Remove Vector<SI> subclassing from SegmentInfos & more refactoring",
        "description": "SegmentInfos carries a bunch of fields beyond the list of SI, but for merging purposes these fields are unused.\n\nWe should cutover to List<SI> instead.\n\nAlso SegmentInfos subclasses Vector<SI>, this should be removed and the collections be hidden inside the class. We can add unmodifiable views on it (asList(), asSet()).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-279",
        "summary": "[PATCH] Javadoc improvements and minor fixes",
        "description": "Javadoc improvements for Scorer.java and Weight.java. \nThis also fixes some recent changes introduced minor warnings when building \nthe javadocs and adds a small comment in Similarity.java. \nThe individual patches will be attached.",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3529",
        "summary": "creating empty field + empty term leads to invalid index",
        "description": "Spinoff from LUCENE-3526.\n\n* if you create new Field(\"\", \"\"), you get IllegalArgumentException from Field's ctor: \"name and value cannot both be empty\"\n* But there are tons of other ways to index an empty term for the empty field (for example initially make it \"garbage\" then .setValue(\"\"), or via tokenstream).\n* If you do this, and you have assertions enabled, you will trip an assert (the assert is fixed in trunk, in LUCENE-3526)\n* But If you don't have assertions enabled, you will create a corrupt index: test: terms, freq, prox...ERROR [term : docFreq=1 != num docs seen 0 + num docs deleted 0]\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3309",
        "summary": "Add narrow API for loading stored fields, to replace FieldSelector",
        "description": "I think we should \"invert\" the FieldSelector API, with a \"push\" API\nwhereby FieldsReader invokes this API once per field in the document\nbeing visited.\n\nImplementations of the API can then do arbitrary things like save away\nthe field's size, load the field, clone the IndexInput for later lazy\nloading, etc.\n\nThis very thin API would be a mirror image of the very thin index time\nAPI we now have (IndexableField) and, importantly, it would have no\ndependence on our \"user space\" Document/Field/FieldType impl, so apps\nare free to do something totally custom.\n\nAfter we have this, we should build the \"sugar\" API that rebuilds a\nDocument instance (ie IR.document(int docID)) on top of this new thin\nAPI.  This'll also be a good test that the API is sufficient.\n\nRelevant discussions from IRC this morning at\nhttp://colabti.org/irclogger/irclogger_log/lucene-dev?date=2011-07-13#l76\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1759",
        "summary": "Implement TokenStream.end() in contrib TokenStreams",
        "description": "See LUCENE-1448. Mike's patch there already contains the necessary fixes.\n\nI'll attach a patch here as soon as LUCENE-1460 is committed.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-398",
        "summary": "ParallelReader crashes when trying to merge into a new index",
        "description": "ParallelReader causes a NullPointerException in\norg.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)\nwhen trying to merge into a new index.\n\nSee test case and sample output:\n\n$ svn diff\nIndex: src/test/org/apache/lucene/index/TestParallelReader.java\n===================================================================\n--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 179785)\n+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)\n@@ -57,6 +57,13 @@\n \n   }\n  \n+  public void testMerge() throws Exception {\n+    Directory dir = new RAMDirectory();\n+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(), true);\n+    w.addIndexes(new IndexReader[] { ((IndexSearcher)\nparallel).getIndexReader() });\n+    w.close();\n+  }\n+\n   private void queryTest(Query query) throws IOException {\n     Hits parallelHits = parallel.search(query);\n     Hits singleHits = single.search(query);\n$ ant -Dtestcase=TestParallelReader test\nBuildfile: build.xml\n[...]\ntest:\n    [mkdir] Created dir:\n/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/build/test\n    [junit] Testsuite: org.apache.lucene.index.TestParallelReader\n    [junit] Tests run: 2, Failures: 0, Errors: 1, Time elapsed: 1.993 sec\n\n    [junit] Testcase: testMerge(org.apache.lucene.index.TestParallelReader):  \nCaused an ERROR\n    [junit] null\n    [junit] java.lang.NullPointerException\n    [junit]     at\norg.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)\n    [junit]     at\norg.apache.lucene.index.ParallelReader$ParallelTermDocs.seek(ParallelReader.java:294)\n    [junit]     at\norg.apache.lucene.index.SegmentMerger.appendPostings(SegmentMerger.java:325)\n    [junit]     at\norg.apache.lucene.index.SegmentMerger.mergeTermInfo(SegmentMerger.java:296)\n    [junit]     at\norg.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentMerger.java:270)\n    [junit]     at\norg.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:234)\n    [junit]     at\norg.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:96)\n    [junit]     at\norg.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:596)\n    [junit]     at\norg.apache.lucene.index.TestParallelReader.testMerge(TestParallelReader.java:63)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit]     at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit]     at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\n\n    [junit] Test org.apache.lucene.index.TestParallelReader FAILED\n\nBUILD FAILED\n/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/common-build.xml:188:\nTests failed!\n\nTotal time: 16 seconds\n$",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1376",
        "summary": "sometimes if a BG merge hits an exception, optimize() will fail to forward the exception",
        "description": "I was seeing an intermittant failure, only on a Windows instance running inside VMWare, of TestIndexWriter.testAddIndexOnDiskFull.\n\nIt is happening because the while loop that checks for merge exceptions that had occurred during optimize fails to catch the case where all the BG optimize merges completed (or hit exceptions) before the while loop begins.  IE, all BG threads finished before the FG thread advanced to the while loop.  In that case the code fails to check if there were any exceptions.\n\nThe fix is straightforward: change the while loop so that it always checks, at least once, whether there were exceptions.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3551",
        "summary": "Yet another race in IW#nrtIsCurrent",
        "description": "In IW#nrtIsCurrent looks like this:\n\n{code}\n  synchronized boolean nrtIsCurrent(SegmentInfos infos) {\n    ensureOpen();\n    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();\n  }\n{code}\n\n* the version changes once we checkpoint the IW\n* docWriter has changes if there are any docs in ram or any deletes in the delQueue\n* bufferedDeletes contain all frozen del packages from the delQueue\n\nyet, what happens is 1. we decrement the numDocsInRam in DWPT#doAfterFlush (which is executed during DWPT#flush) but before we checkpoint. 2. if we freeze deletes (empty the delQueue) we put them in the flushQueue to maintain the order.  This means they are not yet in the bufferedDeleteStream.\n\nBottom line, there is a window where we could see IW#nrtIsCurrent returning true if we check within this particular window. Phew, I am not 100% sure if that is the reason for our latest failure in SOLR-2861 but from what the logs look like this could be what happens. If we randomly hit low values for maxBufferedDocs & maxBufferedDeleteTerms this is absolutely possible.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2365",
        "summary": "Finding Newest Segment In Empty Index",
        "description": "While extending the index writer, I discovered that its newestSegment method does not check to see if there are any segments before accessing the segment infos vector. Specifically, if you call the IndexWriter#newestSegment method on a brand-new index which is essentially empty, then it throws an java.lang.ArrayIndexOutOfBoundsException exception.\n\nThe proposed fix is to return null if no segments exist, as shown below:\n\n--- lucene/src/java/org/apache/lucene/index/IndexWriter.java\t(revision 930788)\n+++ lucene/src/java/org/apache/lucene/index/IndexWriter.java\t(working copy)\n@@ -4587,7 +4587,7 @@\n \n   // utility routines for tests\n   SegmentInfo newestSegment() {\n-    return segmentInfos.info(segmentInfos.size()-1);\n+    return segmentInfos.size() > 0 ? segmentInfos.info(segmentInfos.size()-1) : null;\n   }\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1554",
        "summary": "Problem with IndexWriter.mergeFinish",
        "description": "I'm getting a (very) infrequent assert in IndexWriter.mergeFinish from TestIndexWriter.testAddIndexOnDiskFull. The problem occurs during the rollback when the merge hasn't been registered. I'm not 100% sure this is the correct fix, because it's such an infrequent event. \n\n{code:java}\n  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {\n    \n    // Optimize, addIndexes or finishMerges may be waiting\n    // on merges to finish.\n    notifyAll();\n\n    if (merge.increfDone)\n      decrefMergeSegments(merge);\n\n    assert merge.registerDone;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n    for(int i=0;i<end;i++)\n      mergingSegments.remove(sourceSegments.info(i));\n    mergingSegments.remove(merge.info);\n    merge.registerDone = false;\n  }\n{code}\n\nShould  be something like:\n\n{code:java}\n  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {\n    \n    // Optimize, addIndexes or finishMerges may be waiting\n    // on merges to finish.\n    notifyAll();\n\n    if (merge.increfDone)\n      decrefMergeSegments(merge);\n\n    if (merge.registerDone) {\n      final SegmentInfos sourceSegments = merge.segments;\n      final int end = sourceSegments.size();\n      for(int i=0;i<end;i++)\n        mergingSegments.remove(sourceSegments.info(i));\n      mergingSegments.remove(merge.info);\n      merge.registerDone = false;\n    }\n  }\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-676",
        "summary": "Promote solr's PrefixFilter into Java Lucene's core",
        "description": "Solr's PrefixFilter class is not specific to Solr and seems to be of interest to core lucene users (PyLucene in this case).\nPromoting it into the Lucene core would be helpful.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1462",
        "summary": "Instantiated/IndexWriter discrepanies",
        "description": " * RAMDirectory seems to do a reset on tokenStreams the first time, this permits to initialise some objects before starting streaming, InstantiatedIndex does not.\n * I can Serialize a RAMDirectory but I cannot on a InstantiatedIndex because of : java.io.NotSerializableException: org.apache.lucene.index.TermVectorOffsetInfo\n\nhttp://www.nabble.com/InstatiatedIndex-questions-to20576722.html\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2360",
        "summary": "speedup recycling of per-doc RAM",
        "description": "Robert found one source of slowness when indexing tiny docs, where we use List.toArray to recycle the byte[] buffers used by per-doc doc store state (stored field, term vectors).  This was added in LUCENE-2283, so not yet released.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2594",
        "summary": "cutover oal.index.* tests to use a random IWC to tease out bugs",
        "description": "",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2824",
        "summary": "optimizations for bufferedindexinput",
        "description": "along the same lines as LUCENE-2816:\n* the readVInt/readVLong/readShort/readInt/readLong are not optimal here since they defer to readByte. for example this means checking the buffer's bounds per-byte in readVint instead of per-vint.\n* its an easy win to speed this up, even for the vint case: its essentially always faster, the only slower case is 1024 single-byte vints in a row, in this case we would do a single extra bounds check (1025 instead of 1024)\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1379",
        "summary": "SpanScorer fails when sloppyFreq() returns 0",
        "description": "I think we should fix this for 2.4 (now back to 10)?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1860",
        "summary": "switch MultiTermQuery to \"constant score auto\" rewrite by default",
        "description": "Right now it defaults to scoring BooleanQuery, and that's inconsistent w/ QueryParser which does constant score auto.\n\nThe new multi-term queries already set this default, so the only core queries this will impact are PrefixQuery and WildcardQuery.  FuzzyQuery, which has its own rewrite to BooleanQuery, will keep doing so.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-773",
        "summary": "Deprecate \"create\" method in FSDirectory.getDirectory in favor of IndexWriter's \"create\"",
        "description": "It's confusing that there is a create=true|false at the FSDirectory\nlevel and then also another create=true|false at the IndexWriter\nlevel.  Which one should you use when creating an index?\n\nOur users have been confused by this in the past:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/4792\n\nI think in general we should try to have one obvious way to achieve\nsomething (like Python: http://en.wikipedia.org/wiki/Python_philosophy).\n\nAnd the fact that there are now two code paths that are supposed to do\nthe same (similar?) thing, can more easily lead to sneaky bugs.  One\ncase of LUCENE-140 (already fixed in trunk but not past releases),\nwhich inspired this issue, can happen if you send create=false to the\nFSDirectory and create=true to the IndexWriter.\n\nFinally, as of lockless commits, it is now possible to open an\nexisting index for \"create\" while readers are still using the old\n\"point in time\" index, on Windows.  (At least one user had tried this\npreviously and failed).  To do this, we use the IndexFileDeleter class\n(which retries on failure) and we also look at the segments file to\ndetermine the next segments_N file to write to.\n\nWith future issues like LUCENE-710 even more \"smarts\" may be required\nto know what it takes to \"create\" a new index into an existing\ndirectory.  Given that we have have quite a few Directory\nimplemenations, I think these \"smarts\" logically should live in\nIndexWriter (not replicated in each Directory implementation), and we\nshould leave the Directory as an interface that knows how to make\nchanges to some backing store but does not itself try to make any\nchanges.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1112",
        "summary": "Document is partially indexed on an unhandled exception",
        "description": "With LUCENE-843, it's now possible for a subset of a document's\nfields/terms to be indexed or stored when an exception is hit.  This\nwas not the case in the past (it was \"all or none\").\n\nI plan to make it \"all or none\" again by immediately marking a\ndocument as deleted if any exception is hit while indexing it.\n\nDiscussion leading up to this:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/56103\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2704",
        "summary": "TestIndexWriter.testOptimizeTempSpaceUsage fails w/ SimpleText codec",
        "description": "{noformat}\n   [junit] Testsuite: org.apache.lucene.index.TestIndexWriter\n   [junit] Testcase: testOptimizeTempSpaceUsage(org.apache.lucene.index.TestIndexWriter):      FAILED\n   [junit] optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)\n   [junit] junit.framework.AssertionFailedError: optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)\n   [junit]     at org.apache.lucene.index.TestIndexWriter.testOptimizeTempSpaceUsage(TestIndexWriter.java:662)\n   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n   [junit]\n   [junit]\n   [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 5.284 sec\n   [junit]\n   [junit] ------------- Standard Output ---------------\n   [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testOptimizeTempSpaceUsage -Dtests.seed=-3299990090561349208:2824386407253661541\n   [junit] NOTE: test params are: codec=SimpleText, locale=el_GR, timezone=Africa/Dar_es_Salaam\n{noformat}\n\nIt's not just SimpleText (because -Dtests.codec=SimpleText, alone, sometimes passes)... there must be something else about the RIWC settings.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2084",
        "summary": "remove Byte/CharBuffer wrapping for collation key generation",
        "description": "We can remove the overhead of ByteBuffer and CharBuffer wrapping in CollationKeyFilter and ICUCollationKeyFilter.\n\nthis patch moves the logic in IndexableBinaryStringTools into char[],int,int and byte[],int,int based methods, with the previous Byte/CharBuffer methods delegating to these.\nPreviously, the Byte/CharBuffer methods required a backing array anyway.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1706",
        "summary": "Site search powered by Lucene/Solr",
        "description": "For a number of years now, the Lucene community has been criticized for not eating our own \"dog food\" when it comes to search. My company has built and hosts a site search (http://www.lucidimagination.com/search) that is powered by Apache Solr and Lucene and we'd like to donate it's use to the Lucene community. Additionally, it allows one to search all of the Lucene content from a single place, including web, wiki, JIRA and mail archives. See also http://www.lucidimagination.com/search/document/bf22a570bf9385c7/search_on_lucene_apache_org\n\nYou can see it live on Mahout, Tika and Solr\n\nLucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. We are committed to maintaining and expanding the search capabilities on the site.\n\nThe following patch adds a skin to the Forrest site that enables the Lucene site to search Lucene only content using Lucene/Solr. When a search is submitted, it automatically selects the Lucene facet such that only Lucene content is searched. From there, users can then narrow/broaden their search criteria.\n\n\nI plan on committing in a 3 or 4 days.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3299",
        "summary": "refactoring of Similarity.sloppyFreq() and Similarity.scorePayload",
        "description": "Currently these are top-level, but they only affect the SloppyDocScorer.\nSo it makes more sense to put these into the SloppyDocScorer api, this gives you additional flexibility\n(e.g. combining payloads with CSF or whatever the hell you want to do), and is cleaner.\n\nFurthermore, there are the following confusing existing issues:\n* scorePayload should take bytesref\n* PayloadTermScorer passes a *null* byte[] array to the sim if there are no payloads. I don't think it should do this, and its inconsistent with PayloadNearQuery, which does not do this. Its an undocumented conditional you need to have in the scoring algorithm which we should remove.\n* there is an unused constant for scorepayload (NO_DOC_ID_PROVIDED), which is a documented, but never used anywhere. I think we should remove this conditional too, because its not possible to have a payload without a docid, and we shouldn't be passing fake document ids (-1) to our scoring APIs anyway.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2497",
        "summary": "Revision 949509 (LUCENE-2480) causes IOE \"read past EOF\" when processing older format SegmentInfo data when JVM assertion processing is disabled.",
        "description": "At revision 949509 in org.apache.lucene.index.SegmentInfo at line 155, there is the following code:\n{noformat} \n    if (format > SegmentInfos.FORMAT_4_0) {\n      // pre-4.0 indexes write a byte if there is a single norms file\n      assert 1 == input.readByte();\n    }\n{noformat} \nNote that the assert statement invokes input.readByte().\nIf asserts are disabled for the JVM, input.readByte() will not be invoked, causing the following readInt() to return a bogus value, and then causing an IOE during the (mistakenly entered) loop at line 165.\nThis can occur when processing old format (format \"-9\") index data under Tomcat (whose startup scripts by default do not turn on asserts).\n\nFull stacktrace:\n{noformat} \nSEVERE: java.lang.RuntimeException: java.io.IOException: read past EOF\n\tat org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1066)\n\tat org.apache.solr.core.SolrCore.<init>(SolrCore.java:581)\n\tat org.apache.solr.core.CoreContainer.create(CoreContainer.java:431)\n\tat org.apache.solr.core.CoreContainer.load(CoreContainer.java:286)\n\tat org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:125)\n\tat org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:86)\n\tat org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:275)\n\tat org.apache.catalina.core.ApplicationFilterConfig.setFilterDef(ApplicationFilterConfig.java:397)\n\tat org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:108)\n\tat org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:3800)\n\tat org.apache.catalina.core.StandardContext.start(StandardContext.java:4450)\n\tat org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)\n\tat org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)\n\tat org.apache.catalina.core.StandardHost.addChild(StandardHost.java:526)\n\tat org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:850)\n\tat org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:724)\n\tat org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:493)\n\tat org.apache.catalina.startup.HostConfig.start(HostConfig.java:1206)\n\tat org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:314)\n\tat org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)\n\tat org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1053)\n\tat org.apache.catalina.core.StandardHost.start(StandardHost.java:722)\n\tat org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1045)\n\tat org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443)\n\tat org.apache.catalina.core.StandardService.start(StandardService.java:516)\n\tat org.apache.catalina.core.StandardServer.start(StandardServer.java:710)\n\tat org.apache.catalina.startup.Catalina.start(Catalina.java:583)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:288)\n\tat org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:413)\nCaused by: java.io.IOException: read past EOF\n\tat org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)\n\tat org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)\n\tat org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)\n\tat org.apache.lucene.store.DataInput.readInt(DataInput.java:76)\n\tat org.apache.lucene.store.DataInput.readLong(DataInput.java:99)\n\tat org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:165)\n\tat org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)\n\tat org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:91)\n\tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)\n\tat org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:87)\n\tat org.apache.lucene.index.IndexReader.open(IndexReader.java:415)\n\tat org.apache.lucene.index.IndexReader.open(IndexReader.java:294)\n\tat org.apache.solr.core.StandardIndexReaderFactory.newReader(StandardIndexReaderFactory.java:38)\n\tat org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1055)\n\t... 32 more\n{noformat} ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3141",
        "summary": "FastVectorHighlighter - expose FieldFragList.fragInfo for user-customizable FragmentsBuilder",
        "description": "Needed to build a custom highlightable snippet - snippet should start with the sentence containing the first match, then continue for 250 characters.\n\nSo created a custom FragmentsBuilder extending SimpleFragmentsBuilder and overriding the createFragments(IndexReader reader, int docId, String fieldName, FieldFragList fieldFragList) method - unit test containing the code is attached to the JIRA.\n\nTo get this to work, needed to expose (make public) the FieldFragList.fragInfo member variable. This is currently package private, so only FragmentsBuilder implementations within the lucene-highlighter o.a.l.s.vectorhighlight package (such as SimpleFragmentsBuilder) can access it. Since I am just using the lucene-highlighter.jar as an external dependency to my application, the simplest way to access FieldFragList.fragInfo in my class was to make it public.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1151",
        "summary": "Fix StandardAnalyzer to not mis-identify HOST as ACRONYM by default",
        "description": "Coming out of the discussion around back compatibility, it seems best to default StandardAnalyzer to properly fix LUCENE-1068, while preserving the ability to get the back-compatible behavior in the rare event that it's desired.\n\nThis just means changing the replaceInvalidAcronym = false with = true, and, adding a clear entry to CHANGES.txt that this very slight non back compatible change took place.\n\nSpinoff from here:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/57517#57517\n\nI'll commit that change in a day or two.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1591",
        "summary": "Enable bzip compression in benchmark",
        "description": "bzip compression can aid the benchmark package by not requiring extracting bzip files (such as enwiki) in order to index them. The plan is to add a config parameter bzip.compression=true/false and in the relevant tasks either decompress the input file or compress the output file using the bzip streams.\nIt will add a dependency on ant.jar which contains two classes similar to GZIPOutputStream and GZIPInputStream which compress/decompress files using the bzip algorithm.\n\nbzip is known to be superior in its compression performance to the gzip algorithm (~20% better compression), although it does the compression/decompression a bit slower.\n\nI wil post a patch which adds this parameter and implement it in LineDocMaker, EnwikiDocMaker and WriteLineDoc task. Maybe even add the capability to DocMaker or some of the super classes, so it can be inherited by all sub-classes.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2035",
        "summary": "TokenSources.getTokenStream() does not assign positionIncrement",
        "description": "TokenSources.StoredTokenStream does not assign positionIncrement information. This means that all tokens in the stream are considered adjacent. This has implications for the phrase highlighting in QueryScorer when using non-contiguous tokens.\n\nFor example:\nConsider  a token stream that creates tokens for both the stemmed and unstemmed version of each word - the fox (jump|jumped)\nWhen retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - the fox jump jumped\n\nNow try a search and highlight for the phrase query \"fox jumped\". The search will correctly find the document; the highlighter will fail to highlight the phrase because it thinks that there is an additional word between \"fox\" and \"jumped\". If we use the original (from the analyzer) token stream then the highlighter works.\n\nAlso, consider the converse - the fox did not jump\n\"not\" is a stop word and there is an option to increment the position to account for stop words - (the,0) (fox,1) (did,2) (jump,4)\nWhen retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - (the,0) (fox,1) (did,2) (jump,3).\n\nSo the phrase query \"did jump\" will cause the \"did\" and \"jump\" terms in the text \"did not jump\" to be highlighted. If we use the original (from the analyzer) token stream then the highlighter works correctly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-759",
        "summary": "Add n-gram tokenizers to contrib/analyzers",
        "description": "It would be nice to have some n-gram-capable tokenizers in contrib/analyzers.  Patch coming shortly.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3718",
        "summary": "SamplingWrapperTest failure with certain test seed",
        "description": "Build: https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12231/\n\n1 tests failed.\nREGRESSION:  org.apache.lucene.facet.search.SamplingWrapperTest.testCountUsingSamping\n\nError Message:\nResults are not the same!\n\nStack Trace:\norg.apache.lucene.facet.FacetTestBase$NotSameResultError: Results are not the same!\n       at org.apache.lucene.facet.FacetTestBase.assertSameResults(FacetTestBase.java:333)\n       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.assertSampling(BaseSampleTestTopK.java:104)\n       at org.apache.lucene.facet.search.sampling.BaseSampleTestTopK.testCountUsingSamping(BaseSampleTestTopK.java:82)\n       at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)\n       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n\nNOTE: reproduce with: ant test -Dtestcase=SamplingWrapperTest -Dtestmethod=testCountUsingSamping -Dtests.seed=4a5994491f79fc80:-18509d134c89c159:-34f6ecbb32e930f7 -Dtests.multiplier=3 -Dargs=\"-Dfile.encoding=UTF-8\"\nNOTE: test params are: codec=Lucene40: {$facets=PostingsFormat(name=MockRandom), $full_path$=PostingsFormat(name=MockSep), content=Pulsing40(freqCutoff=19 minBlockSize=65 maxBlockSize=209), $payloads$=PostingsFormat(name=Lucene40WithOrds)}, sim=RandomSimilarityProvider(queryNorm=true,coord=true): {$facets=LM Jelinek-Mercer(0.700000), content=DFR I(n)B3(800.0)}, locale=bg, timezone=Asia/Manila\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3879",
        "summary": "fix more position corrumptions in 4.0 codecs",
        "description": "Spinoff of LUCENE-3876.\n\nSome codecs have invalid asserts, wrong shift operators etc.\n\nIf a position exceeds Integer.MAX_VALUE/2 and then also has a payload,\nit will produce corrumpt indexes or other strange errors.\n\nEasiest way to trigger the bugs is to sometimes add a payload to the test from LUCENE-3876.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1625",
        "summary": "openReaderPassed not populated in CheckIndex.Status.SegmentInfoStatus",
        "description": "When using CheckIndex programatically, the openReaderPassed flag on the SegmentInfoStatus is never populated (so it always comes back false)\n\nlooking at the code, its clear that openReaderPassed is defined, but never used\n\nfurthermore, it appears that not all information that is propagated to the \"InfoStream\" is available via SegmentIinfoStatus\n\nAll of the following information should be able to be gather from public properties on the SegmentInfoStatus:\ntest: open reader.........OK\ntest: fields, norms.......OK [2 fields]\ntest: terms, freq, prox...OK [101 terms; 133 terms/docs pairs; 133 tokens]\ntest: stored fields.......OK [100 total field count; avg 1 fields per doc]\ntest: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-865",
        "summary": "SpellChecker not working because of stale IndexSearcher",
        "description": "The SpellChecker unit test did not work, because of a stale IndexReader and IndexSearcher instance after calling indexDictionary(Dictionary).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2274",
        "summary": "Catch exceptions in Threads created by JUnit tasks",
        "description": "On hudson we had several assertions failed in TestRAMDirectory, that were never caught by the error reportier in JUnit (as the test itsself did not fail). This patch adds a handler for uncaught exceptions to LuceneTestCase(J4) that let the test fail in tearDown().",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2783",
        "summary": "Deadlock in IndexWriter",
        "description": "If autoCommit == true a merge usually triggers a commit. A commit (prepareCommit) can trigger a merge vi the flush method. There is a synchronization mechanism for commit (commitLock) and a separate synchronization mechanism for merging (ConcurrentMergeScheduler.wait). If one thread holds the commitLock monitor and another one holds the ConcurrentMergeScheduler monitor we have a deadlock.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2166",
        "summary": "If you hit the \"max term prefix\" warning when indexing, it never goes away",
        "description": "Silly bug.\n\nIf IW's infoStream is on, we warn whenever we hit a ridiculously long term (> 16 KB in length).  The problem is, we never reset this warning, so, once one doc contains such a massive term, we then keep warning over and over about that same term for future docs.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1312",
        "summary": "InstantiatedIndexReader does not implement getFieldNames properly",
        "description": "Causes error in org.apache.lucene.index.SegmentMerger.mergeFields",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2731",
        "summary": "HyphenationCompoundWordTokenFilter fails to load DTD in Crimson parser (JDK 1.4)",
        "description": "HyphenationCompoundWordTokenFilter loads the DTD in its XML parser from memory by supplying EntityResolver. In Java 1.4 (affects Lucene 2.9, but also later versions if not Apache Xerces is used as XML parser) this does not work, because Cromson does not even ask the entity resolver, if no base URI is known. As the hyphenation file is loaded from Reader/InputStream no base URI is known. Crimson needs at least a non-null systemId to proceed.\n\nThis patch (Lucene 2.9 only)  fakes this by supplying a fake systemId to the InputSource.",
        "label": "NUG",
        "classified": "SPEC",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2569",
        "summary": "TestParallelTermEnum fails with Sep codec",
        "description": "reproduceable in the 'preflexfixes' branch (since we test all codecs there) with: ant test-core -Dtestcase=TestParallelTermEnum -Dtests.codec=Sep\n\nBut I think there are probably more tests like this that have only been run with Standard and we might find more like this.\nI don't think this should block LUCENE-2554.\n\n{noformat}\n    [junit] Testsuite: org.apache.lucene.index.TestParallelTermEnum\n    [junit] Testcase: test1(org.apache.lucene.index.TestParallelTermEnum):      Caused an ERROR\n    [junit] read past EOF\n    [junit] java.io.IOException: read past EOF\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)\n    [junit]     at org.apache.lucene.store.DataInput.readVInt(DataInput.java:86)\n    [junit]     at org.apache.lucene.index.codecs.sep.SingleIntIndexInput$Reader.next(SingleIntIndexInput.java:64)\n    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsEnum.nextDoc(SepPostingsReaderImpl.java:316)\n    [junit]     at org.apache.lucene.index.TestParallelTermEnum.test1(TestParallelTermEnum.java:188)\n    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:316)\n    [junit]\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.009 sec\n    [junit]\n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: random codec of testcase 'test1' was: Sep\n    [junit] ------------- ---------------- ---------------\n{noformat}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1064",
        "summary": "Make TopDocs constructor public",
        "description": "TopDocs constructor is package visible. This prevents instantiating it from outside this package. For example, I wrote a HitColletor that couldn't extend directly from TopDocCollector. I need to create a new TopDocs instance, however since the c'tor is package visible, I can't do that.\nFor now, I completely duplicated the code, but I hope you'll fix it soon.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3185",
        "summary": "NRTCachingDirectory.deleteFile always throws exception",
        "description": "Silly bug.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1145",
        "summary": "DisjunctionSumScorer small tweak",
        "description": "Move ScorerDocQueue initialization from next() and skipTo() methods to the Constructor. Makes DisjunctionSumScorer a bit faster (less than 1% on my tests). \n\nDownside (if this is one, I cannot judge) would be throwing IOException from DisjunctionSumScorer constructors as we touch HardDisk there. I see no problem as this IOException does not propagate too far (the only modification I made is in BooleanScorer2)\n\nif (scorerDocQueue == null) {\n      initScorerDocQueue();\n}\n \n\nAttached test is just quick & dirty rip of  TestScorerPerf from standard Lucene test package. Not included as patch as I do not like it.\n\n\nAll test pass, patch made on trunk revision 613923\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3076",
        "summary": "add -Dtests.codecprovider",
        "description": "Currently to test a codec (or set of codecs) you have to add them to lucene's core and edit a couple of arrays here and there...\n\nIt would be nice if when using the test-framework you could instead specify a codecprovider by classname (possibly containing your own set of huper-duper codecs).\n\nFor example I made the following little codecprovider in contrib:\n{noformat}\npublic class AppendingCodecProvider extends CodecProvider {\n  public AppendingCodecProvider() {\n    register(new AppendingCodec());\n    register(new SimpleTextCodec());\n  }\n}\n{noformat}\n\nThen, I'm able to run tests with 'ant -lib build/contrib/misc/lucene-misc-4.0-SNAPSHOT.jar test-core -Dtests.codecprovider=org.apache.lucene.index.codecs.appending.AppendingCodecProvider', and it always picks from my set of  codecs (in this case Appending and SimpleText), and I can set -Dtests.codec=Appending if i want to set just one.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1125",
        "summary": "Excessive Arrays.fill(0) in DocumentsWriter drastically slows down small docs (3.9X slowdown!)",
        "description": "I've been doing some \"final\" performance testing of 2.3RC1 and\nuncovered a fairly serious bug that adds a large fixed CPU cost when\ndocuments have any term vector enabled fields.\n\nThe bug does not affect correctness, just performance.\n\nBasically, for every document, we were calling Arrays.fill(0) on a\nlarge (32 KB) byte array when in fact we only needed to zero a small\npart of it.  This only happens if term vectors are turned on, and is\nespecially devastating for small documents.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-346",
        "summary": "[PATCH] disable coord for generated BooleanQueries",
        "description": "Here's a patch that disables Similiarty.coord() in the scoring of most\nautomatically generated boolean queries.  The coord() score factor is\nappropriate when clauses are independently specified by a user, but is usually\nnot appropriate when clauses are generated automatically, e.g., by a fuzzy,\nwildcard or range query.  Matches on such automatically generated queries are\ncurrently penalized for not matching all terms.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1987",
        "summary": "Remove rest of analysis deprecations (Token, CharacterCache)",
        "description": "These removes the rest of the deprecations in the analysis package:\n- -Token's termText field-- (DONE)\n- -eventually un-deprecate ctors of Token taking Strings (they are still useful) -> if yes remove deprec in 2.9.1- (DONE)\n- -remove CharacterCache and use Character.valueOf() from Java5- (DONE)\n- Stopwords lists\n- Remove the backwards settings from analyzers (acronym, posIncr,...). They are deprecated, but we still have the VERSION constants. Do not know, how to proceed. Keep the settings alive for index compatibility? Or remove it together with the version constants (which were undeprecated).",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-910",
        "summary": "Add/change warning comments in the javadocs of Payload APIs",
        "description": "Since the payload API is still experimental we should change the comments\nin the javadocs similar to the new search/function package.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-288",
        "summary": "[patch] better support gcj compilation",
        "description": "In order to workaround http://gcc.gnu.org/bugzilla/show_bug.cgi?id=15411 the\nattached patch is necessary.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2802",
        "summary": "DirectoryReader ignores NRT SegmentInfos in #isOptimized()",
        "description": "DirectoryReader  only takes shared (with IW) SegmentInfos into account in DirectoryReader#isOptimized(). This can return true even if the actual realtime reader sees more than one segments. \n\n{code}\npublic boolean isOptimized() {\n    ensureOpen();\n   // if segmentsInfos changes in IW this can return false positive\n    return segmentInfos.size() == 1 && !hasDeletions();\n  }\n{code}\n\nDirectoryReader should check if this reader has a non-nul segmentInfosStart and use that instead",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2455",
        "summary": "Some house cleaning in addIndexes*",
        "description": "Today, the use of addIndexes and addIndexesNoOptimize is confusing - \nespecially on when to invoke each. Also, addIndexes calls optimize() in \nthe beginning, but only on the target index. It also includes the \nfollowing jdoc statement, which from how I understand the code, is \nwrong: _After this completes, the index is optimized._ -- optimize() is \ncalled in the beginning and not in the end. \n\nOn the other hand, addIndexesNoOptimize does not call optimize(), and \nrelies on the MergeScheduler and MergePolicy to handle the merges. \n\nAfter a short discussion about that on the list (Thanks Mike for the \nclarifications!) I understand that there are really two core differences \nbetween the two: \n* addIndexes supports IndexReader extensions\n* addIndexesNoOptimize performs better\n\nThis issue proposes the following:\n# Clear up the documentation of each, spelling out the pros/cons of \n  calling them clearly in the javadocs.\n# Rename addIndexesNoOptimize to addIndexes\n# Remove optimize() call from addIndexes(IndexReader...)\n# Document that clearly in both, w/ a recommendation to call optimize() \n  before on any of the Directories/Indexes if it's a concern. \n\nThat way, we maintain all the flexibility in the API - \naddIndexes(IndexReader...) allows for using IR extensions, \naddIndexes(Directory...) is considered more efficient, by allowing the \nmerges to happen concurrently (depending on MS) and also factors in the \nMP. So unless you have an IR extension, addDirectories is really the one \nyou should be using. And you have the freedom to call optimize() before \neach if you care about it, or don't if you don't care. Either way, \nincurring the cost of optimize() is entirely in the user's hands. \n\nBTW, addIndexes(IndexReader...) does not use neither the MergeScheduler \nnor MergePolicy, but rather call SegmentMerger directly. This might be \nanother place for improvement. I'll look into it, and if it's not too \ncomplicated, I may cover it by this issue as well. If you have any hints \nthat can give me a good head start on that, please don't be shy :). ",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1548",
        "summary": "LevenshteinDistance code normalization is incorrect",
        "description": "The normalization of the edit distance should use the maximum of the two string being compared instead of the minimum.  Otherwise negative distances are possible.  The spell checker filters out edits below a certain threshold so this hasn't been a problem in practice.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-589",
        "summary": "Demo HTML parser doesn't work for international documents",
        "description": "Javacc assumes ASCII so it won't work with, say, japanese documents. Ideally it would read the charset from the HTML markup, but that can by tricky. For now assuming unicode would do the trick:\n\nAdd the following line marked with a + to HTMLParser.jj:\n\noptions {\n  STATIC = false;\n  OPTIMIZE_TOKEN_MANAGER = true;\n  //DEBUG_LOOKAHEAD = true;\n  //DEBUG_TOKEN_MANAGER = true;\n+  UNICODE_INPUT = true;\n}\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-667",
        "summary": "javacc skeleton files not regenerated",
        "description": "Copies of the the character stream files for javacc are checked into svn. These files were generated under javacc 3.0 (at least that's what they say, though javacc 3.2 says this too). javacc 4 complains that they are out of date but won't replace them; they must be removed before it will regenerate them.\n\nThere is one side effect of removing them: local changes are lost.  r387550 removed a couple of deprecated methods. By using the files as generated by javacc, these deprecated  methods will be readded (at least until the javacc team removes them totally). There are other changes being made to the stream files, so I woudl think it's better to live with them unmodified than to keep local versions just for this change.\n\nIf we want javacc to recreate the files, the attached patch will remove them before running javacc.\n\nAll the tests pass using both javacc3.2 and 4.0.\n\n\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3513",
        "summary": "Add SimpleFragListBuilder constructor with margin parameter",
        "description": "{{SimpleFragListBuilder}} would benefit from an additional constructor that takes in {{margin}}. Currently, the margin is defined as a constant, so to \"implement\" a {{FragListBuilder}} with a different margin, one has no choice but to copy and paste {{SimpleFragListBuilder}} into a new class that must be placed in the {{org.apache.lucene.search.vectorhighlight}} package due to accesses of package-protected fields in other classes.\n\nIf this change were made, the precondition check of the constructor's {{fragCharSize}} should probably be altered to ensure that it's less than {{max(1, margin*3)}} to allow for a margin of 0.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2610",
        "summary": "addIndexes(Directory...) should not trigger merge on flush()",
        "description": "IndexWriter.addIndexes(Directory..) calls flush() w/ triggerMerge=true. This beats the purpose of the changes done to addIndexes to not merge any segments and leave it as the application's choice. The change is very simple - pass false instead of true. I don't plan to post a patch, however opened an issue in case some want to comment about it.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2254",
        "summary": "Support more queries (other than just title) in Trec quality pkg",
        "description": "Now that we can properly parse descriptions and narratives from TREC queries (LUCENE-2210), it would be nice to allow the user to easily run quality evaluations on more than just \"Title\"\n\nThis patch adds an optional commandline argument to QueryDriver (the default is Title as before), where you can specify something like:\nT: Title-only\nD: Description-only\nN: Narrative-only\nTD: Title + Description,\nTDN: Title+Description+Narrative,\nDN: Description+Narrative\n\nThe SimpleQQParser has an additional constructor that simply accepts a String[] of these fields, forming a booleanquery across all of them.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1712",
        "summary": "Set default precisionStep for NumericField and NumericRangeFilter",
        "description": "This is a spinoff from LUCENE-1701.\n\nA user using Numeric* should not need to understand what's\n\"under the hood\" in order to do their indexing & searching.\n\nThey should be able to simply:\n{code}\ndoc.add(new NumericField(\"price\", 15.50);\n{code}\n\nAnd have a decent default precisionStep selected for them.\n\nActually, if we add ctors to NumericField for each of the supported\ntypes (so the above code works), we can set the default per-type.  I\nthink we should do that?\n\n4 for int and 6 for long was proposed as good defaults.\n\nThe default need not be \"perfect\", as advanced users can always\noptimize their precisionStep, and for users experiencing slow\nRangeQuery performance, NumericRangeQuery with any of the defaults we\nare discussing will be much faster.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2410",
        "summary": "Optimize PhraseQuery",
        "description": "Looking the scorers for PhraseQuery, I think there are some speedups\nwe could do:\n\n  * The AND part of the scorer (which advances to the next doc that\n    has all the terms), in PhraseScorer.doNext, should do the same\n    optimizing as BooleanQuery's ConjunctionScorer, ie sort terms from\n    rarest to most frequent.  I don't think it should use a linked\n    list/firstToLast() that it does today.\n\n  * We do way too much work now when .score() is not called, because\n    we go and find all occurrences of the phrase in the doc, whereas\n    we should stop only after finding the first and then go and count\n    the rest if .score() is called.\n\n  * For the exact case, I think we can use two int arrays to find the\n    matches.  The first array holds the count of how many times a term\n    in the phrase \"matched\" a phrase starting at that position.  When\n    that count == the number of terms in the phrase, it's a match.\n    The 2nd is a \"gen\" array (holds docID when that count was last\n    touched), to avoid clearing.  Ie when incrementing the count, if\n    the docID != gen, we reset count to 0.  I think this'd be faster\n    than the PQ we now use.  Downside of this is if you have immense\n    docs (position gets very large) we'd need 2 immense arrays.\n\nIt'd be great to do LUCENE-1252 along with this, ie factor\nPhraseScorer into two AND'd sub-scorers (LUCENE-1252 is open for\nthis).  The first one should be ConjunctionScorer, and the 2nd one\nchecks the positions (ie, either the exact or sloppy scorers).  This\nwould mean if the PhraseQuery is AND'd w/ other clauses (or, a filter\nis applied) we would save CPU by not checking the positions for a doc\nunless all other AND'd clauses accepted the doc.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3841",
        "summary": "CloseableThreadLocal does not work well with Tomcat thread pooling",
        "description": "We tracked down a large memory leak (effectively a leak anyway) caused\nby how Analyzer users CloseableThreadLocal.\nCloseableThreadLocal.hardRefs holds references to Thread objects as\nkeys.  The problem is that it only frees these references in the set()\nmethod, and SnowballAnalyzer will only call set() when it is used by a\nNEW thread.\n\nThe problem scenario is as follows:\n\nThe server experiences a spike in usage (say by robots or whatever)\nand many threads are created and referenced by\nCloseableThreadLocal.hardRefs.  The server quiesces and lets many of\nthese threads expire normally.  Now we have a smaller, but adequate\nthread pool.  So CloseableThreadLocal.set() may not be called by\nSnowBallAnalyzer (via Analyzer) for a _long_ time.  The purge code is\nnever called, and these threads along with their thread local storage\n(lucene related or not) is never cleaned up.\n\nI think calling the purge code in both get() and set() would have\navoided this problem, but is potentially expensive.  Perhaps using \nWeakHashMap instead of HashMap may also have helped.  WeakHashMap \npurges on get() and set().  So this might be an efficient way to\nclean up threads in get(), while set() might do the more expensive\nMap.keySet() iteration.\n\nOur current work around is to not share SnowBallAnalyzer instances\namong HTTP searcher threads.  We open and close one on every request.\n\nThanks,\nMatt",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1713",
        "summary": "Rename RangeQuery -> TermRangeQuery",
        "description": "Since we now have NumericRangeQuery (LUCENE-1701) we should rename RangeQuery to TextRangeQuery to make it clear that TextRangeQuery (TermRangeQuery?  StringRangeQuery) is based entirely on text comparison.\n\nAnd, existing users on upgrading to 2.9 and using RangeQuery for [slow] numeric searching would realize they now have a good option for numeric range searching.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3219",
        "summary": "Change SortField types to an Enum",
        "description": "When updating my SOLR-2533 patch, one issue was that the int value I had given my new type had been used by another change in the mean time.  Since we don't use these fields in a bitset kind of way, we can convert them to an enum.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1331",
        "summary": "FSDirectory doesn't detect double-close nor usage after close",
        "description": "FSDirectory.close implements logic to ensure only a single instance of FSDirectory per canonical directory exists.  This means code that synchronizes on the FSDirectory instance is also synchronized against that canonical directory.  I think only IndexModifier (now deprecated) actually makes use of this, but I'm not certain. \n\nBut, the close() method doesn't detect double close, and doesn't catch usage after being closed, and so one can easily get two instances of FSDirectory for the same canonical directory.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2440",
        "summary": "Add support for custom ExecutorServices in ParallelMultiSearcher",
        "description": "Right now, the ParallelMultiSearcher uses a cachedThreadPool, which is limitless and a poor choice for a web application, given the threaded nature of the requests (say a webapp with tomcat-default 200 threads and 100 indexes could be looking at 2000 searching threads pretty easily).  Support for adding a custom ExecutorService is pretty trivial.  Patch forthcoming.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3898",
        "summary": "possible SynonymFilter bug: hudson fail",
        "description": "See https://builds.apache.org/job/Lucene-trunk/1867/consoleText (no seed)",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3719",
        "summary": "FVH: slow performance on very large queries",
        "description": "The change from HashSet to ArrayList for flatQueries in LUCENE-3019 resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3365",
        "summary": "Create or Append mode determined before obtaining write lock",
        "description": "If an IndexWriter(\"writer1\") is opened in CREATE_OR_APPEND mode, it determines whether to CREATE or APPEND before obtaining the write lock.  When another IndexWriter(\"writer2\") is in the process of creating the index, this can result in writer1 entering create mode and then waiting to obtain the lock.  When writer2 commits and releases the lock, writer1 is already in create mode and overwrites the index created by write2.\n\nThis bug was probably effected by LUCENE-2386 as prior to that Lucene generated an empty commit when a new index was created.  I think the issue could still have occurred prior to that but the two IndexWriters would have needed to be opened nearly simultaneously and the first IndexWriter would need to release the lock before the second timed out.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1048",
        "summary": "Lock.obtain(timeout) behaves incorrectly for large timeouts",
        "description": "Because timeout is a long, but internal values derived from timeout\nare ints, its possible to overflow those internal values into negative\nnumbers and cause incorrect behavior.\n\nSpinoff from this thread:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/54376\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-923",
        "summary": "Should SegmentTermPositionVector be public?",
        "description": "I'm wondering why SegmentTermPositionVector is public. It implements the public\ninterface TermPositionVector. Should we remove \"public\"?",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": ""
    },
    {
        "key": "LUCENE-1876",
        "summary": "Some contrib packages are missing a package.html",
        "description": "Dunno if we will get to this one this release, but a few contribs don't have a package.html (or a good overview that would work as a replacement) - I don't think this is hugely important, but I think it is important - you should be able to easily and quickly read a quick overview for each contrib I think.\n\nSo far I have identified collation and spatial.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-924",
        "summary": "IndexWriter has incomplete Javadocs",
        "description": "A couple of getter methods in IndexWriter have no javadocs.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-1835",
        "summary": "Signature changes in AttributeSource for better Generics support of AddAttribute/getAttribute",
        "description": "The last update of Attribute API using AttributeImpl as implementation oif Attributes changed the API a little bit. This change leads to the fact, that in Java 1.5 using generics we are no longer able to add Attributes without casting. addAttribute and getAttribute should return the Attribute interface because the implementation of the attribute is not interesting to the caller. By that in 1.5 using generics, one could add a TermAttribute without casting using:\n{code}\nTermAttribute termAtt = addAttribute(TermAttribute.class);\n{code}\nThe signature to do this is:\n{code}\npublic <T extends Attribute> T addAttribute(Class<T>)\n{code}\n\nThe attached patch applies the mentioned change to the signature (without generic, only returning Attribute). No other code changes are needed, as current code always casts the result to the requested interface. I also added the 1.5 method signature for all these methods to the javadocs.\n\nAll tests pass.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2600",
        "summary": "don't try to cache a composite reader's MultiBits deletedDocs",
        "description": "MultiFields.getDeletedDocs now builds up a MultiBits instance (so that one can check if a top-level docID is deleted), but it now stuffs it into a private cache on IndexReader.\n\nThis is invalid when the composite reader is read/write, and can result in a MultiReader falsely claiming a doc was not deleted.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3458",
        "summary": "Change BooleanFilter to have only a single clauses ArrayList (so toString() works fine, clauses() method could be added) so it behaves more lik BooleanQuery",
        "description": "This is unrelated to the other BF changes, but should be done",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2681",
        "summary": "fix generics violations in contrib/modules",
        "description": "There are some generics violations... we should fix them.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-418",
        "summary": "[PATCH] Contribution: A QueryParser which passes wildcard and prefix queries to analyzer",
        "description": "Lucenes built-in QueryParser class does not analyze prefix nor wildcard queries.\nAttached is a subclass which passes these queries to the analyzer as well.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1124",
        "summary": "short circuit FuzzyQuery.rewrite when input token length is small compared to minSimilarity",
        "description": "I found this (unreplied to) email floating around in my Lucene folder from during the holidays...\n\n{noformat}\nFrom: Timo Nentwig\nTo: java-dev\nSubject: Fuzzy makes no sense for short tokens\nDate: Mon, 31 Dec 2007 16:01:11 +0100\nMessage-Id: <200712311601.12255.lucene@nitwit.de>\n\nHi!\n\nit generally makes no sense to search fuzzy for short tokens because changing\neven only a single character of course already results in a high edit\ndistance. So it actually only makes sense in this case:\n\n           if( token.length() > 1f / (1f - minSimilarity) )\n\nE.g. changing one character in a 3-letter token (foo) results in an edit\ndistance of 0.6. And if minSimilarity (which is by default: 0.5 :-) is higher\nwe can save all the expensive rewrite() logic.\n{noformat}\n\nI don't know much about FuzzyQueries, but this reasoning seems sound ... FuzzyQuery.rewrite should be able to completely skip all TermEnumeration in the event that the input token is shorter then some simple math on the minSimilarity.  (i'm not smart enough to be certain that the math above is right however ... it's been a while since i looked at Levenstein distances ... tests needed)\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1019",
        "summary": "CustomScoreQuery should support multiple ValueSourceQueries",
        "description": "CustomScoreQuery's constructor currently accepts a subQuery, and a ValueSourceQuery.  I would like it to accept multiple ValueSourceQueries.  The workaround of nested CustomScoreQueries works for simple cases, but it quickly becomes either cumbersome to manage, or impossible to implement the desired function.\n\nThis patch implements CustomMultiScoreQuery with my desired functionality, and refactors CustomScoreQuery to implement the special case of a CustomMultiScoreQuery with 0 or 1 ValueSourceQueries.  This keeps the CustomScoreQuery API intact.\n\nThis patch includes basic tests, more or less taken from the original implementation, and customized a bit to cover the new cases.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3144",
        "summary": "FreqProxTermsWriter leaks file handles if exceptions are thrown during flush()",
        "description": "FreqProxTermsWriter leaks open file handles if exceptions are thrown during flush. Code needs to be protected by try-finally clauses.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2417",
        "summary": "Fix IndexCommit hashCode() and equals() to be consistent",
        "description": "IndexCommit's impl of hashCode() and equals() is inconsistent. One uses Dir + version and the other uses Dir + equals. According to hashCode()'s javadoc, if o1.equals(o2), then o1.hashCode() == o2.hashCode(). Simple fix, and I'll add a test case.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1912",
        "summary": "FastVectorHighlighter: latter terms cannot be highlighted if two or more terms are concatenated",
        "description": "My customer found a bug in FastVectorHighlighter. I'm working for the fix. I'll post it as soon as possible. We hope the fix in 2.9.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2862",
        "summary": "Track total term freq per term",
        "description": "Right now we track docFreq for each term (how many docs have the\nterm), but the totalTermFreq (total number of occurrences of this\nterm, ie sum of freq() for each doc that has the term) is also a\nuseful stat (for flex scoring, PulsingCodec, etc.).\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2205",
        "summary": "Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.",
        "description": "Basically packing those three arrays into a byte array with an int array as an index offset.  \n\nThe performance benefits are stagering on my test index (of size 6.2 GB, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size.  From 291.5 MB to 49.7 MB.  The random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full GC's on my JVM were made 7 times faster.\n\nI have already performed the work and am offering this code as a patch.  Currently all test in the trunk pass with this new code enabled.  I did write a system property switch to allow for the original implementation to be used as well.\n\n-Dorg.apache.lucene.index.TermInfosReader=default or small\n\nI have also written a blog about this patch here is the link.\n\nhttp://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html\n\n\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2199",
        "summary": "ShingleFilter skips over trie-shingles if outputUnigram is set to false",
        "description": "Spinoff from http://lucene.markmail.org/message/uq4xdjk26yduvnpa\n\n{quote}\nI noticed that if I set outputUnigrams to false it gives me the same output for\nmaxShingleSize=2 and maxShingleSize=3.\n\nplease divide divide this this sentence\n\nwhen i set maxShingleSize to 4 output is:\n\nplease divide please divide this sentence divide this this sentence\n\nI was expecting the output as follows with maxShingleSize=3 and\noutputUnigrams=false :\n\nplease divide this divide this sentence \n{quote}\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1297",
        "summary": "Allow other string distance measures in spellchecker",
        "description": "Updated spelling code to allow for other string distance measures to be used.\n\nCreated StringDistance interface.\nModified existing Levenshtein distance measure to implement interface (and renamed class).\nVerified that change to Levenshtein distance didn't impact runtime performance.\nImplemented Jaro/Winkler distance metric\nModified SpellChecker to take distacne measure as in constructor or in set method and to use interface when calling.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-701",
        "summary": "Lock-less commits",
        "description": "This is a patch based on discussion a while back on lucene-dev:\n\n    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3c44E5B16D.4010805@mikemccandless.com%3e\n\nThe approach is a small modification over the original discussion (see\nRetry Logic below).  It works correctly in all my cross-machine test\ncase, but I want to open it up for feedback, testing by\nusers/developers in more diverse environments, etc.\n\nThis is a small change to how lucene stores its index that enables\nelimination of the commit lock entirely.  The write lock still\nremains.\n\nOf the two, the commit lock has been more troublesome for users since\nit typically serves an active role in production.  Whereas the write\nlock is usually more of a design check to make sure you only have one\nwriter against the index at a time.\n\nThe basic idea is that filenames are never reused (\"write once\"),\nmeaning, a writer never writes to a file that a reader may be reading\n(there is one exception: the segments.gen file; see \"RETRY LOGIC\"\nbelow).  Instead it writes to generational files, ie, segments_1, then\nsegments_2, etc.  Besides the segments file, the .del files and norm\nfiles (.sX suffix) are also now generational.  A generation is stored\nas an \"_N\" suffix before the file extension (eg, _p_4.s0 is the\nseparate norms file for segment \"p\", generation 4).\n\nOne important benefit of this is it avoids files contents caching\nentirely (the likely cause of errors when readers open an index\nmounted on NFS) since the file is always a new file.\n\nWith this patch I can reliably instantiate readers over NFS when a\nwriter is writing to the index.  However, with NFS, you are still forced to\nrefresh your reader once a writer has committed because \"point in\ntime\" searching doesn't work over NFS (see LUCENE-673 ).\n\nThe changes are fully backwards compatible: you can open an old index\nfor searching, or to add/delete docs, etc.  I've added a new unit test\nto test these cases.\n\nAll units test pass, and I've added a number of additional unit tests,\nsome of which fail on WIN32 in the current lucene but pass with this\npatch.  The \"fileformats.xml\" has been updated to describe the changes\nto the files (but XXX references need to be fixed before committing).\n\nThere are some other important benefits:\n\n  * Readers are now entirely read-only.\n\n  * Readers no longer block one another (false contention) on\n    initialization.\n\n  * On hitting contention, we immediately retry instead of a fixed\n    (default 1.0 second now) pause.\n\n  * No file renaming is ever done.  File renaming has caused sneaky\n    access denied errors on WIN32 (see LUCENE-665 ).  (Yonik, I used\n    your approach here to not rename the segments_N file(try\n    segments_(N-1) on hitting IOException on segments_N): the separate\n    \".done\" file did not work reliably under very high stress testing\n    when a directory listing was not \"point in time\").\n\n  * On WIN32, you can now call IndexReader.setNorm() even if other\n    readers have the index open (fixes a pre-existing minor bug in\n    Lucene).\n\n  * On WIN32, You can now create an IndexWriter with create=true even\n    if readers have the index open (eg see\n    www.gossamer-threads.com/lists/lucene/java-user/39265) .\n\n\nHere's an overview of the changes:\n\n  * Every commit writes to the next segments_(N+1).\n\n  * Loading the segments_N file (& opening the segments) now requires\n    retry logic.  I've captured this logic into a new static class:\n    SegmentInfos.FindSegmentsFile.  All places that need to do\n    something on the current segments file now use this class.\n\n  * No more deletable file.  Instead, the writer computes what's\n    deletable on instantiation and updates this in memory whenever\n    files can be deleted (ie, when it commits).  Created a common\n    class index.IndexFileDeleter shared by reader & writer, to manage\n    deletes.\n\n  * Storing more information into segments info file: whether it has\n    separate deletes (and which generation), whether it has separate\n    norms, per field (and which generation), whether it's compound or\n    not.  This is instead of relying on IO operations (file exists\n    calls).  Note that this fixes the current misleading\n    FileNotFoundException users now see when an _X.cfs file is missing\n    (eg http://www.nabble.com/FileNotFound-Exception-t6987.html).\n\n  * Fixed some small things about RAMDirectory that were not\n    filesystem-like (eg opening a non-existent IndexInput failed to\n    raise IOException; renames were not atomic).  I added a stress\n    test against a RAMDirectory (1 writer thread & 2 reader threads)\n    that uncovered these.\n\n  * Added option to not remove old files when create=true on creating\n    FSDirectory; this is so the writer can do its own [more\n    sophisticated because it retries on errors] removal.\n\n  * Removed all references to commit lock, COMMIT_LOCK_TIMEOUT, etc.\n    (This is an API change).\n\n  * Extended index/IndexFileNames.java and index/IndexFileNameFilter.java\n    with logic for computing generational file names.\n\n  * Changed index/IndexFileNameFilter.java to use a HashSet to check\n    file extentsions for better performance.\n\n  * Fixed the test case TestIndexReader.testLastModified: it was\n    incorrectly (I think?) comparing lastModified to version, of the\n    index.  I fixed that and then added a new test case for version.\n\n\nRetry Logic (in index/SegmentInfos.java)\n\nIf a reader tries to load the segments just as a writer is committing,\nit may hit an IOException.  This is just normal contention.  In\ncurrent Lucene contention causes a [default] 1.0 second pause then\nretry.  With lock-less the contention causes no added delay beyond the\ntime to retry.\n\nWhen this happens, we first try segments_(N-1) if present, because it\ncould be segments_N is still being written.  If that fails, we\nre-check to see if there is now a newer segments_M where M > N and\nadvance if so.  Else we retry segments_N once more (since it could be\nit was in process previously but must now be complete since\nsegments_(N-1) did not load).\n\nIn order to find the current segments_N file, I list the directory and\ntake the biggest segments_N that exists.\n\nHowever, under extreme stress testing (5 threads just opening &\nclosing readers over and over), on one platform (OS X) I found that\nthe directory listing can be incorrect (stale) by up to 1.0 seconds.\nThis means the listing will show a segments_N file but that file does\nnot exist (fileExists() returns false).\n\nIn order to handle this (and other such platforms), I switched to a\nhybrid approach (originally proposed by Doron Cohen in the original\nthread): on committing, the writer writes to a file \"segments.gen\" the\ngeneration it just committed.  It writes 2 identical longs into this\nfile.  The retry logic, on detecting that the directory listing is\nstale falls back to the contents of this file.  If that file is\nconsistent (the two longs are identical), and, the generation is\nindeed newer than the dir listing, it will use that.\n\nFinally, if this approach is also stale, we fallback to stepping\nthrough sequential generations (up to a maximum # tries).  If all 3\nmethods fail, we throw the original exception we hit.\n\nI added a static method SegmentInfos.setInfoStream() which will print\ndetails of retry attempts.  In the patch it's set to System.out right\nnow (we should turn off before a real commit) so if there are problems\nwe can see what retry logic had done.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2030",
        "summary": "CachingSpanFilter synchronizing on a none final protected object",
        "description": "CachingSpanFilter and CachingWrapperFilter expose their internal cache via a protected member which is lazily instantiated in the getDocSetId method. The current code yields the chance to double instantiate the cache and internally synchronizes on a protected none final member. My first guess is that this member was exposed for testing purposes so it should rather be changed to package private. \n\nThis patch breaks backwards compat while I guess the cleanup is kind of worth breaking it.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3033",
        "summary": "TestAddIndexes#testAddIndexesWithThreads fails on Realtime",
        "description": "Selckin reported two failures on LUCENE-3023 which I can unfortunately not reproduce at all. here are the traces\n\n{noformat}\n  [junit] Testsuite: org.apache.lucene.index.TestAddIndexes\n    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):\tFAILED\n    [junit] expected:<3160> but was:<3060>\n    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>\n    [junit] \tat org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)\n    [junit] \n    [junit] \n    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.272 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=6128854208955988865:2552774338676281184\n    [junit] NOTE: test params are: codec=PreFlex, locale=no_NO_NY, timezone=America/Edmonton\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=84731792,total=258080768\n    [junit] ------------- ---------------- ---------------\n{noformat}\nand \n{noformat}\n    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes\n    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):\tFAILED\n    [junit] expected:<3160> but was:<3060>\n    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>\n    [junit] \tat org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)\n    [junit] \n    [junit] \n    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.841 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=4502815121171887759:-6764285049309266272\n    [junit] NOTE: test params are: codec=PreFlex, locale=tr_TR, timezone=Mexico/BajaNorte\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=163663416,total=243335168\n    [junit] ------------- ---------------- ---------------\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2491",
        "summary": "Extend Codec with a SegmentInfos writer / reader",
        "description": "I'm trying to implement a Codec that works with append-only filesystems (HDFS). It's _almost_ done, except for the SegmentInfos.write(dir), which uses ChecksumIndexOutput, which in turn uses IndexOutput.seek() - and seek is not supported on append-only output. I propose to extend the Codec interface to encapsulate also the details of SegmentInfos writing / reading. Patch to follow after some feedback ;)",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3418",
        "summary": "Lucene is not fsync'ing files on commit",
        "description": "Thanks to hurricane Irene, when Mark's electricity became unreliable, he discovered that on power loss Lucene could easily corrumpt the index, which of course should never happen...\n\nI was able to easily repro, by pulling the plug on an Ubuntu box during indexing.  On digging, I discovered, to my horror, that Lucene is failing to fsync any files, ever!\n\nThis bug was unfortunately created when we committed LUCENE-2328... that issue added tracking, in FSDir, of which files have been closed but not sync'd, so that when sync is called during IW.commit we only sync those files that haven't already been sync'd.\n\nThat tracking is done via the FSDir.onIndexOutputClosed callback, called when an FSIndexOutput is closed.  The bug is that we only call it on exception during close:\n\n{noformat}\n\n    @Override\n    public void close() throws IOException {\n      // only close the file if it has not been closed yet\n      if (isOpen) {\n        boolean success = false;\n        try {\n          super.close();\n          success = true;\n        } finally {\n          isOpen = false;\n          if (!success) {\n            try {\n              file.close();\n              parent.onIndexOutputClosed(this);\n            } catch (Throwable t) {\n              // Suppress so we don't mask original exception\n            }\n          } else\n            file.close();\n        }\n      }\n    }\n{noformat}\n\nAnd so FSDir thinks no files need syncing when its sync method is called....\n\nI think instead we should call it up-front; better to over-sync than under-sync.\n\nThe fix is trivial (move the callback up-front), but I'd love to somehow have a test that can catch such a bad regression in the future.... still I think we can do that test separately and commit this fix first.\n\nNote that even though LUCENE-2328 was backported to 2.9.x and 3.0.x, this bug wasn't, ie the backport was a much simpler fix (to just address the original memory leak); it's 3.1, 3.2, 3.3 and trunk when this bug is present.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1730",
        "summary": "TrecContentSource should use a fixed encoding, rather than system dependent",
        "description": "TrecContentSource opens InputStreamReader w/o a fixed encoding. On Windows, this means CP1252 (at least on my machine) which is ok. However, when I opened it on a Linux machine w/ a default of UTF-8, it failed to read the files. The patch changes it to use ISO-8859-1, which seems to be the right one (and http://mg4j.dsi.unimi.it/man/manual/ch01s04.html mentions this encoding in its example of a script which reads the data).\n\nPatch to follow shortly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-235",
        "summary": "[PATCH] Error in GermanStemmer.java,v 1.11",
        "description": "GermanStemmer.java,v 1.11 in  lucene-1.4-final\n\u00c3\u009f at the end of a word is not replaced by ss",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-294",
        "summary": "DisjunctionScorer",
        "description": "This disjunction scorer can match a minimum nr. of docs, \nit provides skipTo() and it uses skipTo() on the subscorers. \nThe score() method is abstract in DisjunctionScorer and implemented \nin DisjunctionSumScorer as an example.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1758",
        "summary": "improve arabic analyzer: light8 -> light10",
        "description": "Someone mentioned on the java user list that the arabic analysis was not as good as they would like.\n\nThis patch adds the \u0644\u0644- prefix (light10 algorithm versus light8 algorithm).\nIn the light10 paper, this improves precision from .390 to .413\nThey mention this is not statistically significant, but it makes linguistic sense and at least has been shown not to hurt.\n\nIn the future, I hope openrelevance will allow us to try some more approaches. \n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1827",
        "summary": "Make the Payload Boosting Queries consistent",
        "description": "BoostingFunctionTermQuery should be consistent with BoostingNearQuery -\n\nRenaming to PayloadNearQuery and PayloadTermQuery",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1609",
        "summary": "Eliminate synchronization contention on initial index reading in TermInfosReader ensureIndexIsRead ",
        "description": "synchronized method ensureIndexIsRead in TermInfosReader causes contention under heavy load\n\nSimple to reproduce: e.g. Under Solr, with all caches turned off, do a simple range search e.g. id:[0 TO 999999] on even a small index (in my case 28K docs) and under a load/stress test application, and later, examining the Thread dump (kill -3) , many threads are blocked on 'waiting for monitor entry' to this method.\n\nRather than using Double-Checked Locking which is known to have issues, this implementation uses a state pattern, where only one thread can move the object from IndexNotRead state to IndexRead, and in doing so alters the objects behavior, i.e. once the index is loaded, the index nolonger needs a synchronized method. \n\nIn my particular test, this uncreased throughput at least 30 times.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1902",
        "summary": "Changes.html not explicitly included in release",
        "description": "None of the release related ant targets explicitly call cahnges-to-html ... this seems like an oversight.  (currently it's only called as part of the nightly target)\n\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2867",
        "summary": "Change contrib QP API that uses CharSequence as string identifier",
        "description": "There are some API methods on contrib queryparser that expects CharSequence as identifier. This is wrong, since it may lead to incorrect or mislead behavior, as shown on LUCENE-2855. To avoid this problem, these APIs will be changed and enforce the use of String instead of CharSequence on version 4. This patch already deprecate the old API methods and add new substitute methods that uses only String.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2572",
        "summary": "Maven artifacts for Lucene 4 are not stored in the correct path",
        "description": "Hello,\n\nI would like to use the maven artifacts for Lucene 4.0 produced by the Hudson build machine. The artifacts are correctly produced (http://hudson.zones.apache.org/hudson/view/Lucene/job/Lucene-trunk/lastSuccessfulBuild/artifact/maven_artifacts/lucene/).\nHowever, the artifacts which should be stored under the path \"org/apache/lucene/\" are currently stored under \"lucene\" which prevents a project using maven to correctly download the Lucene 4.0 artifacts.\n\nThanks again for your help.  ",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2197",
        "summary": "StopFilter should not create a new CharArraySet if the given set is already an instance of CharArraySet",
        "description": "With LUCENE-2094 a new CharArraySet is created no matter what type of set is passed to StopFilter. This does not behave as  documented and could introduce serious performance problems. Yet, according to the javadoc, the instance of CharArraySet should be passed to CharArraySet.copy (which is very fast for CharArraySet instances) instead of \"copied\" via \"new CharArraySet()\"",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-888",
        "summary": "Improve indexing performance by increasing internal buffer sizes",
        "description": "In working on LUCENE-843, I noticed that two buffer sizes have a\nsubstantial impact on overall indexing performance.\n\nFirst is BufferedIndexOutput.BUFFER_SIZE (also used by\nBufferedIndexInput).  Second is CompoundFileWriter's buffer used to\nactually build the compound file.  Both are now 1 KB (1024 bytes).\n\nI ran the same indexing test I'm using for LUCENE-843.  I'm indexing\n~5,500 byte plain text docs derived from the Europarl corpus\n(English).  I index 200,000 docs with compound file enabled and term\nvector positions & offsets stored plus stored fields.  I flush\ndocuments at 16 MB RAM usage, and I set maxBufferedDocs carefully to\nnot hit LUCENE-845.  The resulting index is 1.7 GB.  The index is not\noptimized in the end and I left mergeFactor @ 10.\n\nI ran the tests on a quad-core OS X 10 machine with 4-drive RAID 0 IO\nsystem.\n\nAt 1 KB (current Lucene trunk) it takes 622 sec to build the index; if\nI increase both buffers to 8 KB it takes 554 sec to build the index,\nwhich is an 11% overall gain!\n\nI will run more tests to see if there is a natural knee in the curve\n(buffer size above which we don't really gain much more performance).\n\nI'm guessing we should leave BufferedIndexInput's default BUFFER_SIZE\nat 1024, at least for now.  During searching there can be quite a few\nof this class instantiated, and likely a larger buffer size for the\nfreq/prox streams could actually hurt search performance for those\nsearches that use skipping.\n\nThe CompoundFileWriter buffer is created only briefly, so I think we\ncan use a fairly large (32 KB?) buffer there.  And there should not be\ntoo many BufferedIndexOutputs alive at once so I think a large-ish\nbuffer (16 KB?) should be OK.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-605",
        "summary": "Make Explanation include information about match/non-match",
        "description": "As discussed, I'm looking into the possibility of improving the Explanation class to include some basic info about the \"match\" status of the Explanation -- independent of the value...\n\nhttp://www.nabble.com/BooleanWeight.normalize%28float%29-doesn%27t-normalize-prohibited-clauses--t1596471.html#a4347644\n\nThis is neccesary to deal with things like LUCENE-451",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2805",
        "summary": "SegmentInfos shouldn't blindly increment version on commit",
        "description": "SegmentInfos currently increments version on the assumption that there are always changes.\n\nBut, both DirReader and IW are more careful about tracking whether there are changes.  DirReader has hasChanges and IW has changeCount.  I think these classes should notify the SIS when there are in fact changes; this will fix the case Simon hit on fixing LUCENE-2082 when the NRT reader thought there were changes, but in fact there weren't because IW simply committed the exact SIS it already had.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3139",
        "summary": "LuceneTestCase.afterClass does not print enough information if a temp-test-dir fails to delete",
        "description": "I've hit an exception from LTC.afterClass when _TestUtil.rmDir failed (on write.lock, as if some test did not release resources). However, I had no idea which test caused that (i.e. opened the temp directory and did not release resources).\n\nI think we should do the following:\n* Track in LTC a map from dirName -> StackTraceElement\n* In afterClass if _TestUtil.rmDir fails, print the STE of that particular dir, so we know where was this directory created from\n* Make tempDirs private and create accessor method, so that we control the inserts to this map (today the Set is updated by LTC, _TestUtils and TestBackwards !)",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1158",
        "summary": "DateTools UTC/GMT mismatch",
        "description": "Post from Antony Bowesman on java-user:\n\n-----\n\nI just noticed that although the Javadocs for Lucene 2.2 state that the dates \nfor DateTools use UTC as a timezone, they are actually using GMT.\n\nShould either the Javadocs be corrected or the code corrected to use UTC instead.\n\n-----\n\nI'm attaching a patch that changes the javadoc and will commit it, unless someone knows a reason the javadoc is correct and the code should be changed to UTC. To my understanding, there's no significant difference between UTC and GMT.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3345",
        "summary": "docvalues FNFE",
        "description": "I created a test for LUCENE-3335, and it found an unrelated bug in docvalues.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2514",
        "summary": "Change Term to use bytes",
        "description": "in LUCENE-2426, the sort order was changed to codepoint order.\n\nunfortunately, Term is still using string internally, and more importantly its compareTo() uses the wrong order [utf-16].\nSo MultiTermQuery, etc (especially its priority queues) are currently wrong.\n\nBy changing Term to use bytes, we can also support terms encoded as bytes such as numerics, instead of using\nstrange string encodings.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1843",
        "summary": "Convert some tests to new TokenStream API, better support of cross-impl AttributeImpl.copyTo()",
        "description": "This patch converts some remaining tests to the new TokenStream API and non-deprecated classes.\nThis patch also enhances AttributeImpl.copyTo() of Token and TokenWrapper to also support copying e.g. TermAttributeImpl into Token. The target impl must only support all interfaces but must not be of the same type. Token and TokenWrapper use optimized coping without casting to 6 interfaces where possible.\nMaybe the special tokenizers in contrib (shingle matrix and so on using tokens to cache may be enhanced by that). Also Yonik's request for optimized copying of states between incompatible AttributeSources may be enhanced by that (possibly a new issue).",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2358",
        "summary": "rename KeywordMarkerTokenFilter",
        "description": "I would like to rename KeywordMarkerTokenFilter to KeywordMarkerFilter.\nWe havent released it yet, so its a good time to keep the name brief and consistent.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1992",
        "summary": "intermittent failure in TestIndexWriter. testExceptionDuringSync ",
        "description": "{code}\ncommon.test:\n\n    [mkdir] Created dir: C:\\Projects\\lucene\\trunk-full1\\build\\test\n\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter\n\n    [junit] Tests run: 102, Failures: 0, Errors: 1, Time elapsed: 100,297sec\n\n    [junit]\n\n    [junit] Testcase: testExceptionDuringSync(org.apache.lucene.index.TestIndexWriter): Caused an ERROR\n\n    [junit] _a.fnm\n\n    [junit] java.io.FileNotFoundException: _a.fnm\n\n    [junit]     at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:226)\n\n    [junit]     at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:68)\n\n    [junit]     at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:116)\n\n    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:620)\n\n    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:590)\n\n    [junit]     at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:104)\n\n    [junit]     at org.apache.lucene.index.ReadOnlyDirectoryReader.<init>(ReadOnlyDirectoryReader.java:27)\n\n    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:74)\n\n    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:704)\n\n    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:69)\n\n    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:307)\n\n    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:193)\n\n    [junit]     at org.apache.lucene.index.TestIndexWriter.testExceptionDuringSync(TestIndexWriter.java:2723)\n\n    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:206)\n\n    [junit]\n\n    [junit]\n\n    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2662",
        "summary": "BytesHash",
        "description": "This issue will have the BytesHash separated out from LUCENE-2186",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3375",
        "summary": "processing a synonym in a token stream will remove the following token from the stream",
        "description": "If you do a phrase search on a field derived from a fieldtype with the synonym filter which includes a synonym, the term following the synonym vanishes after synonym expansion.\n\ne.g. http://host:port/solr/corename/select/?q=desc:%22xyzzy%20%20bbb%20pot%20of%20gold%22&version=2.2&start=0&rows=10&indent=on&debugQuery=true   (bbb is in the default synonyms file, desc is a \"text\" fieldtype)\n\noutputs\n....\n<str name=\"rawquerystring\">desc:\"xyzzy  bbb pot of gold\"</str>\n<str name=\"querystring\">desc:\"xyzzy  bbb pot of gold\"</str>\n<str name=\"parsedquery\">PhraseQuery(desc:\"xyzzy bbbb 1 bbbb 2 of gold\")</str>\n<str name=\"parsedquery_toString\">desc:\"xyzzy bbbb 1 bbbb 2 of gold\"</str>\n....\n\nYou can also see this behavior using the admin console analysis.jsp\n\nSolr 3.3 behaves properly.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-615",
        "summary": "[patch] javadoc and comment updates for BooleanClause.",
        "description": "Javadoc and comment updates for BooleanClause, one minor code simplification.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1434",
        "summary": "IndexableBinaryStringTools: convert arbitrary byte sequences into Strings that can be used as index terms, and vice versa",
        "description": "Provides support for converting byte sequences to Strings that can be used as index terms, and back again. The resulting Strings preserve the original byte sequences' sort order (assuming the bytes are interpreted as unsigned).\n\nThe Strings are constructed using a Base 8000h encoding of the original binary data - each char of an encoded String represents a 15-bit chunk from the byte sequence.  Base 8000h was chosen because it allows for all lower 15 bits of char to be used without restriction; the surrogate range [U+D800-U+DFFF] does not represent valid chars, and would require complicated handling to avoid them and allow use of char's high bit.\n\nThis class is intended to serve as a mechanism to allow CollationKeys to serve as index terms.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3416",
        "summary": "Allow to pass an instance of RateLimiter to FSDirectory allowing to rate limit merge IO across several directories / instances",
        "description": "This can come in handy when running several Lucene indices in the same VM, and wishing to rate limit merge across all of them.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2194",
        "summary": "improve efficiency of snowballfilter",
        "description": "snowball stemming currently creates 2 new strings and 1 new stringbuilder for every word.\n\nall of this is unnecessary, so don't do it.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2051",
        "summary": "Contrib Analyzer Setters should be deprecated and replace with ctor arguments",
        "description": "Some analyzers in contrib provide setters for stopword / stem exclusion sets / hashtables etc. Those setters should be deprecated as they yield unexpected behaviour. The way they work is they set the reusable token stream instance to null in a thread local cache which only affects the tokenstream in the current thread. Analyzers itself should be immutable except of the threadlocal. \n\nwill attach a patch soon.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2038",
        "summary": "Systemrequirements should say 1.5 instead of 1.4",
        "description": "The website still says Java 1.4 but it should say 1.5",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2126",
        "summary": "Split up IndexInput and IndexOutput into DataInput and DataOutput",
        "description": "I'd like to introduce the two new classes DataInput and DataOutput\nthat contain all methods from IndexInput and IndexOutput that actually\ndecode or encode data, such as readByte()/writeByte(),\nreadVInt()/writeVInt().\n\nMethods like getFilePointer(), seek(), close(), etc., which are not\nrelated to data encoding, but to files as input/output source stay in\nIndexInput/IndexOutput.\n\nThis patch also changes ByteSliceReader/ByteSliceWriter to extend\nDataInput/DataOutput. Previously ByteSliceReader implemented the\nmethods that stay in IndexInput by throwing RuntimeExceptions.\n\nSee also LUCENE-2125.\n\nAll tests pass.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1676",
        "summary": "New Token filter for adding payloads \"in-stream\"",
        "description": "This TokenFilter is able to split a token based on a delimiter and use one part as the token and the other part as a payload.  This allows someone to include payloads inline with tokens (presumably setup by a pipeline ahead of time).  An example is apropos.  Given a | delimiter, we could have a stream that looks like:\n{quote}The quick|JJ red|JJ fox|NN jumped|VB over the lazy|JJ brown|JJ dogs|NN{quote}\n\nIn this case, this would produce tokens and payloads (assuming whitespace tokenization):\nToken: the\nPayload: null\n\nToken: quick\nPayload: JJ\n\nToken: red\nPay: JJ.\n\nand so on.\n\nThis patch will also support pluggable encoders for the payloads, so it can convert from the character array to byte arrays as appropriate.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1824",
        "summary": "FastVectorHighlighter truncates words at beginning and end of fragments",
        "description": "FastVectorHighlighter does not take word boundaries into consideration when building fragments, so that in most cases the first and last word of a fragment are truncated.  This makes the highlights less legible than they should be.  I will attach a patch to BaseFragmentBuilder that resolves this by expanding the start and end boundaries of the fragment to the first whitespace character on either side of the fragment, or the beginning or end of the source text, whichever comes first.  This significantly improves legibility, at the cost of returning a slightly larger number of characters than specified for the fragment size.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1260",
        "summary": "Norm codec strategy in Similarity",
        "description": "The static span and resolution of the 8 bit norms codec might not fit with all applications. \n\nMy use case requires that 100f-250f is discretized in 60 bags instead of the default.. 10?\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-345",
        "summary": "Weird BooleanQuery behavior",
        "description": "Here's a simple OR-connected query.\n\nT:files T:deleting C:thanks C:exists\n\nThe query above hits 1 document. But following *same* query only\nwith parenthesis results nothing.\n\n(T:files T:deleting) (C:thanks C:exists)\n\nAnother combinations of MUST and SHOULD.\n\n\"T:files T:deleting +C:production +C:optimize\" hits 1 document.\n\"(T:files T:deleting) (+C:production +C:optimize)\" hits 1 document.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1932",
        "summary": "Convert PrecedenceQueryParser to new TokenStream API",
        "description": "Adriano Crestani provided a patch, that updates the PQP to use the new TokenStream API...all tests still pass. \nI hope this helps to keep the PQP \n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1877",
        "summary": "Use NativeFSLockFactory as default for new API (direct ctors & FSDir.open)",
        "description": "A user requested we add a note in IndexWriter alerting the availability of NativeFSLockFactory (allowing you to avoid retaining locks on abnormal jvm exit). Seems reasonable to me - we want users to be able to easily stumble upon this class. The below code looks like a good spot to add a note - could also improve whats there a bit - opening an IndexWriter does not necessarily create a lock file - that would depend on the LockFactory used.\n\n\n{code}  <p>Opening an <code>IndexWriter</code> creates a lock file for the directory in use. Trying to open\n  another <code>IndexWriter</code> on the same directory will lead to a\n  {@link LockObtainFailedException}. The {@link LockObtainFailedException}\n  is also thrown if an IndexReader on the same directory is used to delete documents\n  from the index.</p>{code}\n\nAnyone remember why NativeFSLockFactory is not the default over SimpleFSLockFactory?",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1230",
        "summary": "Source release files missing the *.pom.template files",
        "description": "The source release files should contain the *.pom.template files, otherwise it is not possible to build the maven artifacts using \"ant generate-maven-artifacts\" from official release files.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3040",
        "summary": "analysis consumers should use reusable tokenstreams",
        "description": "Some analysis consumers (highlighter, more like this, memory index, contrib queryparser, ...) are using Analyzer.tokenStream but should be using Analyzer.reusableTokenStream instead for better performance.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2593",
        "summary": "disk full can cause index corruption in certain cases",
        "description": "Robert uncovered this nasty bug, in adding more randomness to\noal.index tests...\n\nI got a standalone test to show the issue; the corruption path is\nas follows:\n\n  * The merge hits an initial exception (eg disk full when merging the\n    postings).\n\n  * In handling this exception, IW closes all the sub-readers,\n    suppressing any further exceptions.\n\n  * If one of these sub-readers has pending deletions, which happens\n    if readers are pooled in IW, it will flush them.  If that flush\n    hits a 2nd exception (eg disk full), then SegmentReader\n    [incorrectly] leaves the SegmentInfo's delGen advanced by 1,\n    referencing a corrupt file, yet the SegmentReader is still\n    forcefully closed.\n\n  * If enough disk frees up such that a later IW.commit/close\n    succeeds, the resulting segments file will reference an invalid\n    deletions file.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1782",
        "summary": "Rename OriginalQueryParserHelper",
        "description": "We should rename the new QueryParser so it's clearer that it's\nLucene's default QueryParser, going forward, and not just a temporary\n\"bridge\" to a future new QueryParser.\n\nHow about we rename oal.queryParser.original -->\noal.queryParser.standard (can't use \"default\": it's a Java keyword)?\nThen, leave the OriginalQueryParserHelper under that package, but\nsimply rename it to QueryParser?\n\nThis way if we create other sub-packages in the future, eg\nComplexPhraseQueryParser, they too can have a QueryParser class under\nthem, to make it clear that's the \"top\" class you use to parse\nqueries.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2941",
        "summary": "custom sort broken if IS uses executorservice",
        "description": "",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1250",
        "summary": "Some equals methods do not check for null argument",
        "description": "The equals methods in the following classes do not check for a null argument and thus would incorrectly fail with a null pointer exception if passed null:\n\n- org.apache.lucene.index.SegmentInfo\n- org.apache.lucene.search.function.CustomScoreQuery\n- org.apache.lucene.search.function.OrdFieldSource\n- org.apache.lucene.search.function.ReverseOrdFieldSource\n- org.apache.lucene.search.function.ValueSourceQuery\n\nIf a null parameter is passed to equals() then false should be returned.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3218",
        "summary": "Make CFS appendable  ",
        "description": "Currently CFS is created once all files are written during a flush / merge. Once on disk the files are copied into the CFS format which is basically a unnecessary for some of the files. We can at any time write at least one file directly into the CFS which can save a reasonable amount of IO. For instance stored fields could be written directly during indexing and during a Codec Flush one of the written files can be appended directly. This optimization is a nice sideeffect for lucene indexing itself but more important for DocValues and LUCENE-3216 we could transparently pack per field files into a single file only for docvalues without changing any code once LUCENE-3216 is resolved.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1796",
        "summary": "Speed up repeated TokenStream init",
        "description": " by caching isMethodOverridden results",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3188",
        "summary": "The class from cotrub directory org.apache.lucene.index.IndexSplitter creates a non correct index",
        "description": "When using the method IndexSplitter.split(File destDir, String[] segs) from the Lucene cotrib directory (contrib/misc/src/java/org/apache/lucene/index) it creates an index with segments descriptor file with wrong data. Namely wrong is the number representing the name of segment that would be created next in this index.\nIf some of the segments of the index already has this name this results either to impossibility to create new segment or in crating of an corrupted segment.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2578",
        "summary": "cutover FunctionQuery tests to use RandomIndexWriter, for better testing",
        "description": "",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-526",
        "summary": "FieldSortedHitQueue - subsequent String sorts with different locales sort identically",
        "description": "From my own post to the java-user list. I have looked into this further and am sure it's a bug.\n\n---\n\nIt seems to me that there's a possible bug in FieldSortedHitQueue, specifically in getCachedComparator(). This is showing up on our 1.4.3 install, but it seems from source code inspection that if it's a bug, it's in 1.9.1 also.\n\nThe issue shows up when you need to sort results from a given IndexReader multiple times, using different locales. On line 180 (all line numbers from the 1.9.1 code), we have this:\n\nScoreDocComparator comparator = lookup (reader, fieldname, type, factory);\n\nThen, if no comparator is found in the cache, a new one is created (line 193) and then stored in the cache (line 202). HOWEVER, both the cache lookup() and store() do NOT take into account locale; if we, on the same index reader, try to do one search sorted by Locale.FRENCH and one by Locale.ITALIAN, the first one will result in a cache miss, a new French comparator will be created, and stored in the cache. Second time through, lookup() finds the cached French comparator -- even though this time, the locale parameter to getCachedComparator() is an Italian locale. Therefore, we don't create a new comparator and we use the wrong one to sort the results.\n\nIt looks to me (unless I'm mistaken) that the FieldCacheImpl.Entry class should have an additional property, .locale, to ensure that different locales get different comparators.\n\n---\n\nPatch (well, most of one) to follow immediately.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2619",
        "summary": "simple improvements to tests",
        "description": "Simon had requested some docs on what all our test options do, so lets clean it up and doc it.\n\ni propose:\n# change all vars to be tests.xxx (e.g. tests.threadspercpu, tests.multiplier, ...)\n# ensure all 6 build systems (lucene, solr, each solr contrib) respect these.\n# add a simple wiki page listing what these do.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1268",
        "summary": "Changes.html should be visible to users for closed releases",
        "description": "Changes.html is currently available only in the dev page, for trunk. \nSee LUCENE-1157 for discussion on where exactly to expose this.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3822",
        "summary": "Inner classes of FilterAtomicReader (trunk) / FilterIndexReader (3.x) do not override all methods to be filtered",
        "description": "This issue adds missing checks in the FilterReader test to also check overridden methods in the enum implementations (inner classes) similar to the checks added by Shai Erea.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3443",
        "summary": "Port 3.x FieldCache.getDocsWithField() to trunk",
        "description": "[Spinoff from LUCENE-3390]\n\nI think the approach in 3.x for handling un-valued docs, and making it\npossible to specify how such docs are sorted, is better than the\nsolution we have in trunk.\n\nI like that FC has a dedicated method to get the Bits for docs with field\n-- easy for apps to directly use.  And I like that the\nbits have their own entry in the FC.\n\nOne downside is that it's 2 passes to get values and valid bits, but\nI think we can fix this by passing optional bool to FC.getXXX methods\nindicating you want the bits, and the populate the FC entry for the\nmissing bits as well.  (We can do that for 3.x and trunk). Then it's\nsingle pass.\n",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-572",
        "summary": "SpanNotQuery.hashCode ignores exclude",
        "description": "filing as bug for tracking/refrence...\n\nOn May 16, 2006, at 3:33 AM, Chris Hostetter wrote:\n\n> SpanNodeQuery's hashCode method makes two refrences to  \n> include.hashCode(),\n> but none to exclude.hashCode() ... this is a mistake yes/no?\n\nDate: Tue, 16 May 2006 05:57:15 -0400\nFrom: Erik Hatcher\nTo: java-dev@lucene.apache.org\nSubject: Re: SpanNotQuery.hashCode cut/paste error?\n\nYes, this is a mistake.  I'm happy to fix it, but looks like you have  \nother patches in progress.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3881",
        "summary": "Create UAX29URLEmailAnalyzer: a standard analyzer that recognizes URLs and emails",
        "description": "This Analyzer should contain the same components as StandardAnalyzer, except for the tokenizer, which should be UAX29URLEmailTokenizer instead of StandardTokenizer.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3725",
        "summary": "Add optional packing to FST building",
        "description": "The FSTs produced by Builder can be further shrunk if you are willing\nto spend highish transient RAM to do so... our Builder today tries\nhard not to use much RAM (and has options to tweak down the RAM usage,\nin exchange for somewhat lager FST), even when building immense FSTs.\n\nBut for apps that can afford highish transient RAM to get a smaller\nnet FST, I think we should offer packing.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2579",
        "summary": "Small imprecision in Search package Javadocs",
        "description": "Search package Javadocs states that Scorer#score(Collector) will be abstract in Lucene 3.0, which is not accurate anymore.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1131",
        "summary": "Add numDeletedDocs to IndexReader",
        "description": "Add numDeletedDocs to IndexReader. Basically, the implementation is as simple as doing:\npublic int numDeletedDocs() {\n  return deletedDocs == null ? 0 : deletedDocs.count();\n}\nin SegmentReader.\nPatch to follow to include in all IndexReader extensions.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1566",
        "summary": "Large Lucene index can hit false OOM due to Sun JRE issue",
        "description": "This is not a Lucene issue, but I want to open this so future google\ndiggers can more easily find it.\n\nThere's this nasty bug in Sun's JRE:\n\n  http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6478546\n\nThe gist seems to be, if you try to read a large (eg 200 MB) number of\nbytes during a single RandomAccessFile.read call, you can incorrectly\nhit OOM.  Lucene does this, with norms, since we read in one byte per\ndoc per field with norms, as a contiguous array of length maxDoc().\n\nThe workaround was a custom patch to do large file reads as several\nsmaller reads.\n\nBackground here:\n\n  http://www.nabble.com/problems-with-large-Lucene-index-td22347854.html\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1051",
        "summary": "Separate javadocs for core and contribs",
        "description": "A while ago we had a discussion on java-dev about separating the javadocs\nfor the contrib modules instead of having only one big javadoc containing \nthe core and contrib classes.\n\nThis patch:\n* Adds new targets to build.xml: \n  ** \"javadocs-all\" Generates Javadocs for the core, demo, and contrib \n    classes\n  ** \"javadocs-core\" Generates Javadocs for the core classes\n  ** \"javadocs-demo\" Generates Javadocs for the demo classes\n  ** \"javadocs-contrib\" Using contrib-crawl it generates the Javadocs for \n    all contrib modules, except \"similarity\" (currently empty) and gdata.\n* Adds submenues to the Javadocs link on the Lucene site with links to\n  the different javadocs\n* Includes the javadocs in the maven artifacts\n\nRemarks:\n- I removed the ant target \"javadocs-internal\", because I didn't want to\n  add corresponding targets for all new javadocs target. Instead I \n  defined a new property \"javadoc.access\", so now  \n  \"ant -Djavadoc.access=package\" can be used in combination with any of\n  the javadocs targets. Is this ok?\n- I didn't include gdata (yet) because it uses build files that don't \n  extend Lucenes standard build files.\n  \nHere's a preview:\nhttp://people.apache.org/~buschmi/site-preview/index.html\n\nPlease let me know what you think about these changes!",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3052",
        "summary": "PerFieldCodecWrapper.loadTermsIndex concurrency problem",
        "description": "Selckin's while(1) testing on RT branch hit another error:\n{noformat}\n    [junit] Testsuite: org.apache.lucene.TestExternalCodecs\n    [junit] Testcase: testPerFieldCodec(org.apache.lucene.TestExternalCodecs):\tCaused an ERROR\n    [junit] (null)\n    [junit] java.lang.NullPointerException\n    [junit] \tat org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.loadTermsIndex(PerFieldCodecWrapper.java:202)\n    [junit] \tat org.apache.lucene.index.SegmentReader.loadTermsIndex(SegmentReader.java:1005)\n    [junit] \tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:652)\n    [junit] \tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:609)\n    [junit] \tat org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:276)\n    [junit] \tat org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2660)\n    [junit] \tat org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:2651)\n    [junit] \tat org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:381)\n    [junit] \tat org.apache.lucene.index.IndexReader.open(IndexReader.java:316)\n    [junit] \tat org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:541)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)\n    [junit] \n    [junit] \n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.909 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExternalCodecs -Dtestmethod=testPerFieldCodec -Dtests.seed=-7296204858082494534:5010909751437000758\n    [junit] WARNING: test method: 'testPerFieldCodec' left thread running: merge thread: _i(4.0):Cv130 _m(4.0):Cv30 _n(4.0):cv10 into _o\n    [junit] RESOURCE LEAK: test method: 'testPerFieldCodec' left 1 thread(s) running\n    [junit] NOTE: test params are: codec=PreFlex, locale=zh_TW, timezone=America/Santo_Domingo\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestDemo, TestExternalCodecs]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=2,free=104153512,total=125632512\n    [junit] ------------- ---------------- ---------------\n    [junit] TEST org.apache.lucene.TestExternalCodecs FAILED\n    [junit] Exception in thread \"Lucene Merge Thread #5\" org.apache.lucene.util.ThreadInterruptedException: java.lang.InterruptedException: sleep interrupted\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:505)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)\n    [junit] Caused by: java.lang.InterruptedException: sleep interrupted\n    [junit] \tat java.lang.Thread.sleep(Native Method)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:503)\n    [junit] \t... 1 more\n{noformat}\n\nI suspect this is also a trunk issue, but I can't reproduce it yet.\n\nI think this is happening because the codecs HashMap is changing (via another thread), while .loadTermsIndex is called.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-516",
        "summary": "TestFSDirectory fails on Windows",
        "description": "\"ant test\" generates the following error consistently when run on a Windows machine even when run as user with Administrator privileges\n\n    [junit] Testcase: testTmpDirIsPlainFile(org.apache.lucene.index.store.TestFSDirectory):     Caused an ERROR\n    [junit] Access is denied\n    [junit] java.io.IOException: Access is denied\n    [junit]     at java.io.WinNTFileSystem.createFileExclusively(Native Method)\n    [junit]     at java.io.File.createNewFile(File.java:828)\n    [junit]     at org.apache.lucene.index.store.TestFSDirectory.testTmpDirIsPlainFile(TestFSDirectory.java:66)",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1329",
        "summary": "Remove synchronization in SegmentReader.isDeleted",
        "description": "Removes SegmentReader.isDeleted synchronization by using a volatile deletedDocs variable on Java 1.5 platforms.  On Java 1.4 platforms synchronization is limited to obtaining the deletedDocs reference.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2677",
        "summary": "Tests failing when run with tests.iter > 1",
        "description": "TestMultiLevelSkipList and TestsFieldReader are falling if run with -Dtests.iter > 1 - not all values are reset though\nI will attach a patch in a second.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2095",
        "summary": "Document not guaranteed to be found after write and commit",
        "description": "after same email on developer list:\n\"I developed a stress test to assert that a new document containing a\nspecific term \"X\" is always found after a commit on the IndexWriter.\nThis works most of the time, but it fails under load in rare occasions.\n\nI'm testing with 40 Threads, both with a SerialMergeScheduler and a\nConcurrentMergeScheduler, all sharing a common IndexWriter.\nAttached testcase is using a RAMDirectory only, but I verified a\nFSDirectory behaves in the same way so I don't believe it's the\nDirectory implementation or the MergeScheduler.\nThis test is slow, so I don't consider it a functional or unit test.\nIt might give false positives: it doesn't always fail, sorry I\ncouldn't find out how to make it more likely to happen, besides\nscheduling it to run for a longer time.\"\n\nI tested this to affect versions 2.4.1 and 2.9.1;\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3279",
        "summary": "Allow CFS be empty",
        "description": "since we changed CFS semantics slightly closing a CFS directory on an error can lead to an exception. Yet, an empty CFS is still a valid CFS so for consistency we should allow CFS to be empty.\nhere is an example:\n\n{noformat}\n1 tests failed.\nREGRESSION:  org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull\n\nError Message:\nCFS has no entries\n\nStack Trace:\njava.lang.IllegalStateException: CFS has no entries\n       at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:139)\n       at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)\n       at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)\n       at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)\n       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4252)\n       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3863)\n       at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)\n       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2715)\n       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2710)\n       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2706)\n       at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3513)\n       at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2064)\n       at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2031)\n       at org.apache.lucene.index.TestIndexWriterOnDiskFull.addDoc(TestIndexWriterOnDiskFull.java:539)\n       at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull(TestIndexWriterOnDiskFull.java:74)\n       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)\n       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)\n{noformat}",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-868",
        "summary": "Making Term Vectors more accessible",
        "description": "One of the big issues with term vector usage is that the information is loaded into parallel arrays as it is loaded, which are then often times manipulated again to use in the application (for instance, they are sorted by frequency).\n\nAdding a callback mechanism that allows the vector loading to be handled by the application would make this a lot more efficient.\n\nI propose to add to IndexReader:\nabstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException;\nand a similar one for the all fields version\n\nWhere TermVectorMapper is an interface with a single method:\nvoid map(String term, int frequency, int offset, int position);\n\nThe TermVectorReader will be modified to just call the TermVectorMapper.  The existing getTermFreqVectors will be reimplemented to use an implementation of TermVectorMapper that creates the parallel arrays.  Additionally, some simple implementations that automatically sort vectors will also be created.\n\nThis is my first draft of this API and is subject to change.  I hope to have a patch soon.\n\nSee http://www.gossamer-threads.com/lists/lucene/java-user/48003?search_string=get%20the%20total%20term%20frequency;#48003 for related information.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3707",
        "summary": "Add a Lucene3x private SegmentInfosFormat implemenation",
        "description": "we still don't have a Lucene3x & preflex version of segment infos format. we need this before we release 4.0",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-904",
        "summary": "Calculate MD5 checksums in target <dist-all>",
        "description": "Trivial patch that extends the ant target <dist-all> to calculate\nthe MD5 checksums for the dist files.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2236",
        "summary": "Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level",
        "description": "Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level, to faciliate this could we pass make field name available to all score methods.\nCurrently it is only passed to some such as lengthNorm() but not others such as tf()",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2517",
        "summary": "changes-to-html: fixes and improvements",
        "description": "The [Lucene Hudson Changes.html|http://hudson.zones.apache.org/hudson/job/Lucene-trunk/lastSuccessfulBuild/artifact/lucene/build/docs/changes/Changes.html] looks bad because changes2html.pl doesn't properly handle some new usages in CHANGES.txt.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1884",
        "summary": "javadocs cleanup",
        "description": "basic cleanup in core/contrib: typos, apache license header as javadoc, missing periods that screw up package summary, etc.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2278",
        "summary": "FastVectorHighlighter: highlighted term is out of alignment in multi-valued NOT_ANALYZED field",
        "description": "",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-820",
        "summary": "SegmentReader.setNorm can fail to remove separate norms file, on Windows",
        "description": "\nWhile working through LUCENE-710 I hit this bug: on Windows\nonly, when SegmentReader.setNorm is called, but separate norms\n(_X_N.sY) had already been previously saved, then, on closing the\nreader, we will write the next gen separate norm file correctly\n(_X_N+1.sY) but fail to delete the current one.\n\nIt's quite minor because the next writer to touch the index will\nremove the stale file.\n\nThis is because the Norm class still holds the IndexInput open when\nthe reader commits.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3239",
        "summary": "drop java 5 \"support\"",
        "description": "its been discussed here and there, but I think we need to drop java 5 \"support\", for these reasons:\n* its totally untested by any continual build process. Testing java5 only when there is a release candidate ready is not enough. If we are to claim \"support\" then we need a hudson actually running the tests with java 5.\n* its now unmaintained, so bugs have to either be hacked around, tests disabled, warnings placed, but some things simply cannot be fixed... we cannot actually \"support\" something that is no longer maintained: we do find JRE bugs (http://wiki.apache.org/lucene-java/SunJavaBugs) and its important that bugs actually get fixed: cannot do everything with hacks.\n* because of its limitations, we do things like allow 20% slower grouping speed. I find it hard to believe we are sacrificing performance for this.\n\nSo, in summary: because we don't test it at all, because its buggy and unmaintained, and because we are sacrificing performance, I think we need to cutover the build system for the next release to require java 6.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1709",
        "summary": "Parallelize Tests",
        "description": "The Lucene tests can be parallelized to make for a faster testing system.  \n\nThis task from ANT can be used: http://ant.apache.org/manual/CoreTasks/parallel.html\n\nPrevious discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/69669\n\nNotes from Mike M.:\n{quote}\nI'd love to see a clean solution here (the tests are embarrassingly\nparallelizable, and we all have machines with good concurrency these\ndays)... I have a rather hacked up solution now, that uses\n\"-Dtestpackage=XXX\" to split the tests up.\n\nIdeally I would be able to say \"use N threads\" and it'd do the right\nthing... like the -j flag to make.\n{quote}",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2449",
        "summary": "Improve random testing",
        "description": "We have quite a few random tests, but there's no way to \"crank\" them.\n\nThe idea here is to add a multiplier which can be increased by a sysprop. For example, we could set this to something higher than 1 for hudson.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3903",
        "summary": "javadocs very very ugly if you generate with java7",
        "description": "Java7 changes its javadocs to look much nicer, but this involves different CSS styles.\n\nLucene overrides the CSS with stylesheet+prettify.css which is a combination of java5/6 stylesheet + google prettify:\nbut there are problems because java7 has totally different styles.\n\nSo if you generate javadocs with java7, its like you have no stylesheet at all.\n\nA solution might be to make stylesheet7+prettify.css and conditionalize a property in ant based on java version.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3624",
        "summary": "Throw exception for \"Multi-SortedSource\" instead of returning null",
        "description": "Spinoff of LUCENE-3623: currently if you addIndexes(FIR) or similar, you get a NPE deep within codecs during merge.\n\nI think the NPE is confusing, it looks like a bug but a clearer exception would be an improvement.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1228",
        "summary": "IndexWriter.commit()  does not update the index version",
        "description": "IndexWriter.commit() can update the index *version* and *generation* but the update of *version* is lost.\nAs result added documents are not seen by IndexReader.reopen().\n(There might be other side effects that I am not aware of).\nThe fix is 1 line - update also the version in SegmentsInfo.updateGeneration().\n(Finding this line involved more lines though... :-) )\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-969",
        "summary": "Optimize the core tokenizers/analyzers & deprecate Token.termText",
        "description": "There is some \"low hanging fruit\" for optimizing the core tokenizers\nand analyzers:\n\n  - Re-use a single Token instance during indexing instead of creating\n    a new one for every term.  To do this, I added a new method \"Token\n    next(Token result)\" (Doron's suggestion) which means TokenStream\n    may use the \"Token result\" as the returned Token, but is not\n    required to (ie, can still return an entirely different Token if\n    that is more convenient).  I added default implementations for\n    both next() methods in TokenStream.java so that a TokenStream can\n    choose to implement only one of the next() methods.\n\n  - Use \"char[] termBuffer\" in Token instead of the \"String\n    termText\".\n\n    Token now maintains a char[] termBuffer for holding the term's\n    text.  Tokenizers & filters should retrieve this buffer and\n    directly alter it to put the term text in or change the term\n    text.\n\n    I only deprecated the termText() method.  I still allow the ctors\n    that pass in String termText, as well as setTermText(String), but\n    added a NOTE about performance cost of using these methods.  I\n    think it's OK to keep these as convenience methods?\n\n    After the next release, when we can remove the deprecated API, we\n    should clean up Token.java to no longer maintain \"either String or\n    char[]\" (and the initTermBuffer() private method) and always use\n    the char[] termBuffer instead.\n\n  - Re-use TokenStream instances across Fields & Documents instead of\n    creating a new one for each doc.  To do this I added an optional\n    \"reusableTokenStream(...)\" to Analyzer which just defaults to\n    calling tokenStream(...), and then I implemented this for the core\n    analyzers.\n\nI'm using the patch from LUCENE-967 for benchmarking just\ntokenization.\n\nThe changes above give 21% speedup (742 seconds -> 585 seconds) for\nLowerCaseTokenizer -> StopFilter -> PorterStemFilter chain, tokenizing\nall of Wikipedia, on JDK 1.6 -server -Xmx1024M, Debian Linux, RAID 5\nIO system (best of 2 runs).\n\nIf I pre-break Wikipedia docs into 100 token docs then it's 37% faster\n(1236 sec -> 774 sec), I think because of re-using TokenStreams across\ndocs.\n\nI'm just running with this alg and recording the elapsed time:\n\n  analyzer=org.apache.lucene.analysis.LowercaseStopPorterAnalyzer\n  doc.tokenize.log.step=50000\n  docs.file=/lucene/wikifull.txt\n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n  doc.tokenized=true\n  doc.maker.forever=false\n\n  {ReadTokens > : *\n\nSee this thread for discussion leading up to this:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/51283\n\nI also fixed Token.toString() to work correctly when termBuffer is\nused (and added unit test).\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1519",
        "summary": "Change Primitive Data Types from int to long in class SegmentMerger.java",
        "description": "Hi\n\nWe are getting an exception while optimize. We are getting this exception \"mergeFields produced an invalid result: docCount is 385282378 but fdx file size is 3082259028; now aborting this merge to prevent index corruption\"\n \nI have  checked the code for class SegmentMerger.java and found this check \n\n***********************************************************************************************************************************************************************\nif (4+docCount*8 != fdxFileLength)\n        // This is most likely a bug in Sun JRE 1.6.0_04/_05;\n        // we detect that the bug has struck, here, and\n        // throw an exception to prevent the corruption from\n        // entering the index.  See LUCENE-1282 for\n        // details.\n        throw new RuntimeException(\"mergeFields produced an invalid result: docCount is \" + docCount + \" but fdx file size is \" + fdxFileLength + \"; now aborting this merge to prevent index corruption\");\n}\n***********************************************************************************************************************************************************************\n\nIn our case docCount is 385282378 and fdxFileLength size is 3082259028, even though 4+385282378*8 is equal to 3082259028, the above code will not work because number 3082259028 is out of int range. So type of variable docCount needs to be changed to long\n\nI have written a small test for this \n\n************************************************************************************************************************************************************************\n\npublic class SegmentMergerTest {\npublic static void main(String[] args) {\nint docCount = 385282378; \nlong fdxFileLength = 3082259028L; \nif(4+docCount*8 != fdxFileLength) \nSystem.out.println(\"No Match\" + (4+docCount*8));\nelse \nSystem.out.println(\"Match\" + (4+docCount*8));\n}\n}\n\n************************************************************************************************************************************************************************\n\nAbove test will print No Match but if you change the data type of docCount to long, it will print Match\n\nCan you please advise us if this issue will be fixed in next release?\n\nRegards\nDeepak\n\n\n\n\n\n\n\n \n\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1993",
        "summary": "MoreLikeThis - allow to exclude terms that appear in too many documents (patch included)",
        "description": "The MoreLikeThis class allows to generate a likeness query based on a given document. So far, it is impossible to suppress words from the likeness query, that appear in almost all documents, making it necessary to use extensive lists of stop words.\n\nTherefore I suggest to allow excluding words for which a certain absolute document count or a certain percentage of documents is exceeded. Depending on the corpus of text, words that appear in more than 50 or even 70% of documents can usually be considered insignificant for classifying a document.      ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2913",
        "summary": "Add missing getter methods to NumericField, NumericTokenStream, NumericRangeQuery, NumericRangeFilter",
        "description": "These classes are missing bean-style getter methods for some basic properties. This is inconsistent and should be fixed.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": ""
    },
    {
        "key": "LUCENE-791",
        "summary": "Update the Wiki",
        "description": "The wiki needs updating.  For starters, the URL is still Jakarta.  I think infrastructure needs to be contacted to do this move.  If someone is so inclined, it might be useful to go through and cleanup/organize what is there.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2649",
        "summary": "FieldCache should include a BitSet for matching docs",
        "description": "The FieldCache returns an array representing the values for each doc.  However there is no way to know if the doc actually has a value.\n\nThis should be changed to return an object representing the values *and* a BitSet for all valid docs.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-609",
        "summary": "Lazy field loading breaks backward compat",
        "description": "Document.getField() and Document.getFields() have changed in a non backward compatible manner.\nSimple code like the following no longer compiles:\n Field x = mydoc.getField(\"x\");",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3401",
        "summary": "need to ensure that sims that use collection-level stats (e.g. sumTotalTermFreq) handle non-existent field",
        "description": "Because of things like queryNorm, unfortunately similarities have to handle the case where they are asked to computeStats() for a term, where the field does not exist at all.\n(Note they will never have to actually score anything, but unless we break how queryNorm works for TFIDF, we have to deal with this case).\n\nI noticed this while doing some benchmarking, so i created a test to test some cases like this across all the sims.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1972",
        "summary": "Remove (deprecated) ExtendedFieldCache and Auto/Custom caches and lot's of deprecated sort logic",
        "description": "Remove (deprecated) ExtendedFieldCache and Auto/Custom caches and sort",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1848",
        "summary": "Remove references to older versions of Lucene in \"per-release\" documentation",
        "description": "Some of the documentation that is \"per release\" contains references to older versions, which is often confusing.  This is most noticeable in the file formats docs, but there might be other places too.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1708",
        "summary": "Improve the use of isDeleted in the indexing code",
        "description": "A spin off from here: http://www.nabble.com/Some-thoughts-around-the-use-of-reader.isDeleted-and-hasDeletions-td23931216.html.\nTwo changes:\n# Optimize SegmentMerger work when a reader has no deletions.\n# IndexReader.document() will no longer check if the document is deleted.\n\nWill post a patch shortly",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-372",
        "summary": "Unmatched right parentheses truncates query",
        "description": "The query processor truncates a query when right parentheses are unmatched.\nE.g.:\n\n secret AND illegal) AND access:confidential\n\nwill not result in a ParseException instead will run as:\n\n secret AND illegal",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3454",
        "summary": "rename optimize to a less cool-sounding name",
        "description": "I think users see the name optimize and feel they must do this, because who wants a suboptimal system? but this probably just results in wasted time and resources.\n\nmaybe rename to collapseSegments or something?",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1285",
        "summary": "WeightedSpanTermExtractor incorrectly treats the same terms occurring in different query types",
        "description": "Given a BooleanQuery with multiple clauses, if a term occurs both in a Span / Phrase query, and in a TermQuery, the results of term extraction are unpredictable and depend on the order of clauses. Concequently, the result of highlighting are incorrect.\n\nExample text: t1 t2 t3 t4 t2\nExample query: t2 t3 \"t1 t2\"\nCurrent highlighting: [t1 t2] [t3] t4 t2\nCorrect highlighting: [t1 t2] [t3] t4 [t2]\n\nThe problem comes from the fact that we keep a Map<termText, WeightedSpanTerm>, and if the same term occurs in a Phrase or Span query the resulting WeightedSpanTerm will have a positionSensitive=true, whereas terms added from TermQuery have positionSensitive=false. The end result for this particular term will depend on the order in which the clauses are processed.\n\nMy fix is to use a subclass of Map, which on put() always sets the result to the most lax setting, i.e. if we already have a term with positionSensitive=true, and we try to put() a term with positionSensitive=false, we set the result positionSensitive=false, as it will match both cases.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1851",
        "summary": "'ant javacc' in root project should also properly create contrib/surround Java files",
        "description": "For consistency after LUCENE-1829 which did the same for contrib/queryparser",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1188",
        "summary": "equals and hashCode implementation in org.apache.lucene.search.* package",
        "description": "I would like to talk about the implementation of equals and hashCode method  in org.apache.lucene.search.* package. \n\nExample One:\n\norg.apache.lucene.search.spans.SpanTermQuery (Super Class)\n\t<- org.apache.lucene.search.payloads.BoostingTermQuery (Sub Class)\n\nObservation:\n\n* BoostingTermQuery defines equals but inherits hashCode from SpanTermQuery. Definition of equals is a code clone of SpanTermQuery with a change in class name. \n\nIntention:\n\nI believe the intention of equals redefinition in BoostingTermQuery is not to make the objects of SpanTermQuery and BoostingTermQuery comparable. ie. spanTermQuery.equals(boostingTermQuery) == false && boostingTermQuery.equals(spanTermQuery) == false.\n\n\nProblem:\n\nWith current implementation, the intention might not be respected as a result of symmetric property violation of equals contract i.e.\nspanTermQuery.equals(boostingTermQuery) == true (can be) && boostingTermQuery.equals(spanTermQuery) == false. (always)\n(Note: Provided their state variables are equal)\n\nSolution:\n\nChange implementation of equals in SpanTermQuery from:\n\n{code:title=SpanTermQuery.java|borderStyle=solid}\n  public boolean equals(Object o) {\n    if (!(o instanceof SpanTermQuery))\n      return false;\n    SpanTermQuery other = (SpanTermQuery)o;\n    return (this.getBoost() == other.getBoost())\n      && this.term.equals(other.term);\n  }\n{code}\n\nTo:\n{code:title=SpanTermQuery.java|borderStyle=solid}\n  public boolean equals(Object o) {\n  \tif(o == this) return true;\n  \tif(o == null || o.getClass() != this.getClass()) return false;\n//    if (!(o instanceof SpanTermQuery))\n//      return false;\n    SpanTermQuery other = (SpanTermQuery)o;\n    return (this.getBoost() == other.getBoost())\n      && this.term.equals(other.term);\n  }\n{code}\n\nAdvantage:\n\n* BoostingTermQuery.equals and BoostingTermQuery.hashCode is not needed while still preserving the same intention as before.\n \n* Any further subclassing that does not add new state variables in the extended classes of SpanTermQuery, does not have to redefine equals and hashCode. \n\n* Even if a new state variable is added in a subclass, the symmetric property of equals contract will still be respected irrespective of implementation (i.e. instanceof / getClass) of equals and hashCode in the subclasses.\n\n\nExample Two:\n\n\norg.apache.lucene.search.CachingWrapperFilter (Super Class)\n\t<- org.apache.lucene.search.CachingWrapperFilterHelper (Sub Class)\n\nObservation:\nSame as Example One.\n\nProblem:\nSame as Example one.\n\nSolution:\nChange equals in CachingWrapperFilter from:\n{code:title=CachingWrapperFilter.java|borderStyle=solid}\n  public boolean equals(Object o) {\n    if (!(o instanceof CachingWrapperFilter)) return false;\n    return this.filter.equals(((CachingWrapperFilter)o).filter);\n  }\n{code}\n\nTo:\n{code:title=CachingWrapperFilter.java|borderStyle=solid}\n  public boolean equals(Object o) {\n//    if (!(o instanceof CachingWrapperFilter)) return false;\n    if(o == this) return true;\n    if(o == null || o.getClass() != this.getClass()) return false;\n    return this.filter.equals(((CachingWrapperFilter)o).filter);\n  }\n{code}\n\nAdvantage:\nSame as Example One. Here, CachingWrapperFilterHelper.equals and CachingWrapperFilterHelper.hashCode is not needed.\n\n\nExample Three:\n\norg.apache.lucene.search.MultiTermQuery (Abstract Parent)\n\t<- org.apache.lucene.search.FuzzyQuery (Concrete Sub)\n\t<- org.apache.lucene.search.WildcardQuery (Concrete Sub)\n\nObservation (Not a problem):\n\n* WildcardQuery defines equals but inherits hashCode from MultiTermQuery.\nDefinition of equals contains just super.equals invocation. \n\n* FuzzyQuery has few state variables added that are referenced in its equals and hashCode.\nIntention:\n\nI believe the intention here is not to make objects of FuzzyQuery and WildcardQuery comparable. ie. fuzzyQuery.equals(wildCardQuery) == false && wildCardQuery.equals(fuzzyQuery) == false.\n\nProposed Implementation:\nHow about changing the implementation of equals in MultiTermQuery from:\n\n{code:title=MultiTermQuery.java|borderStyle=solid}\n    public boolean equals(Object o) {\n      if (this == o) return true;\n      if (!(o instanceof MultiTermQuery)) return false;\n\n      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;\n\n      if (!term.equals(multiTermQuery.term)) return false;\n\n      return getBoost() == multiTermQuery.getBoost();\n    }\n{code}\n\nTo:\n{code:title=MultiTermQuery.java|borderStyle=solid}\n    public boolean equals(Object o) {\n      if (this == o) return true;\n//      if (!(o instanceof MultiTermQuery)) return false;\n      if(o == null || o.getClass() != this.getClass()) return false;\n\n      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;\n\n      if (!term.equals(multiTermQuery.term)) return false;\n\n      return getBoost() == multiTermQuery.getBoost();\n    }\n{code}\n\nAdvantage:\n\nSame as above. Here, WildcardQuery.equals is not needed as it does not define any new state. (FuzzyQuery.equals is still needed because FuzzyQuery defines new state.) \n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1745",
        "summary": "Add ability to specify compilation/matching flags to RegexCapabiltiies implementations",
        "description": "The Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.  Therefore, this improvement request is to add the capability to provide those modification flags to either implementation. \n\nI've developed a working implementation that makes minor additions to the existing code. The default constructor is explicitly defined with no arguments, and then a new constructor with an additional \"int flags\" argument is provided. This provides complete backwards compatibility. For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields. These are pass through to the underlying implementation. They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.\n\nProposed changes:\n\nFor the JavaUtilRegexCapabilities.java, the following is the changes made.\n\n  private int flags = 0;\n  \n  // Define the optional flags from Pattern that can be used.\n  // Do this here to keep Pattern contained within this class.\n  \n  public final int FLAG_CANON_EQ = Pattern.CANON_EQ;\n  public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;\n  public final int FLAG_COMMENTS = Pattern.COMMENTS;\n  public final int FLAG_DOTALL = Pattern.DOTALL;\n  public final int FLAG_LITERAL = Pattern.LITERAL;\n  public final int FLAG_MULTILINE = Pattern.MULTILINE;\n  public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;\n  public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;\n  \n  /**\n   * Default constructor that uses java.util.regex.Pattern \n   * with its default flags.\n   */\n  public JavaUtilRegexCapabilities()  {\n    this.flags = 0;\n  }\n  \n  /**\n   * Constructor that allows for the modification of the flags that\n   * the java.util.regex.Pattern will use to compile the regular expression.\n   * This gives the user the ability to fine-tune how the regular expression \n   * to match the functionlity that they need. \n   * The {@link java.util.regex.Pattern Pattern} class supports specifying \n   * these fields via the regular expression text itself, but this gives the caller\n   * another option to modify the behavior. Useful in cases where the regular expression text\n   * cannot be modified, or if doing so is undesired.\n   * \n   * @flags The flags that are ORed together.\n   */\n  public JavaUtilRegexCapabilities(int flags) {\n    this.flags = flags;\n  }\n  \n  public void compile(String pattern) {\n    this.pattern = Pattern.compile(pattern, this.flags);\n  }\n\n\nFor the JakartaRegexpCapabilties.java, the following is changed:\n\n  private int flags = RE.MATCH_NORMAL;\n\n  /**\n   * Flag to specify normal, case-sensitive matching behaviour. This is the default.\n   */\n  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;\n  \n  /**\n   * Flag to specify that matching should be case-independent (folded)\n   */\n  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;\n \n  /**\n   * Contructs a RegexCapabilities with the default MATCH_NORMAL match style.\n   */\n  public JakartaRegexpCapabilities() {}\n  \n  /**\n   * Constructs a RegexCapabilities with the provided match flags.\n   * Multiple flags should be ORed together.\n   * \n   * @param flags The matching style\n   */\n  public JakartaRegexpCapabilities(int flags)\n  {\n    this.flags = flags;\n  }\n  \n  public void compile(String pattern) {\n    regexp = new RE(pattern, this.flags);\n  }\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1725",
        "summary": "Benchmark package uses new TopFieldCollector but also still uses AUTO without resolving it - result is, our sort algorithms won't run",
        "description": "AUTO does not work with TopFieldCollector. If you want to use AUTO with TopFieldCollector, we have a convienence method called detectType on SortField, but it is package protected and so cannot be used here as a stop gap or by users if they wanted to mix AUTO with TopFieldCollector. Lucene does still handle this for back compat internally. Solr got bit here when it was switched to use TopFieldCollector - no auto resolution was added (detectType help couldn't have been used due to visibility), and the result was that plugin code that used to be able to use AUTO would now blow up. You shouldn't use AUTO in Solr anyway though.\n\nThe Benchmark package got bit as well  when it moved to TopFieldCollector. Sort algorithms allowed auto if you specified it, or if you left off the type. Now our sort algs fail because they didn't specify a type.\n\nI'll change to require the type to be specified to get the algs working again. I was thinking of just putting auto resolution in as a stop gap till 3.0 (when auto is removed), but since detectFieldType is package protected and I don't want to repeat it, disallowing auto seems the best way to go.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1939",
        "summary": "IndexOutOfBoundsException at ShingleMatrixFilter's Iterator#hasNext method",
        "description": "I tried to use the ShingleMatrixFilter within Solr. To test the functionality etc., I first used the built-in field analysis view.The filter was configured to be used only at query time analysis with \"_\" as spacer character and a min. and max. shingle size of 2. The generation of the shingles for query strings with this filter seems to work at this view, but by turn on the highlighting of indexed terms that will match the query terms, the exception was thrown. Also, each time I tried to query the index the exception was immediately thrown.\n\nStacktrace:\n{code}\njava.lang.IndexOutOfBoundsException: Index: 1, Size: 1\n\tat java.util.ArrayList.RangeCheck(Unknown Source)\n\tat java.util.ArrayList.get(Unknown Source)\n\tat org.apache.lucene.analysis.shingle.ShingleMatrixFilter$Matrix$1.hasNext(ShingleMatrixFilter.java:729)\n\tat org.apache.lucene.analysis.shingle.ShingleMatrixFilter.next(ShingleMatrixFilter.java:380)\n\tat org.apache.lucene.analysis.StopFilter.next(StopFilter.java:120)\n\tat org.apache.lucene.analysis.TokenStream.next(TokenStream.java:47)\n\t...\n{code}\n\nWithin the hasNext method, there is the {{s-1}}-th Column from the ArrayList {{columns}} requested, but there isn't this entry within columns.\n\nI created a patch that checks, if {{columns}} contains enough entries.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3177",
        "summary": "Decouple indexer from Document/Field impls",
        "description": "I think we should define minimal iterator interfaces,\nIndexableDocument/Field, that indexer requires to index documents.\n\nIndexer would consume only these bare minimum interfaces, not the\nconcrete Document/Field/FieldType classes from oal.document package.\n\nThen, the Document/Field/FieldType hierarchy is one concrete impl of\nthese interfaces. Apps are free to make their own impls as well.\nMaybe eventually we make another impl that enforces a global schema,\neg factored out of Solr's impl.\n\nI think this frees design pressure on our Document/Field/FieldType\nhierarchy, ie, these classes are free to become concrete\nfully-featured \"user-space\" classes with all sorts of friendly sugar\nAPIs for adding/removing fields, getting/setting values, types, etc.,\nbut they don't need substantial extensibility/hierarchy. Ie, the\nextensibility point shifts to IndexableDocument/Field interface.\n\nI think this means we can collapse the three classes we now have for a\nField (Fieldable/AbstracField/Field) down to a single concrete class\n(well, except for LUCENE-2308 where we want to break out dedicated\nclasses for different field types...).\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-971",
        "summary": "Create enwiki indexable data as line-per-article rather than file-per-article",
        "description": "Create a line per article rather than a file. Consume with indexLineFile task.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-746",
        "summary": "Incorrect error message in AnalyzingQueryParser.getPrefixQuery",
        "description": "The error message of  getPrefixQuery is incorrect when tokens were added, for example by a stemmer. The message is \"token was consumed\" even if tokens were added.\nAttached is a patch, which when applied gives a better description of what actually happened.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2109",
        "summary": "Make DocsEnum subclass of DocIdSetIterator",
        "description": "Spinoff from LUCENE-1458:\n\nOne thing I came along long time ago, but now with a new API it get's interesting again: \nDocsEnum should extend DocIdSetIterator, that would make it simplier to use and implement e.g. in MatchAllDocQuery.Scorer, FieldCacheRangeFilter and so on. You could e.g. write a filter for all documents that simply returns the docs enumeration from IndexReader.\n\nSo it should be an abstract class that extends DocIdSetIterator. It has the same methods, only some methods must be a little bit renamed. The problem is, because java does not support multiple inheritace, we cannot also extends attributesource  Would DocIdSetIterator be an interface it would work (this is one of the cases where interfaces for really simple patterns can be used, like iterators).\n\nThe problem with multiple inheritance could be solved by an additional method attributes() that creates a new AttributeSource on first access then (because constructing an AttributeSource is costly).  The same applies for the other *Enums, it should be separated for lazy init.\n\nDocsEnum could look like this:\n\n{code}\npublic abstract class DocsEnum extends DocIdSetIterator {\n  private AttributeSource atts = null;\n  public int freq()\n  public DontKnowClassName positions()\n  public final AttributeSource attributes() {\n   if (atts==null) atts=new AttributeSource();\n   return atts;\n  }\n  ...default impl of the bulk access using the abstract methods from DocIdSetIterator\n}\n{code}\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1668",
        "summary": "Trunk fails tests, FSD.open() - related",
        "description": "    [junit] Testcase: testReadAfterClose(org.apache.lucene.index.TestCompoundFile):\tFAILED\n    [junit] expected readByte() to throw exception\n    [junit] junit.framework.AssertionFailedError: expected readByte() to throw exception\n    [junit] \tat org.apache.lucene.index.TestCompoundFile.demo_FSIndexInputBug(TestCompoundFile.java:345)\n    [junit] \tat org.apache.lucene.index.TestCompoundFile.testReadAfterClose(TestCompoundFile.java:313)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)\n\nThis one is a non-bug, if you ask me. The test should fail on SimpleFSD, but on my system FSD.open() creates MMapD and that one cannot be closed, so the read succeeds.\n\n    [junit] ------------- Standard Output ---------------\n    [junit] Thread[Thread-34,5,main]: exc\n    [junit] java.nio.BufferUnderflowException\n    [junit] \tat java.nio.Buffer.nextGetIndex(Buffer.java:474)\n    [junit] \tat java.nio.DirectByteBuffer.get(DirectByteBuffer.java:229)\n    [junit] \tat org.apache.lucene.store.MMapDirectory$MMapIndexInput.readByte(MMapDirectory.java:67)\n    [junit] \tat org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:36)\n    [junit] \tat org.apache.lucene.store.IndexInput.readInt(IndexInput.java:70)\n    [junit] \tat org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:238)\n    [junit] \tat org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:106)\n    [junit] \tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:699)\n    [junit] \tat org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:126)\n    [junit] \tat org.apache.lucene.index.IndexReader.open(IndexReader.java:374)\n    [junit] \tat org.apache.lucene.index.IndexReader.open(IndexReader.java:260)\n    [junit] \tat org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:76)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing$SearcherThread.doWork(TestStressIndexing.java:109)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing$TimedThread.run(TestStressIndexing.java:52)\n    [junit] NOTE: random seed of testcase 'testStressIndexAndSearching' was: -7374705829444180151\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testStressIndexAndSearching(org.apache.lucene.index.TestStressIndexing):\tFAILED\n    [junit] null\n    [junit] junit.framework.AssertionFailedError\n    [junit] \tat org.apache.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:155)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing.testStressIndexAndSearching(TestStressIndexing.java:178)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)\n\nThis one suceeds sometimes, sometimes (mostly) fails. Is obviously linked with switch to MMapD, but what is the real cause - I don't know.\n\n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: random seed of testcase 'testSetBufferSize' was: 8481546620770090440\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testSetBufferSize(org.apache.lucene.store.TestBufferedIndexInput):\tCaused an ERROR\n    [junit] org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput\n    [junit] java.lang.ClassCastException: org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput\n    [junit] \tat org.apache.lucene.store.TestBufferedIndexInput$MockFSDirectory.tweakBufferSizes(TestBufferedIndexInput.java:226)\n    [junit] \tat org.apache.lucene.store.TestBufferedIndexInput.testSetBufferSize(TestBufferedIndexInput.java:181)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)\n\nBroken assumptions.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1339",
        "summary": "Add IndexReader.acquire() and release() methods using IndexReader's ref counting",
        "description": "From: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3cPine.OSX.4.64.0807170752080.1708@c5850-a3-2-62-147-22-102.dial.proxad.net%3e\n\nI have a server where a bunch of threads are handling search requests. I\nhave a another process that updates the index used by the search server and\nthat asks the searcher server to reopen its index reader after the updates\ncompleted.\n\nWhen I reopen() the index reader, I also close the old one (if the reopen()\nyielded a new instance). This causes problems for the other threads that\nare currently in the middle of a search request.\n\nI'd like to propose the addition of two methods, acquire() and release() \n(attached to this bug report), that increment/decrement the ref count that IndexReader \ninstances currently maintain for related purposes. That ref count prevents \nthe index reader from being actually closed until it reaches zero.\n\nMy server's search threads, thus acquiring and releasing the index reader \ncan be sure that the index reader they're currently using is good until \nthey're done with the current request, ie, until they release() it.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1247",
        "summary": "Unnecessary assert in org.apache.lucene.index.DocumentsWriterThreadState.trimFields()",
        "description": "In org.apache.lucene.index.DocumentsWriterThreadState.trimFields() is the following code:\n\n      if (fp.lastGen == -1) {\n        // This field was not seen since the previous\n        // flush, so, free up its resources now\n\n        // Unhash\n        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n        DocumentsWriterFieldData last = null;\n        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];\n        while(fp0 != fp) {\n          last = fp0;\n          fp0 = fp0.next;\n        }\n        assert fp0 != null;\n\nThe assert at the end is not necessary as fp0 cannot be null.  The first line in the above code guarantees that fp is not null by the time the while loop is hit.  The while loop is exited when fp0 and fp are equal.  Since fp is not null then fp0 cannot be null when the while loop is exited, thus the assert is guaranteed to never occur.\n\nThis was detected by FindBugs.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2736",
        "summary": "Wrong implementation of DocIdSetIterator.advance ",
        "description": "Implementations of {{DocIdSetIterator}} behave differently when advanced is called. Taking the following test for {{OpenBitSet}}, {{DocIdBitSet}} and {{SortedVIntList}} only {{SortedVIntList}} passes the test:\n{code:title=org.apache.lucene.search.TestDocIdSet.java|borderStyle=solid}\n...\n\tpublic void testAdvanceWithOpenBitSet() throws IOException {\n\t\tDocIdSet idSet = new OpenBitSet( new long[] { 1121 }, 1 );  // bits 0, 5, 6, 10\n\t\tassertAdvance( idSet );\n\t}\n\n\tpublic void testAdvanceDocIdBitSet() throws IOException {\n\t\tBitSet bitSet = new BitSet();\n\t\tbitSet.set( 0 );\n\t\tbitSet.set( 5 );\n\t\tbitSet.set( 6 );\n\t\tbitSet.set( 10 );\n\t\tDocIdSet idSet = new DocIdBitSet(bitSet);\n\t\tassertAdvance( idSet );\n\t}\n\n\tpublic void testAdvanceWithSortedVIntList() throws IOException {\n\t\tDocIdSet idSet = new SortedVIntList( 0, 5, 6, 10 );\n\t\tassertAdvance( idSet );\n\t}\t\n\n\tprivate void assertAdvance(DocIdSet idSet) throws IOException {\n\t\tDocIdSetIterator iter = idSet.iterator();\n\t\tint docId = iter.nextDoc();\n\t\tassertEquals( \"First doc id should be 0\", 0, docId );\n\n\t\tdocId = iter.nextDoc();\n\t\tassertEquals( \"Second doc id should be 5\", 5, docId );\n\n\t\tdocId = iter.advance( 5 );\n\t\tassertEquals( \"Advancing iterator should return the next doc id\", 6, docId );\n\t}\n{code}\n\nThe javadoc for {{advance}} says:\n{quote}\nAdvances to the first *beyond* the current whose document number is greater than or equal to _target_.\n{quote}\nThis seems to indicate that {{SortedVIntList}} behaves correctly, whereas the other two don't. \nJust looking at the {{DocIdBitSet}} implementation advance is implemented as:\n{code}\nbitSet.nextSetBit(target);\n{code}\nwhere the docs of {{nextSetBit}} say:\n{quote}\nReturns the index of the first bit that is set to true that occurs *on or after* the specified starting index\n{quote}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1272",
        "summary": "Support for boost factor in MoreLikeThis",
        "description": "This is a patch I made to be able to boost the terms with a specific factor beside the relevancy returned by MoreLikeThis. This is helpful when having more then 1 MoreLikeThis in the query, so words in the field A (i.e. Title) can be boosted more than words in the field B (i.e. Description).",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2676",
        "summary": "TestIndexWriter failes for SimpleTextCodec",
        "description": "I just ran into this failure since SimpleText obviously takes a lot of disk space though.\n\n{noformat}\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter\n    [junit] Testcase: testCommitOnCloseDiskUsage(org.apache.lucene.index.TestIndexWriter):\tFAILED\n    [junit] writer used too much space while adding documents: mid=608162 start=5293 end=634214\n    [junit] junit.framework.AssertionFailedError: writer used too much space while adding documents: mid=608162 start=5293 end=634214\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit] \tat org.apache.lucene.index.TestIndexWriter.testCommitOnCloseDiskUsage(TestIndexWriter.java:1047)\n    [junit] \n    [junit] \n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 3.281 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitOnCloseDiskUsage -Dtests.seed=-7526585723238322940:-1609544650150801239\n    [junit] NOTE: test params are: codec=SimpleText, locale=th_TH, timezone=UCT\n    [junit] ------------- ---------------- ---------------\n    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED\n{noformat}\n\nI did not look into SimpleText but I guess we need either change the threshold for this test or exclude SimpleText from it.\n\nany ideas?",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1401",
        "summary": "Deprecation of autoCommit in 2.4 leads to compile problems, when autoCommit should be false",
        "description": "I am currently changing my code to be most compatible with 2.4. I switched on deprecation warnings and got a warning about the autoCommit parameter in IndexWriter constructors.\n\nMy code *should* use autoCommit=false, so I want to use the new semantics. The default of IndexWriter is still autoCommit=true. My problem now: How to disable autoCommit whithout deprecation warnings?\n\nMaybe, the \"old\" constructors, that are deprecated should use autoCommit=true. But there are new constructors with this \"IndexWriter.MaxFieldLength mfl\" in it, that appear new in 2.4 but are deprecated:\n\nIndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy, IndexWriter.MaxFieldLength mfl) \n          Deprecated. This will be removed in 3.0, when autoCommit will be hardwired to false. Use IndexWriter(Directory,Analyzer,boolean,IndexDeletionPolicy,MaxFieldLength) instead, and call commit() when needed.\n\nWhat the hell is meant by this, a new constructor that is deprecated? And the hint is wrong. If I use the other constructor in the warning, I get autoCommit=true.\n\nThere is something completely wrong.\n\nIt should be clear, which constructors set autoCommit=true, which set it per default to false (perhaps new ones), and the Deprecated text is wrong, if autoCommit does not default to false.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-232",
        "summary": "Version 1.3 reports IOException when re-creating an index",
        "description": "Version: Lucene 1.3 final \nError reported when I am (re-)doing an initialization on the index created \npreviously:\njava.io.IOException: couldn't delete _26a.f1\n\nThe problem disappearred after a re-start of the jvm, some files may be locked \nafter the index writer action !\nProblem does not appear in Version 1.2.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2388",
        "summary": "the unversioned site points to a dead trunk",
        "description": "The unversioned site needs to point to the new merged trunk.\nCurrently it points to the closed-off dead trunk in two different places.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2812",
        "summary": "IndexReader.indexExists sometimes returns true when an index isn't present",
        "description": "If you open a writer on a new dir and prepareCommit but don't finish the commit, IndexReader.indexExists incorrectly returns true, because it just checks for whether a segments_N file is present and not whether it can be successfully read.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-742",
        "summary": "SpanOrQuery.java: simplification and test",
        "description": "The current SpanOrQuery.java has some unnessary attributes. After removing these, I found that there was no existing test for it, so I added some tests to TestSpans.java.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-362",
        "summary": "[PATCH] Extension to binary Fields that allows fixed byte buffer",
        "description": "This is a very simple patch that supports storing binary values in the index\nmore efficiently.  A new Field constructor accepts a length argument, allowing a\nfixed byte[] to be reused acrossed multiple calls with arguments of different\nsizes.  A companion change to FieldsWriter uses this length when storing and/or\ncompressing the field.\n\nThere is one remaining case in Document.  Intentionally, no direct accessor to\nthe length of a binary field is provided from Document, only from Field.  This\nis because Field's created by FieldReader will never have a specified length and\nthis is usual case for Field's read from Document.  It seems less confusing for\nmost users.\n\nI don't believe any upward incompatibility is introduced here (e.g., from the\npossibility of getting a larger byte[] than actually holds the value from\nDocument), since no such byte[] values are possible without this patch anyway.\n\nThe compression case is still inefficient (much copying), but it is hard to see\nhow Lucene can do too much better.  However, the application can do the\ncompression externally and pass in the reused compression-output buffer as a\nbinary value (which is what I'm doing).  This represents a substantialy\nallocation savings for storing large documents bodies (compressed) into the\nLucene index.\n\nTwo patch files are attached, both created by svn on 3/17/05.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3904",
        "summary": "Similarity javadocs look ugly if created with java7's javadoc",
        "description": "The captions used to illustrate the formulas are tables here:\nin jdk 5/6 the table is centered nicely.\n\nBut with java7's javadocs (I think due to some css styles changes?),\nthe table is not centered but instead stretched.\n\nI think we just need to center this table with a different technique?\n\nHave a look at http://people.apache.org/~rmuir/java7-style-javadocs/org/apache/lucene/search/Similarity.html to see what I mean.\n\nNOTE: these javadocs are under TFIDFSimilarity.java in trunk.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3410",
        "summary": "Make WordDelimiterFilter's instantiation more readable",
        "description": "Currently WordDelimiterFilter's constructor is:\n\n{code}\npublic WordDelimiterFilter(TokenStream in,\n\t                             byte[] charTypeTable,\n\t                             int generateWordParts,\n\t                             int generateNumberParts,\n\t                             int catenateWords,\n\t                             int catenateNumbers,\n\t                             int catenateAll,\n\t                             int splitOnCaseChange,\n\t                             int preserveOriginal,\n\t                             int splitOnNumerics,\n\t                             int stemEnglishPossessive,\n\t                             CharArraySet protWords) {\n{code}\n\nwhich means its instantiation is an unreadable combination of 1s and 0s.  \n\nWe should improve this by either using a Builder, 'int flags' or an EnumSet.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3531",
        "summary": "Improve CachingWrapperFilter to optionally also cache acceptDocs, if identical to liveDocs",
        "description": "Spinoff from LUCENE-1536: This issue removed the different cache modes completely and always applies the acceptDocs using BitsFilteredDocIdSet.wrap(), the cache only contains raw DocIdSet without any deletions/acceptDocs. For IndexReaders that are seldom reopened, this might not be as performant as it could be. If the acceptDocs==IR.liveDocs, those DocIdSet could also be cached with liveDocs applied.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1133",
        "summary": "WikipediaTokenizer needs a way of not tokenizing certain parts of the text",
        "description": "It would be nice if the WikipediaTokenizer had a way of, via a flag, leaving categories, links, etc. as single tokens (or at least some parts of them)\n\nThus, if we came across [[Category:My Big Idea]] there would be a way of outputting, as a single token \"My Big Idea\".  \n\nOptionally, it would be good to output both \"My Big Idea\" and the individual tokens as well.\n\nI am not sure of how to do this in JFlex, so any insight would be appreciated.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2167",
        "summary": "Implement StandardTokenizer with the UAX#29 Standard",
        "description": "It would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex. Then its name would actually make sense.\n\nSuch a transition would involve renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims:\n\nbq. This should be a good tokenizer for most European-language documents\n\nThe new StandardTokenizer could then say\n\nbq. This should be a good tokenizer for most languages.\n\nAll the english/euro-centric stuff like the acronym/company/apostrophe stuff can stay with that EuropeanTokenizer, and it could be used by the european analyzers.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2703",
        "summary": "multitermquery scoring differences between 3x and trunk",
        "description": "try this patch with a test, that applies clean to both 3x and trunk, but fails on trunk.\n\nif you modify the test-data-generator to use TopTerms*BoostOnly* rewrite, then it acts like TestFuzzyQuery2, and passes.\n\nSo the problem is in TopTermsScoringBooleanRewrite, or BooleanQuery, or somewhere else.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2234",
        "summary": "Hindi Analyzer",
        "description": "An analyzer for hindi.\n\nbelow are MAP values on the FIRE 2008 test collection.\nQE means expansion with morelikethis, all defaults, on top 5 docs.\n\n||setup||T||T(QE)||TD||TD(QE)||TDN||TDN(QE)||\n|words only|0.1646|0.1979|0.2241|0.2513|0.2468|0.2735|\n|HindiAnalyzer|0.2875|0.3071|0.3387|*0.3791**|0.3837|0.3810|\n|improvement|74.67%|55.18%|51.14%|50.86%|55.47%|39.31%|\n\n* TD was the official measurement, highest score for this collection in FIRE 2008 was 0.3487: http://www.isical.ac.in/~fire/paper/mcnamee-jhu-fire2008.pdf\n\nneeds a bit of cleanup and more tests",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2784",
        "summary": "Change all FilteredTermsEnum impls into TermsEnum decorators",
        "description": "Currently, FilteredTermsEnum has two ctors:\n* FilteredTermsEnum(IndexReader reader, String field)\n* FilteredTermsEnum(TermsEnum tenum)\n\nBut most of our concrete implementations (e.g. TermsRangeEnum) use the IndexReader+field ctor\n\nIn my opinion we should remove this ctor, and switch over all FilteredTermsEnum implementations to just take a TermsEnum.\n\nAdvantages:\n* This simplifies FilteredTermsEnum and its subclasses, where they are more decorator-like (perhaps in the future we could compose them)\n* Removes silly checks such as if (tenum == null) in every next()\n* Allows for consumers to pass in enumerators however they want: e.g. its their responsibility if they want to use MultiFields or not, it shouldnt be buried in FilteredTermsEnum.\n\nI created a quick patch (all core+contrib+solr tests pass), but i think this opens up more possibilities for refactoring improvements that haven't yet been done in the patch: we should explore these too.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-399",
        "summary": "WordListLoader.java should be able to read stopwords from a Reader",
        "description": "WordListLoader should be able to read the stopwords from a Reader.\n\nThis would (for example) allow stopword lists to be stored as a resource in the\njar file of a Lucene application.\n\nDiff is attached.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3358",
        "summary": "StandardTokenizer disposes of Hiragana combining mark dakuten instead of attaching it to the character it belongs to",
        "description": "Lucene 3.3 (possibly 3.1 onwards) exhibits less than great behaviour for tokenising hiragana, if combining marks are in use.\n\nHere's a unit test:\n\n{code}\n    @Test\n    public void testHiraganaWithCombiningMarkDakuten() throws Exception\n    {\n        // Hiragana 'S' following by the combining mark dakuten\n        TokenStream stream = new StandardTokenizer(Version.LUCENE_33, new StringReader(\"\\u3055\\u3099\"));\n\n        // Should be kept together.\n        List<String> expectedTokens = Arrays.asList(\"\\u3055\\u3099\");\n        List<String> actualTokens = new LinkedList<String>();\n        CharTermAttribute term = stream.addAttribute(CharTermAttribute.class);\n        while (stream.incrementToken())\n        {\n            actualTokens.add(term.toString());\n        }\n\n        assertEquals(\"Wrong tokens\", expectedTokens, actualTokens);\n\n    }\n{code}\n\nThis code fails with:\n{noformat}\njava.lang.AssertionError: Wrong tokens expected:<[\u3056]> but was:<[\u3055]>\n{noformat}\n\nIt seems as if the tokeniser is throwing away the combining mark entirely.\n\n3.0's behaviour was also undesirable:\n{noformat}\njava.lang.AssertionError: Wrong tokens expected:<[\u3056]> but was:<[\u3055, \u3099]>\n{noformat}\n\nBut at least the token was there, so it was possible to write a filter to work around the issue.\n\nKatakana seems to be avoiding this particular problem, because all katakana and combining marks found in a single run seem to be lumped into a single token (this is a problem in its own right, but I'm not sure if it's really a bug.)\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1701",
        "summary": "Add NumericField, make plain text numeric parsers public in FieldCache, move trie parsers to FieldCache",
        "description": "In discussions about LUCENE-1673, Mike & me wanted to add a new NumericField to o.a.l.document specific for easy indexing. An alternative would be to add a NumericUtils.newXxxField() factory, that creates a preconfigured Field instance with norms and tf off, optionally a stored text (LUCENE-1699) and the TokenStream already initialized. On the other hand NumericUtils.newXxxSortField could be moved to NumericSortField.\n\nI and Yonik tend to use the factory for both, Mike tends to create the new classes.\n\nAlso the parsers for string-formatted numerics are not public in FieldCache. As the new SortField API (LUCENE-1478) makes it possible to support a parser in SortField instantiation, it would be good to have the static parsers in FieldCache public available. SortField would init its member variable to them (instead of NULL), so making code a lot easier (FieldComparator has this ugly null checks when retrieving values from the cache).\n\nMoving the Trie parsers also as static instances into FieldCache would make the code cleaner and we would be able to hide the \"hack\" StopFillCacheException by making it private to FieldCache (currently its public because NumericUtils is in o.a.l.util).",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3400",
        "summary": "Deprecate / Remove DutchAnalyzer.setStemDictionary",
        "description": "DutchAnalyzer.setStemDictionary(File) prevents reuse of TokenStreams (and also uses a File which isn't ideal).  It should be deprecated in 3x, removed in trunk.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": ""
    },
    {
        "key": "LUCENE-1976",
        "summary": "isCurrent() and getVersion() on an NRT reader are broken",
        "description": "Right now isCurrent() will always return true for an NRT reader and getVersion() will always return the version of the last commit.  This is because the NRT reader holds the live segmentInfos.\n\nI think isCurrent() should return \"false\" when any further changes have occurred with the writer, else true.   This is actually fairly easy to determine, since the writer tracks how many docs & deletions are buffered in RAM and these counters only increase with each change.\n\ngetVersion should return the version as of when the reader was created.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1989",
        "summary": "CharArraySet cannot be made generic, because it violates the Set<char[]> interface",
        "description": "I tried to make CharArraySet using generics (extends AbstractSet<char[]>) but this is not possible, as it e.g. returns sometimes String instances in the Iterator instead of []. Also its addAll method accepts both String and char[]. I think this class is a complete mis-design and violates almost everything (sorry).\n\nWhat to do? Make it Set<?> or just place a big @SuppressWarnings(\"unchecked\"> in front of it?\n\nBecause of this problem also a lot of Set declarations inside StopAnalyzer cannot be made generic as you never know whats inside.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3790",
        "summary": "benchmark cannot parse highlight-vs-vector-highlight.alg, but only on 3.x?!",
        "description": "A new test (TestPerfTasksParse.testParseExamples) was added in LUCENE-3768 that \nguarantees all .alg files in the conf/ directory can actually be parsed...\n\nBut highlight-vs-vector-highlight.alg cannot be parsed on 3.x (NumberFormatException), \nhowever it works fine on trunk... and the .alg is exactly the same in both cases.\n\n{noformat}\n    [junit] ------------- Standard Error -----------------\n    [junit] java.lang.NumberFormatException: For input string: \"maxFrags[3.0],fields[body]\"\n    [junit] \tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1222)\n    [junit] \tat java.lang.Float.parseFloat(Float.java:422)\n    [junit] \tat org.apache.lucene.benchmark.byTask.tasks.SearchTravTask.setParams(SearchTravTask.java:76)\n    [junit] \tat org.apache.lucene.benchmark.byTask.tasks.SearchTravRetVectorHighlightTask.setParams(SearchTravRetVectorHighlightTask.java:124)\n    [junit] \tat org.apache.lucene.benchmark.byTask.utils.Algorithm.<init>(Algorithm.java:112)\n    [junit] \tat org.apache.lucene.benchmark.byTask.TestPerfTasksParse.testParseExamples(TestPerfTasksParse.java:132)\n{noformat}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3801",
        "summary": "Generify FST shortestPaths() to take a comparator",
        "description": "Not sure we should do this, it costs 5-10% performance for WFSTSuggester.\nBut maybe we can optimize something here, or maybe its just no big deal to us.\n\nBecause in general, this could be pretty powerful, e.g. if you needed to store \nsome custom stuff in the suggester, you could use pairoutputs, or whatever.\n\nAnd the possibility we might need shortestPaths for other cool things... at the\nleast I just wanted to have the patch up here.\n\nI haven't tested this on pairoutputs... but i've tested it with e.g. FloatOutputs\nand other things and it works fine.\n\nI tried to minimize the generics violations, there is only 1 (cannot create generic array).\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2664",
        "summary": "Add SimpleText codec",
        "description": "Inspired by Sahin Buyrukbilen's question here:\n\n  http://www.lucidimagination.com/search/document/b68846e383824653/how_to_export_lucene_index_to_a_simple_text_file#b68846e383824653\n\nI made a simple read/write codec that stores all postings data into a\nsingle text file (_X.pst), looking like this:\n\n{noformat}\nfield contents\n  term file\n    doc 0\n      pos 5\n  term is\n    doc 0\n      pos 1\n  term second\n    doc 0\n      pos 3\n  term test\n    doc 0\n      pos 4\n  term the\n    doc 0\n      pos 2\n  term this\n    doc 0\n      pos 0\nEND\n{noformat}\n\nThe codec is fully funtional -- all Lucene & Solr tests pass with\n-Dtests.codec=SimpleText -- but, its performance is obviously poor.\n\nHowever, it should be useful for debugging, transparency,\nunderstanding just what Lucene stores in its index, etc.  And it's a\nquick way to gain some understanding on how a codec works...\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1933",
        "summary": "Provide an convenience AttributeFactory that implements all default attributes with Token",
        "description": "I found some places in contrib tests, where the Token.class was added using addAttributeImpl(). The problem here is, that you cannot be sure, that the attribute is really added and you may fail later (because you only update your local instance). The tests in contrib will partially fail with 3.0 without backwards layer (because the backwards layer uses Token/TokenWrapper internally and copyTo() will work.\n\nThe correct way to achieve this is using an AttributeFactory. The AttributeFactory is currently private in SingleTokenTokenStream. I want to move it to Token.java as a static class / static member. In this case the tests can be rewritten.\n\nI also want to mark addAttributeImpl() as EXPERT, because you must really know whats happening and what are the traps.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1358",
        "summary": "Deadlock for some Query objects in the equals method (f.ex. PhraseQuery) in a concurrent environment",
        "description": "Some Query objects in lucene 2.3.2 (and previous versions) have internal variables using Vector.   These variables are used during the call to the equals method.   In a concurrent environment a deadlock might occur.    The attached code example shows this happening in lucene 2.3.2, but the patch in LUCENE-1346 fixes this issue (though that doesn't seem to be the intention of that patch according to the description :-)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1885",
        "summary": "NativeFSLockFactory.makeLock(...).isLocked() does not work",
        "description": "IndexWriter.isLocked() or IndexReader.isLocked() do not work with NativeFSLockFactory.\n\nThe problem is, that the method NativeFSLock.isLocked() just checks if the same lock instance was locked before (lock != null). If the LockFactory created a new lock instance, this always returns false, even if its locked.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3899",
        "summary": "Evil up MockDirectoryWrapper.checkIndexOnClose",
        "description": "MockDirectoryWrapper checks any indexes tests create on close(), if they exist.\n\nThe problem is the logic it uses to determine if an index exists could mask real bugs (e.g. segments file corrumption):\n{code}\nif (DirectoryReader.indexExists(this) {\n  ...\n  // evil stuff like crash()\n  ...\n  _TestUtil.checkIndex(this)\n}\n{code}\n\nand for reference DirectoryReader.indexExists is:\n{code}\ntry {\n  new SegmentInfos().read(directory);\n  return true;\n} catch (IOException ioe) {\n  return false;\n}\n{code}\n\nSo if there are segments file problems, we just silently do no checkIndex.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-720",
        "summary": "Unit tests TestBackwardsCompatibility and TestIndexFileDeleter might fail depending on JVM",
        "description": "In the two units tests TestBackwardsCompatibility and TestIndexFileDeleter several index file names are hardcoded. For example, in TestBackwardsCompatibility.testExactFileNames() it is tested if the index directory contains exactly the expected files after several operations like addDocument(), deleteDocument() and setNorm() have been performed. Apparently the unit tests pass on the nightly build machine, but in my environment (Windows XP, IBM JVM 1.5) they fail for the following reason:\n\nWhen IndexReader.setNorm() is called a new norm file for the specified field is created with the file  ending .sx, where x is the number of the field. The problem is that the SegmentMerger can not guarantee to keep the order of the fields, in other words after a merge took place a field can have a different field number. This specific testcase fails, because it expects the file ending .s0, but the file has the ending .s1.\n\nThe reason why the field numbers can be different on different JVMs is the use of HashSet in SegmentReader.getFieldNames(). Depending on the HashSet implementation an iterator might not iterate over the entries in insertion order. When I change HashSet to LinkedHashSet, the two testcases pass.\n\nHowever, even with a LinkedHashSet the order of the field numbers might change during a merge, because the order in which the SegmentMerger merges the FieldInfos depends on the field options like TERMVECTOR, INDEXED... (see SegmentMerger.mergeFields() for details). \n\nSo I think we should not use LinkedHashSet but rather change the problematic testcases. Furthermore I'm not sure if we should have hardcoded filenames in the tests anyway, because if we change the index format or file names in the future these test cases would fail without modification.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2847",
        "summary": "Support all of unicode in StandardTokenizer",
        "description": "StandardTokenizer currently only supports the BMP.\n\nIf it encounters characters outside of the BMP, it just discards them... \nit should instead implement fully implement UAX#29 across all of unicode.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3307",
        "summary": "don't require an analyzer, if all fields are NOT_ANALYZED",
        "description": "This seems wierd, if you analyze only NOT_ANALYZED fields, you must have an analyzer (null will not work)\nbecause documentsinverter wants it for things like offsetGap",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1370",
        "summary": "Add ShingleFilter option to output unigrams if no shingles can be generated",
        "description": "Currently if ShingleFilter.outputUnigrams==false and the underlying token stream is only one token long, then ShingleFilter.next() won't return any tokens. This patch provides a new option, outputUnigramIfNoNgrams; if this option is set and the underlying stream is only one token long, then ShingleFilter will return that token, regardless of the setting of outputUnigrams.\n\nMy use case here is speeding up phrase queries. The technique is as follows:\n\nFirst, doing index-time analysis using ShingleFilter (using outputUnigrams==true), thereby expanding things as follows:\n\n\"please divide this sentence into shingles\" ->\n \"please\", \"please divide\"\n \"divide\", \"divide this\"\n \"this\", \"this sentence\"\n \"sentence\", \"sentence into\"\n \"into\", \"into shingles\"\n \"shingles\"\n\nSecond, do query-time analysis using ShingleFilter (using outputUnigrams==false and outputUnigramIfNoNgrams==true). If the user enters a phrase query, it will get tokenized in the following manner:\n\n\"please divide this sentence into shingles\" ->\n \"please divide\"\n \"divide this\"\n \"this sentence\"\n \"sentence into\"\n \"into shingles\"\n\nBy doing phrase queries with bigrams like this, I can gain a very considerable speedup. Without the outputUnigramIfNoNgrams option, then a single word query would tokenize like this:\n\n\"please\" ->\n   [no tokens]\n\nBut thanks to outputUnigramIfNoNgrams, single words will now tokenize like this:\n\n\"please\" ->\n  \"please\"\n\n****\n\nThe patch also adds a little to the pre-outputUnigramIfNoNgrams option tests.\n\n****\n\nI'm not sure if the patch in this state is useful to anyone else, but I thought I should throw it up here and try to find out.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1549",
        "summary": "Strengthen CheckIndex a bit",
        "description": "A few small improvements to CheckIndex to detect possible \"docs out of order\" cases.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3442",
        "summary": "QueryWrapperFilter gets null DocIdSetIterator when wrapping TermQuery",
        "description": "If you try to get the iterator for the DocIdSet returned by a QueryWrapperFilter which wraps a TermQuery you get null instead of an iterator that returns the same documents as the search on the TermQuery.\n\nCode demonstrating the issue:\n\n{code:java}\nimport java.io.IOException;\nimport org.apache.lucene.analysis.WhitespaceAnalyzer;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.document.Field.Index;\nimport org.apache.lucene.document.Field.Store;\nimport org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.index.IndexWriterConfig;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.store.RAMDirectory;\nimport org.apache.lucene.util.Version;\nimport org.apache.lucene.search.DocIdSet;\nimport org.apache.lucene.search.DocIdSetIterator;\nimport org.apache.lucene.search.Filter;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.QueryWrapperFilter;\nimport org.apache.lucene.search.TermQuery;\nimport org.apache.lucene.search.TopDocs;\n\npublic class TestQueryWrapperFilterIterator {\n   public static void main(String[] args) {\n\t\ttry {\n\t\t\tIndexWriterConfig iwconfig = new IndexWriterConfig(Version.LUCENE_34, new WhitespaceAnalyzer(Version.LUCENE_34));\n\t\t\tiwconfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE);\n\t\t\tRAMDirectory dir = new RAMDirectory();\n\t\t\n\t\t\tIndexWriter writer = new IndexWriter(dir, iwconfig);\n\t\t\tDocument d = new Document();\n\t\t\td.add(new Field(\"id\", \"1001\", Store.YES, Index.NOT_ANALYZED));\n\t\t\td.add(new Field(\"text\", \"headline one group one\", Store.YES, Index.ANALYZED));\n\t\t\td.add(new Field(\"group\", \"grp1\", Store.YES, Index.NOT_ANALYZED));\n\t\t    writer.addDocument(d);\n\t\t\twriter.commit();\n\t\t\twriter.close();\n\t\t\t\n\t\t\tIndexReader rdr = IndexReader.open(dir);\n\t\t\tIndexSearcher searcher = new IndexSearcher(rdr);\n\t\t\t\n\t\t\tTermQuery tq = new TermQuery(new Term(\"text\", \"headline\"));\n\t\t\t\n\t\t\tTopDocs results = searcher.search(tq, 5);\n\t\t\tSystem.out.println(\"Number of search results: \" + results.totalHits);\n\t\t\t\n\t\t\tFilter f = new QueryWrapperFilter(tq);\n\t\t\t\n\t\t\tDocIdSet dis = f.getDocIdSet(rdr);\n\t\t\t\n\t\t\tDocIdSetIterator it = dis.iterator();\n\t\t\tif (it != null) {\n\t\t\t\tint docId = it.nextDoc();\n\t\t\t\twhile (docId != DocIdSetIterator.NO_MORE_DOCS) {\n\t\t\t\t\tDocument doc = rdr.document(docId);\n\t\t\t\t\tSystem.out.println(\"Iterator doc: \" + doc.get(\"id\"));\n\t\t\t\t\tdocId = it.nextDoc();\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tSystem.out.println(\"Iterator was null: \");\n\t\t\t}\n\t\t\t\n\t\t\tsearcher.close();\n\t\t\trdr.close();\n\t\t} catch (IOException ioe) {\n\t\t\tioe.printStackTrace();\n\t\t}\n\n\t}\n}\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3727",
        "summary": "fix assertions/checks that use File.length() to use getFilePointer()",
        "description": "This came up on this thread \"Getting RuntimeException: after flush: fdx size mismatch while Indexing\" \n(http://www.lucidimagination.com/search/document/a8db01a220f0a126)\n\nIn trunk, a side effect of the codec refactoring is that these assertions were pushed into codecs as finish() before close().\nthey check getFilePointer() instead in this computation, which checks that lucene did its part (instead of falsely tripping if directory metadata is stale).\n\nI think we should fix these checks/asserts on 3.x too\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3363",
        "summary": "minimizeHopcroft OOMEs on smallish (2096 states, finite) automaton",
        "description": "Not sure what's up w/ this... if you check out the blocktree branch (LUCENE-3030) and comment out the @Ignore in TestTermsEnum2.testFiniteVersusInfinite then this should hit OOME: {[ant test-core -Dtestcase=TestTermsEnum2 -Dtestmethod=testFiniteVersusInfinite -Dtests.seed=-2577608857970454726:-2463580050179334504}}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2245",
        "summary": "Remaining contrib testcases should use Version based ctors instead of deprecated ones",
        "description": "Many testcases in contrib use deprecated ctors for WhitespaceTokenizer / Analyzer etc.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2079",
        "summary": "Further improvements to contrib/benchmark for testing NRT",
        "description": "Some small changes:\n\n  * Allow specifying a priority for BG threads, after the \"&\"\n    character; priority increment is + or - int that's added to main\n    thread's priority to set child thread's.  For my NRT tests I make\n    the reopen thread +2, the indexing threads +1, and leave searching\n    threads at their default.\n\n  * Added test case\n\n  * NearRealTimeReopenTask now reports @ the end the full array of\n    msec of each reopen latency\n\n  * Added optional breakout of counts by time steps.  If you set\n    log.time.step.msec to eg 1000 then reported counts for serial task\n    sequence is broken out by 1 second windows.  EG you can use this\n    to measure slowdown over time.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-732",
        "summary": "Support DateTools in QueryParser",
        "description": "The QueryParser currently uses the deprecated class DateField to create RangeQueries with date values. However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated. In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.\n\nThis patch replaces the use of DateField in QueryParser by DateTools. Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:\n\n  /**\n   * Sets the default date resolution used by RangeQueries for fields for which no\n   * specific date resolutions has been set. Field specific resolutions can be set\n   * with {@link #setDateResolution(String, DateTools.Resolution)}.\n   *  \n   * @param dateResolution the default date resolution to set\n   */\n  public void setDateResolution(DateTools.Resolution dateResolution);\n  \n  /**\n   * Sets the date resolution used by RangeQueries for a specific field.\n   *  \n   * @param field field for which the date resolution is to be set \n   * @param dateResolution date resolution to set\n   */\n  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);\n\n(I also added the corresponding getter methods).\n\nNow the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.\nThe initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY. \n\nPlease let me know if you think we should use a different resolution as default.\n\nI extended TestQueryParser to test this new feature.\n\nAll unit tests pass.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2759",
        "summary": "We should never open an IndexInput when an IndexOutput is still open",
        "description": "I modified MockDirWrapper to assert this (except for\nsegments_N/segments.gen, where it's expected), and, it uncovered a\ncouple of places involving NRT readers where we open a shared doc\nstore file that's still open for writing.\n\nFirst, if you install a merged segment warmer, we were failing to\nforce the merge of the doc stores in this case, thus potentially\nopening the same doc stores that are also still open for writing.\n\nSecond, if you're actively adding docs in other threads when you call\nIW.getReader(), the other threads could sneak in and flush new\nsegments sharing the doc stores.  The returned reader then opens the\ndoc store files that are still open for writing.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2616",
        "summary": "FastVectorHighlighter: out of alignment when the first value is empty in multiValued field",
        "description": "",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3752",
        "summary": "move preflexrw to lucene3x package",
        "description": "Currently there are a lot of things made public in lucene3x codec, but all marked internal/experimental/deprecated.\n\nA lot of this is just so our test codec (preflexrw) can subclass it. I think we should just move it to the same\npackage, then it call all be package-private.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2895",
        "summary": "MockRandomCodec loads termsIndex even if termsIndexDivisor is set to -1",
        "description": "When working on LUCENE-2891 (on trunk), I found out that if MockRandomCodec is used, then setting IWC.readerTermsIndexDivisor to -1 allows seeking e.g., termDocs, when it shouldn't. Other Codecs fail to seek, as expected by the test. We need to find out why MockRandomCodec does not fail as expected.\n\nTo verify that, run \"ant test-core -Dtestcase=TestIndexWriterReader -Dtestmethod=testNoTermsIndex -Dtests.codec=MockRandom\", but comment out the line which adds MockRandom to the list of illegal codecs in the test.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1071",
        "summary": "SegmentMerger doesn't set payload bit in new optimized code",
        "description": "In the new optimized code in SegmentMerger the payload bit is not set correctly\nin the merged segment. This means that we loose all payloads during a merge!\n\nThe Payloads unit test doesn't catch this. Now that we have the new\nDocumentsWriter we buffer much more docs by default then before. This means\nthat the test cases can't assume anymore that the DocsWriter flushes after 10\ndocs by default. TestPayloads however falsely assumed this, which means that no\nmerges happen anymore in TestPayloads. We should check whether there are\nother testcases that rely on this.\n\nThe fixes for TestPayloads and SegmentMerger are very simple, I'll attach a patch\nsoon.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3268",
        "summary": "TestScoredDocIDsUtils.testWithDeletions test failure",
        "description": "ant test -Dtestcase=TestScoredDocIDsUtils -Dtestmethod=testWithDeletions -Dtests.seed=-2216133137948616963:2693740419732273624 -Dtests.multiplier=5\n\nIn general, on both 3.x and trunk, if you run this test with -Dtests.iter=100 it tends to fail 2% of the time.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-654",
        "summary": "GData-Server - Website sandbox part",
        "description": "Added GData-Server to the sandbox part of the website -- xdocs/sandbox/\n\nBuild of website is fine.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-1256",
        "summary": "Changes.html formatting improvements",
        "description": "Some improvements to the Changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task:\n\n# Simplified the Simple stylesheet (removed monospace font specification) and made it the default.  The Fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but IMHO it's ugly).\n# Moved the monospace style from the Simple stylesheet to a new stylesheet named \"Fixed Width\"\n# Fixed syntax errors in the Fancy stylesheet, so that it displays as intended.\n# Added <span style=\"attrib\">  to change attributions.\n# In the Fancy and Simple stylesheets, change attributions are colored dark green.\n# Now properly handling change attributions in CHANGES.txt that have trailing periods.\n# Clicking on an anchor to expand its children now changes the document location to show the children.\n# Hovering over anchors now causes a tooltip to be displayed - either \"Click to expand\" or \"Click to collapse\" - the tooltip changes appropriately after a collapse or expansion.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1994",
        "summary": "EnwikiConentSource does not work with parallel tasks",
        "description": "",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1677",
        "summary": "Remove GCJ IndexReader specializations",
        "description": "These specializations are outdated, unsupported, most probably pointless due to the speed of modern JVMs and, I bet, nobody uses them (Mike, you said you are going to ask people on java-user, anybody replied that they need it?). While giving nothing, they make SegmentReader instantiation code look real ugly.\n\nIf nobody objects, I'm going to post a patch that removes these from Lucene.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2297",
        "summary": "IndexWriter should let you optionally enable reader pooling",
        "description": "For apps using a large index and frequently need to commit and resolve deletes, the cost of opening the SegmentReaders on demand for every commit can be prohibitive.\n\nWe an already pool readers (NRT does so), but, we only turn it on if NRT readers are in use.\n\nWe should allow separate control.\n\nWe should do this after LUCENE-2294.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3196",
        "summary": "Optimize FixedStraightBytes for bytes size == 1",
        "description": "Currently we read all the bytes in a PagedBytes instance wich is unneeded for single byte values like norms. For fast access this should simply be a straight array.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1049",
        "summary": "Simple toString() for BooleanFilter",
        "description": "While working with BooleanFilter I wanted a basic toString() for debugging.\n\nThis is what I came up.  It works ok for me.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2887",
        "summary": "Remove/deprecate IndexReader.undeleteAll",
        "description": "This API is rather dangerous in that it's \"best effort\" since it can only un-delete docs that have not yet been merged away, or, dropped (as of LUCENE-2010).\n\nGiven that it exposes impl details of how Lucene prunes deleted docs, I think we should remove this API.\n\nAre there legitimate use cases....?",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3284",
        "summary": "Move contribs/modules away from QueryParser dependency",
        "description": "Some contribs and modules depend on the core QueryParser just for simplicity in their tests.  We should apply the same process as I did to the core tests, and move them away from using the QueryParser where possible.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-3351",
        "summary": "DirectSpellChecker throws NPE if field doesn't exist",
        "description": "DirectSpellchecker doesn't check that the resulting Terms is null,\nit should return an empty list here.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1633",
        "summary": "Copy/Paste-Typo in toString() for SpanQueryFilter",
        "description": "   public String toString() {\n-    return \"QueryWrapperFilter(\" + query + \")\";\n+    return \"SpanQueryFilter(\" + query + \")\";\n   }\n\nsays it all.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1812",
        "summary": "Static index pruning by in-document term frequency (Carmel pruning)",
        "description": "This module provides tools to produce a subset of input indexes by removing postings data for those terms where their in-document frequency is below a specified threshold. The net effect of this processing is a much smaller index that for common types of queries returns nearly identical top-N results as compared with the original index, but with increased performance. \n\nOptionally, stored values and term vectors can also be removed. This functionality is largely independent, so it can be used without term pruning (when term freq. threshold is set to 1).\n\nAs the threshold value increases, the total size of the index decreases, search performance increases, and recall decreases (i.e. search quality deteriorates). NOTE: especially phrase recall deteriorates significantly at higher threshold values. \n\nPrimary purpose of this class is to produce small first-tier indexes that fit completely in RAM, and store these indexes using IndexWriter.addIndexes(IndexReader[]). Usually the performance of this class will not be sufficient to use the resulting index view for on-the-fly pruning and searching. \n\nNOTE: If the input index is optimized (i.e. doesn't contain deletions) then the index produced via IndexWriter.addIndexes(IndexReader[]) will preserve internal document id-s so that they are in sync with the original index. This means that all other auxiliary information not necessary for first-tier processing, such as some stored fields, can also be removed, to be quickly retrieved on-demand from the original index using the same internal document id. \n\nThreshold values can be specified globally (for terms in all fields) using defaultThreshold parameter, and can be overriden using per-field or per-term values supplied in a thresholds map. Keys in this map are either field names, or terms in field:text format. The precedence of these values is the following: first a per-term threshold is used if present, then per-field threshold if present, and finally the default threshold.\n\nA command-line tool (PruningTool) is provided for convenience. At this moment it doesn't support all functionality available through API.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2322",
        "summary": "Remove verbosity from tests and make configureable",
        "description": "The parent issue added the functionality to LuceneTestCase(J4), this patch applies it to most tests.",
        "label": "NUG",
        "classified": "TEST",
        "type": ""
    },
    {
        "key": "LUCENE-2320",
        "summary": "Add MergePolicy to IndexWriterConfig",
        "description": "Now that IndexWriterConfig is in place, I'd like to move MergePolicy to it as well. The change is not straightforward and so I've kept it for a separate issue. MergePolicy requires in its ctor an IndexWriter, however none can be passed to it before an IndexWriter actually exists. And today IW may create an MP just for it to be overridden by the application one line afterwards. I don't want to make iw member of MP non-final, or settable by extending classes, however it needs to remain protected so they can access it directly. So the proposed changes are:\n\n* Add a SetOnce object (to o.a.l.util), or Immutable, which can only be set once (hence its name). It'll have the signature SetOnce<T> w/ *synchronized set<T>* and *T get()*. T will be declared volatile, so that get() won't be synchronized.\n* MP will define a *protected final SetOnce<IndexWriter> writer* instead of the current writer. *NOTE: this is a bw break*. any suggestions are welcomed.\n* MP will offer a public default ctor, together with a set(IndexWriter).\n* IndexWriter will set itself on MP using set(this). Note that if set will be called more than once, it will throw an exception (AlreadySetException - or does someone have a better suggestion, preferably an already existing Java exception?).\n\nThat's the core idea. I'd like to post a patch soon, so I'd appreciate your review and proposals.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3182",
        "summary": "TestAddIndexes reproducible test failure on turnk",
        "description": "trunk: r1133385\n\n{code}\n    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes\n    [junit] Tests run: 2843, Failures: 1, Errors: 0, Time elapsed: 137.121 sec\n    [junit]\n    [junit] ------------- Standard Output ---------------\n    [junit] java.io.FileNotFoundException: _cy.fdx\n    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)\n    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)\n    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)\n    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)\n    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)\n    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)\n    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)\n    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)\n    [junit] java.io.FileNotFoundException: _cx.fdx\n    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)\n    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)\n    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)\n    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)\n    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)\n    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)\n    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)\n    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=SimpleText, content=SimpleText, d=MockRandom, c=SimpleText}, locale=fr, timezone=Africa/Kigali\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestAddIndexes]\n    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=68050392,total=446234624\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):       FAILED\n    [junit]\n    [junit] junit.framework.AssertionFailedError:\n    [junit]     at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:932)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.index.TestAddIndexes FAILED\n{code}\n\n\nFails randomly in my while(1) test run, and Fails after a few min of running: \n{code}\nant test -Dtestcase=TestAddIndexes -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3 -Dtests.iter=200 -Dtests.iter.min=1\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3398",
        "summary": "TestCompoundFile fails on windows",
        "description": "ant test-core -Dtestcase=TestCompoundFile -Dtestmethod=testReadNestedCFP -Dtests.seed=-61cb66ec0d71d1ac:-46685c36ec38fd32:568c63299214892c",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1148",
        "summary": "Create a new sub-class of SpanQuery to enable use of a RangeQuery within a SpanQuery",
        "description": "Our users express queries using a syntax which enables them to embed various query types within SpanQuery instances.  One feature they've been asking for is the ability to embed a numeric range query so they could, for example, find documents matching \"[2.0 2.75]MHz\".  The attached patch adds the capability and I hope others will find it useful.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1028",
        "summary": "Weight is not serializable for some of the queries",
        "description": "In order to work with ParallelMultiSearcher, Query weights need to be serializable.  The interface Weight extends java.io.Serializable, but it appears that some of the newer queries unnecessarily store global state in their weights, thus causing serialization errors.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3030",
        "summary": "Block tree terms dict & index",
        "description": "Our default terms index today breaks terms into blocks of fixed size\n(ie, every 32 terms is a new block), and then we build an index on top\nof that (holding the start term for each block).\n\nBut, it should be better to instead break terms according to how they\nshare prefixes.  This results in variable sized blocks, but means\nwithin each block we maximize the shared prefix and minimize the\nresulting terms index.  It should also be a speedup for terms dict\nintensive queries because the terms index becomes a \"true\" prefix\ntrie, and can be used to fast-fail on term lookup (ie returning\nNOT_FOUND without having to seek/scan a terms block).\n\nHaving a true prefix trie should also enable much faster intersection\nwith automaton (but this will be a new issue).\n\nI've made an initial impl for this (called\nBlockTreeTermsWriter/Reader).  It's still a work in progress... lots\nof nocommits, and hairy code, but tests pass (at least once!).\n\nI made two new codecs, temporarily called StandardTree, PulsingTree,\nthat are just like their counterparts but use this new terms dict.\n\nI added a new \"exactOnly\" boolean to TermsEnum.seek.  If that's true\nand the term is NOT_FOUND, we will (quickly) return NOT_FOUND and the\nenum is unpositioned (ie you should not call next(), docs(), etc.).\n\nIn this approach the index and dict are tightly connected, so it does\nnot support a pluggable index impl like BlockTermsWriter/Reader.\nBlocks are stored on certain nodes of the prefix trie, and can contain\nboth terms and pointers to sub-blocks (ie, if the block is not a leaf\nblock).  So there are two trees, tied to one another -- the index\ntrie, and the blocks.  Only certain nodes in the trie map to a block\nin the block tree.\n\nI think this algorithm is similar to burst tries\n(http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499),\nexcept it allows terms to be stored on inner blocks (not just leaf\nblocks).  This is important for Lucene because an [accidental]\n\"adversary\" could produce a terms dict with way too many blocks (way\ntoo much RAM used by the terms index).  Still, with my current patch,\nan adversary can produce too-big blocks... which we may need to fix,\nby letting the terms index not be a true prefix trie on it's leaf\nedges.\n\nExactly how the blocks are picked can be factored out as its own\npolicy (but I haven't done that yet).  Then, burst trie is one policy,\nmy current approach is another, etc.  The policy can be tuned to\nthe terms' expected distribution, eg if it's a primary key field and\nyou only use base 10 for each character then you want block sizes of\nsize 10.  This can make a sizable difference on lookup cost.\n\nI modified the FST Builder to allow for a \"plugin\" that freezes the\n\"tail\" (changed suffix) of each added term, because I use this to find\nthe blocks.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3700",
        "summary": "optionally support naist-jdic for kuromoji",
        "description": "This is an alternative dictionary, somewhat larger (~25%).\n\nwe can support it in build.xml so if a user wants to build with it, they can (the resulting jar file will be 500KB larger)",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3492",
        "summary": "Extract a generic framework for running randomized tests.",
        "description": "{color:red}The work on this issue is temporarily at github{color} (lots of experiments and tweaking):\nhttps://github.com/carrotsearch/randomizedtesting\nOr directly: git clone git://github.com/carrotsearch/randomizedtesting.git\n{color}\n----\n\nRandomizedRunner is a JUnit runner, so it is capable of running @Test-annotated test cases. It\nrespects regular lifecycle hooks such as @Before, @After, @BeforeClass or @AfterClass, but it\nalso adds the following:\n\nRandomized, but repeatable execution and infrastructure for dealing with randomness:\n\n- uses pseudo-randomness (so that a given run can be repeated if given the same starting seed)\n  for many things called \"random\" below,\n- randomly shuffles test methods to ensure they don't depend on each other,\n- randomly shuffles hooks (within a given class) to ensure they don't depend on each other,\n- base class RandomizedTest provides a number of methods for generating random numbers, strings\n  and picking random objects from collections (again, this is fully repeatable given the\n  initial seed if there are no race conditions),\n- the runner provides infrastructure to augment stack traces with information about the initial\n  seeds used for running the test, so that it can be repeated (or it can be determined that\n  the test is not repeatable -- this indicates a problem with the test case itself).\n\nThread control:\n\n- any threads created as part of a test case are assigned the same initial random seed \n  (repeatability),\n- tracks and attempts to terminate any Threads that are created and not terminated inside \n  a test case (not cleaning up causes a test failure),\n- tracks and attempts to terminate test cases that run for too long (default timeout: 60 seconds,\n  adjustable using global property or annotations),\n\nImproved validation and lifecycle support:\n\n- RandomizedRunner uses relaxed contracts of hook methods' accessibility (hook methods _can_ be\n  private). This helps in avoiding problems with method shadowing (static hooks) or overrides\n  that require tedious super.() chaining). Private hooks are always executed and don't affect\n  subclasses in any way, period.\n- @Listeners annotation on a test class allows it to hook into the execution progress and listen\n  to events.\n- @Validators annotation allows a test class to provide custom validation strategies \n  (project-specific). For example a base class can request specific test case naming strategy\n  (or reject JUnit3-like methods, for instance).\n- RandomizedRunner does not \"chain\" or \"suppress\" exceptions happening during execution of \n  a test case (including hooks). All exceptions are reported as soon as they happened and multiple\n  failure reports can occur. Most environments we know of then display these failures sequentially\n  allowing a clearer understanding of what actually happened first.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2015",
        "summary": "ASCIIFoldingFilter: expose folding logic + small improvements to ISOLatin1AccentFilter",
        "description": "This patch adds a couple of non-ascii chars to ISOLatin1AccentFilter (namely: left & right single quotation marks, en dash, em dash) which we very frequently encounter in our projects. I know that this class is now deprecated; this improvement is for legacy code that hasn't migrated yet.\n\nIt also enables easy access to the ascii folding technique use in ASCIIFoldingFilter for potential re-use in non-Lucene-related code.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1900",
        "summary": "Confusing Javadoc in Searchable.java",
        "description": "In Searchable.java, the javadoc for maxdoc() is:\n\n  /** Expert: Returns one greater than the largest possible document number.\n   * Called by search code to compute term weights.\n   * @see org.apache.lucene.index.IndexReader#maxDoc()\n\nThe qualification \"expert\" and the statement \"called by search code to compute term weights\" is a bit confusing, It implies that maxdoc() somehow computes weights, which is obviously not true (what it does is explained in the other sentence). Maybe it is used as one factor of the weight, but do we really need to mention this here? ",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-652",
        "summary": "Compressed fields should be \"externalized\" (from Fields into Document)",
        "description": "Right now, as of 2.0 release, Lucene supports compressed stored fields.  However, after discussion on java-dev, the suggestion arose, from Robert Engels, that it would be better if this logic were moved into the Document level.  This way the indexing level just stores opaque binary fields, and then Document handles compress/uncompressing as needed.\n\nThis approach would have prevented issues like LUCENE-629 because merging of segments would never need to decompress.\n\nSee this thread for the recent discussion:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/38836\n\nWhen we do this we should also work on related issue LUCENE-648.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1429",
        "summary": "close() throws incorrect IllegalStateEx after IndexWriter hit an OOME when autoCommit is true",
        "description": "Spinoff from http://www.nabble.com/IllegalStateEx-thrown-when-calling-close-to20201825.html\n\nWhen IndexWriter hits an OOME, it records this and then if close() is\ncalled it calls rollback() instead.  This is a defensive measure, in\ncase the OOME corrupted the internal buffered state (added/deleted\ndocs).\n\nBut there's a bug: if you opened IndexWriter with autoCommit true,\nclose() then incorrectly throws an IllegalStatException.\n\nThis fix is simple: allow rollback to be called even if autoCommit is\ntrue, internally during close.  (External calls to rollback with\nautoCommmit true is still not allowed).\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-707",
        "summary": "Lucene Java Site docs",
        "description": "It would be really nice if the Java site docs where consistent with the rest of the Lucene family (namely, with navigation tabs, etc.) so that one can easily go between Nutch, Hadoop, etc.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1844",
        "summary": "Speed up junit tests",
        "description": "As Lucene grows, so does the number of JUnit tests. This is obviously a good thing, but it comes with longer and longer test times. Now that we also run back compat tests in a standard test run, this problem is essentially doubled.\n\nThere are some ways this may get better, including running parallel tests. You will need the hardware to fully take advantage, but it should be a nice gain. There is already an issue for this, and Junit 4.6, 4.7 have the beginnings of something we might be able to count on soon. 4.6 was buggy, and 4.7 still doesn't come with nice ant integration. Parallel tests will come though.\n\nBeyond parallel testing, I think we also need to concentrate on keeping our tests lean. We don't want to sacrifice coverage or quality, but I'm sure there is plenty of fat to skim.\n\nI've started making a list of some of the longer tests - I think with some work we can make our tests much faster - and then with parallelization, I think we could see some really great gains.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3445",
        "summary": "Add SearcherManager, to manage IndexSearcher usage across threads and reopens",
        "description": "This is a simple helper class I wrote for Lucene in Action 2nd ed.\nI'd like to commit under Lucene (contrib/misc).\n\nIt simplifies using & reopening an IndexSearcher across multiple\nthreads, by using IndexReader's ref counts to know when it's safe\nto close the reader.\n\nIn the process I also factored out a test base class for tests that\nwant to make lots of simultaneous indexing and searching threads, and\nfixed TestNRTThreads (core), TestNRTManager (contrib/misc) and the new\nTestSearcherManager (contrib/misc) to use this base class.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1786",
        "summary": "improve performance of contrib/TestCompoundWordTokenFilter",
        "description": "contrib/analyzers/compound has some tests that use a hyphenation grammar file.\n\nThe tests are currently for german, and they actually are nice, they show how the combination of the hyphenation rules and dictionary work in tandem.\nThe issue is that the german grammar file is not apache licensed: http://offo.sourceforge.net/hyphenation/licenses.html\nSo the test must download the entire offo zip file from sourceforge to execute.\n\nI happen to think the test is a great example of how this thing works (with a language where it matters), but we could consider using a different grammar file, for a language that is apache licensed.\nThis way it could be included in the source with the test and would be more practical.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1419",
        "summary": "Expert API to specify indexing chain",
        "description": "It would be nice to add an expert API to specify an indexing chain, so that\nwe can make use of Mike's nice LUCENE-1301 feature.\n\nThis patch simply adds a package-protected expert API to IndexWriter and \nDocumentsWriter. It adds a inner, abstract class to DocumentsWriter called \nIndexingChain, and a default implementation that is the currently used one.\n\nThis might not be the final solution, but a nice way to play with different\nmodules in the indexing chain.\n\nCould you take a look at the patch, Mike? ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2901",
        "summary": "KeywordMarkerFilter resets keyword attribute state to false for tokens not in protwords.txt",
        "description": "KeywordMarkerFilter sets true or false for the KeywordAttribute on all tokens. This erases previous state established further up the filter chain, for example in the case where a custom filter wants to prevent a token from being stemmed. \n\nIf a token is already marked as a keyword (KeywordAttribute.isKeyword() == true), perhaps the KeywordMarkerFilterFactory should not re-set the state to false.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3510",
        "summary": "BooleanScorer should not limit number of prohibited clauses",
        "description": "Today it's limited to 32, because it uses a separate bit in the mask\nfor each clause.\n\nBut I don't understand why it does this; I think all prohibited\nclauses can share a single boolean/bit?  Any match on a prohibited\nclause sets this bit and the doc is not collected; we don't need each\nprohibited clause to have a dedicated bit?\n\nWe also use the mask for required clauses, but this code is now\ncommented out (we always use BS2 if there are any required clauses);\nif we re-enable this code (and I think we should, at least in certain\ncases: I suspect it'd be faster than BS2 in many cases), I think we\ncan cutover to an int count instead of bit masks, and then have no\nlimit on the required clauses sent to BooleanScorer also.\n\nSeparately I cleaned a few things up about BooleanScorer: all of the\nembedded scorer methods (nextDoc, docID, advance, score) now throw\nUOE; pre-allocate the buckets instead of doing it lazily\nper-sub-collect.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2943",
        "summary": "ICU collator thread-safety issues",
        "description": "The ICU Collators (unlike the JDK ones) aren't thread safe: http://userguide.icu-project.org/collation/architecture , a little non-obvious since its not mentioned\nin the javadocs, and its not clear if the docs apply to only the C code, but i looked\nat the source and there is all kinds of internal state.\n\nSo in my opinion, we should clone the icu collators (which are passed in from the outside) \nwhen creating a new TokenStream/AttributeImpl to prevent problems. This shouldn't be a big\ndeal since everything uses reusableTokenStream anyway.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1438",
        "summary": "StandardTokenizer splits host names with hyphens into multiple tokens",
        "description": "\nStandardTokenizer does not recognize host names with hyphens as a single HOST token. Specifically \"www.m-w.com\" is tokenized as \"www.m\" and \"w.com\", both of \"<HOST>\" type.\n\nStandardTokenizer should instead output a single HOST token for \"www.m-w.com\", since hyphens are a legitimate character in DNS host names.\n\nWe've a local fix to the grammar file which also required us to significantly simplify the NUM type to get the behavior we needed for host names.\n\nhere's a junit test for the desired behavior;\n\n\tpublic void testWithHyphens() throws Exception {\n\t\tfinal String host = \"www.m-w.com\";\n\t\tfinal StandardTokenizer tokenizer = new StandardTokenizer(\n\t\t\t\tnew StringReader(host));\n\t\tfinal Token token = new Token();\n\t\ttokenizer.next(token);\n\t\tassertEquals(\"<HOST>\", token.type());\n\t\tassertEquals(\"www.m-w.com\", token.term());\n\t}\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-708",
        "summary": "Setup nightly build website links and docs",
        "description": "Per discussion on mailing list, we are going to setup a Nightly Build link on the website linking to the docs (and javadocs) generated by the nightly build process.  The build process may need to be modified to complete this task.\n\nGoing forward, the main website will, for the most part, only be updated per releases (I imagine exceptions will be made for News items and per committer's discretion).  The Javadocs linked to from the main website will always be for the latest release.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2854",
        "summary": "Deprecate SimilarityDelegator and Similarity.lengthNorm",
        "description": "SimilarityDelegator is a back compat trap (see LUCENE-2828).\n\nApps should just [statically] subclass Sim or DefaultSim; if they really need \"runtime subclassing\" then they can make their own app-level delegator.\n\nAlso, Sim.computeNorm subsumes lengthNorm, so we should deprecate lengthNorm in favor of computeNorm.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1357",
        "summary": "SpanScorer does not respect ConstantScoreRangeQuery setting",
        "description": "ConstantScoreRangeQuery is actually on and can't be disabled when it should default to off with the option to turn it on.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1638",
        "summary": "Thread safety issue can cause index corruption when autoCommit=true and multiple threads are committing",
        "description": "This is only present in 2.9 trunk, but has been there since\nLUCENE-1516 was committed I believe.\n\nIt's rare to hit: it only happens if multiple calls to commit() are in\nflight (from different threads) and where at least one of those calls\nis due to a merge calling commit (because autoCommit is true).\n\nWhen it strikes, it leaves the index corrupt because it incorrectly\nremoves an active segment.  It causes exceptions like this:\n{code}\njava.io.FileNotFoundException: _1e.fnm\n\tat org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:246)\n\tat org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:67)\n\tat org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:536)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:468)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:414)\n\tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:641)\n\tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:627)\n\tat org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:923)\n\tat org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4987)\n\tat org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:4165)\n\tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:4025)\n\tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:4016)\n\tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:2077)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2040)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2004)\n\tat org.apache.lucene.index.TestStressIndexing2.indexRandom(TestStressIndexing2.java:210)\n\tat org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:104)\n{code}\n\nIt's caused by failing to increment changeCount inside the same\nsynchronized block where segmentInfos was changed, in commitMerge.\nThe fix is simple -- I plan to commit shortly.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3728",
        "summary": "better handling of files inside/outside CFS by codec",
        "description": "Since norms and deletes were moved under Codec (LUCENE-3606, LUCENE-3661),\nwe never really properly addressed the issue of how Codec.files() should work,\nconsidering these files are always stored outside of CFS.\n\nLUCENE-3606 added a hack, LUCENE-3661 cleaned up the hack a little bit more,\nbut its still a hack.\n\nCurrently the logic in SegmentInfo.files() is:\n{code}\nclearCache()\n\nif (compoundFile) {\n  // don't call Codec.files(), hardcoded CFS extensions, etc\n} else {\n  Codec.files()\n}\n\n// always add files stored outside CFS regardless of CFS setting\nCodec.separateFiles()\n\nif (sharedDocStores) {\n  // hardcoded shared doc store extensions, etc\n}\n{code}\n\nAlso various codec methods take a Directory parameter, but its inconsistent\nwhat this Directory is in the case of CFS: for some parts of the index its\nthe CFS directory, for others (deletes, separate norms) its not.\n\nI wonder if instead we could restructure this so that SegmentInfo.files() logic is:\n{code}\nclearCache()\nCodec.files()\n{code}\n\nand so that Codec is instead responsible.\n\ninstead Codec.files logic by default would do the if (compoundFile) thing, and\nLucene3x codec itself would only have the if (sharedDocStores) thing, and any\npart of the codec that wants to put stuff always outside of CFS (e.g. Lucene3x separate norms, deletes) \ncould just use SegmentInfo.dir. Directory parameters in the case of CFS would always\nconsistently be the CFSDirectory.\n\nI haven't totally tested if this will work but there is definitely some cleanups \nwe can do either way, and I think it would be a good step to try to clean this up\nand simplify it.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-714",
        "summary": "Use a System.arraycopy more than a for",
        "description": "In org.apache.lucene.index.DocumentWriter. The patch will explain by itself. I didn't make any performance test, but I think it is obvious that it will be faster.\nAll tests passed.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3209",
        "summary": "Memory codec",
        "description": "This codec stores all terms/postings in RAM.  It uses an\nFST<BytesRef>.  This is useful on a primary key field to ensure\nlookups don't need to hit disk, to keep NRT reopen time fast even\nunder IO contention.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2162",
        "summary": "ExtendableQueryParser should allow extensions to access the toplevel parser settings/ properties",
        "description": "Based on the latest discussions in LUCENE-2039 this issue will expose the toplevel parser via the ExtensionQuery so that ExtensionParsers can access properties like getAllowLeadingWildcards() from the top level parser.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2479",
        "summary": "need the ability to also sort SpellCheck results by freq, instead of just by Edit Distance+freq",
        "description": "This issue was first noticed and reported in this Solr thread; http://lucene.472066.n3.nabble.com/spellcheck-issues-td489776.html#a489788\n\nBasically, there are situations where it would be useful to sort by freq first, instead of the current \"sort by edit distance, and then subsort by freq if edit distance is equal\"\n\nThe author of the thread suggested \"What I think would work even better than allowing a custom compareTo function would be to incorporate the frequency directly into the distance function.  This would allow for greater control over the trade-off between frequency and edit distance\"\n\nHowever, custom compareTo functions are not always be possible (ie if a certain version of Lucene must be used, because it was release with Solr) and incorporating freq directly into the distance function may be overkill (ie depending on the implementation)\n\nit is suggested that we have a simple modification of the existing compareTo function in Lucene to allow users to specify if they want the existing sort method or if they want to sort by freq.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3643",
        "summary": "Improve FilteredQuery to shortcut on wrapped MatchAllDocsQuery",
        "description": "Since the rewrite of Lucene trunk to delegate all Filter logic to FilteredQuery, by simply wrapping in IndexSearcher.wrapFilter(), we can do more short circuits and improve query execution. A common use case it to pass MatchAllDocsQuery as query to IndexSearcher and a filter. For the underlying hit collection this is stupid and slow, as MatchAllDocsQuery simply increments the docID and checks acceptDocs. If the filter is sparse, this is a big waste. This patch changes FilteredQuery.rewrite() to short circuit and return ConstantScoreQuery, if the query is MatchAllDocs.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3014",
        "summary": "comparator API for segment versions",
        "description": "See LUCENE-3012 for an example.\n\nThings get ugly if you want to use SegmentInfo.getVersion()\n\nFor example, what if we committed my patch, release 3.2, but later released 3.1.1 (will \"3.1.1\" this be whats written and returned by this function?)\nThen suddenly we broke the index format because we are using Strings here without a reasonable comparator API.\n\nIn this case one should be able to compute if the version is < 3.2 safely.\n\nIf we don't do this, and we rely upon this version information internally in lucene, I think we are going to break something.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3435",
        "summary": "Create a Size Estimator model for Lucene and Solr",
        "description": "It is often handy to be able to estimate the amount of memory and disk space that both Lucene and Solr use, given certain assumptions.  I intend to check in an Excel spreadsheet that allows people to estimate memory and disk usage for trunk.  I propose to put it under dev-tools, as I don't think it should be official documentation just yet and like the IDE stuff, we'll see how well it gets maintained.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3908",
        "summary": "when tests fail, sometimes the testmethod in 'reproduce with' is null",
        "description": "an example is the recent fail: https://builds.apache.org/job/Lucene-3.x/680/\n\nit would be better to not populate -Dtestmethod with anything here...",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3589",
        "summary": "BytesRef copy short missed the length setting",
        "description": "when storing a short type integer to BytesRef, BytesRef missed the length setting. then it will cause the storage size is ZERO if no continuous options on this BytesRef",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3286",
        "summary": "Move XML QueryParser to queryparser module",
        "description": "The XML QueryParser will be ported across to queryparser module.\n\nAs part of this work, we'll move the QP's demo into the demo module.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-3522",
        "summary": "TermsFilter.getDocIdSet(context) NPE on missing field",
        "description": "If the context does not contain the field for a term when calling TermsFilter.getDocIdSet(AtomicReaderContext context) then a NullPointerException is thrown due to not checking for null Terms before getting iterator.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1458",
        "summary": "Further steps towards flexible indexing",
        "description": "I attached a very rough checkpoint of my current patch, to get early\nfeedback.  All tests pass, though back compat tests don't pass due to\nchanges to package-private APIs plus certain bugs in tests that\nhappened to work (eg call TermPostions.nextPosition() too many times,\nwhich the new API asserts against).\n\n[Aside: I think, when we commit changes to package-private APIs such\nthat back-compat tests don't pass, we could go back, make a branch on\nthe back-compat tag, commit changes to the tests to use the new\npackage private APIs on that branch, then fix nightly build to use the\ntip of that branch?o]\n\nThere's still plenty to do before this is committable! This is a\nrather large change:\n\n  * Switches to a new more efficient terms dict format.  This still\n    uses tii/tis files, but the tii only stores term & long offset\n    (not a TermInfo).  At seek points, tis encodes term & freq/prox\n    offsets absolutely instead of with deltas delta.  Also, tis/tii\n    are structured by field, so we don't have to record field number\n    in every term.\n.\n    On first 1 M docs of Wikipedia, tii file is 36% smaller (0.99 MB\n    -> 0.64 MB) and tis file is 9% smaller (75.5 MB -> 68.5 MB).\n.\n    RAM usage when loading terms dict index is significantly less\n    since we only load an array of offsets and an array of String (no\n    more TermInfo array).  It should be faster to init too.\n.\n    This part is basically done.\n\n  * Introduces modular reader codec that strongly decouples terms dict\n    from docs/positions readers.  EG there is no more TermInfo used\n    when reading the new format.\n.\n    There's nice symmetry now between reading & writing in the codec\n    chain -- the current docs/prox format is captured in:\n{code}\nFormatPostingsTermsDictWriter/Reader\nFormatPostingsDocsWriter/Reader (.frq file) and\nFormatPostingsPositionsWriter/Reader (.prx file).\n{code}\n    This part is basically done.\n\n  * Introduces a new \"flex\" API for iterating through the fields,\n    terms, docs and positions:\n{code}\nFieldProducer -> TermsEnum -> DocsEnum -> PostingsEnum\n{code}\n    This replaces TermEnum/Docs/Positions.  SegmentReader emulates the\n    old API on top of the new API to keep back-compat.\n    \nNext steps:\n\n  * Plug in new codecs (pulsing, pfor) to exercise the modularity /\n    fix any hidden assumptions.\n\n  * Expose new API out of IndexReader, deprecate old API but emulate\n    old API on top of new one, switch all core/contrib users to the\n    new API.\n\n  * Maybe switch to AttributeSources as the base class for TermsEnum,\n    DocsEnum, PostingsEnum -- this would give readers API flexibility\n    (not just index-file-format flexibility).  EG if someone wanted\n    to store payload at the term-doc level instead of\n    term-doc-position level, you could just add a new attribute.\n\n  * Test performance & iterate.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1266",
        "summary": "IndexWriter.optimize(boolean doWait) ignores doWait parameter",
        "description": "{{IndexWriter.optimize(boolean doWait)}} ignores the doWait parameter and always calls {{optimize(1, true)}}.\n\nThat does not seem to be the intended behavior, based on the doc comment.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-483",
        "summary": "QueryParser.getFieldQuery(String,String) doesn't set default slop on MultiPhraseQuery",
        "description": "there seems to have been an oversight in calling mph.setSlop(phraseSlop) in QueryParser.getFieldQuery(String,String).  The result being that in some cases, the \"default slop\" value doesnt' get set right (sometimes, ... see below).\n\nwhen i tried amending TestMultiAnalyzer to demonstrate the problem, I discovered that the grammer aparently always calls getFieldQuery(String,String,int) -- even if no \"~slop\" was specified in the text being parsed, in which case it passes the default as if it were specified.\n(just to clarify: i haven't comfirmed this from a detailed reading of the grammer/code, it's just what i've deduced based on observation of the test)\n\nThe problem isn't entirely obvious unless you have a subclasses of QueryParser and try to call getFieldQuery(String,String) directly.   \n\nIn my case, I had overridden getFieldQuery(String,String) to call super.getFieldQuery(String,String) and wrap the result in a DisjunctionMaxQuery ... I don't care about supporting the ~slop syntax, but i do care about the default slop and i wasn't getting lucky the way QueryParser does, because getFieldQuery(String,String,int) wasn't getting back something it could call setSlop() with the (default) value it got from the javacc generated code.\n\nMy description may not make much sense, but hopefull the test patch i'm about to attach will.  The fix is also in the patch, and is fairly trivial.\n\n(disclaimer: i don't have javacc installed, so I tested this patch by manually making the change to both QueryParser.java ... it should only be commited by someone with javacc who can regen the java file and confirm that my jj change doesn't have some weird bug in it)\n\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2070",
        "summary": "document LengthFilter wrt Unicode 4.0",
        "description": "LengthFilter calculates its min/max length from TermAttribute.termLength()\nThis is not characters, but instead UTF-16 code units.\n\nIn my opinion this should not be changed, merely documented.\nIf we changed it, it would have an adverse performance impact because we would have to actually calculate Character.codePointCount() on the text.\n\nIf you feel strongly otherwise, fixing it to count codepoints would be a trivial patch, but I'd rather not hurt performance.\nI admit I don't fully understand all the use cases for this filter.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1717",
        "summary": "IndexWriter does not properly account for the RAM consumed by pending deletes",
        "description": "IndexWriter, with autoCommit false, is able to carry buffered deletes for quite some time before materializing them to docIDs (thus freeing up RAM used).\n\nIt's only on triggering a merge (or, commit/close) that the deletes are materialized and the RAM is freed.\n\nI expect this in practice is a smallish amount of RAM, but we should still fix it.\n\nI don't have a patch yet so if someone wants to grab this, feel free!!",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2635",
        "summary": "BQ provides an explanation on a non-match",
        "description": "Plug in seed -6336594106867842617L into TestExplanations then run TestSimpleExplanationsOfNonMatches and you'll hit this:\n{noformat}\n    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanationsOfNonMatches\n    [junit] Testcase: testBQ2(org.apache.lucene.search.TestSimpleExplanationsOfNonMatches):\tFAILED\n    [junit] Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:\n    [junit]   0.17556934 = (MATCH) sum of:\n    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:\n    [junit]       0.5165708 = queryWeight(field:w3), product of:\n    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)\n    [junit]         0.6649502 = queryNorm\n    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:\n    [junit]         1.0 = tf(termFreq(field:w3)=1)\n    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)\n    [junit]         0.4375 = fieldNorm(field=field, doc=0)\n    [junit]   0.5 = coord(1/2)\n    [junit]  expected:<0.0> but was:<0.08778467>\n    [junit] junit.framework.AssertionFailedError: Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:\n    [junit]   0.17556934 = (MATCH) sum of:\n    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:\n    [junit]       0.5165708 = queryWeight(field:w3), product of:\n    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)\n    [junit]         0.6649502 = queryNorm\n    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:\n    [junit]         1.0 = tf(termFreq(field:w3)=1)\n    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)\n    [junit]         0.4375 = fieldNorm(field=field, doc=0)\n    [junit]   0.5 = coord(1/2)\n    [junit]  expected:<0.0> but was:<0.08778467>\n    [junit] \tat org.apache.lucene.search.CheckHits.checkNoMatchExplanations(CheckHits.java:60)\n    [junit] \tat org.apache.lucene.search.TestSimpleExplanationsOfNonMatches.qtest(TestSimpleExplanationsOfNonMatches.java:36)\n    [junit] \tat org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:101)\n    [junit] \tat org.apache.lucene.search.TestSimpleExplanations.testBQ2(TestSimpleExplanations.java:235)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:397)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:389)\n{noformat}\n\nThe bug is real -- BQ's explain method fails to properly enforce required clauses when the sub-scorer is null.  Thank you random testing!\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2260",
        "summary": "AttributeSource holds strong reference to class instances and prevents unloading e.g. in Solr if webapplication reload and custom attributes in separate classloaders are used (e.g. in the Solr plugins classloader)",
        "description": "When working on the dynmaic proxy classes using cglib/javaassist i recognized a problem in the caching code inside AttributeSource:\n- AttributeSource has a static (!) cache map that holds implementation classes for attributes to be faster on creating new attributes (reflection cost)\n- AttributeSource has a static (!) cache map that holds a list of all interfaces implemented by a specific AttributeImpl\n\nAlso:\n- VirtualMethod in 3.1 hold a map of implementation distances keyed by subclasses of the deprecated API\n\nBoth have the problem that this strong reference is inside Lucene's classloader and so persists as long as lucene lives. The classes referenced can never be unloaded therefore, which would be fine if all live in the same classloader. As soon as the Attribute or implementation class or the subclass of the deprecated API are loaded by a different classloder (e.g. Lucene lives in bootclasspath of tomcat, but lucene-consumer with custom attributes lives in a webapp), they can never be unloaded, because a reference exists.\n\nLibs like CGLIB or JavaAssist or JDK's reflect.Proxy have a similar cache for generated class files. They also manage this by a WeakHashMap. The cache will always work perfect and no class will be evicted without reason, as classes are only unloaded when the classloader goes and this will only happen on request (e.g. by Tomcat).",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3478",
        "summary": "TestSimpleExplanations failure",
        "description": "{noformat}\nant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=-7e984babece66153:3e3298ae627b33a9:3093059db62bcc71\n{noformat}\n\nfails w/ this on current trunk... looks like silly floating point precision issue:\n\n{noformat}\n\n    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanations\n    [junit]   1.4508595 = (MATCH) sum of:\n    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:\n    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1\n    [junit] ), product of:\n    [junit]         1.287682 = queryWeight, product of:\n    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]           1.0 = queryNorm\n    [junit]         1.1267219 = fieldWeight in 2, product of:\n    [junit]           1.0 = tf(freq=1.0), with freq of:\n    [junit]             1.0 = termFreq=1\n    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]           0.875 = fieldNorm(doc=2)\n    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:\n    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1\n    [junit] ), product of:\n    [junit]       128768.2 = queryWeight, product of:\n    [junit]         100000.0 = boost\n    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]         1.0 = queryNorm\n    [junit]       1.1267219 = fieldWeight in 2, product of:\n    [junit]         1.0 = tf(freq=1.0), with freq of:\n    [junit]           1.0 = termFreq=1\n    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]         0.875 = fieldNorm(doc=2)\n    [junit]  expected:<145086.66> but was:<145086.69>)\n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.544 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=144152895b276837:eb7ba4953db943f:33373b79a971db02\n    [junit] NOTE: test params are: codec=PreFlex, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {field=DefaultSimilarity, alt=DFR I(ne)LZ(0.3), KEY=IB LL-D2}, locale=en_IN, timezone=Pacific/Samoa\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestSimpleExplanations]\n    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=130426744,total=189988864\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testDMQ8(org.apache.lucene.search.TestSimpleExplanations):\tFAILED\n    [junit] ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:\n    [junit]   1.4508595 = (MATCH) sum of:\n    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:\n    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1\n    [junit] ), product of:\n    [junit]         1.287682 = queryWeight, product of:\n    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]           1.0 = queryNorm\n    [junit]         1.1267219 = fieldWeight in 2, product of:\n    [junit]           1.0 = tf(freq=1.0), with freq of:\n    [junit]             1.0 = termFreq=1\n    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]           0.875 = fieldNorm(doc=2)\n    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:\n    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1\n    [junit] ), product of:\n    [junit]       128768.2 = queryWeight, product of:\n    [junit]         100000.0 = boost\n    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]         1.0 = queryNorm\n    [junit]       1.1267219 = fieldWeight in 2, product of:\n    [junit]         1.0 = tf(freq=1.0), with freq of:\n    [junit]           1.0 = termFreq=1\n    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]         0.875 = fieldNorm(doc=2)\n    [junit]  expected:<145086.66> but was:<145086.69>\n    [junit] junit.framework.AssertionFailedError: ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:\n    [junit]   1.4508595 = (MATCH) sum of:\n    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:\n    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1\n    [junit] ), product of:\n    [junit]         1.287682 = queryWeight, product of:\n    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]           1.0 = queryNorm\n    [junit]         1.1267219 = fieldWeight in 2, product of:\n    [junit]           1.0 = tf(freq=1.0), with freq of:\n    [junit]             1.0 = termFreq=1\n    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]           0.875 = fieldNorm(doc=2)\n    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:\n    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1\n    [junit] ), product of:\n    [junit]       128768.2 = queryWeight, product of:\n    [junit]         100000.0 = boost\n    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]         1.0 = queryNorm\n    [junit]       1.1267219 = fieldWeight in 2, product of:\n    [junit]         1.0 = tf(freq=1.0), with freq of:\n    [junit]           1.0 = termFreq=1\n    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)\n    [junit]         0.875 = fieldNorm(doc=2)\n    [junit]  expected:<145086.66> but was:<145086.69>\n    [junit] \tat org.apache.lucene.search.CheckHits.verifyExplanation(CheckHits.java:324)\n    [junit] \tat org.apache.lucene.search.CheckHits$ExplanationAsserter.collect(CheckHits.java:494)\n    [junit] \tat org.apache.lucene.search.Scorer.score(Scorer.java:60)\n    [junit] \tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:580)\n    [junit] \tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:363)\n    [junit] \tat org.apache.lucene.search.CheckHits.checkExplanations(CheckHits.java:302)\n    [junit] \tat org.apache.lucene.search.QueryUtils.checkExplanations(QueryUtils.java:92)\n    [junit] \tat org.apache.lucene.search.QueryUtils.check(QueryUtils.java:126)\n    [junit] \tat org.apache.lucene.search.QueryUtils.check(QueryUtils.java:122)\n    [junit] \tat org.apache.lucene.search.QueryUtils.check(QueryUtils.java:106)\n    [junit] \tat org.apache.lucene.search.CheckHits.checkHitCollector(CheckHits.java:89)\n    [junit] \tat org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:99)\n    [junit] \tat org.apache.lucene.search.TestSimpleExplanations.testDMQ8(TestSimpleExplanations.java:224)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.search.TestSimpleExplanations FAILED\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2777",
        "summary": "Revise PagedBytes#fillUsingLengthPrefix* methods names",
        "description": "PagedBytes has 3 different variants of fillUsingLengthPrefix. We need better names for that since CSFBranch already added a 4th one.\n\n\nhere are some suggestions:\n\n{code}\n/** Reads length as 1 or 2 byte vInt prefix, starting @ start */\n    public BytesRef fillLengthAndOffset(BytesRef b, long start) \n//    was: public BytesRef fillUsingLengthPrefix(BytesRef b, long start) \n\n\n /** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start.  Returns the block number of the term. */\n    public int getBlockAndFill(BytesRef b, long start) \n//    was: public BytesRef fillUsingLengthPrefix2(BytesRef b, long start) \n\n/** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start. \n     * Returns the start offset of the next part, suitable as start parameter on next call\n     * to sequentially read all BytesRefs. */\n    public long getNextOffsetAndFill(BytesRef b, long start) \n//    was: public BytesRef fillUsingLengthPrefix3(BytesRef b, long start) \n\n{code}",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2157",
        "summary": "DelimitedPayloadTokenFilter copies the bufer over itsself. Instead it should only set the length. Also optimize logic.",
        "description": "This is a small improvement I found when looking around. It is also a bad idea to copy a array over itsself.\n\nAll tests pass, will commit later!",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-379",
        "summary": "Contribution: Efficient Sorting of DateField/DateTools Encoded Timestamp Long Values",
        "description": "Hello Tim,\n\nAs promised, the sort functionality for \"long\" values is included in the\nattached files.\n\npatchTestSort.txt contains the diff info. for my modifications to the\nTestSort.java class\n\norg.apache.lucene.search.ZIP contains the three new class files for\nefficient sorting of \"long\" field values and of encoded timestamp\nfield values as \"long\" values.\n\nLet me know if you have any questions.\n\nRegards,\nRus",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2554",
        "summary": "preflex codec doesn't order terms correctly",
        "description": "The surrogate dance in the preflex codec (which must dynamically remap terms from UTF16 order to unicode code point order) is buggy.\n\nTo better test it, I want to add a test-only codec, preflexrw, that is able to write indices in the pre-flex format.  Then we should also fix tests to randomly pick codecs (including preflexrw) so we better test all of our codecs.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-847",
        "summary": "Factor merge policy out of IndexWriter",
        "description": "If we factor the merge policy out of IndexWriter, we can make it pluggable, making it possible for apps to choose a custom merge policy and for easier experimenting with merge policy variants.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1129",
        "summary": "ReadTask ignores traversalSize",
        "description": "The ReadTask doLogic() method ignores the value of the traversalSize and loops over hits.length() instead, thus falsely reporting the desired number of iterations through the hit list.\n\nThe fix is relatively trivial since we already calculate \n{code}\nint traversalSize = Math.min(hits.length(), traversalSize());\n{code}\nso we just need to use this value in the loop condition.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1682",
        "summary": "unit tests should use private directories",
        "description": "This only affects our unit tests...\n\nI run \"ant test\" and \"ant test-tag\" concurrently, but some tests have false failures (eg TestPayloads) because they use a fixed test directory in the filesystem for testing.\n\nI've added a simple method to _TestUtil to get a temp dir, and switched over those tests that I've hit false failures on.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2052",
        "summary": "Scan method signatures and add varargs where possible",
        "description": "I changed a lot of signatures, but there may be more. The important ones like MultiReader and MultiSearcher are already done. This applies also to contrib. Varargs are no backwards break, they stay arrays as before.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1445",
        "summary": "include junit JAR in source dist",
        "description": "We recently added the junit JAR under \"lib\" so that we can checkout & run tests, but we fail to include it in the source dist.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-515",
        "summary": "Using ConstantScoreQuery on a RemoteSearchable throws java.io.NotSerializableException",
        "description": "Using a ConstantScoreQuery through a MultiSearcher on a Searchable obtained through RMI (RemoteSearchable) will throw a java.io.NotSerializableException\n\nThe problem seems to be the fact that the ConstantScoreQuery.ConstantWeight has a Searcher member variable which is not serializable. Keeping a reference to the Searcher does not seem to be required: the fix seems trivial.\n\nI've created the TestCase to reproduce the issue and the patch to fix it.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-432",
        "summary": "Make FieldSortedHitQueue public",
        "description": "Currently, those who utilize the \"advanced\" search API cannot sort results using\nthe handy FieldSortedHitQueue. I suggest making this class public to facilitate\nthis use, as I can't think of a reason not to.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2458",
        "summary": "queryparser makes all CJK queries phrase queries regardless of analyzer",
        "description": "The queryparser automatically makes *ALL* CJK, Thai, Lao, Myanmar, Tibetan, ... queries into phrase queries, even though you didn't ask for one, and there isn't a way to turn this off.\n\nThis completely breaks lucene for these languages, as it treats all queries like 'grep'.\n\nExample: if you query for f:abcd with standardanalyzer, where a,b,c,d are chinese characters, you get a phrasequery of \"a b c d\". if you use cjk analyzer, its no better, its a phrasequery of  \"ab bc cd\", and if you use smartchinese analyzer, you get a phrasequery like \"ab cd\". But the user didn't ask for one, and they cannot turn it off.\n\nThe reason is that the code to form phrase queries is not internationally appropriate and assumes whitespace tokenization. If more than one token comes out of whitespace delimited text, its automatically a phrase query no matter what.\n\nThe proposed patch fixes the core queryparser (with all backwards compat kept) to only form phrase queries when the double quote operator is used. \n\nImplementing subclasses can always extend the QP and auto-generate whatever kind of queries they want that might completely break search for languages they don't care about, but core general-purpose QPs should be language independent.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3468",
        "summary": "FirstPassGroupingCollector should use pollLast()",
        "description": "Currently FirstPassGroupingCollector uses last and remove method (TreeSet) for replacing a more relevant grouping during grouping.\nThis can be replaced by pollLast since Lucene trunk is now Java 6. TermFirstPassGroupingCollectorJava6 in Solr can be removed as well.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1079",
        "summary": "DocValues cleanup: constructor & getInnerArray()",
        "description": "DocValues constructor taking a numDocs parameter is not very clean.\nGet rid of this.\n\nAlso, it's optional getInnerArray() method is not very clean.\nThis is necessary for better testing, but currently tests will fail if it is not implemented.\nModify it to throw UnSupportedOp exception (rather than returning an empty array).\nModify tests to not fail but just warn if the tested iml does not override it.\n\nThese changes should make it easier to implement DocValues for other ValueSource's, e.g. above payloads, with or without caching.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2496",
        "summary": "NPE if you open IW with CREATE on an index with no segments file",
        "description": "I have a simple test case that hits this NPE:\n\n{noformat}\n    [junit] java.lang.NullPointerException\n    [junit] \tat java.io.File.<init>(File.java:305)\n    [junit] \tat org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:67)\n    [junit] \tat org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:333)\n    [junit] \tat org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:213)\n    [junit] \tat org.apache.lucene.index.IndexFileDeleter.<init>(IndexFileDeleter.java:218)\n    [junit] \tat org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1113)\n    [junit] \tat org.apache.lucene.index.TestIndexWriter.testNoSegmentFile(TestIndexWriter.java:4975)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:277)\n{noformat}\n\nIt happens if you have an aborted index, ie, there are segment files in there (*.frq, *.tis, etc.) but no segments_N file, and then you try to open an IW with CREATE on that index.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2097",
        "summary": "In NRT mode, and CFS enabled, IndexWriter incorrectly ties up disk space",
        "description": "Spinoff of java-user thread titled \"searching while optimize\"...\n\nIf IndexWriter is in NRT mode (you've called getReader() at least\nonce), and CFS is enabled, then internally the writer pools readers.\nHowever, after a merge completes, it opens the reader against het\nnon-CFS segment files, and pools that.  It then builds the CFS file,\nas well, thus tying up the storage for that segment twice.\n\nFunctionally the bug is harmless (it's only a disk space issue).\nAlso, when the segment is merged, the disk space is released again\n(though the newly merged segment will also be double-tied-up).\n\nSimple workaround is to use non-CFS mode, or, don't use getReader.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-966",
        "summary": "A faster JFlex-based replacement for StandardAnalyzer",
        "description": "JFlex (http://www.jflex.de/) can be used to generate a faster (up to several times) replacement for StandardAnalyzer. Will add a patch and a simple benchmark code in a while.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3170",
        "summary": "TestDocValuesIndexing reproducible  test failure",
        "description": "docvalues branch: r1131275\n\n{code}\n    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing\n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.81 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3253978684351194958:-8331223747763543724\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_VAR_STRAIGHT=Pulsing(freqCutoff=12), BYTES_FIXED_SORTED=MockRandom}, locale=es_MX, timezone=Pacific/Chatham\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestDocValuesIndexing]\n    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=89168480,total=125632512\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     FAILED\n    [junit] [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>\n    [junit] junit.framework.AssertionFailedError: [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>\n    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:208)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1348)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1266)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1366",
        "summary": "Rename Field.Index.UN_TOKENIZED/TOKENIZED/NO_NORMS",
        "description": "There is confusion about these current Field options and I think we\nshould rename them, deprecating the old names in 2.4/2.9 and removing\nthem in 3.0.  How about this:\n\n{code}\nTOKENIZED --> ANALYZED\nUN_TOKENIZED --> NOT_ANALYZED\nNO_NORMS --> NOT_ANALYZED_NO_NORMS\n{code}\n\nShould we also add ANALYZED_NO_NORMS?\n\nSpinoff from here:\n\n    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200808.mbox/%3C48a3076a.2679420a.1c53.ffffa5c4%40mx.google.com%3E\n    ",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1321",
        "summary": "Highlight fragment does not extend to maxDocCharsToAnalyze",
        "description": "The current highlighter code checks whether the total length of the text to highlight is strictly smaller than maxDocCharsToAnalyze before adding any text remaining after the last token to the fragment. This means that if maxDocCharsToAnalyse is set to exactly the length of the text and the last token of the text is the term to highlight and is followed by non-token text, this non-token text will not be highlighted.\n\nFor example, consider the phrase \"this is a text with searchterm in it\". \"In\" and \"it\" are not tokenized because they're stopwords. Setting maxDocCharsToAnalyze to 36 (the length of the sentence) and searching for \"searchterm\" gives a fragment ending in \"searchterm\". The expected behaviour is to have \"in it\" at the end of the fragment, since maxDocCharsToAnalyse explicitely states that the whole phrase should be considered.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1138",
        "summary": "SpellChecker.clearIndex calls unlock inappropriately",
        "description": "As noted in LUCENE-1050, fixing a bug in SimpleLockFactory related to not reporting success/filure of lock file deletion has surfaced bad behavior in SpellChecker.clearIndex...\n\nGrant...\n{quote}\nIt seems the SpellChecker is telling the IndexReader to delete the lockFile, but the lockFile doesn't exist.\n  ...\nI don't know much about the locking mechanism, but it seems like this should check to see if the lockFile exists before trying to delete it.\n{quote}\n\nHoss...\n{quote}\nGrant: my take on this is that SpellChecker.clearIndex is in the wrong. it shouldn't be calling unlock unless it has reason to think there is a \"stale lock\" that needs to be closed - ie: this is a bug in SpellChecker that you have only discovered because this bug LUCENE-1050 was fixed.\n\nI would suggest a new issue for tracking, and a patch in which SpellChecker.clearIndex doesn't call unlock unless isLocked returns true. Even then, it might make sense to catch and ignore LockReleaseFailedException and let whatever resulting exception may originate from \"new IndexWriter\" be returned.\n{quote}\n\nmarking for 2.3 since it seems like a fairly trivial fix, and if we don't deal with it now it will be a bug introduced in 2.3.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-400",
        "summary": "NGramFilter -- construct n-grams from a TokenStream",
        "description": "This filter constructs n-grams (token combinations up to a fixed size, sometimes\ncalled \"shingles\") from a token stream.\n\nThe filter sets start offsets, end offsets and position increments, so\nhighlighting and phrase queries should work.\n\nPosition increments > 1 in the input stream are replaced by filler tokens\n(tokens with termText \"_\" and endOffset - startOffset = 0) in the output\nn-grams. (Position increments > 1 in the input stream are usually caused by\nremoving some tokens, eg. stopwords, from a stream.)\n\nThe filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache\nCommons-Collections.\n\nFilter, test case and an analyzer are attached.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-272",
        "summary": "[PATCH] Remove equals() from internal Comparator of ConjunctionScorer",
        "description": "As written, the equals() method is not used. \nThe docs of java.util.Comparator have an equals() with a single \narg to compare the Comparator itself to another one, which is \nhardly ever useful. \nPatch follows",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2715",
        "summary": "optimize fuzzytermsenum per-segment",
        "description": "we can make fuzzyquery about 3% faster by not creating DFA(s) for each segment.\n\ncreating the DFAs is still somewhat heavy: i can address this here too, but this is easy.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3301",
        "summary": "add workaround for jre breakiterator bugs",
        "description": "on some inputs, the java breakiterator support will internally crash.\n\nfor example: ant test -Dtestcase=TestThaiAnalyzer -Dtestmethod=testRandomStrings -Dtests.seed=-8005471002120855329:-2517344653287596566 -Dtests.multiplier=3",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3721",
        "summary": "CharFilters not being invoked in Solr",
        "description": "\nOn Solr trunk, *all* CharFilters have been non-functional since LUCENE-3396 was committed in r1175297 on 25 Sept 2011, until Yonik's fix today in r1235810; Solr 3.x was not affected - CharFilters have been working there all along.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-729",
        "summary": "non-recursive MultiTermDocs",
        "description": "A non-recursive implementation of MultiTermDocs.next() and skipTo() would be nice as it's currently possible to get a stack overflow in very rare situations.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3479",
        "summary": "TestGrouping failure",
        "description": "{noformat}\nant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba\n{noformat}\n\nfails with this on current trunk:\n\n{noformat}\n\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, content=MockSep, sort2=SimpleText, groupend=Pulsing(freqCutoff=3 minBlockSize=65 maxBlockSize=132), sort1=Memory, group=Memory}, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {id=DFR I(F)L2, content=DFR BeL3(800.0), sort2=DFR GL3(800.0), groupend=DFR G2, sort1=DFR GB3(800.0), group=LM Jelinek-Mercer(0.700000)}, locale=zh_TW, timezone=America/Indiana/Indianapolis\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestGrouping]\n    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=143246344,total=281804800\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testRandom(org.apache.lucene.search.grouping.TestGrouping):\tFAILED\n    [junit] expected:<11> but was:<7>\n    [junit] junit.framework.AssertionFailedError: expected:<11> but was:<7>\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)\n    [junit] \tat org.apache.lucene.search.grouping.TestGrouping.assertEquals(TestGrouping.java:980)\n    [junit] \tat org.apache.lucene.search.grouping.TestGrouping.testRandom(TestGrouping.java:865)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)\n    [junit] \n    [junit] \n{noformat}\n\nI dug for a while... the test is a bit sneaky because it compares sorted docs (by score) across 2 indexes.  Index #1 has no deletions; Index #2 has same docs, but organized into doc blocks by group, and has some deletions.  In theory (I think) even though the deletions will cause scores to differ across the two indices, it should not alter the sort order of the docs.  Here is the explain output of the docs that sorted differently:\n\n{noformat}\n#1: top hit in the \"has deletes doc-block\" index (id=239):\n\nexplain: 2.394486 = (MATCH) weight(content:real1 in 292)\n[DFRSimilarity], result of:\n 2.394486 = score(DFRSimilarity, doc=292, freq=1.0), computed from:\n   1.0 = termFreq=1\n   41.944084 = NormalizationH3, computed from:\n     1.0 = tf\n     5.3102274 = avgFieldLength\n     2.56 = len\n   102.829 = BasicModelBE, computed from:\n     41.944084 = tfn\n     880.0 = numberOfDocuments\n     239.0 = totalTermFreq\n   0.023286095 = AfterEffectL, computed from:\n     41.944084 = tfn\n\n\n#2: hit in the \"no deletes normal index\" (id=229)\n\nID=229 explain=2.382285 = (MATCH) weight(content:real1 in 225)\n[DFRSimilarity], result of:\n 2.382285 = score(DFRSimilarity, doc=225, freq=1.0), computed from:\n   1.0 = termFreq=1\n   41.765594 = NormalizationH3, computed from:\n     1.0 = tf\n     5.3218827 = avgFieldLength\n     10.24 = len\n   101.879845 = BasicModelBE, computed from:\n     41.765594 = tfn\n     786.0 = numberOfDocuments\n     215.0 = totalTermFreq\n   0.023383282 = AfterEffectL, computed from:\n     41.765594 = tfn\n\nThen I went and called explain on the \"no deletes normal index\" for\nthe top doc (id=239):\n\nexplain: 2.3822558 = (MATCH) weight(content:real1 in 17)\n[DFRSimilarity], result of:\n 2.3822558 = score(DFRSimilarity, doc=17, freq=1.0), computed from:\n   1.0 = termFreq=1\n   42.165264 = NormalizationH3, computed from:\n     1.0 = tf\n     5.3218827 = avgFieldLength\n     2.56 = len\n   102.8307 = BasicModelBE, computed from:\n     42.165264 = tfn\n     786.0 = numberOfDocuments\n     215.0 = totalTermFreq\n   0.023166776 = AfterEffectL, computed from:\n     42.165264 = tfn\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2029",
        "summary": "Allow separate control over whether body is stored or analyzed",
        "description": "Simple enhancement to DocMaker.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3362",
        "summary": "Initialization error of Junit tests with solr-test-framework with IDEs and Maven",
        "description": "I'm currently developping a new component for Solr. And in my Netbeans project, I have created two Test classes for this component: one class for simple unit tests (derived from  SolrTestCaseJ4 class) and a second one for tests with sharding (derived from  BaseDistributedSearchTestCase).\nWhen I launch a test with these two classes, I have an error in the initialization of the second class of tests (no matter the class is, this is always the second executed class which fails). The error comes from an \"assert\" which failed in the begining of the function \"initRandom()\" of LuceneTestCase class :\n\nassert !random.initialized;\n\nBut, if I launch each test class separatly, all the tests succeed!\n\nAfter a discussion with Mr. Muir, the problems seems to be related to the incompatibility of the class LuceneTestCase with the functioning of Maven projects in IDEs.\n\nAccording to mister Muir:\n\n\"\nThe problem is that via ant, tests work like this (e.g. for 3 test classes):\ncomputeTestMethods\nbeforeClass\nafterClass\ncomputeTestMethods\nbeforeClass\nAfterClass\ncomputeTestMethods\nbeforeClass\nafterClass\n\nbut via an IDE, if you run it from a folder like you did, then it does this:\ncomputeTestMethods\ncomputeTestMethods\ncomputeTestMethods\nbeforeClass\nafterClass\nbeforeClass\nafterClass\nbeforeClass\nafterClass \n\"",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-578",
        "summary": "Summer of Code GDATA Server  --Project structure and simple version to start with--",
        "description": "This is the initial issue for the GDATA - Server project (Google Summer of Code). \nThe purpose of the issue is to create the project structure in the svn repository to kick off the project. The source e.g. the project will be located at URL: http://svn.apache.org/repos/asf/lucene/java/trunk/contrib\nThe attachment includes the diff text file and the jar files included in the lib directory as a tar.gz file.\nTo get some information about the project see http://wiki.apache.org/general/SimonWillnauer/SummerOfCode2006",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-385",
        "summary": "[PATCH] don't delete all files in index directory on index creation",
        "description": "Many people use Lucene to index a part of their file system. The chance that  \nyou some day mix up index directory and document directory isn't that bad.  \nCurrently Lucene will delete *all* files in the index directory when the  \ncreate paramater passed to IndexWriter is true, thus deleting your documents \nif you mixed up the parameters. I'll attach a patch that fixes  \nthis. Any objections?",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3850",
        "summary": "Fix rawtypes warnings for Java 7 compiler",
        "description": "Java 7 changed the warnings a little bit:\n- Java 6 only knew \"unchecked\" warning type, applying for all types of generics violations, like missing generics (raw types)\n- Java 7 still knows \"unchecked\" but only emits warning if the call is really unchecked. Declaration of variables/fields or constructing instances without type param now emits \"rawtypes\" warning.\n\nThe changes above causes the Java 7 compile now emit lots of \"rawtypes\" warnings, where Java 6 is silent. The easy fix is to suppres both warning types: @SuppressWarnings({\"unchecked\",\"rawtypes\"}) for all those places. Changes are easy to do, will provide patch later!",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1325",
        "summary": "add IndexCommit.isOptimized method",
        "description": "Spinoff from here:\n\n    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200807.mbox/%3C69de18140807010347s6269fea5r12c3212e0ec0a12a@mail.gmail.com%3E",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1506",
        "summary": "Adding FilteredDocIdSet and FilteredDocIdSetIterator",
        "description": "Adding 2 convenience classes: FilteredDocIdSet and FilteredDocIDSetIterator.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-598",
        "summary": "GData Server MileStone 1 Revision",
        "description": "Some Improvements to the GData Server.\nCRUD actions for Entries implemented / tested \nStorageComponent storing entries / feeds / users\nDynamic Feed elements like links added.\nDecoupled all server components (storage / ReqeustHandler etc) using lookup service\n\nAdded some JavaDoc ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3714",
        "summary": "add suggester that uses shortest path/wFST instead of buckets",
        "description": "Currently the FST suggester (really an FSA) quantizes weights into buckets (e.g. single byte) and puts them in front of the word.\nThis makes it fast, but you lose granularity in your suggestions.\n\nLately the question was raised, if you build lucene's FST with positiveintoutputs, does it behave the same as a tropical semiring wFST?\n\nIn other words, after completing the word, we instead traverse min(output) at each node to find the 'shortest path' to the \nbest suggestion (with the highest score).\n\nThis means we wouldnt need to quantize weights at all and it might make some operations (e.g. adding fuzzy matching etc) a lot easier.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1639",
        "summary": "intermittent failure in TestIndexWriter. testRandomIWReader",
        "description": "Rarely, this test (which was added with LUCENE-1516) fails in MockRAMDirectory.close because some files were not closed, eg:\n{code}\n   [junit] NOTE: random seed of testcase 'testRandomIWReader' was: -5001333286299627079\n   [junit] ------------- ---------------- ---------------\n   [junit] Testcase: testRandomIWReader(org.apache.lucene.index.TestStressIndexing2):        Caused an ERROR\n   [junit] MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}\n   [junit] java.lang.RuntimeException: MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}\n   [junit]     at org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:292)\n   [junit]     at org.apache.lucene.index.TestStressIndexing2.testRandomIWReader(TestStressIndexing2.java:66)\n   [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-593",
        "summary": "Spellchecker's dictionary iterator misbehaves",
        "description": "In LuceneDictionary, the LuceneIterator.hasNext() method has two issues that makes it misbehave:\n\n1) If hasNext is called more than once, items are skipped\n2) Much more seriously, when comparing fieldnames it is done with != rather than .equals() with the potential result that nothing is indexed\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2566",
        "summary": "+ - operators allow any amount of whitespace",
        "description": "As an example, (foo - bar) is treated like (foo -bar).\nIt seems like for +- to be treated as unary operators, they should be immediately followed by the operand.",
        "label": "NUG",
        "classified": "SPEC",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3815",
        "summary": "Correct copy-paste victim Comment",
        "description": "Correct the doc-comment of FieldsProducer (being a copy-paste victim of FieldsConsumer).\n\"consumes\" replaced with \"produces\".\n\nOne word change to avoid confusion: safe to commit.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2282",
        "summary": "Expose IndexFileNames as public, and make use of its methods in the code",
        "description": "IndexFileNames is useful for applications that extend Lucene, an in particular those who extend Directory or IndexWriter. It provides useful constants and methods to query whether a certain file is a core Lucene file or not. In addition, IndexFileNames should be used by Lucene's code to generate segment file names, or query whether a certain file matches a certain extension.\n\nI'll post the patch shortly.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1948",
        "summary": "Deprecating InstantiatedIndexWriter",
        "description": "http://markmail.org/message/j6ip266fpzuaibf7\n\nI suppose that should have been suggested before 2.9 rather than  \nafter...\n\nThere are at least three reasons to why I want to do this:\n\nThe code is based on the behaviour or the Directory IndexWriter as of  \n2.3 and I have not been touching it since then. If there will be  \nchanges in the future one will have to keep IIW in sync, something  \nthat's easy to forget.\nThere is no locking which will cause concurrent modification  \nexceptions when accessing the index via searcher/reader while  \ncommitting.\nIt use the old token stream API so it has to be upgraded in case it  \nshould stay.\n\nThe java- and package level docs have since it was committed been  \nsuggesting that one should consider using II as if it was immutable  \ndue to the locklessness. My suggestion is that we make it immutable  \nfor real.\n\nSince II is ment for small corpora there is very little time lost by  \nusing the constructor that builts the index from an IndexReader. I.e.  \nrather than using InstantiatedIndexWriter one would have to use a  \nDirectory and an IndexWriter and then pass an IndexReader to a new  \nInstantiatedIndex.\n\nAny objections?",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-554",
        "summary": "Possible index corruption if crashing while replacing segments file",
        "description": "Lucene's indexing is expected to be reasonably tolerant to computer crashes or the indexing process being killed. By reasonably tolerant, I mean that it is ok to lose a few documents (those currently buffered in memory), or have to repeat some work (e.g., a long merge that was in progress) - but it is not ok for the entire index, or large chunks of it, to become irreversebly corrupt.\n\nThe fact that Lucene works by repeated merging of several small segments into a new larger segments, solves most of the crash problems, because until the new segment is fully created, the old segments are still there and fully functional. However, one possibility for corruption remains in the segment replacement code:\n\nAfter a new segment is created, a new segments file is written as a new file \"segments.new\", and then this file is renamed to \"segments\". The problem is that this renaming is done using Directory.renameFile(), and FSDirectory.renameFile is *NOT* atomic: it first deletes the old file, and then renames the new file. A crash between these stages (or perhaps during Java's rename which also isn't guaranteed to be atomic) will potentially leave us without a working \"segments\" file.\n\nI will post here a patch for this bug shortly.\n\nThe patch will also include a change to Directory.renameFile()'s Javadoc. It currently claims \"This replacement should be atomic.\", which is false in FSDirectory. Instead it should make a weaker claim, for example\n   \"This replacement does not have to be atomic, but must at least obey a weaker guarantee: at any time during the replacement, either the \"from\" file is still available, or the \"to\" file is available with either the new or old content.\"\n(or, we can just drop the guaranteee altogether, like Java's File.renameTo() provides no atomic-ness guarantees).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1544",
        "summary": "Deadlock: IndexWriter.addIndexes(IndexReader[])",
        "description": "A deadlock issue occurs under the following circumstances\n- IndexWriter.autoCommit == true\n- IndexWriter.directory contains multiple segments\n- IndexWriter.AddIndex(IndexReader[]) is invoked\n\nI put together a JUnit test that recreates the deadlock, which I've attached.  It is the first test method, 'testAddIndexByIndexReader()'.\n\nIn a nutshell, here is what happens:\n\n        // 1) AddIndexes(IndexReader[]) acquires the write lock,\n        // then begins optimization of destination index (this is\n        // prior to adding any external segments).\n        //\n        // 2) Main thread starts a ConcurrentMergeScheduler.MergeThread\n        // to merge the 2 segments.\n        //\n        // 3) Merging thread tries to acquire the read lock at\n        // IndexWriter.blockAddIndexes(boolean) in\n        // IndexWriter.StartCommit(), but cannot as...\n        //\n        // 4) Main thread still holds the write lock, and is\n        // waiting for the IndexWriter.runningMerges data structure\n        // to be devoid of merges with their optimize flag\n        // set (IndexWriter.optimizeMergesPending()).\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2486",
        "summary": "when opening the merged SegmentReader, IW attempts to open store files that were deleted",
        "description": "The issue happens when a merge runs that does not merge the doc stores, those doc stores are still being written to, IW is using CFS, and while the merge is running the doc stores get closed and turned into a cfx file.\n\nWhen we then try to open the reader (for warming), which as of LUCENE-2311 will now [correctly] open the doc stores, we hit FNFE because the SegmentInfo for the merge does not realize that the doc stores were turned into  a cfx.\n\nThis issue does affect trunk; if you crank up the #docs in the test, it happens consistently (I will tie this to _TestUtil.getRandomMultiplier!).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3473",
        "summary": "CheckIndex should verify numUniqueTerms == recomputedNumUniqueTerms",
        "description": "Just glancing at the code it seems to sorta do this check, but only in the hasOrd==true case maybe (which seems to be testing something else)?\n\nIt would be nice to verify this also for terms dicts that dont support ord.\n\nwe should add explicit checks per-field in 4.x, and for-all-fields in 3.x and preflex",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1780",
        "summary": "deprecate Scorer.explain",
        "description": "Spinoff from LUCENE-1749.\n\nWe already have QueryWeight.explain, which is directly invoked by IndexSearcher.explain.  Some queries in turn will defer to their Scorer impl's explain, but many do not (and their Scorer.explain simply throw UOE).  So we should deprecate & remove Scorer.explain, leaving it up to individual queries to define that method if they need it.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2663",
        "summary": "wrong exception from NativeFSLockFactory (LIA2 test case)",
        "description": "As part of integrating Lucene In Action 2 test cases (LUCENE-2661), I found one of the test cases fail\n\nthe test is pretty simple, and passes on 3.0. The exception you get instead (LockReleaseFailedException) is \npretty confusing and I think we should fix it.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-947",
        "summary": "Some improvements to contrib/benchmark",
        "description": "I've made some small improvements to the contrib/benchmark, mostly\nmerging in the ad-hoc benchmarking code I've been using in LUCENE-843:\n\n  - Fixed thread safety of DirDocMaker's usage of SimpleDateFormat\n\n  - Print the props in sorted order\n\n  - Added new config \"autocommit=true|false\" to CreateIndexTask\n\n  - Added new config \"ram.flush.mb=int\" to AddDocTask\n\n  - Added new configs \"doc.term.vector.positions=true|false\" and\n    \"doc.term.vector.offsets=true|false\" to BasicDocMaker\n\n  - Added WriteLineDocTask.java, so you can make an alg that uses this\n    to build up a single file containing one document per line in a\n    single file.  EG this alg converts the reuters-out tree into a\n    single file that has ~1000 bytes per body field, saved to\n    work/reuters.1000.txt:\n\n      docs.dir=reuters-out\n      doc.maker=org.apache.lucene.benchmark.byTask.feeds.DirDocMaker\n      line.file.out=work/reuters.1000.txt\n      doc.maker.forever=false\n      {WriteLineDoc(1000)}: *\n\n    Each line has tab-separted TITLE, DATE, BODY fields.\n\n  - Created feeds/LineDocMaker.java that creates documents read from\n    the file created by WriteLineDocTask.java.  EG this alg indexes\n    all documents created above:\n\n      analyzer=org.apache.lucene.analysis.SimpleAnalyzer\n      directory=FSDirectory\n      doc.add.log.step=500\n\n      docs.file=work/reuters.1000.txt\n      doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n      doc.tokenized=true\n      doc.maker.forever=false\n\n      ResetSystemErase\n      CreateIndex\n      {AddDoc}: *\n      CloseIndex\n\n      RepSumByPref AddDoc\n\nI'll attach initial patch shortly.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1587",
        "summary": "RangeQuery equals method does not compare collator property fully",
        "description": "The equals method in the range query has the collator comparison implemented as:\n(this.collator != null && ! this.collator.equals(other.collator))\n\nWhen _this.collator = null_ and _other.collator = someCollator_  this method will incorrectly assume they are equal. \n\nSo adding something like\n|| (this.collator == null && other.collator != null)\nwould fix the problem\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2179",
        "summary": "CharArraySet.clear()",
        "description": "I needed CharArraySet.clear() for something I was working on in Solr in a tokenstream.\n\ninstead I ended up using CharArrayMap<Boolean> because it supported .clear()\n\nit would be better to use a set though, currently it will throw UOE for .clear() because AbstractSet will call iterator.remove() which throws UOE.\n\nIn Solr, the very similar CharArrayMap.clear() looks like this:\n{code}\n  @Override\n  public void clear() {\n    count = 0;\n    Arrays.fill(keys,null);\n    Arrays.fill(values,null);\n  }\n{code}\n\nI think we can do a similar thing as long as we throw UOE for the UnmodifiableCharArraySet\n\nwill submit a patch later tonight (unless someone is bored and has nothing better to do)",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1282",
        "summary": "Sun hotspot compiler bug in 1.6.0_04/05 affects Lucene",
        "description": "This is not a Lucene bug.  It's an as-yet not fully characterized Sun\nJRE bug, as best I can tell.  I'm opening this to gather all things we\nknow, and to work around it in Lucene if possible, and maybe open an\nissue with Sun if we can reduce it to a compact test case.\n\nIt's hit at least 3 users:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3c8c4e68610803180438x39737565q9f97b4802ed774a5@mail.gmail.com%3e\n  http://mail-archives.apache.org/mod_mbox/lucene-solr-user/200804.mbox/%3c4807654E.7050900@virginia.edu%3e\n  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c733777220805060156t7fdb8fectf0bc984fbfe48a22@mail.gmail.com%3e\n\nIt's specific to at least JRE 1.6.0_04 and 1.6.0_05, that affects\nLucene.  Whereas 1.6.0_03 works OK and it's unknown whether 1.6.0_06\nshows it.\n\nThe bug affects bulk merging of stored fields.  When it strikes, the\nsegment produced by a merge is corrupt because its fdx file (stored\nfields index file) is missing one document.  After iterating many\ntimes with the first user that hit this, adding diagnostics &\nassertions, its seems that a call to fieldsWriter.addDocument some\neither fails to run entirely, or, fails to invoke its call to\nindexStream.writeLong.  It's as if when hotspot compiles a method,\nthere's some sort of race condition in cutting over to the compiled\ncode whereby a single method call fails to be invoked (speculation).\n\nUnfortunately, this corruption is silent when it occurs and only later\ndetected when a merge tries to merge the bad segment, or an\nIndexReader tries to open it.  Here's a typical merge exception:\n\n{code}\nException in thread \"Thread-10\" \norg.apache.lucene.index.MergePolicy$MergeException: \norg.apache.lucene.index.CorruptIndexException:\n    doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:271)\nCaused by: org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000\n        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)\n        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)\n        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:221)\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3099)\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2834)\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:240)\n{code}\n\nand here's a typical exception hit when opening a searcher:\n\n{code}\norg.apache.lucene.index.CorruptIndexException: doc counts differ for segment _kk: fieldsReader shows 72670 but segmentInfo shows 72671\n        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)\n        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)\n        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:230)\n        at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:73)\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:636)\n        at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)\n        at org.apache.lucene.index.IndexReader.open(IndexReader.java:209)\n        at org.apache.lucene.index.IndexReader.open(IndexReader.java:173)\n        at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:48)\n{code}\n\nSometimes, adding -Xbatch (forces up front compilation) or -Xint\n(disables compilation) to the java command line works around the\nissue.\n\nHere are some of the OS's we've seen the failure on:\n\n{code}\nSuSE 10.0\nLinux phoebe 2.6.13-15-smp #1 SMP Tue Sep 13 14:56:15 UTC 2005 x86_64 \nx86_64 x86_64 GNU/Linux \n\nSuSE 8.2\nLinux phobos 2.4.20-64GB-SMP #1 SMP Mon Mar 17 17:56:03 UTC 2003 i686 \nunknown unknown GNU/Linux \n\nRed Hat Enterprise Linux Server release 5.1 (Tikanga)\nLinux lab8.betech.virginia.edu 2.6.18-53.1.14.el5 #1 SMP Tue Feb 19 \n07:18:21 EST 2008 i686 i686 i386 GNU/Linux\n{code}\n\nI've already added assertions to Lucene to detect when this bug\nstrikes, but since assertions are not usually enabled, I plan to add a\nreal check to catch when this bug strikes *before* we commit the merge\nto the index.  This way we can detect & quarantine the failure and\nprevent corruption from entering the index.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-291",
        "summary": "[PATCH] FSDirectory create() method deletes all files",
        "description": "hi all,\n\nthe current implementation of FSDirectory.create(...) silently deletes all files\n(even empty directories) within the index directory when setting up a new index\nwith create option enabled. Lucene doesn't care when deleting files in the index\ndirectory if they  belong to lucene or not. I don't think that this is a real\nbug, but it can be a pain if somebody whants to store some private information\nin the lucene index directory, e.g some configuration files.\n\nTherefore i implemented a FileFilter which knows about the internal lucene file\nextensions, so that all other files would never get touched when creating a new\nindex. The current patch is an enhancement in FSDirectory only. I don't think\nthat there is a need to make it available in the Directory class and change all\nit's depending classes.\n\nregards\nBernhard",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-771",
        "summary": "Change default write lock file location to index directory (not java.io.tmpdir)",
        "description": "Now that readers are read-only, we no longer need to store lock files\nin a different global lock directory than the index directory.  This\nhas been a source of confusion and caused problems to users in the\npast.\n\nFurthermore, once the write lock is stored in the index directory, it\nno longer needs the big digest prefix that was previously required\nto make sure lock files in the global lock directory, from different\nindexes, did not conflict.\n\nThis way, all files related to an index will appear in a single\ndirectory.  And you can easily list that directory to see if a\n\"write.lock\" is present to check whether a writer is open on the\nindex.\n\nNote that this change just affects how FSDirectory creates its default\nlockFactory if no lockFactory was specified.  It is still possible\n(just no longer the default) to pick a different directory to store\nyour lock files by pre-instantiating your own LockFactory.\n\nAs part of this I would like to remove LOCK_DIR and the no-argument\nconstructor, in SimpleFSLockFactory and NativeFSLockFactory.  I don't\nthink we should have the notion of a global default lock directory\nanymore.  This is actually an API change.  However, neither\nSimpleFSLockFactory nor NativeFSLockFactory haver been released yet,\nso I think this API removal is allowed?\n\nFinally I want to deprecate (but not yet remove, because this has been\nin the API for many releases) the static LOCK_DIR that's in\nFSDirectory.  But it's now entirely unused.\n\nSee here for discussion leading to this:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/43940\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3189",
        "summary": "TestIndexWriter.testThreadInterruptDeadlock failed (can't reproduce)",
        "description": "trunk: r1134163 \n\nran it a few times with tests.iter=200 and couldn't reproduce, but i believe you like an issue anyway.\n\n{code}\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter\n    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):     FAILED\n    [junit]\n    [junit] junit.framework.AssertionFailedError:\n    [junit]     at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:1203)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)\n    [junit]\n    [junit]\n    [junit] Tests run: 40, Failures: 1, Errors: 0, Time elapsed: 23.79 sec\n    [junit]\n    [junit] ------------- Standard Output ---------------\n    [junit] CheckIndex failed\n    [junit] ERROR: could not read any segments file in directory\n    [junit] java.io.FileNotFoundException: segments_2w\n    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)\n    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)\n    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)\n    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)\n    [junit]     at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:287)\n    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)\n    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:533)\n    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:283)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:311)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)\n    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)\n    [junit]\n    [junit] CheckIndex FAILED: unexpected exception\n    [junit] java.lang.RuntimeException: CheckIndex failed\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)\n    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)\n    [junit] IndexReader.open FAILED: unexpected exception\n    [junit] java.io.FileNotFoundException: segments_2w\n    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)\n    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)\n    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)\n    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)\n    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:88)\n    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)\n    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:84)\n    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:500)\n    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:293)\n    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1161)\n\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=6733070832417768606:3130345095020099096\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockRandom, f6=SimpleText, f7=MockRandom, f8=MockSep, f9=Standard, f1=SimpleText, f0=Standard, f3=Standard, f2=MockSep, f5=Pulsing(freqCutoff=12),\n f4=MockFixedIntBlock(blockSize=552), c=MockVariableIntBlock(baseBlockSize=43), d9=MockVariableIntBlock(baseBlockSize=43), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=12), d7=MockFixedIntBlock(blockSize=55\n2), d6=MockVariableIntBlock(baseBlockSize=43), d25=MockSep, d0=MockVariableIntBlock(baseBlockSize=43), c29=MockFixedIntBlock(blockSize=552), d24=Pulsing(freqCutoff=12), d1=MockFixedIntBlock(blockSize=552), c28=\nStandard, d23=SimpleText, d2=SimpleText, c27=MockSep, d22=Standard, d3=MockRandom, d21=MockRandom, d20=SimpleText, c22=MockSep, c21=Pulsing(freqCutoff=12), c20=SimpleText, d29=SimpleText, c26=MockVariableIntBlo\nck(baseBlockSize=43), d28=Standard, c25=MockRandom, d27=MockVariableIntBlock(baseBlockSize=43), c24=Pulsing(freqCutoff=12), d26=MockRandom, c23=MockFixedIntBlock(blockSize=552), e9=Pulsing(freqCutoff=12), e8=St\nandard, e7=MockSep, e6=MockRandom, e5=SimpleText, c17=MockFixedIntBlock(blockSize=552), e3=MockFixedIntBlock(blockSize=552), d12=Pulsing(freqCutoff=12), c16=MockVariableIntBlock(baseBlockSize=43), e4=Pulsing(fr\neqCutoff=12), d11=MockFixedIntBlock(blockSize=552), c19=MockRandom, e1=MockSep, d14=MockVariableIntBlock(baseBlockSize=43), c18=SimpleText, e2=Standard, d13=MockRandom, e0=SimpleText, d10=MockSep, d19=MockVaria\nbleIntBlock(baseBlockSize=43), c11=MockVariableIntBlock(baseBlockSize=43), c10=MockRandom, d16=Standard, c13=MockRandom, c12=SimpleText, d15=MockSep, d18=Pulsing(freqCutoff=12), c15=Standard, d17=MockFixedIntBl\nock(blockSize=552), c14=MockSep, b3=Standard, b2=MockSep, b5=Pulsing(freqCutoff=12), b4=MockFixedIntBlock(blockSize=552), b7=MockFixedIntBlock(blockSize=552), b6=MockVariableIntBlock(baseBlockSize=43), d50=Mock\nRandom, b9=MockRandom, b8=SimpleText, d43=SimpleText, d42=Standard, d41=MockVariableIntBlock(baseBlockSize=43), d40=MockRandom, d47=Pulsing(freqCutoff=12), d46=MockFixedIntBlock(blockSize=552), b0=MockRandom, d\n45=Standard, b1=MockVariableIntBlock(baseBlockSize=43), d44=MockSep, d49=MockRandom, d48=SimpleText, c6=SimpleText, c5=Standard, c4=MockVariableIntBlock(baseBlockSize=43), c3=MockRandom, c9=MockFixedIntBlock(bl\nockSize=552), c8=Standard, c7=MockSep, d30=Standard, d32=Pulsing(freqCutoff=12), d31=MockFixedIntBlock(blockSize=552), c1=Pulsing(freqCutoff=12), d34=MockFixedIntBlock(blockSize=552), c2=MockSep, d33=MockVariab\nleIntBlock(baseBlockSize=43), d36=MockRandom, c0=SimpleText, d35=SimpleText, d38=MockSep, d37=Pulsing(freqCutoff=12), d39=MockVariableIntBlock(baseBlockSize=43), e92=MockFixedIntBlock(blockSize=552), e93=Pulsin\ng(freqCutoff=12), e90=MockSep, e91=Standard, e89=Standard, e88=MockVariableIntBlock(baseBlockSize=43), e87=MockRandom, e86=MockFixedIntBlock(blockSize=552), e85=MockVariableIntBlock(baseBlockSize=43), e84=MockS\nep, e83=Pulsing(freqCutoff=12), e80=MockFixedIntBlock(blockSize=552), e81=SimpleText, e82=MockRandom, e77=Standard, e76=MockSep, e79=Pulsing(freqCutoff=12), e78=MockFixedIntBlock(blockSize=552), e73=MockVariabl\neIntBlock(baseBlockSize=43), e72=MockRandom, e75=SimpleText, e74=Standard, binary=MockSep, f98=MockRandom, f97=SimpleText, f99=MockSep, f94=Pulsing(freqCutoff=12), f93=MockFixedIntBlock(blockSize=552), f96=Mock\nVariableIntBlock(baseBlockSize=43), f95=MockRandom, e95=MockRandom, e94=SimpleText, e97=Standard, e96=MockSep, e99=MockSep, e98=Pulsing(freqCutoff=12), id=Standard, f34=SimpleText, f33=Standard, f32=MockVariabl\neIntBlock(baseBlockSize=43), f31=MockRandom, f30=MockFixedIntBlock(blockSize=552), f39=SimpleText, f38=MockVariableIntBlock(baseBlockSize=43), f37=MockRandom, f36=Pulsing(freqCutoff=12), f35=MockFixedIntBlock(b\nlockSize=552), f43=MockSep, f42=Pulsing(freqCutoff=12), f45=MockFixedIntBlock(blockSize=552), f44=MockVariableIntBlock(baseBlockSize=43), f41=Standard, f40=MockSep, f47=SimpleText, f46=Standard, f49=MockSep, f4\n8=Pulsing(freqCutoff=12), content=Standard, e19=Standard, e18=MockSep, e17=SimpleText, f12=MockRandom, e16=Standard, f11=SimpleText, f10=MockFixedIntBlock(blockSize=552), e15=MockVariableIntBlock(baseBlockSize=\n43), e14=MockRandom, f16=MockFixedIntBlock(blockSize=552), e13=MockSep, e12=Pulsing(freqCutoff=12), f15=MockVariableIntBlock(baseBlockSize=43), e11=SimpleText, f14=MockSep, e10=Standard, f13=Pulsing(freqCutoff=\n12), f19=Standard, f18=MockVariableIntBlock(baseBlockSize=43), f17=MockRandom, e29=MockRandom, e26=MockSep, f21=Standard, e25=Pulsing(freqCutoff=12), f20=MockSep, e28=MockFixedIntBlock(blockSize=552), f23=Pulsi\nng(freqCutoff=12), e27=MockVariableIntBlock(baseBlockSize=43), f22=MockFixedIntBlock(blockSize=552), f25=MockRandom, e22=MockFixedIntBlock(blockSize=552), f24=SimpleText, e21=MockVariableIntBlock(baseBlockSize=\n43), f27=Standard, e24=MockRandom, f26=MockSep, e23=SimpleText, f29=MockSep, f28=Pulsing(freqCutoff=12), e20=Pulsing(freqCutoff=12), field=MockSep, string=MockVariableIntBlock(baseBlockSize=43), e30=MockFixedIn\ntBlock(blockSize=552), e31=Pulsing(freqCutoff=12), a98=MockSep, e34=SimpleText, a99=Standard, e35=MockRandom, f79=MockSep, e32=MockVariableIntBlock(baseBlockSize=43), e33=MockFixedIntBlock(blockSize=552), b97=M\nockRandom, f77=MockRandom, e38=MockVariableIntBlock(baseBlockSize=43), b98=MockVariableIntBlock(baseBlockSize=43), f78=MockVariableIntBlock(baseBlockSize=43), e39=MockFixedIntBlock(blockSize=552), b99=Standard,\n f75=MockFixedIntBlock(blockSize=552), e36=Pulsing(freqCutoff=12), f76=Pulsing(freqCutoff=12), e37=MockSep, f73=Pulsing(freqCutoff=12), f74=MockSep, f71=Standard, f72=SimpleText, f81=Standard, f80=MockSep, e40=\nMockVariableIntBlock(baseBlockSize=43), e41=Standard, e42=SimpleText, e43=MockSep, e44=Standard, e45=MockFixedIntBlock(blockSize=552), e46=Pulsing(freqCutoff=12), f86=Standard, e47=SimpleText, f87=SimpleText, e\n48=MockRandom, f88=Pulsing(freqCutoff=12), e49=MockSep, f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=43), f83=MockFixedIntBlock(blockSize=552), f84=SimpleText, f85=MockRandom, f90=Pulsing(freqCutoff=12),\n f92=MockVariableIntBlock(baseBlockSize=43), f91=MockRandom, str=MockRandom, a76=Standard, e56=Standard, f59=Pulsing(freqCutoff=12), a77=SimpleText, e57=SimpleText, a78=Pulsing(freqCutoff=12), e54=MockRandom, f\n57=Standard, a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=43), f58=SimpleText, e52=MockVariableIntBlock(baseBlockSize=43), e53=MockFixedIntBlock(blockSize=552), e50=Pulsing(freqCutoff=12), e51=MockSep, f\n51=MockSep, f52=Standard, f50=MockRandom, f55=MockVariableIntBlock(baseBlockSize=43), f56=MockFixedIntBlock(blockSize=552), f53=Pulsing(freqCutoff=12), e58=MockFixedIntBlock(blockSize=552), f54=MockSep, e59=Pul\nsing(freqCutoff=12), a80=Pulsing(freqCutoff=12), e60=Pulsing(freqCutoff=12), a82=MockVariableIntBlock(baseBlockSize=43), a81=MockRandom, a84=MockRandom, a83=SimpleText, a86=Standard, a85=MockSep, a89=SimpleText\n, f68=MockVariableIntBlock(baseBlockSize=43), e65=Pulsing(freqCutoff=12), f69=MockFixedIntBlock(blockSize=552), e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=43), e67=MockVariableIntBlock(baseBlockSize=43\n), a88=MockFixedIntBlock(blockSize=552), e68=MockFixedIntBlock(blockSize=552), e61=SimpleText, e62=MockRandom, e63=MockSep, e64=Standard, f60=MockFixedIntBlock(blockSize=552), f61=Pulsing(freq\n\nCutoff=12), f62=MockRandom, f63=MockVariableIntBlock(baseBlockSize=43), e69=Standard, f64=SimpleText, f65=MockRandom, f66=MockSep, f67=Standard, f70=MockFixedIntBlock(blockSize=552), a93=MockSep, a92=Pulsing(freqCutoff=12), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockVariableIntBlock(baseBlockSize=43), a96=MockRandom, a95=Pulsing(freqCutoff=12), a94=MockFixedIntBlock(blockSize=552), c58=MockRandom, a63=MockFixedIntBlock(blockSize=552), a64=Pulsing(freqCutoff=12), c59=MockVariableIntBlock(baseBlockSize=43), c56=MockFixedIntBlock(blockSize=552), d59=MockRandom, a61=MockSep, c57=Pulsing(freqCutoff=12), a62=Standard, c54=Pulsing(freqCutoff=12), c55=MockSep, a60=SimpleText, c52=Standard, c53=SimpleText, d53=SimpleText, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=43), d52=MockFixedIntBlock(blockSize=552), d57=Pulsing(freqCutoff=12), b62=Standard, d58=MockSep, b63=SimpleText, d55=Standard, b60=MockRandom, d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=43), b56=Standard, b55=MockSep, b54=MockRandom, b53=SimpleText, d61=MockVariableIntBlock(baseBlockSize=43), b59=MockVariableIntBlock(baseBlockSize=43), d60=MockRandom, b58=MockSep, b57=Pulsing(freqCutoff=12), c62=Standard, c61=MockSep, a59=MockVariableIntBlock(baseBlockSize=43), c60=MockRandom, a58=MockRandom, a57=MockFixedIntBlock(blockSize=552), a56=MockVariableIntBlock(baseBlockSize=43), a55=MockSep, a54=Pulsing(freqCutoff=12), a72=MockRandom, c67=Standard, a73=MockVariableIntBlock(baseBlockSize=43), c68=SimpleText, a74=Standard, c69=Pulsing(freqCutoff=12), a75=SimpleText, c63=MockVariableIntBlock(baseBlockSize=43), c64=MockFixedIntBlock(blockSize=552), a70=MockVariableIntBlock(baseBlockSize=43), c65=SimpleText, a71=MockFixedIntBlock(blockSize=552), c66=MockRandom, d62=MockSep, d63=Standard, d64=MockFixedIntBlock(blockSize=552), b70=Standard, d65=Pulsing(freqCutoff=12), b71=Pulsing(freqCutoff=12), d66=MockVariableIntBlock(baseBlockSize=43), b72=MockSep, d67=MockFixedIntBlock(blockSize=552), b73=MockVariableIntBlock(baseBlockSize=43), d68=SimpleText, b74=MockFixedIntBlock(blockSize=552), d69=MockRandom, b65=Pulsing(freqCutoff=12), b64=MockFixedIntBlock(blockSize=552), b67=MockVariableIntBlock(baseBlockSize=43), b66=MockRandom, d70=SimpleText, b69=MockRandom, b68=SimpleText, d72=MockSep, d71=Pulsing(freqCutoff=12), c71=Pulsing(freqCutoff=12), c70=MockFixedIntBlock(blockSize=552), a69=Pulsing(freqCutoff=12), c73=MockVariableIntBlock(baseBlockSize=43), c72=MockRandom, a66=MockRandom, a65=SimpleText, a68=Standard, a67=MockSep, c32=MockSep, c33=Standard, c30=SimpleText, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=43), a41=Pulsing(freqCutoff=12), c37=MockFixedIntBlock(blockSize=552), a42=MockSep, a0=MockRandom, c34=Pulsing(freqCutoff=12), c35=MockSep, a40=SimpleText, b84=MockSep, d79=MockFixedIntBlock(blockSize=552), b85=Standard, b82=SimpleText, d77=MockSep, c38=Standard, b83=MockRandom, d78=Standard, c39=SimpleText, b80=MockRandom, d75=Standard, b81=MockVariableIntBlock(baseBlockSize=43), d76=SimpleText, d73=MockRandom, d74=MockVariableIntBlock(baseBlockSize=43), d83=MockRandom, a9=MockFixedIntBlock(blockSize=552), d82=SimpleText, d81=MockFixedIntBlock(blockSize=552), d80=MockVariableIntBlock(baseBlockSize=43), b79=MockFixedIntBlock(blockSize=552), b78=MockSep, b77=Pulsing(freqCutoff=12), b76=SimpleText, b75=Standard, a1=Pulsing(freqCutoff=12), a35=Pulsing(freqCutoff=12), a2=MockSep, a34=MockFixedIntBlock(blockSize=552), a3=MockVariableIntBlock(baseBlockSize=43), a33=Standard, a4=MockFixedIntBlock(blockSize=552), a32=MockSep, a5=MockRandom, a39=MockRandom, c40=SimpleText, a6=MockVariableIntBlock(baseBlockSize=43), a38=SimpleText, a7=Standard, a37=MockFixedIntBlock(blockSize=552), a8=SimpleText, a36=MockVariableIntBlock(baseBlockSize=43), c41=MockFixedIntBlock(blockSize=552), c42=Pulsing(freqCutoff=12), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=43), c45=SimpleText, a50=MockVariableIntBlock(baseBlockSize=43), c46=MockRandom, a51=MockFixedIntBlock(blockSize=552), c47=MockSep, a52=SimpleText, c48=Standard, a53=MockRandom, b93=MockFixedIntBlock(blockSize=552), d88=MockRandom, c49=MockVariableIntBlock(baseBlockSize=43), b94=Pulsing(freqCutoff=12), d89=MockVariableIntBlock(baseBlockSize=43), b95=MockRandom, b96=MockVariableIntBlock(baseBlockSize=43), d84=Pulsing(freqCutoff=12), b90=SimpleText, d85=MockSep, b91=Pulsing(freqCutoff=12), d86=MockVariableIntBlock(baseBlockSize=43), b92=MockSep, d87=MockFixedIntBlock(blockSize=552), d92=Standard, d91=MockSep, d94=Pulsing(freqCutoff=12), d93=MockFixedIntBlock(blockSize=552), b87=MockFixedIntBlock(blockSize=552), b86=MockVariableIntBlock(baseBlockSize=43), d90=SimpleText, b89=MockRandom, b88=SimpleText, a44=MockVariableIntBlock(baseBlockSize=43), a43=MockRandom, a46=SimpleText, a45=Standard, a48=Standard, a47=MockSep, c51=MockFixedIntBlock(blockSize=552), a49=MockFixedIntBlock(blockSize=552), c50=MockVariableIntBlock(baseBlockSize=43), d98=MockFixedIntBlock(blockSize=552), d97=MockVariableIntBlock(baseBlockSize=43), d96=MockSep, d95=Pulsing(freqCutoff=12), d99=MockRandom, a20=MockVariableIntBlock(baseBlockSize=43), c99=SimpleText, c98=Standard, c97=MockVariableIntBlock(baseBlockSize=43), c96=MockRandom, b19=MockVariableIntBlock(baseBlockSize=43), a16=Pulsing(freqCutoff=12), a17=MockSep, b17=Pulsing(freqCutoff=12), a14=Standard, b18=MockSep, a15=SimpleText, a12=SimpleText, a13=MockRandom, a10=MockVariableIntBlock(baseBlockSize=43), a11=MockFixedIntBlock(blockSize=552), b11=MockFixedIntBlock(blockSize=552), b12=Pulsing(freqCutoff=12), b10=Standard, b15=SimpleText, b16=MockRandom, a18=MockRandom, b13=MockVariableIntBlock(baseBlockSize=43), a19=MockVariableIntBlock(baseBlockSize=43), b14=MockFixedIntBlock(blockSize=552), b30=MockRandom, a31=MockSep, a30=Pulsing(freqCutoff=12), b28=SimpleText, a25=MockVariableIntBlock(baseBlockSize=43), b29=MockRandom, a26=MockFixedIntBlock(blockSize=552), a27=SimpleText, a28=MockRandom, a21=MockSep, a22=Standard, a23=MockFixedIntBlock(blockSize=552), a24=Pulsing(freqCutoff=12), b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=43), b22=Standard, b23=SimpleText, a29=Pulsing(freqCutoff=12), b24=MockSep, b25=Standard, b26=MockFixedIntBlock(blockSize=552), b27=Pulsing(freqCutoff=12), b41=Pulsing(freqCutoff=12), b40=MockFixedIntBlock(blockSize=552), c77=MockRandom, c76=SimpleText, c75=MockFixedIntBlock(blockSize=552), c74=MockVariableIntBlock(baseBlockSize=43), c79=SimpleText, c78=Standard, c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=43), c81=MockFixedIntBlock(blockSize=552), b39=MockFixedIntBlock(blockSize=552), c82=Pulsing(freqCutoff=12), b37=Standard, b38=SimpleText, b35=MockRandom, b36=MockVariableIntBlock(baseBlockSize=43), b33=MockVariableIntBlock(baseBlockSize=43), b34=MockFixedIntBlock(blockSize=552), b31=Pulsing(freqCutoff=12), b32=MockSep, str2=MockSep, b50=MockVariableIntBlock(baseBlockSize=43), b52=SimpleText, str3=MockVariableIntBlock(baseBlockSize=43), b51=Standard, c86=Standard, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=12), c87=MockFixedIntBlock(blockSize=552), c89=MockVariableIntBlock(baseBlockSize=43), c90=SimpleText, c91=MockRandom, c92=Standard, c93=SimpleText, c94=Pulsing(freqCutoff=12), c95=MockSep, content1=MockRandom, b46=Pulsing(freqCutoff=12), b47=MockSep, content3=MockFixedIntBlock(blockSize=552), b48=MockVariableIntBlock(baseBlockSize=43), content4=MockVariableIntBlock(baseBlockSize=43), b49=MockFixedIntBlock(blockSize=552), content5=Pulsing(freqCutoff=12), b42=SimpleText, b43=MockRandom, b44=MockSep, b45=Standard}, locale=sk, timezone=America/Rainy_River\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]\n    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=86065896,total=127401984\n    [junit] ------------- ---------------- ---------------\n    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1464",
        "summary": "FSDirectory.getDirectory always creates index path",
        "description": "This was reported to me as a Luke bug, but going deeper it proved to be a non-intuitive (broken?) behavior of FSDirectory.\n\nIf you use FSDirectory.getDirectory(File nonexistent) on a nonexistent path, but one that is located under some existing parent path, then FSDirectory:174 uses file.mkdirs() to create this directory. One would expect a variant of the method with a boolean flag to decide whether or not to create the output path. However, the API with \"create\" flag is now deprecated, with a comment that points to IndexWriter's \"create\" flag. This comment is misleading, because the indicated path is created anyway in the file system just by calling FSDirectory.getDirectory().\n\nI propose to do one of the following:\n\n* reinstate the variant of the method with \"create\" flag. In case if this flag is false, and the index directory is missing, either return null or throw an IOException,\n\n* keep the API as it is now, but either return null or throw IOException if the index dir is missing. This breaks the backwards compatibility, because now users are required to do file.mkdirs() themselves prior to calling FSDirectory.getDirectory().",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-401",
        "summary": "[PATCH] fixes for gcj target.",
        "description": "I've modified the Makefile so that it compiles with GCJ-4.0.\n\nThis involved fixing the CORE_OBJ macro to match the generated jar file as well\nas excluding FieldCacheImpl from being used from its .java source (GCJ has\nproblems with anonymous inner classes, I guess).\n\nAlso, I changed the behaviour of FieldInfos.fieldInfo(int). It depended on\ncatching IndexOutOfBoundsException exception. I've modified it to test the\nbounds first, returning -1 in that case. This helps with gcj since we build with\n-fno-bounds-check.\n\nI compiled with;\n\nGCJ=gcj-4.0 GCJH=gcjh-4.0 GPLUSPLUS=g++-4.0 ant clean gcj\n\npatch to follow.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1163",
        "summary": "CharArraySet.contains(char[] text, int off, int len) does not work",
        "description": "I try to use the CharArraySet for a filter I am writing. I heavily use char-arrays in my code to speed up things. I stumbled upon a bug in CharArraySet while doing that.\n\nThe method _public boolean contains(char[] text, int off, int len)_ seems not to work.\n\nWhen I do \n\n{code}\nif (set.contains(buffer,offset,length) {\n  ...\n}\n{code}\n\nmy code fails.\n\nBut when I do\n\n{code}\nif (set.contains(new String(buffer,offset,length)) {\n   ...\n}\n{code}\n\neverything works as expected.\n\nBoth variants should behave the same. I attach a small piece of code to show the problem.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1719",
        "summary": "Add javadoc notes about ICUCollationKeyFilter's advantages over CollationKeyFilter",
        "description": "contrib/collation's ICUCollationKeyFilter, which uses ICU4J collation, is faster than CollationKeyFilter, the JVM-provided java.text.Collator implementation in the same package.  The javadocs of these classes should be modified to add a note to this effect.\n\nMy curiosity was piqued by [Robert Muir's comment|https://issues.apache.org/jira/browse/LUCENE-1581?focusedCommentId=12720300#action_12720300] on LUCENE-1581, in which he states that ICUCollationKeyFilter is up to 30x faster than CollationKeyFilter.\n\nI timed the operation of these two classes, with Sun JVM versions 1.4.2/32-bit, 1.5.0/32- and 64-bit, and 1.6.0/64-bit, using 90k word lists of 4 languages (taken from the corresponding Debian wordlist packages and truncated to the first 90k words after a fixed random shuffling), using Collators at the default strength, on a Windows Vista 64-bit machine.  I used an analysis pipeline consisting of WhitespaceTokenizer chained to the collation key filter, so to isolate the time taken by the collation key filters, I also timed WhitespaceTokenizer operating alone for each combination.  The rightmost column represents the performance advantage of the ICU4J implemtation (ICU) over the java.text.Collator implementation (JVM), after discounting the WhitespaceTokenizer time (WST): (JVM-ICU) / (ICU-WST). The best times out of 5 runs for each combination, in milliseconds, are as follows:\n\n||Sun JVM||Language||java.text||ICU4J||WhitespaceTokenizer||ICU4J Improvement||\n|1.4.2_17 (32 bit)|English|522|212|13|156%|\n|1.4.2_17 (32 bit)|French|716|243|14|207%|\n|1.4.2_17 (32 bit)|German|669|264|16|163%|\n|1.4.2_17 (32 bit)|Ukranian|931|474|25|102%|\n|1.5.0_15 (32 bit)|English|604|176|16|268%|\n|1.5.0_15 (32 bit)|French|817|209|17|317%|\n|1.5.0_15 (32 bit)|German|799|225|20|280%|\n|1.5.0_15 (32 bit)|Ukranian|1029|436|26|145%|\n|1.5.0_15 (64 bit)|English|431|89|10|433%|\n|1.5.0_15 (64 bit)|French|562|112|11|446%|\n|1.5.0_15 (64 bit)|German|567|116|13|438%|\n|1.5.0_15 (64 bit)|Ukranian|734|281|21|174%|\n|1.6.0_13 (64 bit)|English|162|81|9|113%|\n|1.6.0_13 (64 bit)|French|192|92|10|122%|\n|1.6.0_13 (64 bit)|German|204|99|14|124%|\n|1.6.0_13 (64 bit)|Ukranian|273|202|21|39%|\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1340",
        "summary": "Make it posible not to include TF information in index",
        "description": "Term Frequency is typically not needed  for all fields, some CPU (reading one VInt less and one X>>>1...) and IO can be spared by making pure boolen fields possible in Lucene. This topic has already been discussed and accepted as a part of Flexible Indexing... This issue tries to push things a bit faster forward as I have some concrete customer demands.\n\nbenefits can be expected for fields that are typical candidates for Filters, enumerations, user rights, IDs or very short \"texts\", phone  numbers, zip codes, names...\n\nStatus: just passed standard test (compatibility), commited for early review, I have not tried new feature, missing some asserts and one two unit tests\n\nComplexity: simpler than expected\n\ncan be used via omitTf() (who used omitNorms() will know where to find it :)  ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3562",
        "summary": "Stop storing TermsEnum in CloseableThreadLocal inside Terms instance",
        "description": "We have sugar methods in Terms.java (docFreq, totalTermFreq, docs,\ndocsAndPositions) that use a saved thread-private TermsEnum to do the\nlookups.\n\nBut on apps that send many threads through Lucene, and/or have many\nsegments, this can add up to a lot of RAM, especially if the codecs\nimpl holds onto stuff.\n\nAlso, Terms has a close method (closes the CloseableThreadLocal) which\nmust be called, but we fail to do so in some places.\n\nThese saved enums are the cause of the recent OOME in TestNRTManager\n(TestNRTManager.testNRTManager -seed\n2aa27e1aec20c4a2:-4a5a5ecf46837d0e:-7c4f651f1f0b75d7 -mult 3\n-nightly).\n\nReally sharing these enums is a holdover from before Lucene queries\nwould share state (ie, save the TermState from the first pass, and use\nit later to pull enums, get docFreq, etc.).  It's not helpful anymore,\nand it can use gobbs of RAM, so I'd like to remove it.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2555",
        "summary": "Remove shared doc stores",
        "description": "With per-thread DocumentsWriters sharing doc stores across segments doesn't make much sense anymore.\n\nSee also LUCENE-2324.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-623",
        "summary": "RAMDirectory.close() should have a comment about not releasing any resources",
        "description": "I wrongly assumed that calling RAMDirectory.close() would free up the memory occupied by the RAMDirectory.\nIt might be helpful to add a javadoc comment that warns users that RAMDirectory.close() is a no-op, since it might be a common assumption that close() would release resources.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2996",
        "summary": "addIndexes(IndexReader) incorrectly applies existing deletes",
        "description": "If you perform these operations:\n# deleteDocuments(Term) for all the new documents\n# addIndexes(IndexReader)\n# commit\n\nThen addIndexes applies the previous deletes on the incoming indexes as well, which is incorrect. If you call addIndexes(Directory) instead, the deletes are applied beforehand, as they should. The solution, as Mike indicated here: http://osdir.com/ml/general/2011-03/msg20876.html is to add *flush(false,true)* to addIndexes(IndexReader).\n\nI will create a patch with a matching unit test shortly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2339",
        "summary": "Allow Directory.copy() to accept a collection of file names to be copied",
        "description": "Par example, I want to copy files pertaining to a certain commit, and not everything there is in a Directory.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1130",
        "summary": "Hitting disk full during DocumentWriter.ThreadState.init(...) can cause hang",
        "description": "More testing of RC2 ...\n\nI found one case, if you hit disk full during init() in\nDocumentsWriter.ThreadState, when we first create the term vectors &\nfields writer, such that subsequent calls to\nIndexWriter.add/updateDocument will then hang forever.\n\nWhat's happening in this case is we are incrementing nextDocID even\nthough we never call finishDocument (because we \"thought\" init did not\nsucceed).  Then, when we finish the next document, it will never\nactually write because missing finishDocument call never happens.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2889",
        "summary": "Remove @lucene.experimental from Numeric*",
        "description": "NumericRangeQuery and NumericField are now there since 2.9. It is still marked as experimental. The API stabilized and there are no changes in the public parts (even in Lucene trunk no changes). Also lot's of people ask, if \"experimental\" means \"unstable\" in general, but it means only \"unstable API\".\n\nI will remove the @lucene.experimental from Numeric* classes. NumericUtils* stays with @lucene.internal, as it is not intended for public use. Some people use it to make \"TermQuery\" on a numeric field, but this should be done using a NRQ with upper==lower and included=true, which does not affect scoring (applies also to Solr).",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1382",
        "summary": "Allow storing user data when IndexWriter.commit() is called",
        "description": "Spinoff from here:\n\n    http://www.mail-archive.com/java-user@lucene.apache.org/msg22303.html\n\nThe idea is to allow optionally passing an opaque String commitUserData to the IndexWriter.commit method.  This String would be stored in the segments_N file, and would be retrievable by an IndexReader.  Applications could then use this to assign meaning to each commit.\n\nIt would be nice to get this done for 2.4, but I don't think we should hold the release for it.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2534",
        "summary": "MultiTermsEnum over-shares between different Docs/AndPositionsEnum",
        "description": "Robert found this in working on LUCENE-2352.\n\nMultiTermsEnum incorrectly shared sub-enums on two different invocation of .docs/AndPositionsEnum.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1834",
        "summary": "remove unused code in SmartChineseAnalyzer hmm pkg",
        "description": "there is some unused code in the hmm package.\n\nI would like to remove it before I supply a fix for LUCENE-1817.\n\nonly after this can we refactor any of this analyzer, otherwise we risk breaking custom dictionary support.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1804",
        "summary": "Can't specify AttributeSource for Tokenizer",
        "description": "One can't currently specify the attribute source for a Tokenizer like one can with any other TokenStream.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1455",
        "summary": "org.apache.lucene.ant.HtmlDocument creates a FileInputStream in its constructor that it doesn't close",
        "description": "A look through the jtidy source code doesn't show a close that i can find in parse (seems to be standard that you close your own streams anyway), so this looks like a small descriptor leak to me.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1874",
        "summary": "Further updates to the site scoring page",
        "description": "update the site scoring page - see Appendix:\n{quote}\nClass Diagrams\nKarl Wettin's UML on the Wiki\n{quote}\nKarl's diagrams are outdated - I think this link should be pulled for 2.9\n\n{quote}\nSequence Diagrams\nFILL IN HERE. Volunteers?\n{quote}\nI think this should be pulled - I say put something like this as a task in JIRA - not the published site docs.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3224",
        "summary": "bugs in ByteArrayDataInput",
        "description": "ByteArrayDataInput has a byte[] ctor, but it doesn't actually work (some things like readVint will work, others will fail due to asserts).\n\nThe problem is it doesnt set things like limit in the ctor... I think the ctor should call reset()\nMost code using this passes null to the ctor to initialize it, then uses reset(), instead they could just call ByteArrayInput(BytesRef.EMPTY_BYTES) if they want to do that.\nfinally, reset()'s limit looks like it should be offset + len",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2184",
        "summary": "CartesianPolyFilterBuilder doesn't properly account for which tiers actually exist in the index ",
        "description": "In the CartesianShapeFilterBuilder, there is logic that determines the \"best fit\" tier to create the Filter against.  However, it does not account for which fields actually exist in the index when doing so.  For instance, if you index tiers 1 through 10, but then choose a very small radius to restrict the space to, it will likely choose a tier like 15 or 16, which of course does not exist.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2284",
        "summary": "MatchAllDocsQueryNode toString() creates invalid XML-Tag",
        "description": "MatchAllDocsQueryNode.toString() returns \"<matchAllDocs field='*' term='*'>\", which is inavlid XML should read \"<matchAllDocs field='*' term='*' />.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1294",
        "summary": "Jar manifest should not contain ${user.name} of the person building",
        "description": "Not sure if it is a big deal, but I don't particularly like that my user id for my build machine is in the manifest of the JAR that I constructed.  It's a stretch, security-wise, I know, but I don't see how it serves any useful purpose.  We have signatures/logs/SVN tags so we know who built the particular item w/o needing to know what their local user account name is.\n\nThe fix is:\n\n{code}\nIndex: common-build.xml\n===================================================================\n--- common-build.xml    (revision 661027)\n+++ common-build.xml    (working copy)\n@@ -281,7 +281,7 @@\n                <attribute name=\"Implementation-Title\" value=\"org.apache.lucene\"/>\n                <!-- impl version can be any string -->\n                <attribute name=\"Implementation-Version\"\n-                          value=\"${version} ${svnversion} - ${user.name} - ${DSTAMP} ${TSTAMP}\"/>\n+                          value=\"${version} ${svnversion} - ${DSTAMP} ${TSTAMP}\"/>\n                <attribute name=\"Implementation-Vendor\"\n                           value=\"The Apache Software Foundation\"/>\n                <attribute name=\"X-Compile-Source-JDK\" \n{code} ",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-230",
        "summary": "Search with sort fails when a document has a missing value",
        "description": "Testing on version: lucene-1.4-rc2\n\nCall in question: IndexSearcher.search(Query query, Filter filter, int nDocs, \nSort sort) \n\nDescription: I'm making a call to search with a sort field - in my case I'm \nsorting by date. If any document in the results set (Hits) has a missing value \nin the sort field, the entire call throws an [uncaught] exception during the \nsorting process with no results returned. \n\nThis is an undesireable result, and the prospects for patching this problem \noutside the search classes are ugly, e.g. trying to patch the index itself.\n\nThis is actually a critical function in my application. Thank you for \naddressing it.\n\n-Dan",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1018",
        "summary": "intermittant exceptions in TestConcurrentMergeScheduler",
        "description": "\nThe TestConcurrentMergeScheduler throws intermittant exceptions that\ndo not result in a test failure.\n\nThe exception happens in the \"testNoWaitClose()\" test, which repeated\ntests closing an IndexWriter with \"false\", meaning abort any\nstill-running merges.  When a merge is aborted it can hit various\nexceptions because the files it is reading and/or writing have been\ndeleted, so we ignore these exceptions.\n\nThe bug was just that we were failing to properly check whether the\nrunning merge was actually aborted because of a scoping issue of the\n\"merge\" variable in ConcurrentMergeScheduler.  So the exceptions are\nactually \"harmless\".  Thanks to Ning for spotting it!\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2300",
        "summary": "IndexWriter should never pool readers for external segments",
        "description": "EG when addIndexes is called, it enrolls external segment infos, which are then merged.  But merging will simply ask the pool for the readers, and if writer is pooling (NRT reader has been pooled) it incorrectly pools these readers.\n\nIt shouldn't break anything but it's a waste because these readers are only used for merging, once, and they are not opened by NRT reader.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3796",
        "summary": "Disallow setBoost() on StringField, throw exception if boosts are set if norms are omitted",
        "description": "Occasionally users are confused why index-time boosts are not applied to their norms-omitted fields.\n\nThis is because we silently discard the boost: there is no reason for this!\n\nThe most absurd part: in 4.0 you can make a StringField and call setBoost and nothing complains... (more reasons to remove StringField totally in my opinion)",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-919",
        "summary": "DefaultSkipListReader should not be public",
        "description": "There's no need for org.apache.lucene.index.DefaultSkipListReader to be public.\nThis class hasn't been released yet, so we should fix this before 2.2.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": ""
    },
    {
        "key": "LUCENE-2763",
        "summary": "Swap URL+Email recognizing StandardTokenizer and UAX29Tokenizer",
        "description": "Currently, in addition to implementing the UAX#29 word boundary rules, StandardTokenizer recognizes email adresses and URLs, but doesn't provide a way to turn this behavior off and/or provide overlapping tokens with the components (username from email address, hostname from URL, etc.).\n\nUAX29Tokenizer should become StandardTokenizer, and current StandardTokenizer should be renamed to something like UAX29TokenizerPlusPlus (or something like that).\n\nFor rationale, see [the discussion at the reopened LUCENE-2167|https://issues.apache.org/jira/browse/LUCENE-2167?focusedCommentId=12929325&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12929325].",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2581",
        "summary": "FastVectorHighlighter: Make FragmentsBuilder use Encoder",
        "description": "Make FragmentsBuilder use Encoder, as Highlighter does.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1552",
        "summary": "IndexWriter.addIndexes(IndexReader[] readers) doesn't correctly handle exception success flag.",
        "description": "After this bit of code in addIndexes(IndexReader[] readers)\n\n try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the write lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseWrite();\n      }\n\nThe success flag should be reset to \"false\" because it's used again in another try/catch/finally block.  \n\nTestIndexWriter.testAddIndexOnDiskFull() sometimes will hit this bug; but it's infrequent.\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1551",
        "summary": "Add reopen(IndexCommit) methods to IndexReader",
        "description": "Add reopen(IndexCommit) methods to IndexReader to be able to reopen an index on any previously saved commit points with all advantages of LUCENE-1483.\n\nSimilar to open(IndexCommit) & company available in 2.4.0.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3847",
        "summary": "LuceneTestCase should check for modifications on System properties",
        "description": "- fail the test if changes have been detected.\n- revert the state of system properties before the suite.\n- cleanup after the suite.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2324",
        "summary": "Per thread DocumentsWriters that write their own private segments",
        "description": "See LUCENE-2293 for motivation and more details.\n\nI'm copying here Mike's summary he posted on 2293:\n\nChange the approach for how we buffer in RAM to a more isolated\napproach, whereby IW has N fully independent RAM segments\nin-process and when a doc needs to be indexed it's added to one of\nthem. Each segment would also write its own doc stores and\n\"normal\" segment merging (not the inefficient merge we now do on\nflush) would merge them. This should be a good simplification in\nthe chain (eg maybe we can remove the *PerThread classes). The\nsegments can flush independently, letting us make much better\nconcurrent use of IO & CPU.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2960",
        "summary": "Allow (or bring back) the ability to setRAMBufferSizeMB on an open IndexWriter",
        "description": "In 3.1 the ability to setRAMBufferSizeMB is deprecated, and removed in trunk. It would be great to be able to control that on a live IndexWriter. Other possible two methods that would be great to bring back are setTermIndexInterval and setReaderTermsIndexDivisor. Most of the other setters can actually be set on the MergePolicy itself, so no need for setters for those (I think).",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2776",
        "summary": "indexwriter creates unwanted termvector info",
        "description": "I noticed today that when I build a big index in Solr, I get some unwanted termvector info, even though I didn't request any.\nThis does not happen on 3x - not sure when it started happening on trunk.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2797",
        "summary": "upgrade icu to 4.6",
        "description": "version 4.6 supports unicode 6, new collators (search collators) etc.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1251",
        "summary": "on disk full during close, FSIndexOutput fails to close descriptor",
        "description": "The close method just does this:\n\n{code}\n      if (isOpen) {\n        super.close();\n        file.close();\n        isOpen = false;\n      }\n{code}\n\nBut super.close = BufferedIndexOutput.close, which tries to flush its buffer.  If disk is full (or something else is wrong) then we hit an exception and don't actually close the descriptor.\n\nI will put a try/finally in so we always close, taking care to preserve the original exception. I'll commit shortly & backport to 2.3.2",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3073",
        "summary": "make compoundfilewriter public",
        "description": "CompoundFileReader is public, but CompoundFileWriter is not.\n\nI propose we make it public + @lucene.internal instead (just in case someone \nelse finds themselves wanting to manipulate cfs files)\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3708",
        "summary": "codec postings api (finishDoc) is inconsistent",
        "description": "finishDoc says:\n\n{noformat}\n  /** Called when we are done adding positions & payloads\n   *  for each doc.  Not called  when the field omits term\n   *  freq and positions. */\n   public abstract void finishDoc() throws IOException;\n{noformat}\n\nBut this is confusing (because a field can omit just positions, is it called then?!),\nand wrong (because merging calls it always, even if freq+positions is omitted).\n\nI think we should fix the javadoc and fix FreqProxTermsWriter to always call finish()\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1630",
        "summary": "Mating Collector and Scorer on doc Id orderness",
        "description": "This is a spin off of LUCENE-1593. This issue proposes to expose appropriate API on Scorer and Collector such that one can create an optimized Collector based on a given Scorer's doc-id orderness and vice versa. Copied from LUCENE-1593, here is the list of changes:\n\n# Deprecate Weight and create QueryWeight (abstract class) with a new scorer(reader, scoreDocsInOrder), replacing the current scorer(reader) method. QueryWeight implements Weight, while score(reader) calls score(reader, false /* out-of-order */) and scorer(reader, scoreDocsInOrder) is defined abstract.\n#* Also add QueryWeightWrapper to wrap a given Weight implementation. This one will also be deprecated, as well as package-private.\n#* Add to Query variants of createWeight and weight which return QueryWeight. For now, I prefer to add a default impl which wraps the Weight variant instead of overriding in all Query extensions, and in 3.0 when we remove the Weight variants - override in all extending classes.\n# Add to Scorer isOutOfOrder with a default to false, and override in BS to true.\n# Modify BooleanWeight to extend QueryWeight and implement the new scorer method to return BS2 or BS based on the number of required scorers and setAllowOutOfOrder.\n# Add to Collector an abstract _acceptsDocsOutOfOrder_ which returns true/false.\n#* Use it in IndexSearcher.search methods, that accept a Collector, in order to create the appropriate Scorer, using the new QueryWeight.\n#* Provide a static create method to TFC and TSDC which accept this as an argument and creates the proper instance.\n#* Wherever we create a Collector (TSDC or TFC), always ask for out-of-order Scorer and check on the resulting Scorer isOutOfOrder(), so that we can create the optimized Collector instance.\n# Modify IndexSearcher to use all of the above logic.\n\nThe only class I'm worried about, and would like to verify with you, is Searchable. If we want to deprecate all the search methods on IndexSearcher, Searcher and Searchable which accept Weight and add new ones which accept QueryWeight, we must do the following:\n* Deprecate Searchable in favor of Searcher.\n* Add to Searcher the new QueryWeight variants. Here we have two choices: (1) break back-compat and add them as abstract (like we've done with the new Collector method) or (2) add them with a default impl to call the Weight versions, documenting these will become abstract in 3.0.\n* Have Searcher extend UnicastRemoteObject and have RemoteSearchable extend Searcher. That's the part I'm a little bit worried about - Searchable implements java.rmi.Remote, which means there could be an implementation out there which implements Searchable and extends something different than UnicastRemoteObject, like Activeable. I think there is very small chance this has actually happened, but would like to confirm with you guys first.\n* Add a deprecated, package-private, SearchableWrapper which extends Searcher and delegates all calls to the Searchable member.\n* Deprecate all uses of Searchable and add Searcher instead, defaulting the old ones to use SearchableWrapper.\n* Make all the necessary changes to IndexSearcher, MultiSearcher etc. regarding overriding these new methods.\n\nOne other optimization that was discussed in LUCENE-1593 is to expose a topScorer() API (on Weight) which returns a Scorer that its score(Collector) will be called, and additionally add a start() method to DISI. That will allow Scorers to initialize either on start() or score(Collector). This was proposed mainly because of BS and BS2 which check if they are initialized in every call to next(), skipTo() and score(). Personally I prefer to see that in a separate issue, following that one (as it might add methods to QueryWeight).",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-354",
        "summary": "FIXME in src/test/org/apache/lucene/IndexTest.java",
        "description": "Index: src/test/org/apache/lucene/IndexTest.java\n===============================================================\n====\n--- src/test/org/apache/lucene/IndexTest.java   (revision 155945)\n+++ src/test/org/apache/lucene/IndexTest.java   (working copy)\n@@ -27,8 +27,7 @@   \npublic static void main(String[] args) {\n     try {\n       Date start = new Date();\n-      // FIXME: OG: what's with this hard-coded dirs??\n-      IndexWriter writer = new IndexWriter(\"F:\\\\test\", new SimpleAnalyzer(),\n+      IndexWriter writer = new IndexWriter(File.createTempFile(\"luceneTest\",\"idx\"), new \nSimpleAnalyzer(),\n                                           true);\n        writer.setMergeFactor(20);",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-955",
        "summary": "Bug in SegmentTermPositions if used for first term in the dictionary",
        "description": "When a SegmentTermPositions object is reset via seek() it does not move\nthe proxStream to the correct position in case the term is the first one\nin the dictionary.\n\nThe reason for this behavior is that the skipStream is only moved if\nlazySkipPointer is != 0. But 0 is a valid value for the posting list of\nthe very first term. The fix is easy: We simply have to set lazySkipPointer\nto -1 in case no lazy skip has to be performed and then we only move the\nskipStream if lazySkipPointer!=-1.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1846",
        "summary": "More Locale problems in Lucene",
        "description": "This is a followup to LUCENE-1836: I found some more Locale problems in Lucene with Date Formats. Even for simple date formats only consisting of numbers (like ISO dates), you should always give the US locale. Because the dates in DateTools should sort according to String.compare(), it is important, that the decimal digits are western ones. In some strange locales, this may be different. Whenever you want to format dates for internal formats you exspect to behave somehow, you should at least set the locale to US, which uses ASCII. Dates entered by users and displayed to users, should be formatted according to the default or a custom specified locale.\nI also looked for DecimalFormat (especially used for padding numbers), but found no problems.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3693",
        "summary": "Add a testing implementation for DocumentsWriterPerThreadPool",
        "description": "currently we only have one impl for DocumentsWriterPerThreadPool. We should add some more to make sure the interface is sufficient and to beef up tests. For testing I'm working on a randomized impl. selecting and locking states randomly.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-315",
        "summary": "Documentation Error for FilteredTermEnum",
        "description": "As pointed out in \nhttp://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene-user@jakarta.apache.org&msgNo=11034\nthe documentation of FilteredTermEnum.term() is wrong:\nit says \n'Returns the current Term in the enumeration. Initially invalid, valid after\nnext() called for the first time.'\nbut the implementation of the constructors of the two derived classes\n(FuzzyTermEnum and WildcardTermEnum) already initializes the object to point to\nthe first match. So calling next() before accessing terms will leave out the\nfirst match.\n\nSo I suggest to replace the second sentance by something like\n'Returns null if no Term matches or all terms have been enumerated.'\n(I checked that for WildcardTermEnum only).\nFurther one might add some note to the docs of the constructors of FuzzyTermEnum\nand WildcardTermEnum that they will point to the first element of the\nenumeration (if any).",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-588",
        "summary": "Escaped wildcard character in wildcard term not handled correctly",
        "description": "If an escaped wildcard character is specified in a wildcard query, it is treated as a wildcard instead of a literal.\ne.g., t\\??t is converted by the QueryParser to t??t - the escape character is discarded.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1371",
        "summary": "Add Searcher.search(Query, int)",
        "description": "Now that we've deprecated Hits (LUCENE-1290), I think we should add this trivial convenience method to Searcher, which is just sugar for Searcher.search(Query, null, int) ie null filter, returning a TopDocs.\n\nThis way there is a simple API for users to retrieve the top N results for a Query.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1793",
        "summary": "remove custom encoding support in Greek/Russian Analyzers",
        "description": "The Greek and Russian analyzers support custom encodings such as KOI-8, they define things like Lowercase and tokenization for these.\n\nI think that analyzers should support unicode and that conversion/handling of other charsets belongs somewhere else. \n\nI would like to deprecate/remove the support for these other encodings.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2493",
        "summary": "Rename lucene/solr dev jar files to -SNAPSHOT.jar",
        "description": "Currently the lucene dev jar files end with '-dev.jar' this is all fine, but it makes people using maven jump through a few hoops to get the -SNAPSHOT naming convention required by maven.  If we want to publish snapshot builds with hudson, we would need to either write some crazy scripts or run the build twice.\n\nI suggest we switch to -SNAPSHOT.jar.  Hopefully for the 3.x branch and for the /trunk (4.x) branch",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2412",
        "summary": "Architecture Diagrams needed for Lucene, Solr and Nutch",
        "description": "",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2000",
        "summary": "Use covariant clone() return types",
        "description": "*Paul Cowan wrote in LUCENE-1257:*\n\nOK, thought I'd jump in and help out here with one of my Java 5 favourites. Haven't seen anyone discuss this, and don't believe any of the patches address this, so thought I'd throw a patch out there (against SVN HEAD @ revision 827821) which uses Java 5 covariant return types for (almost) all of the Object#clone() implementations in core. \ni.e. this:\n\npublic Object clone() {\nchanges to:\npublic SpanNotQuery clone() {\n\nwhich lets us get rid of a whole bunch of now-unnecessary casts, so e.g.\n\nif (clone == null) clone = (SpanNotQuery) this.clone();\nbecomes\nif (clone == null) clone = this.clone();\n\nAlmost everything has been done and all downcasts removed, in core, with the exception of\n\nSome SpanQuery stuff, where it's assumed that it's safe to cast the clone() of a SpanQuery to a SpanQuery - this can't be made covariant without declaring \"abstract SpanQuery clone()\" in SpanQuery itself, which breaks those SpanQuerys that don't declare their own clone() \nSome IndexReaders, e.g. DirectoryReader - we can't be more specific than changing .clone() to return IndexReader, because it returns the result of IndexReader.clone(boolean). We could use covariant types for THAT, which would work fine, but that didn't follow the pattern of the others so that could be a later commit. \nTwo changes were also made in contrib/, where not making the changes would have broken code by trying to widen IndexInput#clone() back out to returning Object, which is not permitted. contrib/ was otherwise left untouched.\n\nLet me know what you think, or if you have any other questions.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3909",
        "summary": "Move Kuromoji to analysis.ja and introduce Japanese* naming",
        "description": "Lucene/Solr 3.6 and 4.0 will get out-of-the-box Japanese language support through {{KuromojiAnalyzer}}, {{KuromojiTokenizer}} and various other filters.  These filters currently live in {{org.apache.lucene.analysis.kuromoji}}.\n\nI'm proposing that we move Kuromoji to a new Japanese package {{org.apache.lucene.analysis.ja}} in line with how other languages are organized.  As part of this, I also think we should rename {{KuromojiAnalyzer}} to {{JapaneseAnalyzer}}, etc. to further align naming to our conventions by making it very clear that these analyzers are for Japanese.  (As much as I like the name \"Kuromoji\", I think \"Japanese\" is more fitting.)\n\nA potential issue I see with this that I'd like to raise and get feedback on, is that end-users in Japan and elsewhere who use lucene-gosen could have issues after an upgrade since lucene-gosen is in fact releasing its analyzers under the {{org.apache.lucene.analysis.ja}} namespace (and we'd have a name clash).\n\nI believe users should have the freedom to choose whichever Japanese analyzer, filter, etc. they'd like to use, and I don't want to propose a name change that just creates unnecessary problems for users, but I think the naming proposed above is most fitting for a Lucene/Solr release.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1986",
        "summary": "NPE in NearSpansUnordered from PayloadNearQuery",
        "description": "The following query causes a NPE in NearSpansUnordered, and is reproducible with the the attached unit test. The failure occurs on the last document scored.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-406",
        "summary": "sort missing string fields last",
        "description": "A SortComparatorSource for string fields that orders documents with the sort\nfield missing after documents with the field.  This is the reverse of the\ndefault Lucene implementation.\n\nThe concept and first-pass implementation was done by Chris Hostetter.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3582",
        "summary": "NumericUtils.floatToSortableInt/doubleToSortableLong does not sort certain NaN ranges correctly and NumericRangeQuery produces wrong results for NaNs with half-open ranges",
        "description": "The current implementation of floatToSortableInt does not account for different NaN ranges which may result in NaNs sorted before -Infinity and after +Infinity. The default Java ordering is: all NaNs after Infinity.\n\nA possible fix is to make all NaNs canonic \"quiet NaN\" as in:\n{code}\n// Canonicalize NaN ranges. I assume this check will be faster here than \n// (v == v) == false on the FPU? We don't distinguish between different\n// flavors of NaNs here (see http://en.wikipedia.org/wiki/NaN). I guess\n// in Java this doesn't matter much anyway.\nif ((v & 0x7fffffff) > 0x7f800000) {\n  // Apply the logic below to a canonical \"quiet NaN\"\n  return 0x7fc00000 ^ 0x80000000;\n}\n{code}\n\nI don't commit because I don't know how much of the existing stuff relies on this (nobody should be keeping different NaNs  in their indexes, but who knows...).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2773",
        "summary": "Don't create compound file for large segments by default",
        "description": "Spinoff from LUCENE-2762.\n\nCFS is useful for keeping the open file count down.  But, it costs\nsome added time during indexing to build, and also ties up temporary\ndisk space, causing eg a large spike on the final merge of an\noptimize.\n\nSince MergePolicy dictates which segments should be CFS, we can\nchange it to only build CFS for \"smallish\" merges.\n\nI think we should also set a maxMergeMB by default so that very large\nmerges aren't done.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-933",
        "summary": "QueryParser can produce empty sub BooleanQueries when Analyzer proudces no tokens for input",
        "description": "as triggered by SOLR-261, if you have a query like this...\n\n   +foo:BBB  +(yak:AAA  baz:CCC)\n\n...where the analyzer produces no tokens for the \"yak:AAA\" or \"baz:CCC\" portions of the query (posisbly because they are stop words) the resulting query produced by the QueryParser will be...\n\n  +foo:BBB +()\n\n...that is a BooleanQuery with two required clauses, one of which is an empty BooleanQuery with no clauses.\n\nthis does not appear to be \"good\" behavior.\n\nIn general, QueryParser should be smarter about what it does when parsing encountering parens whose contents result in an empty BooleanQuery -- but what exactly it should do in the following situations...\n\n a)  +foo:BBB +()\n b)  +foo:BBB ()\n c)  +foo:BBB -()\n\n...is up for interpretation.  I would think situation (b) clearly lends itself to dropping the sub-BooleanQuery completely.  situation (c) may also lend itself to that solution, since semanticly it means \"don't allow a match on any queries in the empty set of queries\".  .... I have no idea what the \"right\" thing to do for situation (a) is.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-471",
        "summary": "gcj ant target doesn't work on windows",
        "description": "In order to fix it I made two changes, both really simple.\n\nFirst I added to org/apache/lucene/store/GCJIndexInput.cc some code to use windows memory-mapped I/O instead than unix mmap().\n\nThen I had to rearrange the link order in the Makefile in order to avoid unresolved symbol errors. Also to build repeatedly I had to instruct make to ignore the return code for the mkdir command as on windows it fails if the directory already exists.\n\nI'm attaching two patches corresponding to the changes; please note that with the patches applied, the gcj target still works on linux. Both patches apply cleanly to the current svn head.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1991",
        "summary": "Similarity#score deprecated method - javadoc reference + SimilarityDelegator",
        "description": "Old method  \n\n  public float scorePayload(String fieldName, byte [] payload, int offset, int length)\n\nhas been deprecated by - \n\n  public float scorePayload(int docId, String fieldName, int start, int end, byte [] payload, int offset, int length)\n\n\nReferences in PayLoadNearQuery (javadoc) changed. \n\nAlso - SimilarityDelegator overrides the new method as opposed to the (deprecated) old one. ",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1977",
        "summary": "Remove MultiTermQuery.getTerm()",
        "description": "Removes the field and methods in MTQ that return the pattern term.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1660",
        "summary": "Make StopFilter.enablePositionIncrements explicit",
        "description": "I think the default for this should be true, ie, do not lose\ninformation when filtering (preserve the positions of the original\ntokens).\n\nBut, we can't change this without breaking back-compat.\n\nSo, as workaround, we should make the parameter explicit so one must\ndecide up front.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3082",
        "summary": "Add tool to upgrade all segments of an index to last recent supported index format without optimizing",
        "description": "Currently if you want to upgrade an old index to the format of your current Lucene version, you have to optimize your index or use addIndexes(IndexReader...) [see LUCENE-2893] to copy to a new directory. The optimize() approach fails if your index is already optimized.\n\nI propose to add a custom MergePolicy to upgrade all segments to the last format. This MergePolicy could simply also ignore all segments already up-to-date. All segments in prior formats would be merged to a new segment using another MergePolicy's optimize strategy.\n\nThis issue is different from LUCENE-2893, as it would only support upgrading indexes from previous Lucene versions in-place using the official path. Its a tool for the end user, not a developer tool.\n\nThis addition should also go to Lucene 3.x, as we need to make users with pre-3.0 indexes go the step through 3.x, else they would not be able to open their index with 4.0. With this tool in 3.x the users could safely upgrade their index without relying on optimize to work on already-optimized indexes.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2132",
        "summary": "the demo application does not work as of 3.0",
        "description": "the demo application does not work. QueryParser needs a Version argument.\n\nWhile I am here, remove @author too",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1478",
        "summary": "Missing possibility to supply custom FieldParser when sorting search results",
        "description": "When implementing the new TrieRangeQuery for contrib (LUCENE-1470), I was confronted by the problem that the special trie-encoded values (which are longs in a special encoding) cannot be sorted by Searcher.search() and SortField. The problem is: If you use SortField.LONG, you get NumberFormatExceptions. The trie encoded values may be sorted using SortField.String (as the encoding is in such a way, that they are sortable as Strings), but this is very memory ineffective.\n\nExtendedFieldCache gives the possibility to specify a custom LongParser when retrieving the cached values. But you cannot use this during searching, because there is no possibility to supply this custom LongParser to the SortField.\n\nI propose a change in the sort classes:\nInclude a pointer to the parser instance to be used in SortField (if not given use the default). My idea is to create a SortField using a new constructor\n{code}SortField(String field, int type, Object parser, boolean reverse){code}\n\nThe parser is \"object\" because all current parsers have no super-interface. The ideal solution would be to have:\n\n{code}SortField(String field, int type, FieldCache.Parser parser, boolean reverse){code}\n\nand FieldCache.Parser is a super-interface (just empty, more like a marker-interface) of all other parsers (like LongParser...). The sort implementation then must be changed to respect the given parser (if not NULL), else use the default FieldCache.getXXXX without parser.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1487",
        "summary": "FieldCacheTermsFilter",
        "description": "This is a companion to FieldCacheRangeFilter except it operates on a set of terms rather than a range. It works best when the set is comparatively large or the terms are comparatively common.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1658",
        "summary": "Absorb NIOFSDirectory into FSDirectory",
        "description": "I think whether one uses java.io.* vs java.nio.* or eventually\njava.nio2.*, or some other means, is an under-the-hood implementation\ndetail of FSDirectory and doesn't merit a whole separate class.\n\nI think FSDirectory should be the core class one uses when one's index\nis in the filesystem.\n\nSo, I'd like to deprecate NIOFSDirectory, absorbing it into\nFSDirectory, and add a setting \"useNIO\" to FSDirectory.  It should\ndefault to \"true\" for non-Windows OSs, because it gives far better\nconcurrent performance on all platforms but Windows (due to known Sun\nJRE issue http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734).\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3770",
        "summary": "Rename FilterIndexReader to FilterAtomicReader",
        "description": "This class has to be renamed to be consistent with the new naming.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-451",
        "summary": "BooleanQuery explain with boost==0",
        "description": "BooleanWeight.explain() uses the returned score of subweights to determine if a clause matched.\nIf any required clause has boost==0, the returned score will be zero and the explain for the entire BooleanWeight will be simply  Explanation(0.0f, \"match required\").\n\nI'm not sure what the correct fix is here.  I don't think it can be done based on score alone, since that isn't how scorers work.   Perhaps we need a new method \"boolean Explain.matched()\" that returns true on a match, regardless of what the score may be? \n\nRelated to the problem above, even if no boosts are zero, it it sometimes nice to know *why* a particular query failed to match.  It would mean a longer explanation, but maybe we should include non matching explains too?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-634",
        "summary": "QueryParser is not applicable for the arguments (String, String, Analyzer) error in results.jsp when executing search in the browser (demo from Lucene 2.0)",
        "description": "When executing search in the browser (as described in demo3.html Lucene demo) I get error, because the demo uses the method (QueryParser with three arguments) which is deleted (it was deprecated).\nI checked the demo from Lucene 1.4-final it with Lucene 1.4-final - it works, because those time the method was there.\nBut demo from Lucene 2.0 does not work with Lucene 2.0\n\nThe error stack is here:\nTTP Status 500 -\n\ntype Exception report\n\nmessage\n\ndescription The server encountered an internal error () that prevented it from fulfilling this request.\n\nexception\n\norg.apache.jasper.JasperException: Unable to compile class for JSP\n\nAn error occurred at line: 60 in the jsp file: /results.jsp\nGenerated servlet error:\nThe method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)\n\n\norg.apache.jasper.servlet.JspServletWrapper.handleJspException(JspServletWrapper.java:510)\norg.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:375)\norg.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)\norg.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)\njavax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n\nroot cause\n\norg.apache.jasper.JasperException: Unable to compile class for JSP\n\nAn error occurred at line: 60 in the jsp file: /results.jsp\nGenerated servlet error:\nThe method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)\n\n\norg.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:84)\norg.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:328)\norg.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:409)\norg.apache.jasper.compiler.Compiler.compile(Compiler.java:297)\norg.apache.jasper.compiler.Compiler.compile(Compiler.java:276)\norg.apache.jasper.compiler.Compiler.compile(Compiler.java:264)\norg.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)\norg.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:303)\norg.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)\norg.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)\njavax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n\nnote The full stack trace of the root cause is available in the Apache Tomcat/5.5.15 logs.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3722",
        "summary": "make similarities/term/collectionstats take long (for > 2B docs)",
        "description": "As noted by Yonik and Andrzej on SOLR-1632, this would be useful for distributed scoring.\n\nwe can also add a sugar method add() to both of these to make it easier to sum.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2080",
        "summary": "Improve the documentation of Version",
        "description": "In my opinion, we should elaborate more on the effects of changing the Version parameter.\nParticularly, changing this value, even if you recompile your code, likely involves reindexing your data.\nI do not think this is adequately clear from the current javadocs.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3464",
        "summary": "Rename IndexReader.reopen to make it clear that reopen may not happen",
        "description": "Spinoff from LUCENE-3454 where Shai noted this inconsistency.\n\nIR.reopen sounds like an unconditional operation, which has trapped users in the past into always closing the old reader instead of only closing it if the returned reader is new.\n\nI think this hidden maybe-ness is trappy and we should rename it (maybeReopen?  reopenIfNeeded?).\n\nIn addition, instead of returning \"this\" when the reopen didn't happen, I think we should return null to enforce proper usage of the maybe-ness of this API.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1314",
        "summary": "IndexReader.clone",
        "description": "Based on discussion http://www.nabble.com/IndexReader.reopen-issue-td18070256.html.  The problem is reopen returns the same reader if there are no changes, so if docs are deleted from the new reader, they are also reflected in the previous reader which is not always desired behavior.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1837",
        "summary": "Remove Searcher from Weight#explain",
        "description": "Explain needs to calculate corpus wide stats in a way that is consistent with MultiSearcher.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-748",
        "summary": "Exception during IndexWriter.close() prevents release of the write.lock",
        "description": "After encountering a case of index corruption - see http://issues.apache.org/jira/browse/LUCENE-140 - when the close() method encounters an exception in the flushRamSegments() method, the index write.lock is not released (ie. it is not really closed).\n\nThe writelock is only released when the IndexWriter is GC'd and finalize() is called.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3855",
        "summary": "TestStressNRT failures (reproducible)",
        "description": "Build server logs. Reproduces on at least two machines.\n\n{noformat}\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressNRT -Dtestmethod=test -Dtests.seed=69468941c1bbf693:19e66d58475da929:69e9d2f81769b6d0 -Dargs=\"-Dfile.encoding=UTF-8\"\n    [junit] NOTE: test params are: codec=Lucene3x, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {}, locale=ro, timezone=Etc/GMT+1\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestStressNRT]\n    [junit] NOTE: Linux 3.0.0-16-generic amd64/Sun Microsystems Inc. 1.6.0_27 (64-bit)/cpus=2,threads=1,free=74960064,total=135987200\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: test(org.apache.lucene.index.TestStressNRT):\tCaused an ERROR\n    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}\n    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:555)\n    [junit] \tat org.apache.lucene.index.TestStressNRT.test(TestStressNRT.java:385)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:743)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:639)\n    [junit] \tat org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:538)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n    [junit] \tat org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)\n    [junit] \tat org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)\n    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput: _ng.cfs\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:479)\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper$1.openSlice(MockDirectoryWrapper.java:777)\n    [junit] \tat org.apache.lucene.store.CompoundFileDirectory.openInput(CompoundFileDirectory.java:221)\n    [junit] \tat org.apache.lucene.codecs.lucene3x.TermInfosReader.<init>(TermInfosReader.java:112)\n    [junit] \tat org.apache.lucene.codecs.lucene3x.Lucene3xFields.<init>(Lucene3xFields.java:84)\n    [junit] \tat org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat$1.<init>(PreFlexRWPostingsFormat.java:51)\n    [junit] \tat org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat.fieldsProducer(PreFlexRWPostingsFormat.java:51)\n    [junit] \tat org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:108)\n    [junit] \tat org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:51)\n    [junit] \tat org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.getMergeReader(IndexWriter.java:521)\n    [junit] \tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3587)\n    [junit] \tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.index.TestStressNRT FAILED\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3671",
        "summary": "Add a TypeTokenFilter",
        "description": "It would be convenient to have a TypeTokenFilter that filters tokens by its type, either with an exclude or include list. This might be a stupid thing to provide for people who use Lucene directly, but it would be very useful to later expose it to Solr and other Lucene-backed search solutions.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1794",
        "summary": "implement reusableTokenStream for all contrib analyzers",
        "description": "most contrib analyzers do not have an impl for reusableTokenStream\n\nregardless of how expensive the back compat reflection is for indexing speed, I think we should do this to mitigate any performance costs. hey, overall it might even be an improvement!\n\nthe back compat code for non-final analyzers is already in place so this is easy money in my opinion.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-650",
        "summary": "NPE doing local sensitive sorting when sort field is missing",
        "description": "If you do a local sensitive sort against a field that is missing from some documents in the index an NPE will get thrown.\n\nAttached is a patch which resolved the issue and updates the sort test case to give coverage to this issue.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3685",
        "summary": "Add top-down version of BlockJoinQuery",
        "description": "Today, BlockJoinQuery can join from child docIDs up to parent docIDs.\nEG this works well for product (parent) + many SKUs (child) search.\n\nBut the reverse, which BJQ cannot do, is also useful in some cases.\nEG say you index songs (child) within albums (parent), but you want to\nsearch and present by song not album while involving some fields from\nthe album in the query.  In this case you want to wrap a parent query\n(against album), joining down to the child document space.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2836",
        "summary": "FieldCache rewrite method for MultiTermQueries",
        "description": "For some MultiTermQueries, like RangeQuery we have a FieldCacheRangeFilter etc (in this case its particularly optimized).\n\nBut in the general case, since LUCENE-2784 we can now have a rewrite method to rewrite any MultiTermQuery \nusing the FieldCache, because MultiTermQuery's getEnum no longer takes IndexReader but Terms, and all the \nFilteredTermsEnums are now just real TermsEnum decorators.\n\nIn cases like low frequency queries this is actually slower (I think this has been shown for numeric ranges before too),\nbut for the really high-frequency cases like especially ugly wildcards, regexes, fuzzies, etc, this can be several times faster \nusing the FieldCache instead, since all the terms are in RAM and automaton can blast through them quicker.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-453",
        "summary": "Using MultiSearcher and ParallelMultiSearcher can change the sort order.",
        "description": "When using multiple sort criteria the first criterium that indicates a difference should be used.\nWhen a field does not exist for a given document, special rules apply.\nFrom what I see in the code, it is sorted as 0 for integer and float fields, and null Strings are sorted before others.\n\nThis works correctly in both Lucene 1.4.3 and in trunk as long as you use a single IndexSearcher (except perhaps in special cases, see other bug reports like LUCENE-374).\n\nHowever, in MultiSearcher and ParallelMultiSearcher, the results of the separate IndexSearchers are merged and there an error occurs.\nThe bug is located in FieldDocSortedHitQueue.\n\nIt can even be demonstrated by passing a single indexSearcher to a MultiSearcher.\n\nTestCase and patch follow.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1547",
        "summary": "Rare thread hazard in IndexWriter.commit()",
        "description": "The nightly build 2 nights ago hit this:\n\n{code}\n NOTE: random seed of testcase 'testAtomicUpdates' was: -5065675995121791051\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testAtomicUpdates(org.apache.lucene.index.TestAtomicUpdate):\tFAILED\n    [junit] expected:<100> but was:<91>\n    [junit] junit.framework.AssertionFailedError: expected:<100> but was:<91>\n    [junit] \tat org.apache.lucene.index.TestAtomicUpdate.runTest(TestAtomicUpdate.java:142)\n    [junit] \tat org.apache.lucene.index.TestAtomicUpdate.testAtomicUpdates(TestAtomicUpdate.java:194)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)\n{code}\n\nIt's an intermittant failure that only happens when multiple threads\nare calling commit() at once.  With autoComit=true and\nConcurrentMergeScheduler, this can happen more often because each\nmerge thread calls commit after it's done.\n\nThe problem happens when one thread has already begun the commit\nprocess, but another two or more threads then come along wanting to\nalso commit after further changes have happened.  Those two or more\nthreads would wait until the currently committing thread finished, and\nthen they'd wake up and do their commit.  The problem was, after\nwaking up they would fail to check whether they had been superseded,\nie whether another thread had already committed more up-to-date\nchanges.\n\nThe fix is simple -- after waking up, check again if your commit has\nbeen superseded, and skip your commit if so.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1061",
        "summary": "Adding a factory to QueryParser to instantiate query instances",
        "description": "With the new efforts with Payload and scoring functions, it would be nice to plugin custom query implementations while using the same QueryParser.\nIncluded is a patch with some refactoring the QueryParser to take a factory that produces query instances.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3065",
        "summary": "NumericField should be stored in binary format in index (matching Solr's format)",
        "description": "(Spinoff of LUCENE-3001)\n\nToday when writing stored fields we don't record that the field was a NumericField, and so at IndexReader time you get back an \"ordinary\" Field and your number has turned into a string.  See https://issues.apache.org/jira/browse/LUCENE-1701?focusedCommentId=12721972&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12721972\n\nWe have spare bits already in stored fields, so, we should use one to record that the field is numeric, and then encode the numeric field in Solr's more-compact binary format.\n\nA nice side-effect is we fix the long standing issue that you don't get a NumericField back when loading your document.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3553",
        "summary": "tweak AppendingCodec to write segments_N compatible with 'normal' Lucene",
        "description": "Just an easy improvement from LUCENE-3490:\n\nCurrently AppendingCodec writes a different segments_N format (it writes no checksum at all in commit())\nIf you don't configure your codecprovider correctly in IndexReader, you will get read past EOF.\n(we have some proposed fixes for this stuff in LUCENE-3490 branch)\n\nBut besides this, all it really needs to do is no-op prepareCommit(), it can still write the 'final' checksum\nwhich is a good thing.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1421",
        "summary": "Ability to group search results by field",
        "description": "It would be awesome to group search results by specified field. Some functionality was provided for Apache Solr but I think it should be done in Core Lucene. There could be some useful information like total hits about collapsed data like total count and so on.\n\nThanks,\nArtyom",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2044",
        "summary": "Allow random seed to be set in DeleteByPercentTask",
        "description": "Need this to make index identical on multiple runs.  ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2742",
        "summary": "Enable native per-field codec support ",
        "description": "Currently the codec name is stored for every segment and PerFieldCodecWrapper is used to enable codecs per fields which has recently brought up some issues (LUCENE-2740 and LUCENE-2741). When a codec name is stored lucene does not respect the actual codec used to encode a fields postings but rather the \"top-level\" Codec in such a case the name of the top-level codec is  \"PerField\" instead of \"Pulsing\" or \"Standard\" etc. The way this composite pattern works make the indexing part of codecs simpler but also limits its capabilities. By recoding the top-level codec in the segments file we rely on the user to \"configure\" the PerFieldCodecWrapper correctly to open a SegmentReader. If a fields codec has changed in the meanwhile we won't be able to open the segment.\n\nThe issues LUCENE-2741 and LUCENE-2740 are actually closely related to the way PFCW is implemented right now. PFCW blindly creates codecs per field on request and at the same time doesn't have any control over the file naming nor if a two codec instances are created for two distinct fields even if the codec instance is the same. If so FieldsConsumer will throw an exception since the files it relies on are already created.\n\nHaving PerFieldCodecWrapper AND a CodecProvider overcomplicates things IMO. In order to use per field codec a user should on the one hand register its custom codecs AND needs to build a PFCW which needs to be maintained in the \"user-land\" an must not change incompatible once a new IW of IR is created. What I would expect from Lucene is to enable me to register a codec in a provider and then tell the Field which codec it should use for indexing. For reading lucene should determ the codec automatically once a segment is opened. if the codec is not available in the provider that is a different story. Once we instantiate the composite codec in SegmentsReader we only have the codecs which are really used in this segment for free which in turn solves LUCENE-2740. \n\nYet, instead of relying on the user to configure PFCW I suggest to move composite codec functionality inside the core an record the distinct codecs per segment in the segments info. We only really need the distinct codecs used in that segment since the codec instance should be reused to prevent additional files to be created. Lets say we have the follwing codec mapping :\n{noformat}\nfield_a:Pulsing\nfield_b:Standard\nfield_c:Pulsing\n{noformat}\n\nthen we create the following mapping:\n{noformat}\nSegmentInfo:\n[Pulsing, Standard]\n\nPerField:\n[field_a:0, field_b:1, field_c:0]\n{noformat}\n\nthat way we can easily determ which codec is used for which field an build the composite - codec internally on opening SegmentsReader. This ordering has another advantage, if like in that case pulsing and standard use really the same type of files we need a way to distinguish the used files per codec within a segment. We can in turn pass the codec's ord (implicit in the SegmentInfo) to the FieldConsumer on creation to create files with segmentname_ord.ext (or something similar). This solvel LUCENE-2741). \n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-696",
        "summary": "Scorer.skipTo() doesn't always work if called before next()",
        "description": "skipTo() doesn't work for all scorers if called before next().",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2897",
        "summary": "apply delete-by-Term and docID immediately to newly flushed segments",
        "description": "Spinoff from LUCENE-2324.\n\nWhen we flush deletes today, we keep them as buffered Term/Query/docIDs that need to be deleted.  But, for a newly flushed segment (ie fresh out of the DWPT), this is silly, because during flush we visit all terms and we know their docIDs.  So it's more efficient to apply the deletes (for this one segment) at that time.\n\nWe still must buffer deletes for all prior segments, but these deletes don't need to map to a docIDUpto anymore; ie we just need a Set.\n\nThis issue should wait until LUCENE-1076 is in since that issue cuts over buffered deletes to a transactional stream.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1621",
        "summary": "deprecate term and getTerm in MultiTermQuery",
        "description": "This means moving getTerm and term up to sub classes as appropriate and reimplementing equals, hashcode as appropriate in sub classes.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2977",
        "summary": "WriteLineDocTask should write gzip/bzip2/txt according to the extension of specified output file name",
        "description": "Since the readers behave this way it would be nice and handy if also this line writer would.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2683",
        "summary": "upgrade icu libraries to 4.4.2",
        "description": "modules/analysis uses 4.4\nsolr/contrib/extraction uses 4.2.1\n\nI think we should keep them the same version, for consistency, and go to 4.4.2 since it has bugfixes.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2111",
        "summary": "Wrapup flexible indexing",
        "description": "Spinoff from LUCENE-1458.\n\nThe flex branch is in fairly good shape -- all tests pass, initial search performance testing looks good, it survived several visits from the Unicode policeman ;)\n\nBut it still has a number of nocommits, could use some more scrutiny especially on the \"emulate old API on flex index\" and vice/versa code paths, and still needs some more performance testing.  I'll do these under this issue, and we should open separate issues for other self contained fixes.\n\nThe end is in sight!",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1561",
        "summary": "Maybe rename Field.omitTf, and strengthen the javadocs",
        "description": "Spinoff from here:\n\n  http://www.nabble.com/search-problem-when-indexed-using-Field.setOmitTf()-td22456141.html\n\nMaybe rename omitTf to something like omitTermPositions, and make it clear what queries will silently fail to work as a result.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2259",
        "summary": "add IndexWriter.removeUnferencedFiles, so apps can more immediately delete index files when readers are closed",
        "description": "This has come up several times on the user's list.\n\nOn Windows, which prevents deletion of still-open files, IndexWriter cannot remove files that are in-use by open IndexReaders.  This is fine, and IndexWriter periodically retries the delete, but it doesn't retry very often (only on open, on flushing a new segment, and on committing a merge).  So it lacks immediacy.\n\nWith this expert method, apps that want faster deletion can call this method.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2216",
        "summary": "OpenBitSet#hashCode() may return false for identical sets.",
        "description": "OpenBitSet uses an internal buffer of long variables to store set bits and an additional 'wlen' index that points \nto the highest used component inside {@link #bits} buffer.\n\nUnlike in JDK, the wlen field is not continuously maintained (on clearing bits, for example). This leads to a situation when wlen may point\nfar beyond the last set bit. \n\nThe hashCode implementation iterates over all long components of the bits buffer, rotating the hash even for empty components. This is against the contract of hashCode-equals. The following test case illustrates this:\n\n{code}\n// initialize two bitsets with different capacity (bits length).\nBitSet bs1 = new BitSet(200);\nBitSet bs2 = new BitSet(64);\n// set the same bit.\nbs1.set(3);\nbs2.set(3);\n        \n// equals returns true (passes).\nassertEquals(bs1, bs2);\n// hashCode returns false (against contract).\nassertEquals(bs1.hashCode(), bs2.hashCode());\n{code}\n\nFix and test case attached.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3186",
        "summary": "DocValues type should be recored in FNX file to early fail if user specifies incompatible type",
        "description": "Currently segment merger fails if the docvalues type is not compatible across segments. We already catch this problem if somebody changes the values type for a field within one segment but not across segments. in order to do that we should record the type in the fnx fiel alone with the field numbers.\n\nI marked this 4.0 since it should not block the landing on trunk",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2211",
        "summary": "Improve BaseTokenStreamTestCase to uses a fake attribute to check if clearAttributes() was called correctly - found bugs in contrib/analyzers",
        "description": "Robert had the idea to use a fake attribute inside BaseTokenStreamTestCase that records if its clear() method was called. If this is not the case after incrementToken(), asserTokenStreamContents fails. It also uses the attribute in TeeSinkTokenFilter, because there a lot of copying, captureState and restoreState() is used. By the attribute, you can track wonderful, if save/restore and clearAttributes is correctly implemented. It also verifies that *before* a captureState() it was also cleared (as the state will also contain the clear call). Because if you consume tokens in a filter, capture the consumed tokens and insert them, the capturedStates must also be cleared before.\n\nIn contrib analyzers are some test that fail to pass this additional assertion. They are not fixed in the attached patch.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-963",
        "summary": "Add setters to Field to allow re-use of Field instances during indexing",
        "description": "If we add setters to Field it makes it possible to re-use Field\ninstances during indexing which is a sizable performance gain for\nsmall documents.  See here for some discussion:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/51041\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3175",
        "summary": "speed up core tests",
        "description": "Our core tests have gotten slower and slower, if you don't have a really fast computer its probably frustrating.\n\nI think we should:\n1. still have random parameters, but make the 'obscene' settings like SimpleText rarer... we can always make them happen more on NIGHTLY\n2. tests that make a lot of documents can conditionalize on NIGHTLY so that they are still doing a reasonable test on ordinary runs e.g. numdocs = (NIGHTLY ? 10000 : 1000) * multiplier\n3. refactor some of the slow huge classes with lots of tests like TestIW/TestIR, at least pull out really slow methods like TestIR.testDiskFull into its own class. this gives better parallelization.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3509",
        "summary": "Add settings to IWC to optimize IDV indices for CPU or RAM respectivly",
        "description": "spinnoff from LUCENE-3496 - we are seeing much better performance if required bits for PackedInts are rounded up to a 8/16/32/64. We should add this option to IWC and default to round up ie. more RAM & faster lookups.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1365",
        "summary": "deprecate IndexWriter.addIndexes(Directory[])",
        "description": "Since addIndexesNoOptimize accomplishes the same thing, more efficiently, and you can always then call optimize() if you really wanted to, I think we should deprecate the older addIndexes(Directory[]).",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1502",
        "summary": "CharArraySet behaves inconsistently in add(Object) and contains(Object)",
        "description": "CharArraySet's add(Object) method looks like this:\n    if (o instanceof char[]) {\n      return add((char[])o);\n    } else if (o instanceof String) {\n      return add((String)o);\n    } else if (o instanceof CharSequence) {\n      return add((CharSequence)o);\n    } else {\n      return add(o.toString());\n    }\nYou'll notice that in the case of an Object (for example, Integer), the o.toString() is added. However, its contains(Object) method looks like this:\n    if (o instanceof char[]) {\n      char[] text = (char[])o;\n      return contains(text, 0, text.length);\n    } else if (o instanceof CharSequence) {\n      return contains((CharSequence)o);\n    }\n    return false;\nIn case of contains(Integer), it always returns false. I've added a simple test to TestCharArraySet, which reproduces the problem:\n  public void testObjectContains() {\n    CharArraySet set = new CharArraySet(10, true);\n    Integer val = new Integer(1);\n    set.add(val);\n    assertTrue(set.contains(val));\n    assertTrue(set.contains(new Integer(1)));\n  }\nChanging contains(Object) to this, solves the problem:\n    if (o instanceof char[]) {\n      char[] text = (char[])o;\n      return contains(text, 0, text.length);\n    } \n    return contains(o.toString());\n\nThe patch also includes few minor improvements (which were discussed on the mailing list) such as the removal of the following dead code from getHashCode(CharSequence):\n      if (false && text instanceof String) {\n        code = text.hashCode();\nand simplifying add(Object):\n    if (o instanceof char[]) {\n      return add((char[])o);\n    }\n    return add(o.toString());\n(which also aligns with the equivalent contains() method).\n\nOne thing that's still left open is whether we can avoid the calls to Character.toLowerCase calls in all the char[] array methods, by first converting the char[] to lowercase, and then passing it through the equals() and getHashCode() methods. It works for add(), but fails for contains(char[]) since it modifies the input array.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2413",
        "summary": "Consolidate all (Solr's & Lucene's) analyzers into modules/analysis",
        "description": "We've been wanting to do this for quite some time now...  I think, now that Solr/Lucene are merged, and we're looking at opening an unstable line of development for Solr/Lucene, now is the right time to do it.\n\nA standalone module for all analyzers also empowers apps to separately version the analyzers from which version of Solr/Lucene they use, possibly enabling us to remove Version entirely from the analyzers.\n\nWe should also do LUCENE-2309 (decouple, as much as possible, indexer from the analysis API), but I don't think that issue needs to block this consolidation.\n\nOnce we do this, there is one place where our users can find all the analyzers that Solr/Lucene provide.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-222",
        "summary": "[PATCH] Better \"lock obtain timed out\" error message",
        "description": "The attached patch prints the complete path and name of the lock file. This \nshould simplify debugging (it's actually a wish from the Wiki).",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3745",
        "summary": "Need stopwords and stoptags lists for default Japanese configuration",
        "description": "Stopwords and stoptags lists for Japanese needs to be developed, tested and integrated into Lucene.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3586",
        "summary": "Choose a specific Directory implementation running the CheckIndex main",
        "description": "It should be possible to choose a specific Directory implementation to use during the CheckIndex process when we run it from its main.\nWhat about an additional main parameter?\nIn fact, I'm experiencing some problems with MMapDirectory working with a big segment, and after some failed attempts playing with maxChunkSize, I decided to switch to another FSDirectory implementation but I needed to do that on my own main.\nShould we also consider to use a FileSwitchDirectory?\nI'm willing to contribute, could you please let me know your thoughts about it?",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2778",
        "summary": "Allow easy extension of RAMDirectory",
        "description": "RAMDirectory uses RAMFiles to store the data. RAMFile offers a newBuffer() method for extensions to override and allocate buffers differently, from e.g. a pool or something. However, RAMDirectory always allocates RAMFile and doesn't allow allocating a RAMFile extension, which makes RAMFile.newBuffer() unusable.\n\nI think we can simply introduce a newRAMFile() method on RAMDirectory and make the RAMFiles map protected, and it will allow really extending RAMDir.\n\nI will post a patch later.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1944",
        "summary": "Remove deprecated Directory stuff and IR/IW open/ctor hell",
        "description": "This patch removes primarily the deprecated Directory stuff. This also removes parts of the ctor/open hell in IR and IW. IndexModifier is completely removed as deprecated, too.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1856",
        "summary": "Remove Hits",
        "description": "LUCENE-1290 removed all references to Hits from core.\n\nMost work to be done here is to remove all references from the contrib modules and some new ones that crept into core after 1290.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2387",
        "summary": "IndexWriter retains references to Readers used in Fields (memory leak)",
        "description": "As described in [1] IndexWriter retains references to Reader used in Fields and that can lead to big memory leaks when using tika's ParsingReaders (as those can take 1MB per ParsingReader). \n\n[2] shows a screenshot of the reference chain to the Reader from the IndexWriter taken with Eclipse MAT (Memory Analysis Tool) . The chain is the following:\n\nIndexWriter -> DocumentsWriter -> DocumentsWriterThreadState -> DocFieldProcessorPerThread  -> DocFieldProcessorPerField -> Fieldable -> Field (fieldsData) \n\n\n-------------\n[1] http://markmail.org/thread/ndmcgffg2mnwjo47\n[2] http://skitch.com/ecerulm/n7643/eclipse-memory-analyzer\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1184",
        "summary": "Allow SnapshotDeletionPolicy to be reused across writer close/open",
        "description": "If you re-use the same instance of SnapshotDeletionPolicy across a\nclose/open of your writer, and you had a snapshot open, it can still\nbe removed when the 2nd writer is opened.  This is because SDP is\ncomparing IndexCommitPoint instances.\n\nThe fix is to instead compare segments file names.\n\nI've also changed the inner class IndexFileDeleter.CommitPoint to be\nstatic so an instance of SnapshotDeletionPolicy does not hold\nreferences to IndexFileDeleter, DocumentsWriter, etc.\n\nSpinoff from here:\n\n  http://markmail.org/message/bojgqfgyxkkv4fyb\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2201",
        "summary": "more performance improvements for snowball",
        "description": "i took a more serious look at snowball after LUCENE-2194.\n\nThis gives greatly improved performance, but note it has some minor breaks to snowball internals:\n* Among.s becomes a char[] instead of a string\n* SnowballProgram.current becomes a char[] instead of a StringBuilder\n* SnowballProgram.eq_s(int, String) becomes eq_s(int, CharSequence), so that eq_v(StringBuilder) doesnt need to create an extra string.\n* same as the above with eq_s_b and eq_v_b\n* replace_s(int, int, String) becomes replace_s(int, int, CharSequence), so that StringBuilder-based slice and insertion methods don't need to create an extra string.\n\nall of these \"breaks\" imho are only theoretical, the problem is just that pretty much everything is public or protected in the snowball internals.\n\nthe performance improvement here depends heavily upon the snowball language in use, but its way more significant than LUCENE-2194.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2353",
        "summary": "Config incorrectly handles Windows absolute pathnames",
        "description": "I have no idea how no one ran into this so far, but I tried to execute an .alg file which used ReutersContentSource and referenced both docs.dir and work.dir as Windows absolute pathnames (e.g. d:\\something). Surprisingly, the run reported an error of missing content under benchmark\\work\\something.\n\nI've traced the problem back to Config, where get(String, String) includes the following code:\n{code}\n    if (sval.indexOf(\":\") < 0) {\n      return sval;\n    }\n    // first time this prop is extracted by round\n    int k = sval.indexOf(\":\");\n    String colName = sval.substring(0, k);\n    sval = sval.substring(k + 1);\n    ...\n{code}\n\nIt detects \":\" in the value and so it thinks it's a per-round property, thus stripping \"d:\" from the value ... fix is very simple:\n{code}\n    if (sval.indexOf(\":\") < 0) {\n      return sval;\n    } else if (sval.indexOf(\":\\\\\") >= 0) {\n      // this previously messed up absolute path names on Windows. Assuming\n      // there is no real value that starts with \\\\\n      return sval;\n    }\n    // first time this prop is extracted by round\n    int k = sval.indexOf(\":\");\n    String colName = sval.substring(0, k);\n    sval = sval.substring(k + 1);\n{code}\n\nI'll post a patch w/ the above fix + test shortly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3264",
        "summary": "crank up faceting module tests",
        "description": "The faceting module has a large set of good tests.\n\nlets switch them over to use all of our test infra (randomindexwriter, random iwconfig, mockanalyzer, newDirectory, ...)\nI don't want to address multipliers and atLeast() etc on this issue, I think we should follow up with that on a separate issue, that also looks at speed and making sure the nightly build is exhaustive.\n\nfor now, lets just get the coverage in, it will be good to do before any refactoring.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1867",
        "summary": "replace collation/lib/icu4j.jar with a smaller icu jar",
        "description": "Collation does not need all the icu data.\nwe can shrink the jar file a bit by using the data customizer, and excluding things like character set conversion tables.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3515",
        "summary": "Possible slowdown of indexing/merging on 3.x vs trunk",
        "description": "Opening an issue to pursue the possible slowdown Marc Sturlese uncovered.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-340",
        "summary": "[PATCH] Highlighter: Delegate output escaping to Formatter",
        "description": "Patch for jakarta-lucene-sandbox/contributions/highlighter\nCVS version 3rd February 2005\n\nThis patch allows the highlighter Formatter to control escaping of the non\nhighlighted text as well as the highlighting of the matching text.\n\nThe example formatters highlight the matching text using XML/HTML tags. This\nworks fine if the plain text does not contain any characters that need to be\nescaped for HTML output (i.e. <, &, and \"), however this cannot be guaranteed.\nAs the formatter controls the method of highlighting (in the examples this is\nHTML, but it could be any other form of markup) it should also be responsible\nfor escaping the rest of the output.\n\nThis patch adds a method, encodeText(String), to the Formatter interface. This\nis a breaking change. This method is called from the Highlighter with the text\nthat is not passed to the formatter's highlightTerm method. \nThe SimpleHTMLFormatter has a public static method for performing simple HTML\nescaping called htmlEncode. \nThe SimpleHTMLFormatter, GradientFormatter, and SpanGradientFormatter have been\nupdated to implement the encodeText method and call the htmlEncode method to\nescape the output.\n\nFor existing formatter to maintain exactly the same behaviour as before applying\nthis patch they would need to implement the encodeText method to return the\nargument value without modification, e.g.:\n\npublic String encodeText(String originalText)\n{\n  return originalText;\n}",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-870",
        "summary": "add concurrent merge policy",
        "description": "Provide the ability to handle merges in one or more concurrent threads, i.e., concurrent with other IndexWriter operations.\n\nI'm factoring the code from LUCENE-847 for this.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1654",
        "summary": "Include diagnostics per-segment when writing a new segment",
        "description": "It would be very helpful if each segment in an index included\ndiagnostic information, such as the current version of Lucene.\n\nEG, in LUCENE-1474 this would be very helpful to see if certain\nsegments were written under 2.4.0.\n\nWe can start with just the current version.\n\nWe could also consider making this extensible, so you could provide\nyour own arbitrary diagnostics, but SegmentInfo/s is not public so I\nthink such an API would be \"one-way\" in that you'd have to use\nCheckIndex to check on it later.  Or we could wait on such extensibility\nuntil we provide some consistent way to access per-segment details\nin the index.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2576",
        "summary": "Intermittent failure in TestIndexWriter.testCommitThreadSafety",
        "description": "Mark's while(1) hudson box found this failure (and I can repro it too):\n\n{noformat}\nError Message\n\nMockRAMDirectory: cannot close: there are still open files: {_1m.cfs=1,\n_1k.cfs=1, _14.cfs=1, _1g.cfs=1, _1h.cfs=1, _1f.cfs=1, _1n.cfs=1,\n_1i.cfs=1, _1j.cfs=1, _1l.cfs=1}\n\nStacktrace\n\njava.lang.RuntimeException: MockRAMDirectory: cannot close: there are\nstill open files: {_1m.cfs=1, _1k.cfs=1, _14.cfs=1, _1g.cfs=1,\n_1h.cfs=1, _1f.cfs=1, _1n.cfs=1, _1i.cfs=1, _1j.cfs=1, _1l.cfs=1}\n       at\norg.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:282)\n       at\norg.apache.lucene.index.TestIndexWriter.testCommitThreadSafety(TestIndexWriter.java:4616)\n       at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:328)\n\nStandard Output\n\nNOTE: random codec of testcase 'testCommitThreadSafety' was: Sep\n\nStandard Error\n\nThe following exceptions were thrown by threads:\n*** Thread: Thread-1784 ***\njava.lang.RuntimeException: junit.framework.AssertionFailedError: null\n       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4606)\nCaused by: junit.framework.AssertionFailedError: null\n       at junit.framework.Assert.fail(Assert.java:47)\n       at junit.framework.Assert.assertTrue(Assert.java:20)\n       at junit.framework.Assert.assertTrue(Assert.java:27)\n       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4597)\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1002",
        "summary": "Nightly Builds",
        "description": "Nightly builds for Lucene are HUGE due to the inclusion of the contrib/benchmark temp and work directories.  These directories should be excluded.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2774",
        "summary": "ant generate-maven-artifacts target broken for contrib",
        "description": "When executing 'ant generate-maven-artifacts' from a pristine checkout of branch_3x/lucene or trunk/lucene the following error is encountered:\n\n{code}\ndist-maven:\n     [copy] Copying 1 file to /home/drew/lucene/branch_3x/lucene/build/contrib/analyzers/common\n[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-ssh:jar:1.0-beta-2:runtime\n[artifact:pom] An error has occurred while processing the Maven artifact tasks.\n[artifact:pom]  Diagnosis:\n[artifact:pom] \n[artifact:pom] Unable to initialize POM pom.xml.template: Cannot find parent: org.apache.lucene:lucene-contrib for project: org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT for project org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT\n[artifact:pom] Unable to download the artifact from any repository\n{code}\n\n\nThe contrib portion of the ant build is executed in a subant task which does not pick up the pom definitions for lucene-parent and lucene-contrib from the main build.xml, so the lucene-parent and lucene-controb poms must be loaded specifically as a part of the contrib build using the artifact:pom task.\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1585",
        "summary": "Allow to control how payloads are merged",
        "description": "Lucene handles backwards-compatibility of its data structures by\nconverting them from the old into the new formats during segment\nmerging. \n\nPayloads are simply byte arrays in which users can store arbitrary\ndata. Applications that use payloads might want to convert the format\nof their payloads in a similar fashion. Otherwise it's not easily\npossible to ever change the encoding of a payload without reindexing.\n\nSo I propose to introduce a PayloadMerger class that the SegmentMerger\ninvokes to merge the payloads from multiple segments. Users can then\nimplement their own PayloadMerger to convert payloads from an old into\na new format.\n\nIn the future we need this kind of flexibility also for column-stride\nfields (LUCENE-1231) and flexible indexing codecs.\n\nIn addition to that it would be nice if users could store version\ninformation in the segments file. E.g. they could store \"in segment _2\nthe term a:b uses payloads of format x.y\".\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-512",
        "summary": "ClassCastException in ParallelReader class",
        "description": "ClassCastException in ParalleReader when calling getTermFreqVectors on line 153\n\nReason : \n\n cast of key and value is swapped\n\nFixed with : \n\n      IndexReader reader = (IndexReader)e.getValue();\n      String field = (String)e.getKey();\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1661",
        "summary": "Change visibility of getComparator method in SortField from protected to public",
        "description": "Hi,\n\nCurrently I'm using SortField for the creation of FieldComparators, but I ran into an issue.\nI cannot invoke SortField.getComparator(...) directly from my code, which forces me to use a  workaround. (subclass SortField and override the getComparator method with visiblity public)\nI'm proposing to make this method public. Currently I do not see any problems changing the visibility to public, I do not know if there are any (and the reason why this method is currently protected)\nI think that this is a cleaner solution then the workaround I used and also other developers can benefit from it. I will also attach a patch to this issue based on the code in the trunk (26th of May). place). \nPlease let me know your thoughts about this.\n\nCheers,\n\nMartijn\n\n ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-656",
        "summary": "FieldsInfo uses deprecated API",
        "description": "The class FieldsInfo.java uses deprecated API in method \"public void add(Document doc)\"\nI rused the replacement and created the patch -> see attachment",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2372",
        "summary": "Replace deprecated TermAttribute by new CharTermAttribute",
        "description": "After LUCENE-2302 is merged to trunk with flex, we need to carry over all tokenizers and consumers of the TokenStreams to the new CharTermAttribute.\n\nWe should also think about adding a AttributeFactory that creates a subclass of CharTermAttributeImpl that returns collation keys in toBytesRef() accessor. CollationKeyFilter is then obsolete, instead you can simply convert every TokenStream to indexing only CollationKeys by changing the attribute implementation.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-717",
        "summary": "src builds fail because of no \"lib\" directory",
        "description": "I just downloaded http://mirrors.ibiblio.org/pub/mirrors/apache/lucene/java/lucene-2.0.0-src.tar.gz and noticed that you can't compile and run the tests from that src build because it doesn't inlcude the lib dir (and the build file won't attempt to make it if it doesn't exist) ...\n\nhossman@coaster:~/tmp/l2$ tar -xzvf lucene-2.0.0-src.tar.gz\n  ...\nhossman@coaster:~/tmp/l2$ cd lucene-2.0.0/\nhossman@coaster:~/tmp/l2/lucene-2.0.0$ ant test\n  ...\ntest:\n    [mkdir] Created dir: /home/hossman/tmp/l2/lucene-2.0.0/build/test\n\nBUILD FAILED\n/home/hossman/tmp/l2/lucene-2.0.0/common-build.xml:169: /home/hossman/tmp/l2/lucene-2.0.0/lib not found.\n\n(it's refrenced in junit.classpath, but i'm not relaly sure why)\n\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1832",
        "summary": "minor/nitpick TermInfoReader bug ?",
        "description": "Some code flagged by a bytecode static analyzer - I guess a nitpick, but we should just drop the null check in the if? If its null it will fall to the below code and then throw a NullPointer exception anyway. Keeping the nullpointer check implies we expect its possible - but then we don't handle it correctly.\n\n{code}\n  /** Returns the nth term in the set. */\n  final Term get(int position) throws IOException {\n    if (size == 0) return null;\n\n    SegmentTermEnum enumerator = getThreadResources().termEnum;\n    if (enumerator != null && enumerator.term() != null &&\n        position >= enumerator.position &&\n\tposition < (enumerator.position + totalIndexInterval))\n      return scanEnum(enumerator, position);      // can avoid seek\n\n    seekEnum(enumerator, position/totalIndexInterval); // must seek\n    return scanEnum(enumerator, position);\n  }\n\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1825",
        "summary": "Incorrect usage of AttributeSource.addAttribute/getAttribute leads to failures when onlyUseNewAPI=true",
        "description": "when seting \"use only new API\" for TokenStream, i received the following exception:\n\n{code}\n   [junit] Caused by: java.lang.IllegalArgumentException: This AttributeSource does not have the attribute 'interface org.apache.lucene.analysis.tokenattributes.TermAttribute'.\n    [junit] \tat org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:249)\n    [junit] \tat org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:252)\n    [junit] \tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:145)\n    [junit] \tat org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:244)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:772)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:755)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2613)\n{code}\n\nHowever, i can't actually see the culprit that caused this exception\n\nsuggest that the IllegalArgumentException include \"getClass().getName()\" in order to be able to identify which TokenStream implementation actually caused this\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3387",
        "summary": "Get javadoc for the similarities package in shape",
        "description": "1. Create a package.html in the similarities package.\n2. Update the javadoc of the search package (package.html mentions Similarity)?\n3. Compile the javadoc to see if there are any warnings.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": ""
    },
    {
        "key": "LUCENE-3637",
        "summary": "Make IndexReader.decRef() call refCount.decrementAndGet instead of getAndDecrement",
        "description": "IndexReader.decRef() has this code:\n\n{code}\n    final int rc = refCount.getAndDecrement();\n    if (rc == 1) {\n{code}\n\nI think it will be clearer if it was written like this:\n\n{code}\n    final int rc = refCount.decrementAndGet();\n    if (rc == 0) {\n{code}\n\nIt's a very simple change, which makes reading the code (at least IMO) easier. Will post a patch shortly.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2793",
        "summary": "Directory createOutput and openInput should take an IOContext",
        "description": "Today for merging we pass down a larger readBufferSize than for searching because we get better performance.\n\nI think we should generalize this to a class (IOContext), which would hold the buffer size, but then could hold other flags like DIRECT (bypass OS's buffer cache), SEQUENTIAL, etc.\n\nThen, we can make the DirectIOLinuxDirectory fully usable because we would only use DIRECT/SEQUENTIAL during merging.\n\nThis will require fixing how IW pools readers, so that a reader opened for merging is not then used for searching, and vice/versa.  Really, it's only all the open file handles that need to be different -- we could in theory share del docs, norms, etc, if that were somehow possible.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3059",
        "summary": "PulsingTermState.clone leaks memory",
        "description": "I looked at the heap dump from the OOME this morning (thank you Uwe\nfor turning this on!), and I think it's a real memory leak.\n\nWell, not really a leak; rather, the cloned PulsingTermState, which we\ncache in the terms dict cache, is hanging onto large byte[]\nunnecessarily.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1084",
        "summary": "increase default maxFieldLength?",
        "description": "To my understanding, Lucene 2.3 will easily index large documents. So shouldn't we get rid of the 10,000 default limit for the field length? 10,000 isn't that much and as Lucene doesn't have any error logging by default, this is a common problem for users that is difficult to debug if you don't know where to look.\n\nA better new default might be Integer.MAX_VALUE.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2785",
        "summary": "TopFieldCollector throws AIOOBE if numHits is 0",
        "description": "See solr-user thread \"ArrayIndexOutOfBoundsException for query with rows=0 and sort param\".\n\nI think we should just create a null collector (only tallies up totalHits) if numHits is 0?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2537",
        "summary": "FSDirectory.copy() impl is unsafe",
        "description": "There are a couple of issues with it:\n\n# FileChannel.transferFrom documents that it may not copy the number of bytes requested, however we don't check the return value. So need to fix the code to read in a loop until all bytes were copied..\n# When calling addIndexes() w/ very large segments (few hundred MBs in size), I ran into the following exception (Java 1.6 -- Java 1.5's exception was cryptic):\n{code}\nException in thread \"main\" java.io.IOException: Map failed\n    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:770)\n    at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:450)\n    at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:523)\n    at org.apache.lucene.store.FSDirectory.copy(FSDirectory.java:450)\n    at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3019)\nCaused by: java.lang.OutOfMemoryError: Map failed\n    at sun.nio.ch.FileChannelImpl.map0(Native Method)\n    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:767)\n    ... 7 more\n{code}\n\nI changed the impl to something like this:\n{code}\nlong numWritten = 0;\nlong numToWrite = input.size();\nlong bufSize = 1 << 26;\nwhile (numWritten < numToWrite) {\n  numWritten += output.transferFrom(input, numWritten, bufSize);\n}\n{code}\n\nAnd the code successfully adds the indexes. This code uses chunks of 64MB, however that might be too large for some applications, so we definitely need a smaller one. The question is how small so that performance won't be affected, and it'd be great if we can let it be configurable, however since that API is called by other API, such as addIndexes, not sure it's easily controllable.\n\nAlso, I read somewhere (can't remember now where) that on Linux the native impl is better and does copy in chunks. So perhaps we should make a Linux specific impl?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2137",
        "summary": "Replace SegmentReader.Ref with AtomicInteger",
        "description": "I think the patch should be applied to backcompat tag in its entirety.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1479",
        "summary": "TrecDocMaker skips over documents when \"Date\" is missing from documents",
        "description": "TrecDocMaker skips over Trec documents if they do not have a \"Date\" line. When such a document is encountered, the code may skip over several documents until the next tag that is searched for is found.\nThe result is, instead of reading ~25M documents from the GOV2 collection, the code reads only ~23M (don't remember the actual numbers).\n\nThe fix adds a terminatingTag to read() such that the code looks for prefix, but only until terminatingTag is found. Appropriate changes were made in getNextDocData().\n\nPatch to follow",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1875",
        "summary": "Javadoc of TokenStream.end() somehow confusing",
        "description": "The Javadocs of TokenStream.end() are somehow confusing, because they also refer to the old TokenStream API (\"after next() returned null\"). But one who implements his TokenStream with the old API cannot make use of the end() feature, as he would not use attributes and so cannot update the end offsets (he could, but then he should rewrite the whole TokenStream). To be conform to the old API, there must be an end(Token) method, which we will not add.\n\nI would drop the old API from this docs.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3452",
        "summary": "The native FS lock used in test-framework's o.a.l.util.LuceneJUnitResultFormatter prohibits testing on a multi-user system",
        "description": "{{LuceneJUnitResultFormatter}} uses a lock to buffer test suites' output, so that when run in parallel, they don't interrupt each other when they are displayed on the console.\n\nThe current implementation uses a fixed directory ({{lucene_junit_lock/}} in {{java.io.tmpdir}} (by default {{/tmp/}} on Unix/Linux systems) as the location of this lock.  This functionality was introduced on SOLR-1835.\n\nAs Shawn Heisey reported on SOLR-2739, some tests fail when run as root, but succeed when run as a non-root user.  \n\nOn #lucene IRC today, Shawn wrote:\n{quote}\n(2:06:07 PM) elyograg: Now that I know I can't run the tests as root, I have discovered /tmp/lucene_junit_lock.  Once you run the tests as user A, you cannot run them again as user B until that directory is deleted, and only root or the original user can do so.\n{quote}\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-985",
        "summary": "AIOOB thrown when length of termText is longer than 16384 characters (ArrayIndexOutOfBoundsException)",
        "description": "DocumentsWriter has a max term length of 16384; if you cross that you\nget an unfriendly ArrayIndexOutOfBoundsException.  We should fix to raise a clearer exception.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2285",
        "summary": "Code cleanup from all sorts of (trivial) warnings",
        "description": "I would like to do some code cleanup and remove all sorts of trivial warnings, like unnecessary casts, problems w/ javadocs, unused variables, redundant null checks, unnecessary semicolon etc. These are all very trivial and should not pose any problem.\n\nI'll create another issue for getting rid of deprecated code usage, like LuceneTestCase and all sorts of deprecated constructors. That's also trivial because it only affects Lucene code, but it's a different type of change.\n\nAnother issue I'd like to create is about introducing more generics in the code, where it's missing today - not changing existing API. There are many places in the code like that.\n\nSo, with you permission, I'll start with the trivial ones first, and then move on to the others.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-970",
        "summary": "FilterIndexReader should overwrite isOptimized()",
        "description": "A call of FilterIndexReader.isOptimized() results in a NPE because FilterIndexReader does not overwrite isOptimized().",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1550",
        "summary": "Add N-Gram String Matching for Spell Checking",
        "description": "N-Gram version of edit distance based on paper by Grzegorz Kondrak, \"N-gram similarity and distance\". Proceedings of the Twelfth International Conference on String Processing and Information Retrieval (SPIRE 2005), pp. 115-126,  Buenos Aires, Argentina, November 2005. \nhttp://www.cs.ualberta.ca/~kondrak/papers/spire05.pdf\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-745",
        "summary": "Make inspection of BooleanQuery more efficient",
        "description": "Just attempting to inspect a BooleanQuery allocates two new arrays.  This could be cheaper.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-209",
        "summary": "Lucene requires ant 1.6?",
        "description": "The latest version in CVS as of April 3rd only builds with ant 1.6.   If this is intentional, BUILD.txt should \nbe updated.\n\nHere's the error I get with ant 1.5:\n\nBUILD FAILED\nfile:/Users/skybrian/remote-cvs/jakarta-lucene/build.xml:11: Unexpected element \"tstamp\"",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2626",
        "summary": "FastVectorHighlighter: enable FragListBuilder and FragmentsBuilder to be set per-field override",
        "description": "",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-246",
        "summary": "[PATCH] Make a getter for SortField[] fields in org.apache.lucene.search.Sort",
        "description": "I'm have my own Collector and I would like to use the Sort object within my\ncollector, but SortField[] fields; is not accessible outside Lucene's package.\nCan you please consider making a public getFields() method in the Sort object so\nwe can use it in our implementation?",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-926",
        "summary": "Document Package level javadocs need improving",
        "description": "The document package package level javadocs could use some improving, such as:\n1. Info on what a Document is, as well as Field and Fieldable\n2. Examples of FieldSelectors and how to implement\n3. Samples of using DateTools and NumberTools",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-3767",
        "summary": "Explore streaming Viterbi search in Kuromoji",
        "description": "I've been playing with the idea of changing the Kuromoji viterbi\nsearch to be 2 passes (intersect, backtrace) instead of 4 passes\n(break into sentences, intersect, score, backtrace)... this is very\nmuch a work in progress, so I'm just getting my current state up.\nIt's got tons of nocommits, doesn't properly handle the user dict nor\nextended modes yet, etc.\n\nOne thing I'm playing with is to add a double backtrace for the long\ncompound tokens, ie, instead of penalizing these tokens so that\nshorter tokens are picked, leave the scores unchanged but on backtrace\ntake that penalty and use it as a threshold for a 2nd best\nsegmentation...\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-622",
        "summary": "Provide More of Lucene For Maven",
        "description": "Please provide javadoc & source jars for lucene-core.  Also, please provide the rest of lucene (the jars inside of \"contrib\" in the download bundle) if possible.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1038",
        "summary": "TermVectorMapper.setDocumentNumber()",
        "description": "Passes down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader.  \n\nSee http://www.nabble.com/Allowing-IOExceptions-in-TermVectorMapper--tf4687704.html#a13397341",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-229",
        "summary": "[PATCH] Binary stored fields",
        "description": "Provides a binary Field type that can be used to store byte arrays in the Lucene\nindex. Can be used for a variety of applications from compressed text storage,\nimage storage or as a basis for implementing typed storage (e.g: Integers,\nFloats, etc.)\n\nBased on discussion from lucene-dev list started here:\nhttp://marc.theaimsgroup.com/?l=lucene-dev&m=108455161204687&w=2\n\nDirectly based on design fleshed out here:\nhttp://marc.theaimsgroup.com/?l=lucene-dev&m=108456898230542&w=2\n\nPatch includes updated code and unit tests not included in the patch sent do the\nlucene-dev list.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-556",
        "summary": "MatchAllDocsQuery, MultiSearcher and a custom HitCollector throwing exception",
        "description": "I have encountered an issue with lucene1.9.1. It involves MatchAllDocsQuery, MultiSearcher and a custom HitCollector. The following code throws  java.lang.UnsupportedOperationException.\n\nIf I remove the MatchAllDocsQuery  condition (comment whole //1 block), or if I dont use the custom hitcollector (ms.search(mbq); instead of ms.search(mbq, allcoll);) the exception goes away. By stepping into the source I can see it seems due to MatchAllDocsQuery no implementing extractTerms()....\n\n\n           Searcher searcher = new\nIndexSearcher(\"c:\\\\projects\\\\mig\\\\runtime\\\\index\\\\01Aug16\\\\\");\n           Searchable[] indexes = new IndexSearcher[1];\n           indexes[0] = searcher;\n           MultiSearcher ms = new MultiSearcher(indexes);\n\n           AllCollector allcoll = new AllCollector(ms);\n\n           BooleanQuery mbq = new BooleanQuery();\n           mbq.add(new TermQuery(new Term(\"body\", \"value1\")),\nBooleanClause.Occur.MUST_NOT);\n// 1\n           MatchAllDocsQuery alld = new MatchAllDocsQuery();\n           mbq.add(alld, BooleanClause.Occur.MUST);\n//\n\n           System.out.println(\"Query: \" + mbq.toString());\n\n           // 2\n           ms.search(mbq, allcoll);\n           //ms.search(mbq);",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1737",
        "summary": "Always use bulk-copy when merging stored fields and term vectors",
        "description": "Lucene has nice optimizations in place during merging of stored fields\n(LUCENE-1043) and term vectors (LUCENE-1120) whereby the bytes are\nbulk copied to the new segmetn.  This is much faster than decoding &\nrewriting one document at a time.\n\nHowever the optimization is rather brittle: it relies on the mapping\nof field name to number to be the same (\"congruent\") for the segment\nbeing merged.\n\nUnfortunately, the field mapping will be congruent only if the app\nadds the same fields in precisely the same order to each document.\n\nI think we should fix IndexWriter to assign the same field number for\na given field that has been assigned in the past.  Ie, when writing a\nnew segment, we pre-seed the field numbers based on past segments.\nAll other aspects of FieldInfo would remain fully dynamic.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3251",
        "summary": "Directory#copy leaks file handles",
        "description": "Directory#copy doesn't close the target directories output stream if sourceDir.openInput(srcFile) throws an Exception. Before LUCENE-3218 Directory#copy wasn't used extensively so this wasn't likely to happen during tests. Today we had a failure on the 3.x branch that is likely caused by this bug:\n\n{noformat}\n[junit] Testsuite: org.apache.lucene.index.TestAddIndexes\n    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):\tCaused an ERROR\n    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}\n    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:483)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads.closeDir(TestAddIndexes.java:693)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:924)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)\n    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput: _co.cfs\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:410)\n    [junit] \tat org.apache.lucene.store.MockCompoundFileDirectoryWrapper.<init>(MockCompoundFileDirectoryWrapper.java:39)\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.createCompoundOutput(MockDirectoryWrapper.java:439)\n    [junit] \tat org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:128)\n    [junit] \tat org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)\n    [junit] \n    [junit] \n    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 9.034 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] java.lang.IllegalStateException: CFS has pending open files\n    [junit] \tat org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:143)\n    [junit] \tat org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)\n    [junit] \tat org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)\n    [junit] \tat org.apache.lucene.store.MockCompoundFileDirectoryWrapper.close(MockCompoundFileDirectoryWrapper.java:55)\n    [junit] \tat org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)\n    [junit] \tat org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1623",
        "summary": "Back-compat break with non-ascii field names",
        "description": "If a field name contains non-ascii characters in a 2.3.x index, then\non upgrade to 2.4.x unexpected problems are hit.  It's possible to hit\na \"read past EOF\" IOException; it's also possible to not hit an\nexception but get an incorrect field name.\n\nThis was caused by LUCENE-510, because the FieldInfos (*.fnm) file is\nnot properly versioned.\n\nSpinoff from http://www.nabble.com/Read-past-EOF-td23276171.html\n",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1104",
        "summary": "Clean up old JIRA issues in component \"Analysis\"",
        "description": "A list of all JIRA issues in component \"Analysis\" that haven't been updated in 2007:\n\n   *\t LUCENE-760  \t Spellchecker could/should use n-gram tokenizers instead of rolling its own n-gramming   \n   *\tLUCENE-677 \tItalian Analyzer \n   *\tLUCENE-571 \tStandardTokenizer parses decimal number as <HOST> \n   *\tLUCENE-566 \tEsperanto Analyzer \n   *\tLUCENE-559 \tTurkish Analyzer for Lucene \n   *\tLUCENE-494 \tAnalyzer for preventing overload of search service by queries with common terms in large indexes \n   *\tLUCENE-424 \t[PATCH] Submissiom form simple Romanian Analyzer \n   *\tLUCENE-417 \tStandardTokenizer has problems with comma-separated values \n   *\tLUCENE-400 \tNGramFilter -- construct n-grams from a TokenStream \n   *\tLUCENE-396 \t[PATCH] Add position increment back into StopFilter \n   *\tLUCENE-387 \tContrib: Main memory based SynonymMap and SynonymTokenFilter \n   *\tLUCENE-321 \t[PATCH] Submissiom of my Tswana Analyzer \n   *\tLUCENE-233 \t[PATCH] analyzer refactoring based on CVS HEAD from 6/21/2004 \n   *\tLUCENE-210 \t[PATCH] Never write an Analyzer again \n   *\tLUCENE-205 \t[PATCH] Patches for RussianAnalyzer \n   *\tLUCENE-185 \t[PATCH] Thai Analysis Enhancement \n   *\tLUCENE-152 \t[PATCH] KStem for Lucene \n   *\tLUCENE-82 \t[PATCH] HTMLParser: IOException: Pipe closed \n\n",
        "label": "NUG",
        "classified": "OTHER",
        "type": "TASK"
    },
    {
        "key": "LUCENE-447",
        "summary": "Make \"ant -projecthelp\" show the javadocs and docs targets as well",
        "description": "Added a description to the targets \"javadocs\" and \"docs\".\nThis makes ant show them when the executes \"ant -projecthelp\"\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2609",
        "summary": "Generate jar containing test classes.",
        "description": "The test classes are useful for writing unit tests for code external to the Lucene project. It would be helpful to build a jar of these classes and publish them as a maven dependency.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1598",
        "summary": "While you could use a custom Sort Comparator source with remote searchable before, you can no longer do so with FieldComparatorSource",
        "description": "FieldComparatorSource is not serializable, but can live on a SortField",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-438",
        "summary": "add Token.setTermText(), remove final",
        "description": "The Token class should be more friendly to classes not in it's package:\n  1) add setTermText()\n  2) remove final from class and toString()\n  3) add clone()\n\nSupport for (1):\n  TokenFilters in the same package as Token are able to do things like \n   \"t.termText = t.termText.toLowerCase();\" which is more efficient, but more importantly less error prone.  Without the ability to change *only* the term text, a new Token must be created, and one must remember to set all the properties correctly.  This exact issue caused this bug:\nhttp://issues.apache.org/jira/browse/LUCENE-437\n\nSupport for (2):\n  Removing final allows one to subclass Token.  I didn't see any performance impact after removing final.\nI can go into more detail on why I want to subclass Token if anyone is interested.\n\nSupport for (3):\n  - support for a synonym TokenFilter, where one needs to make two tokens from one (same args that support (1), and esp important if instance is a subclass of Token).",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3246",
        "summary": "Invert IR.getDelDocs -> IR.getLiveDocs",
        "description": "Spinoff from LUCENE-1536, where we need to fix the low level filtering\nwe do for deleted docs to \"match\" Filters (ie, a set bit means the doc\nis accepted) so that filters can be pushed all the way down to the\nenums when possible/appropriate.\n\nThis change also inverts the meaning first arg to\nTermsEnum.docs/AndPositions (renames from skipDocs to liveDocs).\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2186",
        "summary": "First cut at column-stride fields (index values storage)",
        "description": "I created an initial basic impl for storing \"index values\" (ie\ncolumn-stride value storage).  This is still a work in progress... but\nthe approach looks compelling.  I'm posting my current status/patch\nhere to get feedback/iterate, etc.\n\nThe code is standalone now, and lives under new package\noal.index.values (plus some util changes, refactorings) -- I have yet\nto integrate into Lucene so eg you can mark that a given Field's value\nshould be stored into the index values, sorting will use these values\ninstead of field cache, etc.\n\nIt handles 3 types of values:\n\n  * Six variants of byte[] per doc, all combinations of fixed vs\n    variable length, and stored either \"straight\" (good for eg a\n    \"title\" field), \"deref\" (good when many docs share the same value,\n    but you won't do any sorting) or \"sorted\".\n\n  * Integers (variable bit precision used as necessary, ie this can\n    store byte/short/int/long, and all precisions in between)\n\n  * Floats (4 or 8 byte precision)\n\nString fields are stored as the UTF8 byte[].  This patch adds a\nBytesRef, which does the same thing as flex's TermRef (we should merge\nthem).\n\nThis patch also adds basic initial impl of PackedInts (LUCENE-1990);\nwe can swap that out if/when we get a better impl.\n\nThis storage is dense (like field cache), so it's appropriate when the\nfield occurs in all/most docs.  It's just like field cache, except the\nreading API is a get() method invocation, per document.\n\nNext step is to do basic integration with Lucene, and then compare\nsort performance of this vs field cache.\n\nFor the \"sort by String value\" case, I think RAM usage & GC load of\nthis index values API should be much better than field caache, since\nit does not create object per document (instead shares big long[] and\nbyte[] across all docs), and because the values are stored in RAM as\ntheir UTF8 bytes.\n\nThere are abstract Writer/Reader classes.  The current reader impls\nare entirely RAM resident (like field cache), but the API is (I think)\nagnostic, ie, one could make an MMAP impl instead.\n\nI think this is the first baby step towards LUCENE-1231.  Ie, it\ncannot yet update values, and the reading API is fully random-access\nby docID (like field cache), not like a posting list, though I\ndo think we should add an iterator() api (to return flex's DocsEnum)\n-- eg I think this would be a good way to track avg doc/field length\nfor BM25/lnu.ltc scoring.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-937",
        "summary": "Make CachingTokenFilter faster",
        "description": "The LinkedList used by CachingTokenFilter is accessed using the get() method. Direct access on a LinkedList is slow and an Iterator should be used instead. For more than a handful of tokens, the difference in speed grows exponentially.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1211",
        "summary": "Small speedups to DocumentsWriter's quickSort",
        "description": "In working on LUCENE-510 I found that DocumentsWriter's quickSort can\nbe further optimized to handle the common case of sorting only 2\nvalues.\n\nI ran with this alg:\n\n  analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\n  \n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n  \n  docs.file=/Volumes/External/lucene/wiki.txt\n  doc.stored = true\n  doc.term.vector = true\n  doc.add.log.step=2000\n  doc.maker.forever = false\n  \n  directory=FSDirectory\n  autocommit=false\n  compound=false\n  \n  ram.flush.mb=64\n  \n  { \"Rounds\"\n    ResetSystemErase\n    { \"BuildIndex\"\n      CreateIndex\n      { \"AddDocs\" AddDoc > : 200000\n      - CloseIndex\n    }\n    NewRound\n  } : 5\n  \n  RepSumByPrefRound BuildIndex\n\nBest of 5 was 857.3 docs/sec before the optimization and 881.6 after =\n2.8% speedup, on a quad-core Mac Pro with 4-drive RAID 0 array.\n\nThe fix is trivial.  I will commit shortly.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1001",
        "summary": "Add Payload retrieval to Spans",
        "description": "It will be nice to have access to payloads when doing SpanQuerys.\n\nSee http://www.gossamer-threads.com/lists/lucene/java-dev/52270 and http://www.gossamer-threads.com/lists/lucene/java-dev/51134\n\nCurrent API, added to Spans.java is below.  I will try to post a patch as soon as I can figure out how to make it work for unordered spans (I believe I have all the other cases working).\n\n{noformat}\n /**\n   * Returns the payload data for the current span.\n   * This is invalid until {@link #next()} is called for\n   * the first time.\n   * This method must not be called more than once after each call\n   * of {@link #next()}. However, payloads are loaded lazily,\n   * so if the payload data for the current position is not needed,\n   * this method may not be called at all for performance reasons.<br>\n   * <br>\n   * <p><font color=\"#FF0000\">\n   * WARNING: The status of the <b>Payloads</b> feature is experimental.\n   * The APIs introduced here might change in the future and will not be\n   * supported anymore in such a case.</font>\n   *\n   * @return a List of byte arrays containing the data of this payload\n   * @throws IOException\n   */\n  // TODO: Remove warning after API has been finalized\n  List/*<byte[]>*/ getPayload() throws IOException;\n\n  /**\n   * Checks if a payload can be loaded at this position.\n   * <p/>\n   * Payloads can only be loaded once per call to\n   * {@link #next()}.\n   * <p/>\n   * <p><font color=\"#FF0000\">\n   * WARNING: The status of the <b>Payloads</b> feature is experimental.\n   * The APIs introduced here might change in the future and will not be\n   * supported anymore in such a case.</font>\n   *\n   * @return true if there is a payload available at this position that can be loaded\n   */\n  // TODO: Remove warning after API has been finalized\n  public boolean isPayloadAvailable();\n{noformat}",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1947",
        "summary": "Snowball package contains BSD licensed code with ASL header",
        "description": "All classes in org.tartarus.snowball (but not in org.tartarus.snowball.ext) has for some reason been given an ASL header. These classes are licensed with BSD. Thus the ASL header should be removed. I suppose this a misstake or possible due to the ASL header automation tool.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1226",
        "summary": "IndexWriter.addIndexes(IndexReader[]) fails to create compound files",
        "description": "Even if no exception is thrown while writing the compound file at the end of the \naddIndexes() call, the transaction is rolled back and the successfully written cfs \nfile deleted. The fix is simple: There is just the \n{code:java}\nsuccess = true;\n{code}\nstatement missing at the end of the try{} clause.\n\nAll tests pass. I'll commit this soon to trunk and 2.3.2.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1753",
        "summary": "Make not yet final core/contrib TokenStream/Filter implementations final",
        "description": "Lucene's analysis package is designed in a way, that you can plug different *implementations* of analysis in chains of TokenStreams and TokenFilters. An analyzer is build of several TokenStreams/Filters that do the tokenization of text. If you want to modify the behaviour of tokenization, you implement a new subclass of TokenStream/-Filter/Tokenizer.\n\nMost classes in the core are correctly implemented like that. They are itsself final or their implementation methods are final (CharTokenizer).\n\nA lot of problems with backwards-compatibility of LUCENE-1693 are some classes in Lucene's core/contrib not yet final:\n- KeywordTokenizer should be declared final or its implementation methods should be final\n- StandardTokenizer should be declared final or its implementation methods should be final\n- ISOLatin1Filter is deprecated, so it will be removed in 3.0, nothing to do.\n\nCharTokenizer is the abstract base class of several other classes. The design is correct: Child classes cannot override the implementation, they can only change the behaviour of this final implementation.\n\nContrib should be checked, that all implementation classes are at least final or they are designed in the same way like CharTokenizer.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1563",
        "summary": "Add example test case for surround query language",
        "description": "",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1724",
        "summary": "Analysis package calls Java 1.5 API",
        "description": "I found compile errors when I tried to compile trunk with 1.4 JVM.\norg.apache.lucene.analysis.NormalizeCharMap\norg.apache.lucene.analysis.MappingCharFilter\n\nuses Character.valueOf() which has been added in 1.5.\nI added a CharacterCache (+ testcase) with a valueOf method as a replacement for that quite useful method.\n\norg.apache.lucene.analysis.BaseTokenTestCase\n\nuses StringBuilder instead of the synchronized version StringBuffer (available in 1.4)\n\nI will attach a patch shortly.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1847",
        "summary": "PhraseQuery/TermQuery/SpanQuery use IndexReader specific stats in their explains",
        "description": "PhraseQuery uses IndexReader in explainfor top level stats - as mentioned by Mike McCandless in LUCENE-1837.\nTermQuery uses IndexReader in explain for top level stats\n\nAlways been a bug with MultiSearcher, but per segment search makes it worse.\n\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-907",
        "summary": "Demo and contrib jars should contain NOTICE.TXT and LICENSE.TXT",
        "description": "We should include NOTICE.TXT and LICENSE.TXT not only in the core jar but also\nin the demo and contrib jars.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1119",
        "summary": "Optimize TermInfosWriter.add",
        "description": "I found one more optimization, in how terms are written in\nTermInfosWriter.  Previously, each term required a new Term() and a\nnew String().  Looking at the cpu time (using YourKit), I could see\nthis was adding a non-trivial cost to flush() when indexing Wikipedia.\n\nI changed TermInfosWriter.add to accept char[] directly, instead.\n\nI ran a quick test building first 200K docs of Wikipedia.  With this\nfix it took 231.31 sec (best of 3) and without the fix it took 236.05\nsec (best of 3) = ~2% speedup.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-792",
        "summary": "PrecedenceQueryParser misinterprets queries starting with NOT",
        "description": "\"NOT foo AND baz\" is parsed as \"-(+foo +baz)\" instead of \"-foo +bar\".\n\n(I'm setting parser.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR) but the issue applies otherwise too.)\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2556",
        "summary": "(Char)TermAttribute cloning memory consumption",
        "description": "The memory consumption problem with cloning a (Char)TermAttributeImpl object was raised on thread http://markmail.org/thread/bybuerugbk5w2u6z",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1755",
        "summary": "WriteLineDocTask should keep docs w/ just title and no body",
        "description": "WriteLineDocTask throws away a document if it does not have a body element. However, if the document has a title, then it should be kept. Some documents, such as emails, may not have a body which is legitimate. I'll post a patch + a test case.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2939",
        "summary": "Highlighter should try and use maxDocCharsToAnalyze in WeightedSpanTermExtractor when adding a new field to MemoryIndex as well as when using CachingTokenStream",
        "description": "huge documents can be drastically slower than need be because the entire field is added to the memory index\nthis cost can be greatly reduced in many cases if we try and respect maxDocCharsToAnalyze\n\nthings can be improved even further by respecting this setting with CachingTokenStream\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2781",
        "summary": "Drop deprecations from trunk",
        "description": "subj.\nAlso, to each remaining deprecation add release version when it first appeared.\n\nPatch incoming.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-755",
        "summary": "Payloads",
        "description": "This patch adds the possibility to store arbitrary metadata (payloads) together with each position of a term in its posting lists. A while ago this was discussed on the dev mailing list, where I proposed an initial design. This patch has a much improved design with modifications, that make this new feature easier to use and more efficient.\n\nA payload is an array of bytes that can be stored inline in the ProxFile (.prx). Therefore this patch provides low-level APIs to simply store and retrieve byte arrays in the posting lists in an efficient way. \n\nAPI and Usage\n------------------------------   \nThe new class index.Payload is basically just a wrapper around a byte[] array together with int variables for offset and length. So a user does not have to create a byte array for every payload, but can rather allocate one array for all payloads of a document and provide offset and length information. This reduces object allocations on the application side.\n\nIn order to store payloads in the posting lists one has to provide a TokenStream or TokenFilter that produces Tokens with payloads. I added the following two methods to the Token class:\n  /** Sets this Token's payload. */\n  public void setPayload(Payload payload);\n  \n  /** Returns this Token's payload. */\n  public Payload getPayload();\n\nIn order to retrieve the data from the index the interface TermPositions now offers two new methods:\n  /** Returns the payload length of the current term position.\n   *  This is invalid until {@link #nextPosition()} is called for\n   *  the first time.\n   * \n   * @return length of the current payload in number of bytes\n   */\n  int getPayloadLength();\n  \n  /** Returns the payload data of the current term position.\n   * This is invalid until {@link #nextPosition()} is called for\n   * the first time.\n   * This method must not be called more than once after each call\n   * of {@link #nextPosition()}. However, payloads are loaded lazily,\n   * so if the payload data for the current position is not needed,\n   * this method may not be called at all for performance reasons.\n   * \n   * @param data the array into which the data of this payload is to be\n   *             stored, if it is big enough; otherwise, a new byte[] array\n   *             is allocated for this purpose. \n   * @param offset the offset in the array into which the data of this payload\n   *               is to be stored.\n   * @return a byte[] array containing the data of this payload\n   * @throws IOException\n   */\n  byte[] getPayload(byte[] data, int offset) throws IOException;\n\nFurthermore, this patch indroduces the new method IndexOutput.writeBytes(byte[] b, int offset, int length). So far there was only a writeBytes()-method without an offset argument. \n\nImplementation details\n------------------------------\n- One field bit in FieldInfos is used to indicate if payloads are enabled for a field. The user does not have to enable payloads for a field, this is done automatically:\n   * The DocumentWriter enables payloads for a field, if one ore more Tokens carry payloads.\n   * The SegmentMerger enables payloads for a field during a merge, if payloads are enabled for that field in one or more segments.\n- Backwards compatible: If payloads are not used, then the formats of the ProxFile and FreqFile don't change\n- Payloads are stored inline in the posting list of a term in the ProxFile. A payload of a term occurrence is stored right after its PositionDelta.\n- Same-length compression: If payloads are enabled for a field, then the PositionDelta is shifted one bit. The lowest bit is used to indicate whether the length of the following payload is stored explicitly. If not, i. e. the bit is false, then the payload has the same length as the payload of the previous term occurrence.\n- In order to support skipping on the ProxFile the length of the payload at every skip point has to be known. Therefore the payload length is also stored in the skip list located in the FreqFile. Here the same-length compression is also used: The lowest bit of DocSkip is used to indicate if the payload length is stored for a SkipDatum or if the length is the same as in the last SkipDatum.\n- Payloads are loaded lazily. When a user calls TermPositions.nextPosition() then only the position and the payload length is loaded from the ProxFile. If the user calls getPayload() then the payload is actually loaded. If getPayload() is not called before nextPosition() is called again, then the payload data is just skipped.\n  \nChanges of file formats\n------------------------------\n- FieldInfos (.fnm)\nThe format of the .fnm file does not change. The only change is the use of the sixth lowest-order bit (0x20) of the FieldBits. If this bit is set, then payloads are enabled for the corresponding field. \n\n- ProxFile (.prx)\nProxFile (.prx) -->  <TermPositions>^TermCount\nTermPositions   --> <Positions>^DocFreq\nPositions       --> <PositionDelta, Payload?>^Freq\nPayload         --> <PayloadLength?, PayloadData>\nPositionDelta   --> VInt\nPayloadLength   --> VInt \nPayloadData     --> byte^PayloadLength\n\nFor payloads disabled (unchanged):\nPositionDelta is the difference between the position of the current occurrence in the document and the previous occurrence (or zero, if this is the first   occurrence in this document).\n  \nFor Payloads enabled:\nPositionDelta/2 is the difference between the position of the current occurrence in the document and the previous occurrence. If PositionDelta is odd, then PayloadLength is stored. If PositionDelta is even, then the length of the current payload equals the length of the previous payload and thus PayloadLength is omitted.\n\n- FreqFile (.frq)\n\nSkipDatum     --> DocSkip, PayloadLength?, FreqSkip, ProxSkip\nPayloadLength --> VInt\n\nFor payloads disabled (unchanged):\nDocSkip records the document number before every SkipInterval th document in TermFreqs. Document numbers are represented as differences from the previous value in the sequence.\n\nFor payloads enabled:\nDocSkip/2 records the document number before every SkipInterval th  document in TermFreqs. If DocSkip is odd, then PayloadLength follows. If DocSkip is even, then the length of the payload at the current skip point equals the length of the payload at the last skip point and thus PayloadLength is omitted.\n\n\nThis encoding is space efficient for different use cases:\n   * If only some fields of an index have payloads, then there's no space overhead for the fields with payloads disabled.\n   * If the payloads of consecutive term positions have the same length, then the length only has to be stored once for every term. This should be a common case, because users probably use the same format for all payloads.\n   * If only a few terms of a field have payloads, then we don't waste much space because we benefit again from the same-length-compression since we only have to store the length zero for the empty payloads once per term.\n\nAll unit tests pass.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3848",
        "summary": "basetokenstreamtestcase should fail if tokenstream starts with posinc=0",
        "description": "This is meaningless for a tokenstream to start with posinc=0,\n\nIts also caused problems and hairiness in the indexer (LUCENE-1255, LUCENE-1542),\nand it makes senseless tokenstreams. We should add a check and fix any that do this.\n\nFurthermore the same bug can exist in removing-filters if they have enablePositionIncrements=false.\nI think this option is useful: but it shouldnt mean 'allow broken tokenstream', it just means we\ndon't add gaps. \n\nIf you remove tokens with enablePositionIncrements=false it should not cause the TS to start with\npositionincrement=0, and it shouldnt 'restructure' the tokenstream (e.g. moving synonyms on top of a different word).\nIt should just not add any 'holes'.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2238",
        "summary": "deprecate ChineseAnalyzer",
        "description": "The ChineseAnalyzer, ChineseTokenizer, and ChineseFilter (not the smart one, or CJK) indexes chinese text as individual characters and removes english stopwords, etc.\n\nIn my opinion we should simply deprecate all of this in favor of StandardAnalyzer, StandardTokenizer, and StopFilter, which does the same thing.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2813",
        "summary": "TestIndexWriterDelete fails randomly",
        "description": "10 out of 9 runs with that see fail on my trunk:\n\nant test-core -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField\n\n\nwith this result:\n\n{code}\n\njunit-sequential:\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete\n    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 1.725 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, contents=SimpleText, city=MockSep}, locale=ar_QA, timezone=VST\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestIndexWriterDelete]\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):\tFAILED\n    [junit] \n    [junit] junit.framework.AssertionFailedError: \n    [junit] \tat org.apache.lucene.index.TestIndexWriterDelete.testErrorAfterApplyDeletes(TestIndexWriterDelete.java:736)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)\n    [junit] \n    [junit] \n    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):\tFAILED\n    [junit] ConcurrentMergeScheduler hit unhandled exceptions\n    [junit] junit.framework.AssertionFailedError: ConcurrentMergeScheduler hit unhandled exceptions\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:503)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.index.TestIndexWriterDelete FAILED\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-689",
        "summary": "NullPointerException thrown by equals method in SpanOrQuery",
        "description": "Part of our code utilizes the equals method in SpanOrQuery and, in certain cases (details to follow, if necessary), a NullPointerException gets thrown as a result of the String \"field\" being null.  After applying the following patch, the problem disappeared:\n\nIndex: src/java/org/apache/lucene/search/spans/SpanOrQuery.java\n===================================================================\n--- src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (revision 465065)\n+++ src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (working copy)\n@@ -121,7 +121,8 @@\n     final SpanOrQuery that = (SpanOrQuery) o;\n\n     if (!clauses.equals(that.clauses)) return false;\n-    if (!field.equals(that.field)) return false;\n+    if (field != null && !field.equals(that.field)) return false;\n+    if (field == null && that.field != null) return false;\n\n     return getBoost() == that.getBoost();\n   }\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-238",
        "summary": "[PATCH] import cleanup",
        "description": "This patch just removes useless imports so you get less warnings in Eclipse.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-964",
        "summary": "Remove DocumentWriter",
        "description": "DocumentWriter has been replaced by DocumentsWriter (from LUCENE-843) so we need to remove it & fix the unit tests that directly use it...",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3238",
        "summary": "SpanMultiTermQueryWrapper with Prefix Query issue",
        "description": "If we try to do a search with SpanQuery and a PrefixQuery this message is returned:\n\n\"You can only use SpanMultiTermQueryWrapper with a suitable SpanRewriteMethod.\"\n\nThe problem is in the WildcardQuery rewrite function.\n\nIf the wildcard query is a prefix, a new prefix query is created, the rewrite method is set with the SpanRewriteMethod and the prefix query is returned.\n\nBut, that's the rewritten prefix query which should be returned:\n\n-      return rewritten;\n+      return rewritten.rewrite(reader);\n\nI will attach a patch with a unit test included.\n\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3851",
        "summary": "TestTermInfosReaderIndex failing (always reproducible)",
        "description": "Always fails on branch (use reproduce string below):\ngit clone --depth 1 -b rr git@github.com:dweiss/lucene_solr.git\n\n{noformat}\n[junit4] Running org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex\n[junit4] FAILURE 0.04s J0 | TestTermInfosReaderIndex.testSeekEnum\n[junit4]    > Throwable #1: java.lang.AssertionError: expected:<field9:z91ob3wozm6d> but was:<:>\n[junit4]    > \tat __randomizedtesting.SeedInfo.seed([C7597DFBBE0B3D7D:C6D9CEDD0700AAFF]:0)\n[junit4]    > \tat org.junit.Assert.fail(Assert.java:93)\n[junit4]    > \tat org.junit.Assert.failNotEquals(Assert.java:647)\n[junit4]    > \tat org.junit.Assert.assertEquals(Assert.java:128)\n[junit4]    > \tat org.junit.Assert.assertEquals(Assert.java:147)\n[junit4]    > \tat org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex.testSeekEnum(TestTermInfosReaderIndex.java:137)\n[junit4]    > \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n[junit4]    > \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n[junit4]    > \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n[junit4]    > \tat java.lang.reflect.Method.invoke(Method.java:597)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1766)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.access$1000(RandomizedRunner.java:141)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:728)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:789)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:803)\n[junit4]    > \tat org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:744)\n[junit4]    > \tat org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:636)\n[junit4]    > \tat org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)\n[junit4]    > \tat org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:550)\n[junit4]    > \tat org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:735)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.access$600(RandomizedRunner.java:141)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$3$1.run(RandomizedRunner.java:586)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:605)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:641)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:652)\n[junit4]    > \tat org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:533)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.access$400(RandomizedRunner.java:141)\n[junit4]    > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:479)\n[junit4]    > \n[junit4]   2> NOTE: reproduce with: ant test -Dtests.filter=*.TestTermInfosReaderIndex -Dtests.filter.method=testSeekEnum -Drt.seed=C7597DFBBE0B3D7D -Dargs=\"-Dfile.encoding=UTF-8\"\n[junit4]   2>\n[junit4]    > (@AfterClass output)\n[junit4]   2> NOTE: test params are: codec=Appending, sim=DefaultSimilarity, locale=en, timezone=Atlantic/Stanley\n[junit4]   2> NOTE: all tests run in this JVM:\n[junit4]   2> [TestLock, TestFileSwitchDirectory, TestWildcardRandom, TestVersionComparator, TestTermdocPerf, TestBitVector, TestParallelTermEnum, TestSimpleSearchEquivalence, TestNumericRangeQuery64, TestSort, TestIsCurrent, TestToken, TestIntBlockCodec, TestDocumentsWriterDeleteQueue, TestPagedBytes, TestThreadedForceMerge, TestOmitTf, TestSegmentTermEnum, TestIndexWriterConfig, TestCheckIndex, TestTermVectorsWriter, TestNumericTokenStream, TestSearchAfter, TestRegexpQuery, InBeforeClass, InAfterClass, InTestMethod, NonStringProperties, TestIndexWriterMergePolicy, TestVirtualMethod, TestFieldCache, TestSurrogates, TestSegmentTermDocs, TestMultiValuedNumericRangeQuery, TestBasicOperations, TestCodecs, TestDateSort, TestPositiveScoresOnlyCollector, TestBooleanQuery, TestIndexInput, TestMinimize, TestNumericRangeQuery32, TestBoolean2, TestSloppyPhraseQuery, TestNoDeletionPolicy, TestFieldCacheTermsFilter, TestRandomStoredFields, TestDocBoost, TestTransactionRollback, TestUnicodeUtil, TestIndexWriterLockRelease, TestUTF32ToUTF8, TestFixedBitSet, TestDoubleBarrelLRUCache, TestTimeLimitingCollector, TestSpanFirstQuery, TestDirectory, TestSpansAdvanced2, TestConcurrentMergeScheduler, TestIndexWriterExceptions, TestDocValues, TestCustomNorms, TestFieldValueFilter, TestTermVectors, TestTermInfosReaderIndex]\n[junit4]   2> NOTE: Linux 2.6.32-38-server amd64/Sun Microsystems Inc. 1.6.0_20 (64-bit)/cpus=4,threads=1,free=100102360,total=243859456\n[junit4]   2> \n{noformat}",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-818",
        "summary": "IndexWriter should detect when it's used after being closed",
        "description": "Spinoff from this thread on java-user:\n\n    http://www.gossamer-threads.com/lists/lucene/java-user/45986\n\nIf you call addDocument on IndexWriter after it's closed you'll hit a\nhard-to-explain NullPointerException (because the RAMDirectory was\nclosed).  Before 2.1, apparently you won't hit any exception and the\nIndexWrite will keep running but will have released it's write lock (I\nthink).\n\nI plan to fix IndexWriter methods to throw an IllegalStateException if\nit has been closed.\n",
        "label": "NUG",
        "classified": "SPEC",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2638",
        "summary": "Make HighFreqTerms.TermStats class public",
        "description": "It's not possible to use public methods in contrib/misc/... /HighFreqTerms from outside the package because the return type has package visibility. I propose to move TermStats class to a separate file and make it public.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-979",
        "summary": "Remove Deprecated Benchmarking Utilities from contrib/benchmark",
        "description": "The old Benchmark utilities in contrib/benchmark have been deprecated and should be removed in 2.9 of Lucene.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2961",
        "summary": "Remove benchmark/lib/xml-apis.jar - JVM 1.5 already contains the required JAXP 1.3 implementation",
        "description": "On [LUCENE-2957|https://issues.apache.org/jira/browse/LUCENE-2957?focusedCommentId=13004991&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13004991], Uwe wrote:\n{quote}\nxml-apis.jar is not needed with xerces-2.9 and Java 5, as Java 5 already has these interface classes (JAXP 1.3). Xerces 2.11 needs it, because it ships with Java 6's JAXP release (containing STAX & Co. not available in Java 5).\n{quote}\n\nOn the #lucene IRC channel, Uwe also wrote:\n{noformat}\nsince we are on java 5 since 3.0\nwe have the javax APIs already available in the JVM\nxerces until 2.9.x only needs JAXP 1.3\nso the only thing you need is xercesImpl.jar\nand serializer.jar\nserializer.jar is shared between all apache xml projects, dont know the exact version number\nok you dont need it whan you only parse xml\nas soon as you want to serialize a dom tree or result of an xsl transformation you need it\n[...]\nbut if we upgrade to latest xerces we need it [the xml-apis jar] again unless we are on java 6\nso the one shipped with xerces 2.11 is the 1.4 one\nbecause xerces 2.11 supports Stax\n{noformat}",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1959",
        "summary": "Index Splitter",
        "description": "If an index has multiple segments, this tool allows splitting those segments into separate directories.  ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3897",
        "summary": "KuromojiTokenizer fails with large docs",
        "description": "just shoving largeish random docs triggers asserts like:\n\n{noformat}\n    [junit] Caused by: java.lang.AssertionError: backPos=4100 vs lastBackTracePos=5120\n    [junit] \tat org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.backtrace(KuromojiTokenizer.java:907)\n    [junit] \tat org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.parse(KuromojiTokenizer.java:756)\n    [junit] \tat org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.incrementToken(KuromojiTokenizer.java:403)\n    [junit] \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:404)\n{noformat}\n\nBut, you get no seed...\n\nI'll commit the test case and @Ignore it.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3471",
        "summary": "TestNRTManager test failure",
        "description": "reproduces for me",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2283",
        "summary": "Possible Memory Leak in StoredFieldsWriter",
        "description": "StoredFieldsWriter creates a pool of PerDoc instances\n\nthis pool will grow but never be reclaimed by any mechanism\n\nfurthermore, each PerDoc instance contains a RAMFile.\nthis RAMFile will also never be truncated (and will only ever grow) (as far as i can tell)\n\nWhen feeding documents with large number of stored fields (or one large dominating stored field) this can result in memory being consumed in the RAMFile but never reclaimed. Eventually, each pooled PerDoc could grow very large, even if large documents are rare.\n\nSeems like there should be some attempt to reclaim memory from the PerDoc[] instance pool (or otherwise limit the size of RAMFiles that are cached) etc\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-485",
        "summary": "IndexWriter.mergeSegments should not hold the commit lock while cleaning up.",
        "description": "Same happens in IndexWriter.addIndexes(IndexReader[] readers).\n\nThe commit lock should be obtained whenever the Index structure/version is read or written.  It should be kept for as short a period as possible.\n\nThe write lock is needed to make sure only one IndexWriter or IndexReader instance can update the index (multiple IndexReaders can of course use the index for searching).\n\nThe list of files that can be deleted is stored in the file \"deletable\".  It is only read or written by the IndexWriter instance that holds the write lock, so there's no need to have the commit lock to to update it.\n\nOn my production system deleting the obsolete segment files after a mergeSegments() happens can occasionally take several seconds(!) and the commit lock blocks the searcher machines from updating their IndexReader instance.\nEven on a standalone machine, the time to update the segments file is about 3ms, the time to delete the obsolete segments about 30ms.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3223",
        "summary": "SearchWithSortTask ignores sorting by Doc",
        "description": "During my work in LUCENE-3912, I found the following code:\n\n{code}\nif (field.equals(\"doc\")) {\n    sortField0 = SortField.FIELD_DOC;\n} if (field.equals(\"score\")) {\n    sortField0 = SortField.FIELD_SCORE;\n} ...\n{code}\n\nThis means the setting of SortField.FIELD_DOC is ignored.  While I don't know much about this code, this seems like a valid setting and obviously just a bug.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3895",
        "summary": "Not getting random-seed/reproduce-with if a test fails from another thread",
        "description": "See https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12822/console as an example.\n\nThis is at least affecting 4.0, maybe 3.x too",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-227",
        "summary": "[PATCH] documentation typo",
        "description": "Just a small patch that fixes a typo and changes the first sentence, as that \none is used by Javadoc as a kind of summary so it should be something more \nuseful than \"The Jakarta Lucene API is divided into several packages.\"",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1090",
        "summary": "remove relative paths assumptions from benchmark code",
        "description": "Also see Eric comments in:\n   http://www.nabble.com/forum/ViewPost.jtp?post=14347924&framed=y\n\nBenchmark's config.xml relies on relative paths, more or less like this;\n- base-dir\n   -- conf-dir\n   -- work-dir\n       --- docs-dir\n       --- indexes-dir\n\nThese assumptions are also in the Java code, and so it is inconvenient for\nusing absolute paths, e.g. for specifying a docs dir that is not under work-dir.\n\nRelax this by modifying in build.xml to replace \"value\" and \"line\" props by \n\"location\" and \"file\" and by requiring absolute paths in the Java code.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3572",
        "summary": "MultiIndexDocValues pretends it can merge sorted sources",
        "description": "Nightly build hit this failure:\n\n{noformat}\nant test-core -Dtestcase=TestSort -Dtestmethod=testReverseSort -Dtests.seed=791b126576b0cfab:-48895c7243ecc5d0:743c683d1c9f7768 -Dtests.multiplier=3 -Dargs=\"-Dfile.encoding=ISO8859-1\"\n\n    [junit] Testcase: testReverseSort(org.apache.lucene.search.TestSort):\tCaused an ERROR\n    [junit] expected:<[CEGIA]> but was:<[ACEGI]>\n    [junit] \tat org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1248)\n    [junit] \tat org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)\n    [junit] \tat org.apache.lucene.search.TestSort.testReverseSort(TestSort.java:759)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)\n{noformat}\n\nIt's happening in the test for reverse-sort of a string field with DocValues, when the test had gotten SlowMultiReaderWrapper.\n\nI committed a fix to the test to avoid testing this case, but we need a better fix to the underlying bug.\n\nMultiIndexDocValues cannot merge sorted sources (I think?), yet somehow it's pretending it can (in the above test, the three subs had BYTES_FIXED_SORTED type, and the TypePromoter happily claims to merge these to BYTES_FIXED_SORTED; I think MultiIndexDocValues should return null for the sorted source in this case?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2571",
        "summary": "Indexing performance tests with realtime branch",
        "description": "We should run indexing performance tests with the DWPT changes and compare to trunk.\n\nWe need to test both single-threaded and multi-threaded performance.\n\nNOTE:  flush by RAM isn't implemented just yet, so either we wait with the tests or flush by doc count.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1507",
        "summary": "adding EmptyDocIdSet/Iterator",
        "description": "Adding convenience classes for EmptyDocIdSet and EmptyDocIdSetIterator",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-941",
        "summary": "Benchmark alg line -  {[AddDoc(4000)]: 4} : * - causes an infinite loop",
        "description": "Background in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html \nThe line  \n   {[AddDoc(4000)]: 4} : * \ncauses an infinite loop because the parallel sequence would mask the exhaustion from the outer sequential sequence.\n\nTo fix this the DocMaker exhaustion check should be modified to rely  on the doc maker instance only, and to be reset when the inputs are being reset. ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-709",
        "summary": "[PATCH] Enable application-level management of IndexWriter.ramDirectory size",
        "description": "IndexWriter currently only supports bounding of in the in-memory index cache using maxBufferedDocs, which limits it to a fixed number of documents.  When document sizes vary substantially, especially when documents cannot be truncated, this leads either to inefficiencies from a too-small value or OutOfMemoryErrors from a too large value.\n\nThis simple patch exposes IndexWriter.flushRamSegments(), and provides access to size information about IndexWriter.ramDirectory so that an application can manage this based on total number of bytes consumed by the in-memory cache, thereby allow a larger number of smaller documents or a smaller number of larger documents.  This can lead to much better performance while elimianting the possibility of OutOfMemoryErrors.\n\nThe actual job of managing to a size constraint, or any other constraint, is left up the applicatation.\n\nThe addition of synchronized to flushRamSegments() is only for safety of an external call.  It has no significant effect on internal calls since they all come from a sychronized caller.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2674",
        "summary": "improve how MTQs interact with the terms dict cache",
        "description": "Some small improvements:\n\n  * Adds a TermsEnum.cacheCurrentTerm \"hint\" (codec can make this a no-op)\n\n  * Removes the FTE.useTermsCache\n\n  * Changes MTQ's TermCollector API to accept the TermsEnum so collectors can eg call .docFreq directly\n\n  * Adds expert ctor to TermQuery allowing you to pass in the docFreq",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2059",
        "summary": "benchmark pkg: allow TrecContentSource not to change the docname",
        "description": "TrecContentSource currently appends 'iteration number' to the docname field.\nExample: if the original docname is DOC0001 then it will be indexed as DOC0001_0\n\nthis presents a problem for relevance testing, because when judging results, the expected docname will never be present.\nThis patch adds an option to disable this behavior, defaulting to the existing behavior (which is to append the iteration number).\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3106",
        "summary": "commongrams filter calls incrementToken() after it returns false",
        "description": "In LUCENE-3064, we beefed up MockTokenizer with assertions, and I started cutting over some analysis tests to use MockTokenizer for better coverage.\n\nThe commongrams tests fail, because they call incrementToken() after it already returns false. \n\nIn general its my understanding consumers should not do this (and i know of a few tokenizers that will actually throw exceptions if you do this, just like java iterators and such).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3676",
        "summary": "Support SortedSource in MultiDocValues",
        "description": "MultiDocValues doesn't support Sorted variant ie. SortedSource but throws UnsupportedOperationException. This forces users to work per segment. For consistency we should support sortedsource also if we wrap the DocValues in MDV.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1762",
        "summary": "Slightly more readable code in Token/TermAttributeImpl",
        "description": "No big deal. \n\ngrowTermBuffer(int newSize) was using correct, but slightly hard to follow code. \n\nthe method was returning null as a hint that the current termBuffer has enough space to the upstream code or reallocated buffer.\n\nthis patch simplifies logic   making this method to only reallocate buffer, nothing more.  \nIt reduces number of if(null) checks in a few methods and reduces amount of code. \nall tests pass.\n\nThis also adds tests for the new basic attribute impls (copies of the Token tests).",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2356",
        "summary": "Enable setting the terms index divisor used by IndexWriter whenever it opens internal readers",
        "description": "Opening a place holder issue... if all the refactoring being discussed don't make this possible, then we should add a setting to IWC to do so.\n\nApps with very large numbers of unique terms must set the terms index divisor to control RAM usage.\n\n(NOTE: flex's RAM terms dict index RAM usage is more efficient, so this will help such apps).\n\nBut, when IW resolves deletes internally it always uses default 1 terms index divisor, and the app cannot change that.  Though one workaround is to call getReader(termInfosIndexDivisor) which will pool the reader with the right divisor.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1678",
        "summary": "Deprecate Analyzer.tokenStream",
        "description": "The addition of reusableTokenStream to the core analyzers unfortunately broke back compat of external subclasses:\n\n    http://www.nabble.com/Extending-StandardAnalyzer-considered-harmful-td23863822.html\n\nOn upgrading, such subclasses would silently not be used anymore, since Lucene's indexing invokes reusableTokenStream.\n\nI think we should should at least deprecate Analyzer.tokenStream, today, so that users see deprecation warnings if their classes override this method.  But going forward when we want to change the API of core classes that are extended, I think we have to  introduce entirely new classes, to keep back compatibility.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1653",
        "summary": "Change DateTools to not create a Calendar in every call to dateToString or timeToString",
        "description": "DateTools creates a Calendar instance on every call to dateToString and timeToString. Specifically:\n\n# timeToString calls Calendar.getInstance on every call.\n# dateToString calls timeToString(date.getTime()), which then instantiates a new Date(). I think we should change the order of the calls, or not have each call the other.\n# round(), which is called from timeToString (after creating a Calendar instance) creates another (!) Calendar instance ...\n\nSeems that if we synchronize the methods and create the Calendar instance once (static), it should solve it.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3340",
        "summary": "Buffered deletes are not flushed by RAM or count",
        "description": "When a segment is flushed, we will generally NOT flush the deletes, ie we simply buffer up the pending delete terms/queries, and the only apply them if 1) a segment is going to be merged (so we can remove the del docs in that segment), or 2) the buffered deletes' RAM exceeds 1/2 of IW's RAM limit when we are flushing a segment, or 3) the buffered deletes count exceeds IWC's maxBufferedDeleteTerms.\n\nBut the latter 2 triggers are currently broken on trunk; I suspect (but I'm not sure) when we landed DWPT we introduced this bug.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2140",
        "summary": "TopTermsScoringBooleanQueryRewrite minscore",
        "description": "when using the TopTermsScoringBooleanQueryRewrite (LUCENE-2123), it would be nice if MultiTermQuery could set an attribute specifying the minimum required score once the Priority Queue is filled. \n\nThis way, FilteredTermsEnums could adjust their behavior accordingly based on the minimal score needed to actually be a useful term (i.e. not just pass thru the pq)\n\nAn example is FuzzyTermsEnum: at some point the bottom of the priority queue contains words with edit distance of 1 and enumerating any further terms is simply a waste of time.\nThis is because terms are compared by score, then termtext. So in this case FuzzyTermsEnum could simply seek to the exact match, then end.\n\nThis behavior could be also generalized for all n, for a different impl of fuzzyquery where it is only looking in the term dictionary for words within edit distance of n' which is the lowest scoring term in the pq (they adjust their behavior during enumeration of the terms depending upon this attribute).\n\nOther FilteredTermsEnums could make use of this minimal score in their own way, to drive the most efficient behavior so that they do not waste time enumerating useless terms.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2257",
        "summary": "relax the per-segment max unique term limit",
        "description": "Lucene can't handle more than 2.1B (limit of signed 32 bit int) unique terms in a single segment.\n\nBut I think we can improve this to termIndexInterval (default 128) * 2.1B.  There is one place (internal API only) where Lucene uses an int but should use a long.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1771",
        "summary": "Using explain may double ram reqs for fieldcaches when using ValueSourceQuery/CustomScoreQuery or for ConstantScoreQuerys that use a caching Filter.",
        "description": "",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2366",
        "summary": "LLRect.createBox returned box does not contains all points in (center,distance) disc",
        "description": "LLRect,createBox computation of a bouding box for a disc given center and distance doest not contains all the point in the distance.\n\nExample : the point north by distance doest not have Lat inferior of Lat of the UpperRight corner of the returned box",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3507",
        "summary": "Improve Memory Consumption for merging DocValues SortedBytes variants",
        "description": "Currently SortedBytes are loaded into memory during merge which could be a potential trap. Instead of loading them into Heap memory we can merge those sorted values with much smaller memory and without loading all values into ram.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2817",
        "summary": "SimpleText has a bulk enum buffer reuse bug",
        "description": "testBulkPostingsBufferReuse fails with SimpleText codec.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1927",
        "summary": "Lucene-core 2.9.0 missing from Maven Central Repository",
        "description": "Sub-projects like lucene-demos, lucene-contrib, etc. exist in central, and depend on 2.9.0 of lucene-core. However, the lucene-core 2.9.0 artifact itself is missing.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3692",
        "summary": "DocumentsWriter blocks flushes when applyDeletes takes forever - memory not released",
        "description": "In DocumentsWriter we have a safety check that applies all deletes if the deletes consume too much RAM to prevent too-frequent flushing of a long tail of tiny segments. If we enter applyAllDeletes we essentially lock on IW -> BufferedDeletes which is fine since this usually doesn't take long and doesn't keep DWPTs from indexing. Yet, if that takes long and at the same time a semgent is flushed and subsequently published to the IW we take the lock on the ticket queue and the IW. Now this prevents all other threads to append to the ticketQueue which is done BEFORE we actually flush the segment concurrently and free up the RAM.\n\nEssentially its ok to block on the IW lock but we should not keep concurrent flushed from execution just because we apply deletes. The threads will block once they try to execute maybeMerge after the segment is flushed so we don't pile up subsequent memory but we should actually allow the DWPT to be flushed since we actually try to get rid of memory.\n\nI ran into this by accident due to a coding bug using delete queries instead of terms for each document. This thread dump show the problem:\n\n{noformat}\n\"Application Worker Thread\" prio=10 tid=0x00007fdda0238000 nid=0x3256 waiting for\nmonitor entry [0x00007fddad3c2000]\n  java.lang.Thread.State: BLOCKED (on object monitor)\n       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)\n       - waiting to lock <0x00007fddb74ff990> (a\norg.apache.lucene.index.DocumentsWriter$TicketQueue)\n       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)\n       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)\n       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)\n\n\"Application Worker Thread\" prio=10 tid=0x00007fdda0236000 nid=0x3255 waiting for\nmonitor entry [0x00007fddad4c3000]\n  java.lang.Thread.State: BLOCKED (on object monitor)\n       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)\n       - waiting to lock <0x00007fddb74fe350> (a\norg.apache.solr.update.SolrIndexWriter)\n       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)\n       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)\n       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)\n\n\"Application Worker Thread\" prio=10 tid=0x00007fdda0234000 nid=0x3254 waiting for\nmonitor entry [0x00007fddad5c4000]\n  java.lang.Thread.State: BLOCKED (on object monitor)\n       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)\n       - waiting to lock <0x00007fddb74fe350> (a\norg.apache.solr.update.SolrIndexWriter)\n       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)\n       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)\n       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)\n\n\"Application Worker Thread\" prio=10 tid=0x00007fdda0232000 nid=0x3253 waiting for\nmonitor entry [0x00007fddad6c5000]\n  java.lang.Thread.State: BLOCKED (on object monitor)\n       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)\n       - waiting to lock <0x00007fddb74ff990> (a\norg.apache.lucene.index.DocumentsWriter$TicketQueue)\n       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)\n       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)\n       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)\n\n\"Application Worker Thread\" prio=10 tid=0x00007fdda0230800 nid=0x3252 waiting for\nmonitor entry [0x00007fddad7c6000]\n  java.lang.Thread.State: BLOCKED (on object monitor)\n       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)\n       - waiting to lock <0x00007fddb74fe350> (a\norg.apache.solr.update.SolrIndexWriter)\n       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)\n       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)\n       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)\n\n\"Application Worker Thread\" prio=10 tid=0x00007fdda022e800 nid=0x3251 runnable\n[0x00007fddad8c6000]\n  java.lang.Thread.State: RUNNABLE\n       at java.nio.Bits.copyToArray(Bits.java:715)\n       at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:233)\n       at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readBytes(MMapDirectory.java:319)\n       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum$Frame.loadBlock(BlockTreeTermsReader.java:2283)\n       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.seekExact(BlockTreeTermsReader.java:1600)\n       at org.apache.lucene.util.TermContext.build(TermContext.java:97)\n       at org.apache.lucene.search.TermQuery.createWeight(TermQuery.java:180)\n       at org.apache.lucene.search.BooleanQuery$BooleanWeight.<init>(BooleanQuery.java:186)\n       at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:423)\n       at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:583)\n       at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:55)\n       at org.apache.lucene.index.BufferedDeletesStream.applyQueryDeletes(BufferedDeletesStream.java:431)\n       at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:268)\n       - locked <0x00007fddb751e1e8> (a\norg.apache.lucene.index.BufferedDeletesStream)\n       at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2852)\n       - locked <0x00007fddb74fe350> (a org.apache.solr.update.SolrIndexWriter)\n       at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:188)\n       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:470)\n       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)\n       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)\n       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)\n       \n\n\"Application Worker Thread\" prio=10 tid=0x00007fdda022d800 nid=0x3250 waiting for\nmonitor entry [0x00007fddad9c8000]\n  java.lang.Thread.State: BLOCKED (on object monitor)\n       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)\n       - waiting to lock <0x00007fddb74ff990> (a\norg.apache.lucene.index.DocumentsWriter$TicketQueue)\n       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)\n       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)\n       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)\n   \n\"Application Worker Thread\" prio=10 tid=0x00007fdda022d000 nid=0x324f waiting for\nmonitor entry [0x00007fddadac9000]\n  java.lang.Thread.State: BLOCKED (on object monitor)\n       at org.apache.lucene.index.IndexWriter.useCompoundFile(IndexWriter.java:2274)\n       - waiting to lock <0x00007fddb74fe350> (a\norg.apache.solr.update.SolrIndexWriter)\n       at org.apache.lucene.index.IndexWriter.prepareFlushedSegment(IndexWriter.java:2156)\n       at org.apache.lucene.index.DocumentsWriter.publishFlushedSegment(DocumentsWriter.java:526)\n       at org.apache.lucene.index.DocumentsWriter.finishFlush(DocumentsWriter.java:506)\n       at org.apache.lucene.index.DocumentsWriter.applyFlushTickets(DocumentsWriter.java:483)\n       - locked <0x00007fddb74ff990> (a\norg.apache.lucene.index.DocumentsWriter$TicketQueue)\n       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:449)\n       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)\n       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)\n       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)\n       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)\n\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-857",
        "summary": "Remove BitSet caching from QueryFilter",
        "description": "Since caching is built into the public BitSet bits(IndexReader reader)  method, I don't see a way to deprecate that, which means I'll just cut it out and document it in CHANGES.txt.  Anyone who wants QueryFilter caching will be able to get the caching back by wrapping the QueryFilter in the CachingWrapperFilter.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1784",
        "summary": "Make BooleanWeight and DisjunctionMaxWeight protected",
        "description": "Currently, BooleanWeight is private, yet it has 2 protected members (similarity, weights) which are unaccessible from custom code\n\ni have some use cases where it would be very useful to crawl a BooleanWeight to get at the sub Weight objects\n\nhowever, since BooleanWeight is private, i have no way of doing this\n\nIf BooleanWeight is protected, then i can subclass BooleanQuery to hook in and wrap BooleanWeight with a subclass to facilitate this walking of the Weight objects\n\nWould also want DisjunctionMaxWeight to be protected, along with its \"weights\" member\n\nWould be even better if these Weights were made public with accessors to their sub \"weights\" objects (then no subclassing would be necessary on my part)\n\nthis should be really trivial and would be great if it can get into 2.9\n\nmore generally, it would be nice if all Weight classes were public with nice accessors to relevant \"sub weights\"/etc so custom code can get its hooks in where and when desired",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-203",
        "summary": "[PATCH] GermanAnalyzer fails silently + doesn't close files",
        "description": "As mentioned on the developer list, the German analyzer will assume an empty list of \nstopwords if the stopword file isn't found. I'll attach a patch that makes it throw an \nIOException instead. Also the patch makes sure the file readers are closed.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3016",
        "summary": "Analyzer for Latvian",
        "description": "Less aggressive form of Kreslins' phd thesis: A stemming algorithm for Latvian.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1452",
        "summary": "Binary field content lost during optimize",
        "description": "Scenario:\n\n* create an index with arbitrary content, and close it\n* open IndexWriter again, and add a document with binary field (stored but not compressed)\n* close IndexWriter _without_ optimizing, so that the new document is in a separate segment.\n* open IndexReader. You can read the last document and its binary field just fine.\n* open IndexWriter, optimize the index, close IndexWriter\n* open IndexReader. Now the field is still present (not null) and is marked as binary, but the data is not there - Field.getBinaryLength() returns 0.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1234",
        "summary": "BoostingTermQuery's BoostingSpanScorer class should be protected instead of package access",
        "description": "Currently, BoostingTermScorer, an inner class of BoostingTermQuery is not accessible from outside the search.payloads\nmaking it difficult to write an extension of BoostingTermQuery. The other inner classes are protected already, as they should be.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1132",
        "summary": "Highlighter Documentation updates",
        "description": "Various places in the Highlighter documentation refer to bytes (i.e. SimpleFragmenter) when it should be chars.  See http://www.gossamer-threads.com/lists/lucene/java-user/56986",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2743",
        "summary": "SimpleText is too slow",
        "description": "If you are unlucky enough to get SimpleText codec on the TestBasics (span query) test then it runs very slowly...",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3723",
        "summary": "Remove FieldMaskingSpanQuery (or fix its scoring)",
        "description": "In Lucene 4.0 we added new scoring mechanisms, but FieldMaskingSpanQuery is a serious problem:\n\nBecause it lies about the fields of its terms, this sometimes results in totally bogus\nstatistics, cases where a single terms totalTermFreq exceeds sumTotalTermFreq for the entire field (since its lying about it).\n\nSuch lying could result in NaN/Inf/Negative scores, exceptions, divide by zero, and other problems,\nbecause the statistics are impossibly bogus.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-664",
        "summary": "[PATCH] small fixes to the new scoring.html doc",
        "description": "This is an awesome initiative.  We need more docs that cleanly explain the inner workings of Lucene in general... thanks Grant & Steve & others!\n\nI have a few small initial proposed fixes, largely just adding some more description around the components of the formula.  But also a couple typos, another link out to Wikipedia, a missing closing ), etc.  I've only made it through the \"Understanding the Scoring Formula\" section so far.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-326",
        "summary": "IndexWriter.addIndexes results in java.lang.OutOfMemoryError",
        "description": "I'm re-opening a bug I logged previously. My previous bug report has \ndisappeared. \n\nIssue: IndexWriter.addIndexes results in java.lang.OutOfMemoryError for large \nmerges.\n\nUntil this writing, I've been merging successfully only through repetition, \ni.e. I keep repeating merges until a success. As my index size has grown, my \nsuccess rate has steadily declined. I've reached the point where merges now \nfail 100% of the time. I can't merge.\n\nMy tests indicate the threshold is ~30GB on P4/800MB VM with 6 indexes. I have \nrepeated my tests on many different machines (not machine dependent). I have \nrepeated my test using local and attached storage devices (not storage \ndependent).\n\nFor what its worth, I believe the exception occurs entirely during the optimize \nprocess which is called implicitly after the merge. I say this because each \ntime it appears the correct amount of bytes are written to the new index. Is it \npossible to decouple the merge and optimize processes?\n\n\nThe code snippet follows. I can send you the class file and 120GB data set. Let \nme know how you want it.\n\n>>>>> code sample >>>>>\n\nDirectory[] sources = new Directory[paths.length];\n...\n\nDirectory dest = FSDirectory.getDirectory( path, true);\nIndexWriter writer = new IndexWriter( dest, new TermAnalyzer( \nStopWords.SEARCH_MAP), true);\n\nwriter.addIndexes( sources);\nwriter.close();",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1043",
        "summary": "Speedup merging of stored fields when field mapping \"matches\"",
        "description": "Robert Engels suggested the following idea, here:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/54217\n\nWhen merging in the stored fields from a segment, if the field name ->\nnumber mapping is identical then we can simply bulk copy the entire\nentry for the document rather than re-interpreting and then re-writing\nthe actual stored fields.\n\nI've pulled the code from the above thread and got it working on the\ncurrent trunk.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1516",
        "summary": "Integrate IndexReader with IndexWriter ",
        "description": "The current problem is an IndexReader and IndexWriter cannot be open\nat the same time and perform updates as they both require a write\nlock to the index. While methods such as IW.deleteDocuments enables\ndeleting from IW, methods such as IR.deleteDocument(int doc) and\nnorms updating are not available from IW. This limits the\ncapabilities of performing updates to the index dynamically or in\nrealtime without closing the IW and opening an IR, deleting or\nupdating norms, flushing, then opening the IW again, a process which\ncan be detrimental to realtime updates. \n\nThis patch will expose an IndexWriter.getReader method that returns\nthe currently flushed state of the index as a class that implements\nIndexReader. The new IR implementation will differ from existing IR\nimplementations such as MultiSegmentReader in that flushing will\nsynchronize updates with IW in part by sharing the write lock. All\nmethods of IR will be usable including reopen and clone. \n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-649",
        "summary": "Fixed Spelling mailinglist.xml",
        "description": "Just fixed some spelling in the mailinglist.xml in /java/trunk/xdocs\n\n\n\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3701",
        "summary": "FST apis out of sync between trunk/3.x",
        "description": "Looks like the offender is LUCENE-3030 :)\n\nNot sure if everything is generally useful but it does change the public API (e.g. you can specify FreezeTail to the super-scary Builder ctor among other things).\n\nMaybe we should sync up for 3.x? ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1491",
        "summary": "EdgeNGramTokenFilter stops on tokens smaller then minimum gram size.",
        "description": "If a token is encountered in the stream that is shorter in length than the min gram size, the filter will stop processing the token stream.\n\nWorking up a unit test now, but may be a few days before I can provide it. Wanted to get it in the system.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1384",
        "summary": "addIndexesNoOptimize intermittantly throws incorrect \"segment exists in external directory...\" exception",
        "description": "Spinoff from here:\n\n    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200809.mbox/%3Cba72f77f0809111418l29cf215dnd45bf679832d7d42%40mail.gmail.com%3E\n\nHere's my response on that thread:\n\nThe bug only happens when you call addIndexesNoOptimize, and one\nsimple workaround would be to use SerialMergeScheduler.\n\nI think this is already fixed in trunk (soonish to be 2.4) as a side\neffect of https://issues.apache.org/jira/browse/LUCENE-1335.\n\nIn 2.3, merges that involve external segments (which are segments\nfolded in by addIndexesNoOptimize) are not supposed to run in a BG\nthread.  This is to prevent addIndexesNoOptimize from returning until\nafter all external segments have been carried over (merged or copied)\ninto the index, so that if there is an exception (eg disk full),\naddIndexesNoOptimize is able to rollback to the index to the starting\npoint.\n\nThe primary merge() method of CMS indeed does not BG any external\nmerges, but the bug is that when a BG merge finishes it then selects a\nnew merge to kick off and that selection is happy to pick an external\nsegment.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2161",
        "summary": "Some concurrency improvements for NRT",
        "description": "Some concurrency improvements for NRT\n\nI found & fixed some silly thread bottlenecks that affect NRT:\n\n  * Multi/DirectoryReader.numDocs is synchronized, I think so only 1\n    thread computes numDocs if it's -1.  I removed this sync, and made\n    numDocs volatile, instead.  Yes, multiple threads may compute the\n    numDocs for the first time, but I think that's harmless?\n\n  * Fixed BitVector's ctor to set count to 0 on creating a new BV, and\n    clone to copy the count over; this saves CPU computing the count\n    unecessarily.\n\n  * Also strengthened assertions done in SR, testing the delete docs\n    count.\n\nI also found an annoying thread bottleneck that happens, due to CMS.\nWhenever CMS hits the max running merges (default changed from 3 to 1\nrecently), and the merge policy now wants to launch another merge, it\nforces the incoming thread to wait until one of the BG threads\nfinishes.\n\nThis is a basic crude throttling mechanism -- you force the mutators\n(whoever is causing new segments to appear) to stop, so that merging\ncan catch up.\n\nUnfortunately, when stressing NRT, that thread is the one that's\nopening a new NRT reader.\n\nSo, the first serious problem happens when you call .reopen() on your\nNRT reader -- this call simply forwards to IW.getReader if the reader\nwas an NRT reader.  But, because DirectoryReader.doReopen is\nsynchronized, this had the horrible effect of holding the monitor lock\non your main IR.  In my test, this blocked all searches (since each\nsearch uses incRef/decRef, still sync'd until LUCENE-2156, at least).\nI fixed this by making doReopen only sync'd on this if it's not simply\nforwarding to getWriter.  So that's a good step forward.\n\nThis prevents searches from being blocked while trying to reopen to a\nnew NRT.\n\nHowever... it doesn't fix the problem that when an immense merge is\noff and running, opening an NRT reader could hit a tremendous delay\nbecause CMS blocks it.  The BalancedSegmentMergePolicy should help\nhere... by avoiding such immense merges.\n\nBut, I think we should also pursue an improvement to CMS.  EG, if it\nhas 2 merges running, where one is huge and one is tiny, it ought to\nincrease thread priority of the tiny one.  I think with such a change\nwe could increase the max thread count again, to prevent this\nstarvation.  I'll open a separate issue....\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3269",
        "summary": "Speed up Top-K sampling tests",
        "description": "speed up the top-k sampling tests (but make sure they are thorough on nightly etc still)\n\nusually we would do this with use of atLeast(), but these tests are somewhat tricky,\nso maybe a different approach is needed.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1062",
        "summary": "Improved Payloads API",
        "description": "We want to make some optimizations to the Payloads API.\n\nSee following thread for related discussions:\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/54708",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2548",
        "summary": "Remove all interning of field names from flex API",
        "description": "In previous versions of Lucene, interning of fields was important to minimize string comparison cost when iterating TermEnums, to detect changes in field name. As we separated field names from terms in flex, no query compares field names anymore, so the whole performance problematic interning can be removed. I will start with doing this, but we need to carefully review some places e.g. in preflex codec.\n\nMaybe before this issue we should remove the Term class completely. :-) Robert?",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2275",
        "summary": "DocumentsWriter.applyDeletes should not create TermDocs or IndexSearcher if not needed",
        "description": "DocumentsWriter.applyDeletes(IndexReader, int) always creates TermDocs and IndexSearcher, even if there were no deletes by Term or by Query. The attached patch wraps those creations w/ checks on whether there were any deletes by these two. Additionally, the searcher wasn't closed in a finally block, so I fixed that as well.\n\nI'll attach a patch shortly.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2536",
        "summary": "Rollback doesn't preserve integrity of original index",
        "description": "After several \"updateDocuments\" calls a rollback call does not return the index to the prior state.\nThis seems to occur if the number of updates exceeds the RAM buffer size i.e. when some flushing of updates occurs.\n\nTest fails in Lucene 2.4, 2.9, 3.0.1 and 3.0.2\n\nJUnit to follow.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1760",
        "summary": "TokenStream API javadoc improvements",
        "description": "- Change or remove experimental warnings of new TokenStream API\n- Improve javadocs for deprecated Token constructors\n- javadocs for TeeSinkTokenStream.SinkFilter",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-376",
        "summary": "[PATCH] Ant macro for javadocs and javadocs-internal",
        "description": "This removes the duplication introduced in build.xml \nwhen the javadocs-internal build target was added. \n \nRegards, \nPaul Elschot",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1979",
        "summary": "Remove remaining deprecations from indexer package",
        "description": "",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2065",
        "summary": "Java 5 port phase II ",
        "description": "LUCENE-1257 addresses the public API changes ( generics , mainly ) and other j.u.c. package changes related to the API .  The changes are frozen and closed for 3.0 . This would be a placeholder JIRA for 3.0+ version to address the pending changes ( tests for generics etc.) and any other internal API changes as necessary. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2147",
        "summary": "Improve Spatial Utility like classes",
        "description": "- DistanceUnits can be improved by giving functionality to the enum, such as being able to convert between different units, and adding tests.  \n\n- GeoHashUtils can be improved through some code tidying, documentation, and tests.\n\n- SpatialConstants allows us to move all constants, such as the radii and circumferences of Earth, to a single consistent location that we can then use throughout the contrib.  This also allows us to improve the transparency of calculations done in the contrib, as users of the contrib can easily see the values being used.  Currently this issues does not migrate classes to use these constants, that will happen in issues related to the appropriate classes.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1073",
        "summary": "Add unit test showing how to do a \"live backup\" of an index",
        "description": "The question of how to backup an index comes up every so often on the\nlists.  Backing up and index is also clearly an important fundamental\nadmin task that many applications need to do for fault tolerance.\n\nIn the past you were forced to stop & block all changes to your index,\nperform the backup, and then resume changes.  But many applications\ncannot afford a potentially long pause in their indexing.\n\nWith the addition of DeletionPolicy (LUCENE-710), it's now possible to\ndo a \"live backup\", which means backup your index in the background\nwithout pausing ongoing changes to the index.  This\nSnapshotDeletionPolicy just has to mark the chosen commit point as not\ndeletable, until the backup finishes.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1180",
        "summary": "Syns2Index fails",
        "description": "Running Syns2Index fails with a\njava.lang.IllegalArgumentException: maxBufferedDocs must at least be 2 when enabled exception.\nat org.apache.lucene.index.IndexWriter.setMaxBufferedDocs(IndexWriter.java:883)\nat org.apache.lucene.wordnet.Syns2Index.index(Syns2Index.java:249)\nat org.apache.lucene.wordnet.Syns2Index.main(Syns2Index.java:208)\n\nThe code is here\n\t\t// blindly up these parameters for speed\n\t\twriter.setMergeFactor( writer.getMergeFactor() * 2);\n\t\twriter.setMaxBufferedDocs( writer.getMaxBufferedDocs() * 2);\n\nIt looks like getMaxBufferedDocs used to return 10, and now it returns -1, not sure when that started happening.\n\nMy suggestion would be to just remove these three lines.  Since speed has already improved vastly, there isn't a need to speed things up.\n\nTo run this, Syns2Index requires two args.  The first is the location of the wn_s.pl file, and the second is the directory to create the index in.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-301",
        "summary": "Index Writer constructor flags unclear - and annoying in certain cases",
        "description": "Wouldn't it make more sense if the constructor for the IndexWriter always\ncreated an index if it doesn't exist - and the boolean parameter should be\n\"clear\" (instead of \"create\")\n\nSo instead of this (from javadoc):\n\nIndexWriter\n\npublic IndexWriter(Directory d,\n                   Analyzer a,\n                   boolean create)\n            throws IOException\n\n    Constructs an IndexWriter for the index in d. Text will be analyzed with a.\nIf create is true, then a new, empty index will be created in d, replacing the\nindex already there, if any.\n\nParameters:\n    d - the index directory\n    a - the analyzer to use\n    create - true to create the index or overwrite the existing one; false to\nappend to the existing index \nThrows:\n    IOException - if the directory cannot be read/written to, or if it does not\nexist, and create is false\n\n\nWe would have this:\n\nIndexWriter\n\npublic IndexWriter(Directory d,\n                   Analyzer a,\n                   boolean clear)\n            throws IOException\n\n    Constructs an IndexWriter for the index in d. Text will be analyzed with a.\nIf clear is true, and a index exists at location d, then it will be erased, and\na new, empty index will be created in d.\n\nParameters:\n    d - the index directory\n    a - the analyzer to use\n    clear - true to overwrite the existing one; false to append to the existing\nindex \nThrows:\n    IOException - if the directory cannot be read/written to, or if it does not\nexist.\n\n\n\nIts current behavior is kind of annoying, because I have an app that should\nnever clear an existing index, it should always append.  So I want create set to\nfalse.  But when I am starting a brand new index, then I have to change the\ncreate flag to keep it from throwing an exception...  I guess for now I will\nhave to write code to check if a index actually has content yet, and if it\ndoesn't, change the flag on the fly.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3533",
        "summary": "Nuke SpanFilters and CachingSpanFilter (maybe move to sandbox)",
        "description": "SpanFilters are inefficient and OOM easily (they don't scale at all: Create large Lists of Objects for every match, also filtering deleted docs is a pain). Some talks with Grant on Eurocon and also the fact that caching of them is still broken in 3.x (but fixed on trunk) - I assume nobody uses them, so let's nuke them. They are also in wrong package, so standard statement: \"Die, SpanFilters, die!\"",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1400",
        "summary": "Add Apache RAT (Release Audit Tool) target to build.xml",
        "description": "\nApache RAT is a useful tool to check for common mistakes in our source code (eg missing copyright headers):\n\n    http://incubator.apache.org/rat/\n\nI'm just copying the patch Grant worked out for Solr (SOLR-762).  I plan to commit to 2.4 & 2.9.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-962",
        "summary": "I/O exception in DocsWriter add or updateDocument may not delete unreferenced files",
        "description": "If an I/O exception is thrown in DocumentsWriter#addDocument or #updateDocument, the stored fields files may not be cleaned up.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2286",
        "summary": "enable DefaultSimilarity.setDiscountOverlaps by default",
        "description": "I think we should enable setDiscountOverlaps in DefaultSimilarity by default.\n\nIf you are using synonyms or commongrams or a number of other 0-posInc-term-injecting methods, these currently screw up your length normalization.\nThese terms have a position increment of zero, so they shouldnt count towards the length of the document.\n\nI've done relevance tests with persian showing the difference is significant, and i think its a big trap to anyone using synonyms, etc: your relevance can actually get worse if you don't flip this boolean flag.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3249",
        "summary": "Move Solr's FunctionQuery impls to Queries Module",
        "description": "Now that we have the main interfaces in the Queries module, we can move the actual impls over.\n\nImpls that won't be moved are:\n\nfunction/distance/* (to be moved to a spatial module)\nfunction/FileFloatSource.java (depends on Solr's Schema, data directories and exposes a RequestHandler)",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-1168",
        "summary": "TermVectors index files can become corrupt when autoCommit=false",
        "description": "Spinoff from this thread:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/55951\n\nThere are actually 2 separate cases here, both only happening when\nautoCommit=false:\n\n  * First issue was caused by LUCENE-843 (sigh): if you add a bunch of\n    docs with no term vectors, such that 1 or more flushes happen;\n    then you add docs that do have term vectors, the tvx file will not\n    have enough entries (= corruption).\n\n  * Second issue was caused by bulk merging of term vectors\n    (LUCENE-1120 -- only in trunk) and bulk merging of stored fields\n    (LUCENE-1043, in 2.3), and only shows when autoCommit=false, and,\n    the bulk merging optimization runs.  In this case, the code that\n    reads the rawDocs tries to read too far in the tvx/fdx files (it's\n    not really index corruption but rather a bug in the rawDocs\n    reading).\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1298",
        "summary": "MoreLikeThis ignores custom similarity",
        "description": "MoreLikeThis only allows the use of the DefaultSimilarity.  Patch shortly",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3127",
        "summary": "pull CoreReaders out of SegmentReader",
        "description": "Similar to LUCENE-3117, I think we should pull the CoreReaders class out of SR,\nto make it easier to navigate the code.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-495",
        "summary": "Suggested Patches to MultiPhraseQuery and QueryTermExtractor (for use with HighLighter)",
        "description": "I encountered a problem with the Highlighter, where it was not recognizing MultiPhraseQuery.\nTo fix this, I developed the following two patches:\n\n=====================================================\n1. Addition to org.apache.lucene.search.MultiPhraseQuery:\n\nAdd the following method:\n\n/** Returns the set of terms in this phrase. */\npublic Term[] getTerms() {\n  ArrayList allTerms = new ArrayList();\n  Iterator iterator = termArrays.iterator();\n  while (iterator.hasNext()) {\n    Term[] terms = (Term[])iterator.next();\n    for (int i = 0, n = terms.length; i < n; ++i) {\n      allTerms.add(terms[i]);\n    }\n  }\n  return (Term[])allTerms.toArray(new Term[0]);\n}\n\n=====================================================\n2. Patch to org.apache.lucene.search.highlight.QueryTermExtractor:\n\na) Add the following import:\nimport org.apache.lucene.search.MultiPhraseQuery;\n\nb) Add the following code to the end of the getTerms(...) method:\n      else  if(query instanceof MultiPhraseQuery)\n              getTermsFromMultiPhraseQuery((MultiPhraseQuery) query, terms, fieldName);\n  }\n\nc) Add the following method:\n private static final void getTermsFromMultiPhraseQuery(MultiPhraseQuery query, HashSet terms, String fieldName)\n {\n   Term[] queryTerms = query.getTerms();\n   int i;\n\n   for (i = 0; i < queryTerms.length; i++)\n   {\n       if((fieldName==null)||(queryTerms[i].field()==fieldName))\n       {\n           terms.add(new WeightedTerm(query.getBoost(),queryTerms[i].text()));\n       }\n   }\n }\n\n\n=====================================================\n\nCan the team update the repository?\n\nThanks\nMichael Harhen ",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-975",
        "summary": "Position based TermVectorMapper",
        "description": "As part of the new TermVectorMapper approach to TermVectors, the ensuing patch loads term vectors and stores the term info by position.  This should let people directly index into a term vector given a position.  Actually, it does it through Maps, b/c the array based bookkeeping is a pain given the way positions are stored.  \n\nThe map looks like:\nMap<String,   Map<Integer, TVPositionInfo>>\n\nwhere the String is the field name, the integer is the position, and TVPositionInfo is a storage mechanism for the terms and offsets that occur at a position.  It _should_ handle multiple terms per position (which is always my downfall! )\n\nI have not tested performance of this approach.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2747",
        "summary": "Deprecate/remove language-specific tokenizers in favor of StandardTokenizer",
        "description": "As of Lucene 3.1, StandardTokenizer implements UAX#29 word boundary rules to provide language-neutral tokenization.  Lucene contains several language-specific tokenizers that should be replaced by UAX#29-based StandardTokenizer (deprecated in 3.1 and removed in 4.0).  The language-specific *analyzers*, by contrast, should remain, because they contain language-specific post-tokenization filters.  The language-specific analyzers should switch to StandardTokenizer in 3.1.\n\nSome usages of language-specific tokenizers will need additional work beyond just replacing the tokenizer in the language-specific analyzer.  \n\nFor example, PersianAnalyzer currently uses ArabicLetterTokenizer, and depends on the fact that this tokenizer breaks tokens on the ZWNJ character (zero-width non-joiner; U+200C), but in the UAX#29 word boundary rules, ZWNJ is not a word boundary.  Robert Muir has suggested using a char filter converting ZWNJ to spaces prior to StandardTokenizer in the converted PersianAnalyzer.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2588",
        "summary": "terms index should not store useless suffixes",
        "description": "This idea came up when discussing w/ Robert how to improve our terms index...\n\nThe terms dict index today simply grabs whatever term was at a 0 mod 128 index (by default).\n\nBut this is wasteful because you often don't need the suffix of the term at that point.\n\nEG if the 127th term is aa and the 128th (indexed) term is abcd123456789, instead of storing that full term you only need to store ab.  The suffix is useless, and uses up RAM since we load the terms index into RAM.\n\nThe patch is very simple.  The optimization is particularly easy because terms are now byte[] and we sort in binary order.\n\nI tested on first 10M 1KB Wikipedia docs, and this reduces the terms index (tii) file from 3.9 MB -> 3.3 MB = 16% smaller (using StandardAnalyzer, indexing body field tokenized but title / date fields untokenized).  I expect on noisier terms dicts, especially ones w/ bad terms accidentally indexed, that the savings will be even more.\n\nIn the future we could do crazier things.  EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say \"approximately\" every N, but favor terms that have a smaller net prefix.  We can also index more sparsely in regions where the net docFreq is lowish, since we can afford somewhat higher seek+scan time to these terms since enuming their docs will be much faster.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-783",
        "summary": "Store all metadata in human-readable segments file",
        "description": "Various index-reading components in Lucene need metadata in addition to data.\nThis metadata is presently stored in arbitrary binary headers and spread out\nover several files.  We should move to concentrate it in a single file, and \nthis file should be encoded using a human-readable, extensible, standardized \ndata serialization language -- either XML or YAML.\n\n* Making metadata human-readable makes debugging easier.  Centralizing it\n  makes debugging easier still.  Developers benefit from being able to scan\n  and locate relevant information quickly and with less debug printing.  Users\n  get a new window through which to peer into the index structure.\n* Since metadata is written to a separate file, there would no longer be a \n  need to seek back to the beginning of any data file to finish a header, \n  solving issue LUCENE-532.\n* Special-case parsing code needed for extracting metadata supplied by \n  different index formats can be pared down.  If a value is no longer \n  necessary, it can just be ignored/discarded.\n* Removing headers from the data files simplifies them and makes the file\n  format easier to implement. \n* With headers removed, all or nearly all data structures can take the\n  form of records stacked end to end, so that once a decoder has been\n  selected, an iterator can read the file from top to tail.  To an extent,\n  this allows us to separate our data-processing algorithms from our\n  serialization algorithms, decoupling Lucene's code base from its file\n  format.  For instance, instead of further subclassing TermDocs to deal with\n  \"flexible indexing\" formats, we might replace it with a PostingList which\n  returns a subclass of Posting.  The deserialization code would be wholly\n  contained within the Posting subclass rather than spread out over several\n  subclasses of TermDocs.\n* YAML and XML are equally well suited for the task of storing metadata, \n  but in either case a complete parser would not be needed -- a small subset \n  of the language will do.  KinoSearch 0.20's custom-coded YAML parser \n  occupies about 600 lines of C -- not too bad, considering how miserable C's \n  string handling capabilities are. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-862",
        "summary": "Contrib query org.apache.lucene.search.BoostingQuery sets boost on constructor Query, not cloned copy",
        "description": "BoostingQuery sets the boost value on the passed context Query\n\n    public BoostingQuery(Query match, Query context, float boost) {\n      this.match = match;\n      this.context = (Query)context.clone();        // clone before boost\n      this.boost = boost;\n\n      context.setBoost(0.0f);                      // ignore context-only matches\n    }\n\nThis should be \n      this.context.setBoost(0.0f);                      // ignore context-only matches\n\nAlso, boost value of 0.0 may have wrong effect - see discussion at\n\nhttp://www.mail-archive.com/java-user@lucene.apache.org/msg12243.html \n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3823",
        "summary": "Add a field-filtering FilterAtomicReader to 4.0 so ParallelReaders can be better tested (in LTC.maybeWrapReader)",
        "description": "In addition to the filters in contrib/misc for horizontally filtering (by doc-id) AtomicReader, it would be good to have the same vertically (by field). For now I will add this implementation to test-framework, as it cannot stay in contrib/misc, because LTC will need it for maybeWrapReader.\n\nLTC will use this FilterAtomicReader to construct a ParallelAtomicReader out of two (or maybe more) FieldFilterAtomicReaders.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2217",
        "summary": "Remaining reallocation should use ArrayUtil.getNextSize()",
        "description": "See recent discussion on ArrayUtils.getNextSize().",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3434",
        "summary": "Make ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper immutable",
        "description": "Both ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper have setters which change some state which impacts their analysis stack.  If these are going to become reusable, then the state must be immutable as changing it will have no effect.\n\nProcess will be similar to QueryAutoStopWordAnalyzer, I will remove in trunk and deprecate in 3x.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3802",
        "summary": "Grouping collector that computes grouped facet counts",
        "description": "Spinoff from issue SOLR-2898. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3258",
        "summary": "File leak when IOException occurs during index optimization.",
        "description": "I am not sure if this issue requires a fix due to the nature of its occurrence, or if it exists in other versions of Lucene.\n\nI am using Lucene Java 3.0.3 on a SUSE Linux machine with Java 6 and have noticed there are a number of file handles that are not being released from my java application. There are IOExceptions in my log regarding disk full, which causes a merge and the optimization to fail. The index is not currupt upon encountering the IOException. I am using CFS for my index format, so 3X my largest index size during optimization certainly consumes all of my available disk. \n\nI realize that I need to add more disk space to my machine, but I investigated how to clean up the leaking file handles. After failing to find a misuse of Lucene's IndexWriter in the code I have wrapping Lucene, I did a quick search for close() being invoked in the Lucene Jave source code. I found a number of source files that attempt to close more than one object within the same close() method. I think a try/catch should be put around each of these close() attempts to avoid skipping a subsequent closes. The catch may be able to ignore a caught exception to avoid masking the original exception like done in SimpleFSDirectory.close().\n\nLocations in Lucene Java source where I suggest a try/catch should be used:\n- org.apache.lucene.index.FormatPostingFieldsWriter.finish()\n- org.apache.lucene.index.TermInfosWriter.close()\n- org.apache.lucene.index.SegmentTermPositions.close()\n- org.apache.lucene.index.SegmentMergeInfo.close()\n- org.apache.lucene.index.SegmentMerger.mergeTerms() (The finally block)\n- org.apache.lucene.index.DirectoryReader.close()\n- org.apache.lucene.index.FieldsReader.close()\n- org.apache.lucene.index.MultiLevelSkipListReader.close()\n- org.apache.lucene.index.MultipleTermPositions.close()\n- org.apache.lucene.index.SegmentMergeQueue.close()\n- org.apache.lucene.index.SegmentMergeDocs.close()\n- org.apache.lucene.index.TermInfosReader.close()",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2830",
        "summary": "Use StringBuilder instead of StringBuffer in benchmark",
        "description": "Minor change - use StringBuilder instead of StringBuffer in benchmark's code. We don't need the synchronization of StringBuffer in all the places that I've checked.\n\nThe only place where it _could_ be a problem is in HtmlParser's API - one method accepts a StringBuffer and it's an interface. But I think it's ok to change benchmark's API, back-compat wise and so I'd like to either change it to accept a String, or remove the method altogether -- no code in benchmark uses it, and if anyone needs it, he can pass StringReader to the other method.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-702",
        "summary": "Disk full during addIndexes(Directory[]) can corrupt index",
        "description": "This is a spinoff of LUCENE-555\n\nIf the disk fills up during this call then the committed segments file can reference segments that were not written.  Then the whole index becomes unusable.\n\nDoes anyone know of any other cases where disk full could corrupt the index?\n\nI think disk full should worse lose the documents that were \"in flight\" at the time.  It shouldn't corrupt the index.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3735",
        "summary": "Fix PayloadProcessorProvider to no longer use Directory for lookup, instead AtomicReader",
        "description": "The PayloadProcessorProvider has a broken API, this should be fixed. The current trunk mimics the old behaviour, but not 100%.\n\nThe PayloadProcessorProvider API should return a PayloadProcessor based on the AtomicReader instance that gets merged. As AtomicReader do no longer know the directory they are reside (they could be e.g. FilterIndexReaders, MemoryIndexes,...) a selection by Directory is no longer possible.\n\nThe current code in Lucene trunk mimics the old behavior by doing an instanceof SegmentReader check and then asking for a DirProvider. If something else is merged in, Payload processing is not supported. This should be changed, the old API could be kept backwards compatible by moving the instanceof check in a \"convenience class\" DirPayloadProcessorProvider, extending PayloadProcessorProvider.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": ""
    },
    {
        "key": "LUCENE-3487",
        "summary": "TestBooleanMinShouldMatch test failure",
        "description": "ant test -Dtestcase=TestBooleanMinShouldMatch -Dtestmethod=testRandomQueries -Dtests.seed=505d62a62e9f90d0:-60daa428161b404b:-406411290a98f416\n\nI think its an absolute/relative epsilon issue",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-938",
        "summary": "I/O exceptions can cause loss of buffered deletes",
        "description": "Some I/O exceptions that result in segmentInfos rollback operations can cause buffered deletes that existed before the rollback creation point to be incorrectly lost when the IOException triggers a rollback.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-335",
        "summary": "Testcase for StandardAnalyzer",
        "description": "As per our discussion on lucene-user, I'm attaching a unit test for \nStandardAnalyzer.  I wrote most of the tests from reading the comments in the \nStandardTokenizer.jj grammar.  Someone familiar with the grammar (and its \nintent) should review the tests.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3097",
        "summary": "Post grouping faceting",
        "description": "This issues focuses on implementing post grouping faceting.\n* How to handle multivalued fields. What field value to show with the facet.\n* Where the facet counts should be based on\n** Facet counts can be based on the normal documents. Ungrouped counts. \n** Facet counts can be based on the groups. Grouped counts.\n** Facet counts can be based on the combination of group value and facet value. Matrix counts.   \n\nAnd properly more implementation options.\n\nThe first two methods are implemented in the SOLR-236 patch. For the first option it calculates a DocSet based on the individual documents from the query result. For the second option it calculates a DocSet for all the most relevant documents of a group. Once the DocSet is computed the FacetComponent and StatsComponent use one the DocSet to create facets and statistics.  \n\nThis last one is a bit more complex. I think it is best explained with an example. Lets say we search on travel offers:\n|||hotel||departure_airport||duration||\n|Hotel a|AMS|5\n|Hotel a|DUS|10\n|Hotel b|AMS|5\n|Hotel b|AMS|10\n\nIf we group by hotel and have a facet for airport. Most end users expect (according to my experience off course) the following airport facet:\nAMS: 2\nDUS: 1\n\nThe above result can't be achieved by the first two methods. You either get counts AMS:3 and DUS:1 or 1 for both airports.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1858",
        "summary": "Update site level documentation",
        "description": "ugg - a fun one - my brain is sliding to the bottom of my skull from excitement.\n\nMust update all of the site level pages to current API usage.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-909",
        "summary": "Demo targets for running the demo",
        "description": "Now that the demo build targets are working and build the jar/war, it may be useful for users to also be able to run the demo with something like 'ant run-demo'. This complements existing docs/demo.html.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    }
]