[
  {
        "key": "LUCENE-1821",
        "summary": "Weight.scorer() not passed doc offset for \"sub reader\"",
        "description": "Now that searching is done on a per segment basis, there is no way for a Scorer to know the \"actual\" doc id for the document's it matches (only the relative doc offset into the segment)\n\nIf using caches in your scorer that are based on the \"entire\" index (all segments), there is now no way to index into them properly from inside a Scorer because the scorer is not passed the needed offset to calculate the \"real\" docid\n\nsuggest having Weight.scorer() method also take a integer for the doc offset\n\nAbstract Weight class should have a constructor that takes this offset as well as a method to get the offset\nAll Weights that have \"sub\" weights must pass this offset down to created \"sub\" weights\n\n\nDetails on workaround:\nIn order to work around this, you must do the following:\n* Subclass IndexSearcher\n* Add \"int getIndexReaderBase(IndexReader)\" method to your subclass\n* during Weight creation, the Weight must hold onto a reference to the passed in Searcher (casted to your sub class)\n* during Scorer creation, the Scorer must be passed the result of YourSearcher.getIndexReaderBase(reader)\n* Scorer can now rebase any collected docids using this offset\n\nExample implementation of getIndexReaderBase():\n{code}\n// NOTE: more efficient implementation can be done if you cache the result if gatherSubReaders in your constructor\npublic int getIndexReaderBase(IndexReader reader) {\n  if (reader == getReader()) {\n    return 0;\n  } else {\n    List readers = new ArrayList();\n    gatherSubReaders(readers);\n    Iterator iter = readers.iterator();\n    int maxDoc = 0;\n    while (iter.hasNext()) {\n      IndexReader r = (IndexReader)iter.next();\n      if (r == reader) {\n        return maxDoc;\n      } \n      maxDoc += r.maxDoc();\n    } \n  }\n  return -1; // reader not in searcher\n}\n{code}\n\nNotes:\n* This workaround makes it so you cannot serialize your custom Weight implementation\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1871",
        "summary": "Highlighter wraps caching token filters that are not CachingTokenFilter in CachingTokenFilter",
        "description": "I figured this was fine and a rare case that you would have another caching tokenstream to feed the highlighter with - but I guess if its happening to you, especially depending on what you are doing - its not an ideal situation.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1127",
        "summary": "TokenSources.getTokenStream(Document...) ",
        "description": "Sometimes, one already has the Document, and just needs to generate a TokenStream from it, so I am going to add a convenience method to TokenSources.  Sometimes, you also already have just the string, so I will add a convenience method for that.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1069",
        "summary": "CheckIndex incorrectly sees deletes as index corruption",
        "description": "There is a silly bug in CheckIndex whereby any segment with deletes is\nconsidered corrupt.\n\nThanks to Bogdan Ghidireac for reporting this.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3334",
        "summary": "IOUtils.closeSafely should log suppressed Exceptions in stack trace of original Exception (a new feature of Java 7)",
        "description": "I was always against Java 6 support, as it brings no really helpful new features into Lucene. But there are several things that make life easier in coming Java 7 (hopefully on July 28th, 2011). One of those is simplier Exception handling and suppression on Closeable, called \"Try-With-Resources\" (see http://docs.google.com/View?id=ddv8ts74_3fs7483dp, by the way all Lucene classes support these semantics in Java 7 automatically, the cool try-code below would work e.g. for IndexWriter, TokenStreams,...).\n\nWe already have this functionality in Lucene since adding the IOUtils.closeSafely() utility (which can be removed when Java 7 is the minimum requirement of Lucene - maybe in 10 years):\n\n{code:java}\ntry (Closeable a = new ...; Closeable b = new ...) {\n  ... use Closeables ...\n} catch (Exception e) {\n  dosomething;\n  throw e;\n}\n{code}\n\nThis code will close a and b in an autogenerated finally block and supress any exception. This is identical to our IOUtils.closeSafely:\n\n{code:java}\nException priorException = null;\nCloseable a,b;\ntry (Closeable a = new ...; Closeable b = new ...) {\n  a = new ...;\n  b = new ...\n  ... use Closeables ...\n} catch (Exception e) {\n  priorException = e;\n  dosomething;\n} finally {\n  IOUtils.closeSafely(priorException, a, b);\n}\n{code}\n\nSo this means we have the same functionality without Java 7, but there is one thing that makes logging/debugging much nicer:\nThe above Java 7 code also adds maybe suppressed Exceptions in those Closeables to the priorException, so when you print the stacktrace, it not only shows the stacktrace of the original Exception, it also prints all Exceptions that were suppressed to throw this Exception (all Closeable.close() failures):\n\n{noformat}\norg.apache.lucene.util.TestIOUtils$TestException: BASE-EXCEPTION\n    at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:61)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:601)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1486)\n    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1404)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)\n    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)\n    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)\n    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-1\n            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)\n            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)\n            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)\n            ... 26 more\n    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-2\n            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)\n            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)\n            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)\n            ... 26 more\n{noformat}\n\nFor this in Java 7 a new method was added to Throwable, that allows logging such suppressed Exceptions (it is called automatically by the synthetic bytecode emitted by javac). This patch simply adds this functionality conditionally to IOUtils, so it \"registers\" all suppressed Exceptions, if running on Java 7. This is done by reflection: once it looks for this method in Throwable.class and if found, it invokes it in closeSafely, so the exceptions thrown on Closeable.close() don't get lost.\n\nThis makes debugging much easier and logs all problems that may occur.\n\nThis patch does *not* change functionality or behaviour, it just adds more nformation to the stack trace in a Java-7-way (similar to the way how Java 1.4 added causes). It works here locally on Java 6 and Java 7, but only Java 7 gets the additional stack traces. For Java 6 nothing changes. Same for Java 5 (if we backport to 3.x).\n\nThis would be our first Java 7 improvement (a minor one). Next would be NIO2... - but thats not easy to do with reflection only, so we have to wait 10 years :-)",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3215",
        "summary": "SloppyPhraseScorer sometimes computes Infinite freq",
        "description": "reported on user list:\nhttp://www.lucidimagination.com/search/document/400cbc528ed63db9/score_of_infinity_on_dismax_query\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-306",
        "summary": "[PATCH]multiple wildcards ? at the end of search pattern return incorrect hits",
        "description": "The problem is if you search on \"ca??\", the hit includes 'cat', 'CA', \netc, while the user only wants 4 letter words start with CA, such as \n'card', 'cash', to be returned. This happens only when multiple '?' at \nthe end of search pattern. The solution is to check if the word that is \nmatching against search pattern ends while there is still '?' left. If \nthis is the case, match should return false.\n\nAttached is the patch code I generated use 'diff'\n********************************************************************\n\n--- WildcardTermEnum.org\t2004-05-11 11:42:10.000000000 -0400\n+++ WildcardTermEnum.java\t2004-11-08 14:35:14.823610500 -0500\n@@ -132,6 +132,10 @@\n             }\n             else\n             {\n+\t      //to prevent \"cat\" matches \"ca??\"\n+\t      if(wildchar == WILDCARD_CHAR){\n+\t\treturn false;\n+\t      }\t      \n               // Look at the next character\n               wildcardSearchPos++;\n             }\n**********************************************************************",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3518",
        "summary": "Add sort-by-term with DocValues",
        "description": "There are two sorted byte[] types with DocValues (BYTES_VAR_SORTED,\nBYTES_FIXED_SORTED), so you can index this type, but you can't yet\nsort by it.\n\nSo I added a FieldComparator just like TermOrdValComparator, except it\npulls from the doc values instead.\n\nThere are some small diffs, eg with doc values there are never null\nvalues (see LUCENE-3504).\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1655",
        "summary": "remove 1.5 only unit test code that snuck in",
        "description": "I just tried to run unit tests w/ Java 1.4.2, but hit this:\n\n{code}\ncommon.compile-test:\n    [mkdir] Created dir: /lucene/src/diagnostics.1654/build/classes/test\n    [javac] Compiling 191 source files to /lucene/src/diagnostics.1654/build/classes/test\n    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:26: package java.util.concurrent.atomic does not exist\n    [javac] import java.util.concurrent.atomic.AtomicInteger;\n    [javac]                                    ^\n    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:262: cannot resolve symbol\n    [javac] symbol  : class AtomicInteger \n    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.DeleteThreads\n    [javac]     AtomicInteger delCount = new AtomicInteger();\n    [javac]     ^\n    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:332: cannot resolve symbol\n    [javac] symbol  : class AtomicInteger \n    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads\n    [javac]     AtomicInteger count = new AtomicInteger(0);\n    [javac]     ^\n    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:333: cannot resolve symbol\n    [javac] symbol  : class AtomicInteger \n    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads\n    [javac]     AtomicInteger numAddIndexesNoOptimize = new AtomicInteger(0);\n    [javac]     ^\n    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:262: cannot resolve symbol\n    [javac] symbol  : class AtomicInteger \n    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.DeleteThreads\n    [javac]     AtomicInteger delCount = new AtomicInteger();\n    [javac]                                  ^\n    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:332: cannot resolve symbol\n    [javac] symbol  : class AtomicInteger \n    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads\n    [javac]     AtomicInteger count = new AtomicInteger(0);\n    [javac]                               ^\n    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:333: cannot resolve symbol\n    [javac] symbol  : class AtomicInteger \n    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads\n    [javac]     AtomicInteger numAddIndexesNoOptimize = new AtomicInteger(0);\n    [javac]                                                 ^\n    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/search/TestSort.java:932: cannot resolve symbol\n    [javac] symbol  : method getSimpleName ()\n    [javac] location: class java.lang.Class\n    [javac]           assertEquals(actualTFCClasses[j], tdc.getClass().getSimpleName());\n    [javac]                                                         ^\n    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/search/TestTopScoreDocCollector.java:70: cannot resolve symbol\n    [javac] symbol  : method getSimpleName ()\n    [javac] location: class java.lang.Class\n    [javac]         assertEquals(actualTSDCClass[i], tdc.getClass().getSimpleName());\n    [javac]                                                      ^\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -deprecation for details.\n    [javac] 9 errors\n{code}",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1420",
        "summary": "Similarity.lengthNorm and positionIncrement=0",
        "description": "Calculation of lengthNorm factor should in some cases take into account the number of tokens with positionIncrement=0. This should be made optional, to support two different scenarios:\n\n* when analyzers insert artificially constructed tokens into TokenStream (e.g. ASCII-fied versions of accented terms, stemmed terms), and it's unlikely that users submit queries containing both versions of tokens: in this case lengthNorm calculation should ignore the tokens with positionIncrement=0.\n\n* when analyzers insert synonyms, and it's likely that users may submit queries that contain multiple synonymous terms: in this case the lengthNorm should be calculated as it is now, i.e. it should take into account all terms no matter what is their positionIncrement.\n\nThe default should be backward-compatible, i.e. it should count all tokens.\n\n(See also the discussion here: http://markmail.org/message/vfvmzrzhr6pya22h )",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-678",
        "summary": "[PATCH] LockFactory implementation based on OS native locks (java.nio.*)",
        "description": "The current default locking for FSDirectory is SimpleFSLockFactory.\nIt uses java.io.File.createNewFile for its locking, which has this\nspooky warning in Sun's javadocs:\n\n    Note: this method should not be used for file-locking, as the\n    resulting protocol cannot be made to work reliably. The FileLock\n    facility should be used instead.\n\nSo, this patch provides a LockFactory implementation based on FileLock\n(using java.nio.*).\n\nAll unit tests pass with this patch, on OS X (10.4.8), Linux (Ubuntu\n6.06), and Windows XP SP2.\n\nAnother benefit of native locks is the OS automatically frees them if\nthe JVM exits before Lucene can free its locks.  Many people seem to\nhit this (old lock files still on disk) now.\n\nI've created this new class:\n\n  org.apache.lucene.store.NativeFSLockFactory\n\nand added a couple test cases to the existing TestLockFactory.\n\nI've left SimpleFSLockFactory as the default locking for FSDirectory\nfor now.  I think we should get some usage / experience with\nNativeFSLockFactory and then later on make it the default locking\nimplementation?\n\nI also tested changing FSDirectory's default locking to\nNativeFSLockFactory and all unit tests still pass (on the above\nplatforms).\n\nOne important note about locking over NFS: some NFS servers and/or\nclients do not support it, or, it's a configuration option or mode\nthat must be explicitly enabled.  When it's misconfigured it's able to\ntake a long time (35 seconds in my case) before throwing an exception.\nTo handle this, I acquire & release a random test lock on creating the\nNativeFSLockFactory to verify locking is configured properly.\n\nA few other small changes in the patch:\n\n    - Added a \"failure reason\" to Lock.java so that in\n      obtain(lockWaitTimeout), if there is a persistent IOException\n      in trying to obtain the lock, this can be messaged & included in\n      the \"Lock obtain timed out\" that's raised.\n\n    - Corrected javadoc in SimpleFSLockFactory: it previously said the\n      wrong system property for overriding lock class via system\n      properties\n\n    - Fixed unhandled IOException when opening an IndexWriter for\n      create, if the locks dir does not exist (just added\n      lockDir.exists() check in clearAllLocks method of\n      SimpleFSLockFactory & NativeFSLockFactory.\n\n    - Fixed a few small unrelated issues with TestLockFactory, and\n      also fixed tests to accept NativeFSLockFactory as the default\n      locking implementation for FSDirectory.\n\n    - Fixed a typo in javadoc in FieldsReader.java\n\n    - Added some more javadoc for the LockFactory.setLockPrefix\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2195",
        "summary": "Speedup CharArraySet if set is empty",
        "description": "CharArraySet#contains(...) always creates a HashCode of the String, Char[] or CharSequence even if the set is empty. \ncontains should return false if set it empty",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1894",
        "summary": "Spatial checks for a string in an int,double map",
        "description": "{code}\n  private Map<Integer,Double> distances;\n{code}\n\n{code}\n    if (precise != null) {\n      double xLat = getPrecision(lat, precise);\n      double xLng = getPrecision(lng, precise);\n      \n      String k = new Double(xLat).toString() +\",\"+ new Double(xLng).toString();\n    \n      Double d = (distances.get(k));\n      if (d != null){\n        return d.doubleValue();\n      }\n    }\n{code}\n\nSomething is off here eh?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-738",
        "summary": "read/write .del as d-gaps when the deleted bit vector is sufficiently sparse",
        "description": ".del file of a segment maintains info on deleted documents in that segment. The file exists only for segments having deleted docs, so it does not exists for newly created segments (e.g. resulted from merge). Each time closing an index reader that deleted any document, the .del file is rewritten. In fact, since the lock-less commits change a new (generation of) .del file is created in each such occasion.\n\nFor small indexes there is no real problem with current situation. But for very large indexes, each time such an index reader is closed, creating such new bit-vector seems like unnecessary overhead in cases that the bit vector is sparse (just a few docs were deleted). For instance, for an index with a segment of 1M docs, the sequence: {open reader; delete 1 doc from that segment; close reader;} would write a file of ~128KB. Repeat this sequence 8 times: 8 new files of total size of 1MB are written to disk.\n\nWhether this is a bottleneck or not depends on the application deletes pattern, but for the case that deleted docs are sparse, writing just the d-gaps would save space and time. \n\nI have this (simple) change to BitVector running and currently trying some performance tests to, yet, convince myself on the worthiness of this.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-386",
        "summary": "rev. 169301: wrong directory name in build.xml",
        "description": "build.xml mentions non-existing directory contrib/WordNet/ which should read\ncontrib/wordnet in line 418.\n\nbelow the result of svn diff against the corrected and working version of build.xml\n\n--- build.xml   (revision 169301)\n+++ build.xml   (working copy)\n@@ -415,7 +415,7 @@\n         <!-- TODO: find a dynamic way to do include multiple source roots -->\n         <packageset dir=\"src/java\"/>\n         <packageset dir=\"contrib/analyzers/src/java\"/>\n-        <packageset dir=\"contrib/WordNet/src/java\"/>\n+        <packageset dir=\"contrib/wordnet/src/java\"/>\n         <packageset dir=\"contrib/highlighter/src/java\"/>\n         <packageset dir=\"contrib/similarity/src/java\"/>\n         <packageset dir=\"contrib/spellchecker/src/java\"/>",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3253",
        "summary": "TestIndexwriterWithThreads#testCloseWithThreads hangs if a thread hit an exception before indexing its first document",
        "description": "in TestIndexwriterWithThreads#testCloseWithThreads we loop until all threads have indexed a single document but if one or more threads fail on before they index the first doc the test hangs forever. We should check if the thread is still alive unless it has indexed a document and fail if it already died.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2299",
        "summary": "if you open an NRT reader while addIndexes* is running it may miss segments",
        "description": "Earwin spotted this in pending ongoing refactoring of Dir/MultiReader, but I wanted to open this separately just to make sure we fix it for 3.1...\n\nThis is the fix:\n{code}\nIndex: src/java/org/apache/lucene/index/DirectoryReader.java\n===================================================================\n--- src/java/org/apache/lucene/index/DirectoryReader.java\t(revision 919119)\n+++ src/java/org/apache/lucene/index/DirectoryReader.java\t(working copy)\n@@ -145,7 +145,7 @@\n     for (int i=0;i<numSegments;i++) {\n       boolean success = false;\n       try {\n-        final SegmentInfo info = infos.info(upto);\n+        final SegmentInfo info = infos.info(i);\n         if (info.dir == dir) {\n           readers[upto++] = writer.readerPool.getReadOnlyClone(info, true, termInfosIndexDivisor);\n         }\n{code}\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1355",
        "summary": "Using the WeightedTerms option in the Highlighter can cause fragments to be supressed for indexes with deletes",
        "description": "An index with a few documents and many deletes can report a lower total docs than docFreq for a term - total docs will account for deletes while docFreq will not - this causes the idf to be negative and the fragment to score < 0.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2820",
        "summary": "CMS fails to cleanly stop threads",
        "description": "When you close IW, it waits for (or aborts and then waits for) all running merges.\n\nHowever, it's wait criteria is wrong -- it waits for the threads to be done w/ their merges, not for the threads to actually die.\n\nCMS already has a sync() method, to wait for running threads, which we can call from CMS.close.  However it has a thread hazard because a MergeThread removes itself from mergeThreads before it actually exits.  So sync() is able to return even while a merge thread is still running.\n\nThis was uncovered by LUCENE-2819 on the test case TestCustomScoreQuery.testCustomExternalQuery, though I expect other test cases would show it.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3573",
        "summary": "TaxonomyReader.refresh() is broken, replace its logic with reopen(), following IR.reopen pattern",
        "description": "When recreating the taxonomy index, TR's assumption that categories are only added does not hold anymore.\nAs result, calling TR.refresh() will be incorrect at best, but usually throw an AIOOBE.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1990",
        "summary": "Add unsigned packed int impls in oal.util",
        "description": "There are various places in Lucene that could take advantage of an\nefficient packed unsigned int/long impl.  EG the terms dict index in\nthe standard codec in LUCENE-1458 could subsantially reduce it's RAM\nusage.  FieldCache.StringIndex could as well.  And I think \"load into\nRAM\" codecs like the one in TestExternalCodecs could use this too.\n\nI'm picturing something very basic like:\n{code}\ninterface PackedUnsignedLongs  {\n  long get(long index);\n  void set(long index, long value);\n}\n{code}\n\nPlus maybe an iterator for getting and maybe also for setting.  If it\nhelps, most of the usages of this inside Lucene will be \"write once\"\nso eg the set could make that an assumption/requirement.\n\nAnd a factory somewhere:\n\n{code}\n  PackedUnsignedLongs create(int count, long maxValue);\n{code}\n\nI think we should simply autogen the code (we can start from the\nautogen code in LUCENE-1410), or, if there is an good existing impl\nthat has a compatible license that'd be great.\n\nI don't have time near-term to do this... so if anyone has the itch,\nplease jump!\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2198",
        "summary": "support protected words in Stemming TokenFilters",
        "description": "This is from LUCENE-1515\n\nI propose that all stemming TokenFilters have an 'exclusion set' that bypasses any stemming for words in this set.\nSome stemming tokenfilters have this, some do not.\n\nThis would be one way for Karl to implement his new swedish stemmer (as a text file of ignore words).\nAdditionally, it would remove duplication between lucene and solr, as they reimplement snowballfilter since it does not have this functionality.\nFinally, I think this is a pretty common use case, where people want to ignore things like proper nouns in the stemming.\n\nAs an alternative design I considered a case where we generalized this to CharArrayMap (and ignoring words would mean mapping them to themselves), which would also provide a mechanism to override the stemming algorithm. But I think this is too expert, could be its own filter, and the only example of this i can find is in the Dutch stemmer.\n\nSo I think we should just provide ignore with CharArraySet, but if you feel otherwise please comment.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1201",
        "summary": "Add getIndexCommit method to IndexReader",
        "description": "Spinoff from this thread:\n\n  http://markmail.org/message/bojgqfgyxkkv4fyb\n\nI think it makes sense ask an IndexReader for the commit point it has\nopen.  This enables the use case described in the above thread, which\nis to create a deletion policy that is able to query all open readers\nfor what commit points they are using, and prevent deletion of them.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-479",
        "summary": "MultiReader.numDocs incorrect after undeleteAll",
        "description": "Calling MultiReader.undeleteAll does not clear cached numDocs value. So the subsequent numDocs() call returns a wrong value if there were deleted documents in the index. Following patch fixes the bug and adds a test showing the issue.\n\n\nIndex: src/test/org/apache/lucene/index/TestMultiReader.java\n===================================================================\n--- src/test/org/apache/lucene/index/TestMultiReader.java       (revision 354923)\n+++ src/test/org/apache/lucene/index/TestMultiReader.java       (working copy)\n@@ -69,6 +69,18 @@\n     assertTrue(vector != null);\n     TestSegmentReader.checkNorms(reader);\n   }\n+\n+  public void testUndeleteAll() throws IOException {\n+    sis.read(dir);\n+    MultiReader reader = new MultiReader(dir, sis, false, readers);\n+    assertTrue(reader != null);\n+    assertEquals( 2, reader.numDocs() );\n+    reader.delete(0);\n+    assertEquals( 1, reader.numDocs() );\n+    reader.undeleteAll();\n+    assertEquals( 2, reader.numDocs() );\n+  }\n+\n\n   public void testTermVectors() {\n     MultiReader reader = new MultiReader(dir, sis, false, readers);\nIndex: src/java/org/apache/lucene/index/MultiReader.java\n===================================================================\n--- src/java/org/apache/lucene/index/MultiReader.java   (revision 354923)\n+++ src/java/org/apache/lucene/index/MultiReader.java   (working copy)\n@@ -122,6 +122,7 @@\n     for (int i = 0; i < subReaders.length; i++)\n       subReaders[i].undeleteAll();\n     hasDeletions = false;\n+    numDocs = -1;      // invalidate cache\n   }\n\n   private int readerIndex(int n) {    // find reader for doc n:",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1052",
        "summary": "Add an \"termInfosIndexDivisor\" to IndexReader",
        "description": "The termIndexInterval, set during indexing time, let's you tradeoff\nhow much RAM is used by a reader to load the indexed terms vs cost of\nseeking to the specific term you want to load.\n\nBut the downside is you must set it at indexing time.\n\nThis issue adds an indexDivisor to TermInfosReader so that on opening\na reader you could further sub-sample the the termIndexInterval to use\nless RAM.  EG a setting of 2 means every 2 * termIndexInterval is\nloaded into RAM.\n\nThis is particularly useful if your index has a great many terms (eg\nyou accidentally indexed binary terms).\n\nSpinoff from this thread:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/54371\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3274",
        "summary": "Collapse Common module into Lucene core util",
        "description": "It was suggested by Robert in [http://markmail.org/message/wbfuzfamtn2qdvii] that we should try to limit the dependency graph between modules and where there is something 'common' it should probably go into lucene core.  Given that I haven't added anything to this module except the MutableValue classes, I'm going to collapse them into the util package, remove the module, and correct the dependencies.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3437",
        "summary": "Detect the test thread by reference, not by name.",
        "description": "Get rid of this:\n{code}\n      if (doFail && (Thread.currentThread().getName().equals(\"main\") \n          || Thread.currentThread().getName().equals(\"Main Thread\"))) {\n{code}",
        "label": "NUG",
        "classified": "TEST",
        "type": ""
    },
    {
        "key": "LUCENE-1882",
        "summary": "move SmartChineseAnalyzer into the smartcn package",
        "description": "an offshoot of LUCENE-1862, org.apache.lucene.analysis.cn.SmartChineseAnalyzer should become org.apache.lucene.analysis.cn.smartcn.SmartChineseAnalyzer",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1367",
        "summary": "Add a isDeleted method to IndexCommit",
        "description": "I wish to add a IndexCommit.isDeleted() method.\n\nThe use-case is that Solr will now support configurable IndexDeletionPolicy (SOLR-617). For the new replication (SOLR-561) to work, we need access to a list of IndexCommit instances which haven't been deleted yet. I can wrap the user specified IndexDeletionPolicy but since the IndexCommit does not have a isDeleted method, I may store a reference to an IndexCommit on which delete() has been called by the deletion policy. I can wrap the IndexCommit objects too just for having a isDeleted() method so a workaround exists. Not a big pain but if it can be managed on the lucene side easily, I'll appreciate it. It would save me from writing some delegate code.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1042",
        "summary": "discrepancy in getTermFreqVector-methods ",
        "description": "getTermFreqVector(int, TermVectorMapper) never calls the mapper if there is no term vector, consitent with all the other getTermFreqVector methods that returns null. \n\ngetTermFreqVector(int, String, TermVectorMapper) throws an IOException when a field does not contain the term vector.\n\nMy suggestion:\n\n{code}\nIndex: src/java/org/apache/lucene/index/SegmentReader.java\n===================================================================\n--- src/java/org/apache/lucene/index/SegmentReader.java (revision 590149)\n+++ src/java/org/apache/lucene/index/SegmentReader.java (working copy)\n@@ -648,7 +648,7 @@\n     ensureOpen();\n     FieldInfo fi = fieldInfos.fieldInfo(field);\n     if (fi == null || !fi.storeTermVector || termVectorsReaderOrig == null)\n-      throw new IOException(\"field does not contain term vectors\");\n+      return; \n{code}",
        "label": "NUG",
        "classified": "SPEC",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1393",
        "summary": "Lucene's nightly Hudson builds don't have svn version in MANIFEST.MF",
        "description": "Solr had the same issue but apparently made a configuration change to the Hudson configuration to get it working:\n\n    https://issues.apache.org/jira/browse/SOLR-684\n\nAlso I opened this INFRA issue:\n\n    https://issues.apache.org/jira/browse/INFRA-1721\n\nwhich says the svnversion exe is located in this path:\n\n    /opt/subversion-current/bin\n\nIn that INRA issue, /etc/init.d/tomcat was also fixed in theory so that svnversion would be on the path the next time Hudson is restarted.  Still, in case that doesn't work, or it changes in the future, it seems a good idea to make the same change that Solr made to Lucene's Hudson configuration.\n\nHoss can you detail what you needed to do for Solr?  Or maybe just do it also for Lucene ;)  Thanks!",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1154",
        "summary": "System Reqs page should be release specific",
        "description": "The System Requirements page, currently under the Main->Resources section of the website should be part of a given version's documentation, since it will be changing for a given release.  \n\nI will \"deprecate\" the existing one, but leave it in place(with a message) to cover the existing releases that don't have this, but will also add it to the release docs for future releases.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1500",
        "summary": "Highlighter throws StringIndexOutOfBoundsException",
        "description": "Using the canonical Solr example (ant run-example) I added this document (using exampledocs/post.sh):\n\n<add><doc>\n  <field name=\"id\">Test for Highlighting StringIndexOutOfBoundsExcdption</field>\n  <field name=\"name\">Some Name</field>\n  <field name=\"manu\">Acme, Inc.</field>\n  <field name=\"features\">Description of the features, mentioning various things</field>\n  <field name=\"features\">Features also is multivalued</field>\n  <field name=\"popularity\">6</field>\n  <field name=\"inStock\">true</field>\n</doc></add>\n\nand then the URL http://localhost:8983/solr/select/?q=features&hl=true&hl.fl=features caused the exception.\n\nI have a patch.  I don't know if it is completely correct, but it avoids this exception.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3294",
        "summary": "Some code still compares string equality instead using equals",
        "description": "I found a couple of places where we still use string == otherstring which don't look correct. I will attache a patch soon.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1107",
        "summary": "Clean up old JIRA issues in component \"Other\"",
        "description": "A list of all JIRA issues in component \"Other\" that haven't been updated in 2007:\n\n   *\t LUCENE-746  \t Incorrect error message in AnalyzingQueryParser.getPrefixQuery   \n   *\tLUCENE-644 \tContrib: another highlighter approach \n   *\tLUCENE-574 \tsupport for vjc java compiler, also known as J# \n   *\tLUCENE-471 \tgcj ant target doesn't work on windows \n   *\tLUCENE-434 \tLucene database bindings \n   *\tLUCENE-254 \t[PATCH] pseudo-relevance feedback enhancement \n   *\tLUCENE-180 \t[PATCH] Language guesser contribution \n",
        "label": "NUG",
        "classified": "OTHER",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1950",
        "summary": "Remove autoCommit from IndexWriter",
        "description": "IndexWriter's autoCommit is deprecated; in 3.0 it will be hardwired to false.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-347",
        "summary": "[PATCH] Javadoc correction for Scorer.java",
        "description": " ",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2402",
        "summary": "Add an explicit method to invoke IndexDeletionPolicy",
        "description": "Today, if one uses an IDP which holds onto segments, such as SnapshotDeletionPolicy, or any other IDP in the tests, those segments are left in the index even if the IDP no longer references them, until IW.commit() is called (and actually does something). I'd like to add a specific method to IW which will invoke the IDP's logic and get rid of the unused segments w/o forcing the user to call IW.commit(). There are a couple of reasons for that:\n\n* Segments take up sometimes valuable HD space, and the application may wish to reclaim that space immediately. In some scenarios, the index is updated once in several hours (or even days), and waiting until then may not be acceptable.\n* I think it's a cleaner solution than waiting for the next commit() to happen. One can still wait for it if one wants, but otherwise it will give you the ability to immediately get rid of those segments.\n* TestSnapshotDeletionPolicy includes this code, which only strengthens (IMO) the need for such method:\n{code}\n// Add one more document to force writer to commit a\n// final segment, so deletion policy has a chance to\n// delete again:\nDocument doc = new Document();\ndoc.add(new Field(\"content\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\nwriter.addDocument(doc);\n{code}\n\nIf IW had an explicit method, that code would not need to exist there at all ...\n\nHere comes the fun part - naming the baby:\n* invokeDeletionPolicy -- describes exactly what is going to happen. However, if the user did not set IDP at all (relying on default, which I think many do), users won't understand what is it.\n* deleteUnusedSegments - more user-friendly, assuming users understand what 'segments' are.\n\nBTW, IW already has deleteUnusedFiles() which only tries to delete unreferenced files that failed to delete before (such as on Windows, due to e.g. open readers). Perhaps instead of inventing a new name, we can change IW.deleteUnusedFiles to call IndexFileDeleter.checkpoint (instead of deletePendingFiles) which deletes those files + calls IDP.onCommit().",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2882",
        "summary": "Cut over SpanQuery#getSpans to AtomicReaderContext",
        "description": "Followup from LUCENE-2831 - SpanQuery#getSpans(IR) seems to be the last remaining artifact that doesn't enforce per-segments context while it should really work on AtomicReaderContext (SpanQuery#getSpans(AtomicReaderContext) instead of a naked IR.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1504",
        "summary": "Contrib-Spatial should use DocSet API rather then deprecated BitSet API",
        "description": "Contrib-Spatial should be rewritten to use the new DocIdSet Filter API with OpenBitSets instead of j.u.BitSets. FilteredDocIdSet can be used to replace (I)SerialChainFilter.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1447",
        "summary": "Improve payload error handling/reporting",
        "description": "If you try to load a payload more than once you get the exception:  IOException(\"Payload cannot be loaded more than once for the same term position.\");\n\nYou also get this exception if their is no payload to load, and its a bit confusing, as the message doesn't relate to the actual problem.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-925",
        "summary": "Analysis Package Level Javadocs",
        "description": "Analysis package level javadocs need improving.  An overview of what an Analyzer does, and maybe some sample code showing how to write you own Analyzer, Tokenizer and TokenFilter would be really helpful.  Bonus would be some discussion on best practices for achieving performance during analysis. ",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-3729",
        "summary": "Allow using FST to hold terms data in DocValues.BYTES_*_SORTED",
        "description": "",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2688",
        "summary": "NativeFSLockFactory throws an exception on Android 2.2 platform as java.lang.management package is not available on android.",
        "description": "NativeFSLockFactory throws an exception on Android 2.2 platform as java.lang.management package is not available on android.\n   Looks like FSDirectory defaults to NativeFSLockFactory, and this class refers to java.lang.management package to generate a unique lock. java.lang.management is not available in Android 2.2 and hence a runtime exception is raised. The workaround is to use another custom LockFactory or SimpleFSLockFactory, but Fixing NativeFSLockFactroy will help.\n\nThanks,\nSurinder",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2549",
        "summary": "TimeLimitingCollector's TimeExceededException contains useless relative docid",
        "description": "We found another bug with the RandomIndexWriter: When TimeLimitingCollector breaks collection after timeout, it records the last/next collected docid. It does this without rebasing, so the docid is useless. TestTimeLimitingCollector checks the docid, but correctly rebases it (as only this makes sense). Because the RandomIndexWriter uses different merge settings, the index is now sometimes not optimized and so the test fails (which is correct, as the docid is useless for non-optimized index).\n\nAttached is a patch that fixes this. Please tell me if I should backport to 2.9 and 3.0!",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-242",
        "summary": "Lucene Search has poor cpu utilization on a 4-CPU machine",
        "description": "I noticed that the class org.apache.lucene.index.FieldInfos uses private class\nmembers Vector byNumber and Hashtable byName, both of which are synchronized\nobjects, thus resulting in unessesary locks. \n\nBy changing the Vector byNumber to ArrayList byNumber, and Hashtable byName to\nHashMap byName, both are not synchronized objects, I was able to get 110%\nimprovement in performance (number of searches per second).\n\n\nHere is a sample of blocked thread\n\"Thread-32\" daemon prio=1 tid=0x082334c0 nid=0xa66 waiting for monitor entry\n[4f385000..4f38687c]\n        at java.util.Vector.elementAt(Vector.java:430)\n        - waiting to lock <0x452b93a8> (a java.util.Vector)\n        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)\n        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)\n        at\norg.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java:149)\n        at org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:115)\n        at\norg.apache.lucene.index.TermInfosReader.scanEnum(TermInfosReader.java:143)\n        at org.apache.lucene.index.TermInfosReader.get(TermInfosReader.java:137)\n        at org.apache.lucene.index.SegmentTermDocs.seek(SegmentTermDocs.java:51)\n        at org.apache.lucene.index.IndexReader.termDocs(IndexReader.java:364)\n        at org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:59)\n        at\norg.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)\n        at\norg.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:154)\n        at\ngov.gsa.search.SearcherByPageAndSortedField.search(SearcherByPageAndSortedField.java:317)\n        at\ngov.gsa.search.SearcherByPageAndSortedField.search(SearcherByPageAndSortedField.java:203)\n        at\ngov.gsa.search.grants.SearchGrants.searchByPageAndSortedField(SearchGrants.java:308)\n        at\ngov.gsa.search.grants.SearchServlet.searchByIndex(SearchServlet.java:1541)\n        at gov.gsa.search.grants.SearchServlet.getResults(SearchServlet.java:1325)\n        at gov.gsa.search.grants.SearchServlet.doGet(SearchServlet.java:500)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:740)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:853)\n        at\norg.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:247)\n        at\norg.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:193)\n        at\norg.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:256)\n        at\norg.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)\n        at\norg.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)\n        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)\n        at\norg.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at\norg.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)\n        at\norg.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)\n        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)\n        at\norg.apache.catalina.core.StandardContext.invoke(StandardContext.java:2415)\n        at\norg.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:180)\n        at\norg.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)\n        at\norg.apache.catalina.valves.ErrorDispatcherValve.invoke(ErrorDispatcherValve.java:171)\n        at\norg.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:641)\n        at\norg.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:172)\n        at\norg.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:641)\n        at\norg.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)\n        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)\n        at\norg.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:174)\n        at\norg.apache.catalina.core.StandardPipeline$StandardPipelineValveContext.invokeNext(StandardPipeline.java:643)\n        at\norg.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:480)\n        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:995)\n        at org.apache.coyote.tomcat4.CoyoteAdapter.service(CoyoteAdapter.java:223)\n        at org.apache.jk.server.JkCoyoteHandler.invoke(JkCoyoteHandler.java:261)\n        at org.apache.jk.common.HandlerRequest.invoke(HandlerRequest.java:360)\n        at org.apache.jk.common.ChannelSocket.invoke(ChannelSocket.java:604)\n        at\norg.apache.jk.common.ChannelSocket.processConnection(ChannelSocket.java:562)\n        at org.apache.jk.common.SocketConnection.runIt(ChannelSocket.java:679)\n        at\norg.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:619)\n        at java.lang.Thread.run(Thread.java:534)",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-987",
        "summary": "Deprecate IndexModifier",
        "description": "See discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/52017?search_string=deprecating%20indexmodifier;#52017\n\nThis is to deprecate IndexModifier before 3.0 and remove it in 3.0.\n\nThis patch includes:\n  1 IndexModifier and TestIndexModifier are deprecated.\n  2 TestIndexWriterModify is added. It is similar to TestIndexModifer but uses IndexWriter and has a few other changes. The changes are because of the difference between IndexModifier and IndexWriter.\n  3 TestIndexWriterLockRelease and TestStressIndexing are switched to use IndexWriter instead of IndexModifier.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-853",
        "summary": "Caching does not work when using RMI",
        "description": "Filters and caching uses transient maps so that caching does not work if you are using RMI and a remote searcher \n\nI want to add a new RemoteCachededFilter that will make sure that the caching is done on the remote searcher side \n ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-392",
        "summary": "[PATCH] Some Field methods use Classcast check instead of instanceof which is slow",
        "description": "I am not sure if this is because Lucene historically needed to work with older\nJVM's but with modern JVM's, instanceof is much quicker. \n\nThe Field.stringValue(), .readerValue(), and .binaryValue() methods all use\nClassCastException checking.\n\nUsing the following test-bed class, you will see that instanceof is miles quicker:\n\npackage com.aconex.index;\n\npublic class ClassCastExceptionTest {\n\n    private static final long ITERATIONS = 100000;\n\n    /**\n     * @param args\n     */\n    public static void main(String[] args) {\n\n        runClassCastTest(1); // once for warm up\n        runClassCastTest(2);\n        \n        runInstanceOfCheck(1);\n        runInstanceOfCheck(2);\n\n    }\n    private static void runInstanceOfCheck(int run) {\n        long start = System.currentTimeMillis();\n\n        Object foo = new Foo();\n        for (int i = 0; i < ITERATIONS; i++) {\n            String test;\n            if(foo instanceof String) {\n                System.out.println(\"Is a string\"); // should never print\n            }\n        }\n        long end = System.currentTimeMillis();\n        long diff = end - start;\n        System.out.println(\"InstanceOf checking run #\" + run + \": \" + diff + \"ms\");\n        \n    }\n\n    private static void runClassCastTest(int run) {\n        long start = System.currentTimeMillis();\n\n        Object foo = new Foo();\n        for (int i = 0; i < ITERATIONS; i++) {\n            String test;\n            try {\n                test = (String)foo;\n            } catch (ClassCastException c) {\n                // ignore\n            }\n        }\n        long end = System.currentTimeMillis();\n        long diff = end - start;\n        System.out.println(\"ClassCast checking run #\" + run + \": \" + diff + \"ms\");\n    }\n\n    private static final class Foo {\n    }\n\n}\n\n\nResults\n=======\n\nRun #1\n\nClassCast checking run #1: 1660ms\nClassCast checking run #2: 1374ms\nInstanceOf checking run #1: 8ms\nInstanceOf checking run #2: 4ms\n\n\nRun #2\nClassCast checking run #1: 1280ms\nClassCast checking run #2: 1344ms\nInstanceOf checking run #1: 7ms\nInstanceOf checking run #2: 2ms\n\n\nRun #3\nClassCast checking run #1: 1347ms\nClassCast checking run #2: 1250ms\nInstanceOf checking run #1: 7ms\nInstanceOf checking run #2: 2ms\n\nThis could explain why Documents with more Fields scales worse, as in, for lots\nof Documents with lots of Fields, the effect is exacerbated.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-690",
        "summary": "LazyField use of IndexInput not thread safe",
        "description": "Hypothetical problem: IndexInput.clone() of an active IndexInput could result in a corrupt copy.\nLazyField clones the FieldsReader.fieldsStream, which could be in use via IndexReader.document()",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1923",
        "summary": "Add toString() or getName() method to IndexReader",
        "description": "It would be very useful for debugging if IndexReader either had a getName() method, or a toString() implementation that would get a string identification for the reader.\n\nfor SegmentReader, this would return the same as getSegmentName()\nfor Directory readers, this would return the \"generation id\"?\nfor MultiReader, this could return something like \"multi(sub reader name, sub reader name, sub reader name, ...)\n\nright now, i have to check instanceof for SegmentReader, then call getSegmentName(), and for all other IndexReader types, i would have to do something like get the IndexCommit and get the generation off it (and this may throw UnsupportedOperationException, at which point i have would have to recursively walk sub readers and try again)\n\nI could work up a patch if others like this idea",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-1233",
        "summary": "Fix Document.getFieldables and others to never return null",
        "description": "Document.getFieldables (and other similar methods) returns null if there are no fields matching the name.  We can avoid NPE in consumers of this API if instead we return an empty array.\n\nSpinoff from http://markmail.org/message/g2nzstmce4cnf3zj",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3094",
        "summary": "optimize lev automata construction",
        "description": "in our lev automata algorithm, we compute an upperbound of the maximum possible states (not the true number), and\ncreate some \"useless\" unconnected states \"floating around\".\n\nthis isn't harmful, in the original impl we did the Automaton is simply a pointer to the initial state, and all algorithms\ntraverse this list, so effectively the useless states were dropped immediately. But recently we changed automaton to\ncache its numberedStates, and we set them here, so these useless states are being kept around.\n\nit has no impact on performance, but can be really confusing if you are debugging (e.g. toString). Thanks to Dawid Weiss\nfor noticing this. \n\nat the same time, forcing an extra traversal is a bit scary, so i did some benchmarking with really long strings and found\nthat actually its helpful to reduce() the number of transitions (typically cuts them in half) for these long strings, as it\nspeeds up some later algorithms. \n\nwon't see any speedup for short terms, but I think its easier to work with these simpler automata anyway, and it eliminates\nthe confusion of seeing the redundant states without slowing anything down.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-752",
        "summary": "Some tests fail due to common use of java.io.tmpdir",
        "description": "Some tests use java.io.tmpdir, while others use tempDir (which is defined in common-build.xml).  Those that rely on java.io.tmpdir can fail when being run on the same machine as someone else who is running tests (this came up in testing the new nightly build scripts on lucene.zones.a.o)\n\nProposed fix is to map java.io.tmpdir in the ANT Junit task to be the same value as tempDir.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1099",
        "summary": "Making Tokenizer.reset(Reader) public",
        "description": "In order to implement reusableTokenStream and be able to reset a Tokenizer, Tokenizer defines a reset(Reader) method. The problem is that this method is protected. I need to call this reset(Reader) method without having to know in advance what will be the type of the Tokenizer (I plan to have several).\nI noticed that almost all Tokenizer extensions define this method as public, and I wonder if this can be changed for Tokenizer also (I can't simply create my general Tokenizer extension and inherit from it because I want to use StandardTokenizer as well). ",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1040",
        "summary": "Can't quickly create StopFilter",
        "description": "Due to the use of CharArraySet by StopFilter, one can no longer efficiently pre-create a Set for use by future StopFilter instances.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3619",
        "summary": "in trunk if you switch up omitNorms while indexing, you get a corrumpt norms file",
        "description": "document 1 has \n  body: norms=true\n  title: norms=true\ndocument 2 has \n  body: norms=false\n  title: norms=true\n\nwhen seeing 'body' for the first time, normswriterperfield gets 'initial fieldinfo' and \nsaves it away, which says norms=true\n\nhowever, at flush time we dont check, so we write the norms happily anyway.\nthen SegmentReader reads the norms later: it skips \"body\" since it omits norms\nand if you ask for the norms of 'title' it instead returns the bogus \"body\" norms.\n\nasserting that SegmentReader \"plans to\" read the whole .nrm file exposes the bug.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2990",
        "summary": "Improve ArrayUtil/CollectionUtil.*Sort() methods to early-reaturn on empty or one-element lists/arrays",
        "description": "It might be a good idea to make CollectionUtil or ArrayUtil return early if the passed-in list or array's length <= 1 because sorting is unneeded then. This improves maybe automaton or other places, as for empty or one-element lists no SorterTermplate is created.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3660",
        "summary": "If indexwriter hits a non-ioexception from indexExists it leaks a write.lock",
        "description": "the rest of IW's ctor is careful about this.\n\nIndexReader.indexExists catches any IOException and returns false, but the problem\noccurs if some other exception (in my test, UnsupportedOperationException, but you\ncan imagine others are possible), when trying to e.g. read in the segments file.\n\nI think we just need to move the IR.exists stuff inside the try / finally",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-674",
        "summary": "Error in FSDirectory if java.io.tmpdir incorrectly specified",
        "description": "A user of the JAMWiki project (http://jamwiki.org/) reported an error with the following stack trace:\n\nSEVERE: Unable to create search instance /usr/share/tomcat5/webapps/jamwiki-0.3.4-beta7/test/base/search/indexen\njava.io.IOException: Cannot create directory: /temp\n        at org.apache.lucene.store.FSDirectory.init(FSDirectory.java:171)\n        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:141)\n        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)\n        at org.jamwiki.search.LuceneSearchEngine.getSearchIndexPath(LuceneSearchEngine.java:318)\n\nThe culprit is that the java.io.tmpdir property was incorrectly specified on the user's system.  Lucene could easily handle this issue by modifying the FSDirectory.init() method.  Currently the code uses the index directory if java.io.tmpdir and org.apache.lucene.lockDir are unspecified, but it could use that directory if those values are unspecified OR if they are invalid.  Doing so would make Lucene a bit more robust without breaking any existing installations.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1249",
        "summary": "Bugs in org.apache.lucene.index.TermVectorsReader.clone()",
        "description": "A couple of things:\n\n- The implementation can return null which is not allowed.  It should throw a CloneNotSupportedException if that's the case.\n\n- Part of the code reads:\n\n    TermVectorsReader clone = null;\n    try {\n      clone = (TermVectorsReader) super.clone();\n    } catch (CloneNotSupportedException e) {}\n\n    clone.tvx = (IndexInput) tvx.clone();\n\nIf a CloneNotSupportedException is caught then \"clone\" will be null and the assignment to clone.tvx will fail with a null pointer exception.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2792",
        "summary": "Add a simple FST impl to Lucene",
        "description": "\nI implemented the algo described at\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698 for\nincrementally building a finite state transducer (FST) from sorted\ninputs.\n\nThis is not a fully general FST impl -- it's only able to build up an\nFST incrementally from input/output pairs that are pre-sorted.\n\nCurrently the inputs are BytesRefs, and the outputs are pluggable --\nNoOutputs gets you a simple FSA, PositiveIntOutputs maps to a long,\nByteSequenceOutput maps to a BytesRef.\n\nThe implementation has a low memory overhead, so that it can handle a\nfairly large set of terms.  For example, it can build the FSA for the\n9.8M terms from a 10M document wikipedia index in ~8 seconds (on\nbeast), using ~256 MB peak RAM, resulting in an FSA that's ~60 MB.\n\nIt packs the FST as-it-builds into a compact byte[], and then exposes\nthe API to read nodes/arcs directly from the byte[].  The FST can be\nquickly saved/loaded to/from a Directory since it's just a big byte[].\n\nThe format is similar to what Morfologik uses\n(http://sourceforge.net/projects/morfologik/).\n\nI think there are a number of possible places we can use this in\nLucene.  For example, I think many apps could hold the entire terms\ndict in RAM, either at the multi-reader level or maybe per-segment\n(mapping to file offset or to something else custom to the app), which\nmay possibly be a good speedup for certain MTQs (though, because the\nformat is packed into a byte[], there is a decode cost when visiting\narcs).\n\nThe builder can also prune as it goes, so you get a prefix trie pruned\naccording to how many terms run through the nodes, which makes it\nfaster and even less memory consuming.  This may be useful as a\nreplacement for our current binary search terms index since it can\nachieve higher term density for the same RAM consumption of our\ncurrent index.\n\nAs an initial usage to make sure this is exercised, I cutover the\nSimpleText codec, which currently fully loads all terms into a\nTreeMap (and has caused intermittent OOME in some tests), to use an FST\ninstead.  SimpleText uses a PairOutputs which is able to \"pair up\" any\ntwo other outputs, since it needs to map each input term to an int\ndocFreq and long filePosition.\n\nAll tests pass w/ SimpleText forced codec, and I think this is\ncommittable except I'd love to get some help w/ the generics\n(confession to the policeman: I had to add\n@SuppressWarnings({\"unchecked\"})) all over!!  Ideally an FST is\nparameterized by its output type (Integer, BytesRef, etc.).\n\nI even added a new @nightly test that makes a largeish set of random\nterms and tests the resulting FST on different outputs :)\n\nI think it would also be easy to make a variant that uses char[]\ninstead of byte[] as its inputs, so we could eg use this during analysis\n(Robert's idea).  It's already be easy to have a CharSequence\noutput type since the outputs are pluggable.\n\nDawid Weiss (author of HPPC -- http://labs.carrotsearch.com/hppc.html -- and\nMorfologik -- http://sourceforge.net/projects/morfologik/)\nwas very helpful iterating with me on this (thank you!).\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-997",
        "summary": "Add search timeout support to Lucene",
        "description": "This patch is based on Nutch-308. \n\nThis patch adds support for a maximum search time limit. After this time is exceeded, the search thread is stopped, partial results (if any) are returned and the total number of results is estimated.\n\nThis patch tries to minimize the overhead related to time-keeping by using a version of safe unsynchronized timer.\n\nThis was also discussed in an e-mail thread.\nhttp://www.nabble.com/search-timeout-tf3410206.html#a9501029",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1953",
        "summary": "FastVectorHighlighter: small fragCharSize can cause StringIndexOutOfBoundsException ",
        "description": "If fragCharSize is smaller than Query string, StringIndexOutOfBoundsException is thrown.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1807",
        "summary": "Add convenient constructor to PerFieldAnalyzerWrapper for Dependency Injection",
        "description": "It would be good if PerFieldAnalyzerWrapper had a constructor which took in an analyzer map, rather than having to repeatedly call addAnalyzer -- this would make it much easier/cleaner to use this class in e.g. Spring XML configurations.\n\nRelatively trivial change, patch to be attached.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3502",
        "summary": "Packed ints: move .getArray into Reader API",
        "description": "This is a simple code cleanup... it's messy that a consumer of\nPackedInts.Reader must check whether the impl is Direct8/16/32/64 in\norder to get an array; it's better to move up the .getArray into the\nReader interface and then make the DirectN impls package private.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2074",
        "summary": "Use a separate JFlex generated Unicode 4 by Java 5 compatible StandardTokenizer",
        "description": "The current trunk version of StandardTokenizerImpl was generated by Java 1.4 (according to the warning). In Java 3.0 we switch to Java 1.5, so we should regenerate the file.\n\nAfter regeneration the Tokenizer behaves different for some characters. Because of that we should only use the new TokenizerImpl when Version.LUCENE_30 or LUCENE_31 is used as matchVersion.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-569",
        "summary": "NearSpans skipTo bug",
        "description": "NearSpans appears to have a bug in skipTo that causes it to skip over some matching documents completely.  I discovered this bug while investigating problems with SpanWeight.explain, but as far as I can tell the Bug is not specific to Explanations ... it seems like it could potentially result in incorrect matching in some situations where a SpanNearQuery is nested in another query such thatskipTo will be used ... I tried to create a high level test case to exploit the bug when searching, but i could not.  TestCase exploiting the class using NearSpan and SpanScorer will follow...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-468",
        "summary": "Searchable.java: The info in the @deprecated tags do not refer to the search(Weight, etc...) versions...",
        "description": "E.g.\n\nThe javadoc for \n          void search(Query query, Filter filter, HitCollector results)\nstates:\n          Deprecated. use search(Query, Filter, HitCollector) instead.\ninstead of:\n          Deprecated. use search(Weight, Filter, HitCollector) instead.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2958",
        "summary": "WriteLineDocTask improvements",
        "description": "Make WriteLineDocTask and LineDocSource more flexible/extendable:\n* allow to emit lines also for empty docs (keep current behavior as default)\n* allow more/less/other fields",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-762",
        "summary": "[PATCH] Efficiently retrieve sizes of field values",
        "description": "Sometimes an application would like to know how large a document is before retrieving it.  This can be important for memory management or choosing between algorithms, especially in cases where documents might be very large.\n\nThis patch extends the existing FieldSelector mechanism with two new FieldSelectorResults:  SIZE and SIZE_AND_BREAK.  SIZE creates fields on the retrieved document that store field sizes instead of actual values.  SIZE_AND_BREAK is especially efficient if one field comprises the bulk of the document size (e.g., the body field) and can thus be used as a reasonable size approximation.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "RFE"
    },
    {
        "key": "LUCENE-550",
        "summary": "InstantiatedIndex - faster but memory consuming index",
        "description": "Represented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric RAMDirectory at the cost of greater RAM consumption.\n\nPerformance seems to be a little bit better than log2n (binary search). No real data on that, just my eyes.\n\nPopulated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex.    \n\nAt 20,000 document 10-50 characters long InstantiatedIndex outperforms RAMDirectory some 30x,\n15x at 100 documents of 2000 charachters length,\nand is linear to RAMDirectory at 10,000 documents of 2000 characters length.\n\nMileage may vary depending on term saturation.\n\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-917",
        "summary": "Javadoc improvements for Payload class",
        "description": "Some methods in org.apache.lucene.index.Payload don't have javadocs",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-2689",
        "summary": "remove NativeFSLockFactory's attempt to acquire a test lock",
        "description": "NativeFSLockFactory tries to acquire a test lock the first time a lock is created.  It's the only LF to do this, and, it's caused us hassle (LUCENE-2421,  LUCENE-2688).\n\nI think we should just remove it.  The caller of .makeLock will presumably immediately thereafter acquire the lock and at the point hit any exception that acquireTestLock would've hit.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2839",
        "summary": "Visibility of Scorer.score(Collector, int, int) is wrong",
        "description": "The method for scoring subsets in Scorer has wrong visibility, its marked protected, but protected methods should not be called from other classes. Protected methods are intended for methods that should be overridden by subclasses and are called by (often) final methods of the same class. They should never be called from foreign classes.\n\nThis method is called from another class out-of-scope: BooleanScorer(2) - so it must be public, but it's protected. This does not lead to a compiler error because BS(2) is in same package, but may lead to problems if subclasses from other packages override it. When implementing LUCENE-2838 I hit a trap, as I thought tis method should only be called from the class or Scorer itsself, but in fact its called from outside, leading to bugs, because I had not overridden it. As ConstantScorer did not use it I have overridden it with throw UOE and suddenly BooleanQuery was broken, which made it clear that it's called from outside (which is not the intention of protected methods).\n\nWe cannot fix this in 3.x, as it would break backwards for classes that overwrite this method, but we can fix visibility in trunk.\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2693",
        "summary": "Add delete term and query need to more precisely record the bytes used",
        "description": "DocumentsWriter's add delete query and add delete term add to the number of bytes used regardless of the query or term already existing in the respective map.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1310",
        "summary": "Phrase query with term repeated 3 times requires more slop than expected",
        "description": "Consider a document with the text \"A A A\".\nThe phrase query \"A A A\" (exact match) succeeds.\nThe query \"A A A\"~1 (same document and query, just increasing the slop value by one) fails.\n\"A A A\"~2 succeeds again.\n\nIf the exact match succeeds, I wouldn't expect the same query but with more slop to fail.  The fault seems to require some term to be repeated at least three times in the query, but the three occurrences do not need to be adjacent.  I will attach a file that contains a set of JUnit tests that demonstrate what I mean.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-528",
        "summary": "Optimization for IndexWriter.addIndexes()",
        "description": "One big performance problem with IndexWriter.addIndexes() is that it has to optimize the index both before and after adding the segments.  When you have a very large index, to which you are adding batches of small updates, these calls to optimize make using addIndexes() impossible.  It makes parallel updates very frustrating.\n\nHere is an optimized function that helps out by calling mergeSegments only on the newly added documents.  It will try to avoid calling mergeSegments until the end, unless you're adding a lot of documents at once.\n\nI also have an extensive unit test that verifies that this function works correctly if people are interested.  I gave it a different name because it has very different performance characteristics which can make querying take longer.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3272",
        "summary": "Consolidate Lucene's QueryParsers into a module",
        "description": "Lucene has a lot of QueryParsers and we should have them all in a single consistent place.  \n\nThe following are QueryParsers I can find that warrant moving to the new module:\n\n- Lucene Core's QueryParser\n- AnalyzingQueryParser\n- ComplexPhraseQueryParser\n- ExtendableQueryParser\n- Surround's QueryParser\n- PrecedenceQueryParser\n- StandardQueryParser\n- XML-Query-Parser's CoreParser\n\nAll seem to do a good job at their kind of parsing with extensive tests.\n\nOne challenge of consolidating these is that many tests use Lucene Core's QueryParser.  One option is to just replicate this class in src/test and call it TestingQueryParser.  Another option is to convert all tests over to programmatically building their queries (seems like alot of work).",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3368",
        "summary": "IndexWriter commits update documents without corresponding delete",
        "description": "while backporting the testcase from LUCENE-3348 I ran into this thread hazard in the 3.x branch. We actually fixed this issue in LUCENE-3348 for Lucene 4.0 but since DWPT has a slightly different behavior when committing segments I create a new issue to track this down in 3.x. when we prepare a commit we sync on IW flush the DW and apply all deletes then release the lock, maybeMerge and start the commit (IW#startCommit(userdata)). Yet, a new segment could be flushed via getReader and sneak into the SegementInfos which are cloned in IW#startCommit instead of in prepareCommit right after the flush. ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2825",
        "summary": "FSDirectory.open should return MMap on 64-bit Solaris",
        "description": "MMap is ~ 30% faster than NIOFS on this platform.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3773",
        "summary": "small improvements to DWPTThreadPool",
        "description": "While working on another issue I cleaned up DWTPThreadPool a little, fixed some naming issues and fixed some todos... patch is coming soon...",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-500",
        "summary": "Lucene 2.0 requirements - Remove all deprecated code",
        "description": "Per the move to Lucene 2.0 from 1.9, remove all deprecated code and update documentation, etc.\n\nPatch to follow shortly.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2918",
        "summary": "IndexWriter should prune 100% deleted segs even in the NRT case",
        "description": "We now prune 100% deleted segs on commit from IW or IR (LUCENE-2010),\nbut this isn't quite aggressive enough, because in the NRT case you\nrarely call commit.\n\nInstead, the moment we delete the last doc of a segment, it should be\npruned from the in-memory segmentInfos.  This way, if you open an NRT\nreader, or a merge kicks off, or commit is called, the 100% deleted\nsegment is already gone.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3642",
        "summary": "EdgeNgrams creates invalid offsets",
        "description": "A user reported this because it was causing his highlighting to throw an error.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1374",
        "summary": "Merging of compressed string Fields may hit NPE",
        "description": "This bug was introduced with LUCENE-1219 (only present on 2.4).\n\nThe bug happens when merging compressed string fields, but only if bulk-merging code does not apply because the FieldInfos for the segment being merged are not congruent.  This test shows the bug:\n\n{code}\n  public void testMergeCompressedFields() throws IOException {\n    File indexDir = new File(System.getProperty(\"tempDir\"), \"mergecompressedfields\");\n    Directory dir = FSDirectory.getDirectory(indexDir);\n    try {\n      for(int i=0;i<5;i++) {\n        // Must make a new writer & doc each time, w/\n        // different fields, so bulk merge of stored fields\n        // cannot run:\n        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);\n        w.setMergeFactor(5);\n        w.setMergeScheduler(new SerialMergeScheduler());\n        Document doc = new Document();\n        doc.add(new Field(\"test1\", \"this is some data that will be compressed this this this\", Field.Store.COMPRESS, Field.Index.NO));\n        doc.add(new Field(\"test2\", new byte[20], Field.Store.COMPRESS));\n        doc.add(new Field(\"field\" + i, \"random field\", Field.Store.NO, Field.Index.TOKENIZED));\n        w.addDocument(doc);\n        w.close();\n      }\n\n      byte[] cmp = new byte[20];\n\n      IndexReader r = IndexReader.open(dir);\n      for(int i=0;i<5;i++) {\n        Document doc = r.document(i);\n        assertEquals(\"this is some data that will be compressed this this this\", doc.getField(\"test1\").stringValue());\n        byte[] b = doc.getField(\"test2\").binaryValue();\n        assertTrue(Arrays.equals(b, cmp));\n      }\n    } finally {\n      dir.close();\n      _TestUtil.rmDir(indexDir);\n    }\n  }\n{code}\n\nIt's because in FieldsReader, when we load a field \"for merge\" we create a FieldForMerge instance which subsequently does not return the right values for getBinary{Value,Length,Offset}.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1166",
        "summary": "A tokenfilter to decompose compound words",
        "description": "A tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens.\n\nAn example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter \"Schiff\".\n\nI use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project.\n\nMy question now:\nWould it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well.\n\nWhat do you think?",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-943",
        "summary": "ComparatorKey in Locale based sorting",
        "description": "This is a reply/follow-up on Chris Hostetter's message on Lucene developers list (aug 2006):\nhttp://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3cPine.LNX.4.58.0608211050330.5081@hal.rescomp.berkeley.edu%3e\n\n> perhaps it would be worthwhile for comparatorStringLocale to convert the String[] it gets back from FieldCache.DEFAULT.getStrings to a new CollationKey[]? or maybe even for FieldCache.DEFAULT.getStrings to be deprecated, and replaced with a FieldCache.DEFAULT.getCollationKeys(reader,field,Collator)?\n\nI think the best is to keep the default behavior as it is today. There is a cost of building caches for sort fields which I think not everyone wants. However for some international production environments there are indeed possible performance gains in comparing precalculated keys instead of comparing strings with rulebased collators.\n\nSince Lucene's Sort architecture is pluggable it is easy to create a custom locale-based comparator, which utilizes the built-in caching/warming mechanism of FieldCache, and may be used in SortField constructor.\n\nI'm not sure whether there should be classes for this in Lucene core or not, but it could be nice to have the option of performance vs. memory consumption in localized sorting without having to use additional jars.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3542",
        "summary": "StandardQueryParser ignores AND operator for tokenized query terms",
        "description": "The standard query parser uses the default query operator for query clauses that are created from tokenization in the query parser instead of the actual operator for the source term.\n\nhere is an example:\n{code}\nStandardQueryParser parser = new StandardQueryParser(new StandardAnalyzer(Version.LUCENE_34));\nparser.setDefaultOperator(Operator.OR);\nSystem.out.println(((BooleanQuery)parser.parse(\"_deleted:true AND title:\u6771\u4eac\", \"f\")));\n{code}\n\nthis should yield:\n+_deleted:true +(title:\u6771 title:\u4eac)\n\nas our former core query parser does but actually yields:\n+_deleted:true title:\u6771 title:\u4eac\n\nseems like a bug to me, looking at the tests seems we don't test for this kind of queries in the standard query parser tests too.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1219",
        "summary": "support array/offset/ length setters for Field with binary data",
        "description": "currently Field/Fieldable interface supports only compact, zero based byte arrays. This forces end users to create and copy content of new objects before passing them to Lucene as such fields are often of variable size. Depending on use case, this can bring far from negligible  performance  improvement. \n\nthis approach extends Fieldable interface with 3 new methods   \ngetOffset(); gettLenght(); and getBinaryValue() (this only returns reference to the array)\n\n   ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1173",
        "summary": "index corruption autoCommit=false",
        "description": "In both Lucene 2.3 and trunk, the index becomes corrupted when autoCommit=false",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-880",
        "summary": "DocumentWriter closes TokenStreams too early",
        "description": "The DocumentWriter closes a TokenStream as soon as it has consumed its tokens. The javadoc of TokenStream.close() says that it releases resources associated with the stream. However, the DocumentWriter keeps references of the resources (i. e. payload byte arrays, term strings) until it writes the postings to the new segment, which means that DocumentWriter should call TokenStream.close() after it has written the postings.\n\nThis problem occurs in multithreaded applications where e. g. pooling is used for the resources. My patch adds a new test to TestPayloads which shows this problem. Multiple threads add documents with payloads to an index and use a pool of byte arrays for the payloads. TokenStream.close() puts the byte arrays back into the pool. The test fails with the old version but runs successfully with the patched version. \n\nAll other units tests pass as well.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1579",
        "summary": "Cloned SegmentReaders fail to share FieldCache entries",
        "description": "I just hit this on LUCENE-1516, which returns a cloned readOnly\nreaders from IndexWriter.\n\nThe problem is, when cloning, we create a new [thin] cloned\nSegmentReader for each segment.  FieldCache keys directly off this\nobject, so if you clone the reader and do a search that requires the\nFieldCache (eg, sorting) then that first search is always very slow\nbecause every single segment is reloading the FieldCache.\n\nThis is of course a complete showstopper for LUCENE-1516.\n\nWith LUCENE-831 we'll switch to a new FieldCache API; we should ensure\nthis bug is not present there.  We should also fix the bug in the\ncurrent FieldCache API since for 2.9, users may hit this.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2771",
        "summary": "Remove norms() support from non-atomic IndexReaders",
        "description": "Spin-off from LUCENE-2769:\nCurrently all IndexReaders support norms(), but the core of Lucene never uses it and its even dangerous because of memory usage. We should do the same like with MultiFields and factor it out and throw UOE on non-atomic readers.\n\nThe SlowMultiReaderWrapper can then manage the norms. Also ParallelReader needs to be fixed.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3347",
        "summary": "XMLparser drops user boosting",
        "description": "The lucene XML parser seems to convert user defined boosting back to default 1.0 and thus boosting value is dropped from the query...\n\ne.g.\n\n{code:xml}\n<BooleanQuery>\n\t<Clause occurs=\"must\">\n\t\t<BooleanQuery>\n\t\t\t<Clause occurs=\"should\">\n\t\t\t\t<UserQuery fieldName=\"Vehicle.Colour\">red^66 blue~^8</UserQuery>\n\t\t\t</Clause>\n\t\t</BooleanQuery>\n\t</Clause>\n\t<Clause occurs=\"should\">\n\t\t<BooleanQuery>\n\t\t\t<Clause occurs=\"should\">\n\t\t\t\t<UserQuery fieldName=\"Vehicle.Colour\">black^0.01</UserQuery>\n\t\t\t</Clause>\n\t\t</BooleanQuery>\n\t</Clause>\n</BooleanQuery>\n{code}\n\nproduces a lucene query: +( ( Vehicle.Colour:red^66 Vehicle.Colour:blue~0.5^8 ) ) ( Vehicle.Colour:black )\n\nThe expected query : +( ( Vehicle.Colour:red^66 Vehicle.Colour:blue~0.5^8 ) ) ( Vehicle.Colour:black^0.01 )\n\nI have developed a work around by modifying line 77 of UserInputQueryBuilder.java \n\nfrom:\n\n{code:java}\nq.setBoost(DOMUtils.getAttribute(e,\"boost\",1.0f));\n{code}\n\nto:\n\n{code:java}\nq.setBoost( DOMUtils.getAttribute( e, \"boost\", q.getBoost() ) );\n{code}\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3680",
        "summary": "exception consistency in o.a.l.store",
        "description": "just some minor improvements:\n* always use EOFException when its eof\n* always include the inputstream too so we know filename etc\n* use FileNotFoundException consistently in CFS when a sub-file is not found\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1801",
        "summary": "Tokenizers (which are the source of Tokens) should call AttributeSource.clearAttributes() first",
        "description": "This is a followup for LUCENE-1796:\n{quote}\nToken.clear() used to be called by the consumer... but then it was switched to the producer here: LUCENE-1101 \nI don't know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it's reusableToken... and now it doesn't.\n{quote}\n\nAs alternative to changing all core/contrib Tokenizers to call clearAttributes first, we could do this in the indexer, what would be a overhead for old token streams that itsself clear their reusable token. This issue should also update the Javadocs, to clearly state inside Tokenizer.java, that the source TokenStream (normally the Tokenizer) should clear *all* Attributes. If it does not do it and e.g. the positionIncrement is changed to 0 by any TokenFilter, but the filter does not change it back to 1, the TokenStream would stay with 0. If the TokenFilter would call PositionIncrementAttribute.clear() (because he is responsible), it could also break the TokenStream, because clear() is a general method for the whole attribute instance. If e.g. Token is used as AttributeImpl, a call to clear() would also clear offsets and termLength, which is not wanted. So the source of the Tokenization should rest the attributes to default values.\n\nLUCENE-1796 removed the iterator creation cost, so clearAttributes should run fast, but is an additional cost during Tokenization, as it was not done consistently before, so a small speed degradion is caused by this, but has nothing to do with the new TokenStream API.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1641",
        "summary": "Correct spatial and trie documentation links in JavaDocs and site",
        "description": "After updating myself in the site docs, I have some changes to the site and javadocs of Lucene 2.9:\n- Add spatial contrib to javadocs\n- Add trie package to the contrib/queries package\nBoth changes prevent these pacakges from a apearing in core's pacakge list on the javadocs/all homepage. I also adjusted the documentation page to reflect the changes.\n\nI will commit the attached patch, if nobody objects.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-945",
        "summary": "contrib/benchmark tests fail find data dirs",
        "description": "This was exposed by LUCENE-940 - a test was added that uses the Reuters collection. Then tests succeed when ran from contrib/benchmark (e.g. by IDE) but fail when running as part of \"ant test-contrib\" because the test expects to find the Reuters data under trunk/work. \n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1503",
        "summary": "refactor spatial contrib \"Filter\" \"Query\" classes",
        "description": "From erik's comments in LUCENE-1387\n\n    * DistanceQuery is awkwardly named. It's not an (extends) Query.... it's a POJO with helpers. Maybe DistanceQueryFactory? (but it creates a Filter also)\n\n    * CartesianPolyFilter is not a Filter (but CartesianShapeFilter is)\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1963",
        "summary": "ArabicAnalyzer: Lowercase before Stopfilter",
        "description": "ArabicAnalyzer lowercases text in case you have some non-Arabic text around.\nIt also allows you to set a custom stopword list (you might augment the Arabic list with some English ones, for example).\n\nIn this case its helpful for these non-Arabic stopwords, to lowercase before stopfilter.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2755",
        "summary": "Some improvements to CMS",
        "description": "While running optimize on a large index, I've noticed several things that got me to read CMS code more carefully, and find these issues:\n\n* CMS may hold onto a merge if maxMergeCount is hit. That results in the MergeThreads taking merges from the IndexWriter until they are exhausted, and only then that blocked merge will run. I think it's unnecessary that that merge will be blocked.\n\n* CMS sorts merges by segments size, doc-based and not bytes-based. Since the default MP is LogByteSizeMP, and I hardly believe people care about doc-based size segments anymore, I think we should switch the default impl. There are two ways to make it extensible, if we want:\n** Have an overridable member/method in CMS that you can extend and override - easy.\n** Have OneMerge be comparable and let the MP determine the order (e.g. by bytes, docs, calibrate deletes etc.). Better, but will need to tap into several places in the code, so more risky and complicated.\n\nOn the go, I'd like to add some documentation to CMS - it's not very easy to read and follow.\n\nI'll work on a patch.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2597",
        "summary": "Query scorers should not use MultiFields",
        "description": "Lucene does all searching/filtering per-segment, today, but there are a number of tests that directly invoke Scorer.scorer or Filter.getDocIdSet on a composite reader.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3308",
        "summary": "Cleanup 'good' queries code",
        "description": "Before moving some of the classes from the queries contrib to the queries module, I want to just pass over them and clean them up, since we want code in modules to be of the same calibre as core code.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": ""
    },
    {
        "key": "LUCENE-3552",
        "summary": "TaxonomyReader/Writer and their Lucene* implementation",
        "description": "The facet module contains two interfaces TaxonomyWriter and TaxonomyReader, with two implementations Lucene*. We've never actually implemented two TaxonomyWriters/Readers, so I'm not sure if these interfaces are useful anymore. Therefore I'd like to propose that we do either of the following:\n\n# Remove the interfaces and remove the Lucene part from the implementation classes (to end up with TW/TR impls). Or,\n# Keep the interfaces, but rename the Lucene* impls to Directory*.\n\nWhatever we do, I'd like to make the impls/interfaces impl also TwoPhaseCommit.\n\nAny preferences?",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3113",
        "summary": "fix analyzer bugs found by MockTokenizer",
        "description": "In LUCENE-3064, we beefed up MockTokenizer with assertions, and I've switched over the analysis tests to use MockTokenizer for better coverage.\n\nHowever, this found a few bugs (one of which is LUCENE-3106):\n* incrementToken() after it returns false in CommonGramsQueryFilter, HyphenatedWordsFilter, ShingleFilter, SynonymFilter\n* missing end() implementation for PrefixAwareTokenFilter\n* double reset() in QueryAutoStopWordAnalyzer and ReusableAnalyzerBase\n* missing correctOffset()s in MockTokenizer itself.\n\nI think it would be nice to just fix all the bugs on one issue... I've fixed everything except Shingle and Synonym",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1773",
        "summary": "Add benchmark task for FastVectorHighlighter",
        "description": "",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3749",
        "summary": "Similarity.java javadocs and simplifications for 4.0",
        "description": "As part of adding additional scoring systems to lucene, we made a lower-level Similarity\nand the existing stuff became e.g. TFIDFSimilarity which extends it.\n\nHowever, I always feel bad about the complexity introduced here (though I do feel there\nare some \"excuses\", that its a difficult challenge).\n\nIn order to try to mitigate this, we also exposed an easier API (SimilarityBase) on top of \nit that makes some assumptions (and trades off some performance) to try to provide something \nconsumable for e.g. experiments.\n\nStill, we can cleanup a few things with the low-level api: fix outdated documentation and\nshoot for better/clearer naming etc.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3407",
        "summary": "wrong stats/scoring from MemoryCodec",
        "description": "I hit some random failures in the flexscoring branch: wierd because its not a random test.\n\nI noticed the test always failed with memorycodec, and wrote a specific test for it.\n\nI haven't traced thru it yet, but I think its likely the issue that memorycodec is somehow returning wrong stats here?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3726",
        "summary": "Default KuromojiAnalyzer to use search mode",
        "description": "Kuromoji supports an option to segment text in a way more suitable for search,\nby preventing long compound nouns as indexing terms.\n\nIn general 'how you segment' can be important depending on the application \n(see http://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf for some studies on this in chinese)\n\nThe current algorithm punishes the cost based on some parameters (SEARCH_MODE_PENALTY, SEARCH_MODE_LENGTH, etc)\nfor long runs of kanji.\n\nSome questions (these can be separate future issues if any useful ideas come out):\n* should these parameters continue to be static-final, or configurable?\n* should POS also play a role in the algorithm (can/should we refine exactly what we decompound)?\n* is the Tokenizer the best place to do this, or should we do it in a tokenfilter? or both?\n  with a tokenfilter, one idea would be to also preserve the original indexing term, overlapping it: e.g. ABCD -> AB, CD, ABCD(posInc=0)\n  from my understanding this tends to help with noun compounds in other languages, because IDF of the original term boosts 'exact' compound matches.\n  but does a tokenfilter provide the segmenter enough 'context' to do this properly?\n\nEither way, I think as a start we should turn on what we have by default: its likely a very easy win.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-882",
        "summary": "Spellchecker doesn't need to store ngrams",
        "description": "The spellchecker in contrib stores the ngrams although this doesn't seem to be necessary. This patch changes that, I will commit it unless someone objects. This improves indexing speed and index size. Some numbers on a small test I did:\n\nInput of the original index: 2200 text files, index size 5.3 MB, indexing took 17 seconds\n\nSpell index before patch: about 60.000 documents, index size 13 MB, indexing took 62 seconds\nSpell index after patch: about 60.000 documents, index size 6.3 MB, indexing took 52 seconds\n\nBTW, the test case fails even before this patch. I'll probaby submit another issue about how to fix that.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3678",
        "summary": "TestAddIndexes fails (norms file not found)",
        "description": "ant test-core -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=2f55291b308dc34b:-4d72bfad34f3f357:4bc5ec894269c041 -Dargs=\"-Dfile.encoding=UTF-8\" -Dtests.iter=100\n\nfails about 4 or 5 times out of 100.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2472",
        "summary": "The terms index divisor in IW should be set via IWC not via getReader",
        "description": "The getReader call gives a false sense of security... since if deletions have already been applied (and IW is pooling) the readers have already been loaded with a divisor of 1.\n\nBetter to set the divisor up front in IWC.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3896",
        "summary": "CharTokenizer has bugs for large documents.",
        "description": "Initially found by hudson from additional testing added in LUCENE-3894, but \ncurrently not reproducable (see LUCENE-3895).\n\nBut its easy to reproduce for a simple single-threaded case in TestDuelingAnalyzers.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-829",
        "summary": "StandardBenchmarker#makeDocument does not explicitly close opened files",
        "description": "StandardBenchmarker#makeDocument(File in, String[] tags, boolean stored, boolean tokenized, boolean tfv)\n\n        BufferedReader reader = new BufferedReader(new FileReader(in));\n\nAbove reader is not closed until GC hits it. Can cause problems in cases where ulimit is set too low.\n\nI did this:\n\n        while ((line = reader.readLine()) != null)\n        {\n            body.append(line).append(' ');\n        }\n+        reader.close();",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2790",
        "summary": "IndexWriter should call MP.useCompoundFile and not LogMP.getUseCompoundFile",
        "description": "Spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/112311.\n\nI will attach a patch shortly that addresses the issue on trunk.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1209",
        "summary": "If setConfig(Config config) is called in resetInputs(), you can turn term vectors off and on by round",
        "description": "I want to be able to run one benchmark that tests things using term vectors and not using term vectors.\n\nCurrently this is not easy because you cannot specify term vectors per round.\n\nWhile you do have to create a new index per round, this automation is preferable to me in comparison to running two separate tests.\n\nIf it doesn't affect anything else, it would be great to have setConfig(Config config) called in BasicDocMaker.resetInputs(). This would keep the term vector options up to date per round if you reset.\n\n- Mark",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2311",
        "summary": "Pass potent SR to IRWarmer.warm(), and also call warm() for new segments",
        "description": "Currently warm() receives a SegmentReader without terms index and docstores.\nIt would be arguably more useful for the app to receive a fully loaded reader, so it can actually fire up some caches. If the warmer is undefined on IW, we probably leave things as they are.\n\nIt is also arguably more concise and clear to call warm() on all newly created segments, so there is a single point of warming readers in NRT context, and every subreader coming from getReader is guaranteed to be warmed up -> you don't have to introduce even more mess in your code by rechecking it.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3563",
        "summary": "TestPagedBytes failure",
        "description": "ant test -Dtestcase=TestPagedBytes -Dtestmethod=testDataInputOutput -Dtests.seed=268db1f3329b70d:3125365bc9c56c90:116e02aa4a70ec2f -Dtests.multiplier=5",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1553",
        "summary": "ConcurrentScheduleManager.addMyself() has wrong inted",
        "description": "This method has the wrong index for the 'size' variable, I think it should b allInstances.size.\n\n{code:java}\nprivate void addMyself() {\n    synchronized(allInstances) {\n      final int size=0;\n      int upto = 0;\n      for(int i=0;i<size;i++) {\n        final ConcurrentMergeScheduler other = (ConcurrentMergeScheduler) allInstances.get(i);\n        if (!(other.closed && 0 == other.mergeThreadCount()))\n          // Keep this one for now: it still has threads or\n          // may spawn new threads\n          allInstances.set(upto++, other);\n      }\n      allInstances.subList(upto, allInstances.size()).clear();\n      allInstances.add(this);\n    }\n  }\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2815",
        "summary": "MultiFields not thread safe",
        "description": "MultiFields looks like it has thread safety issues",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2316",
        "summary": "Define clear semantics for Directory.fileLength",
        "description": "On this thread: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201003.mbox/%3C126142c1003121525v24499625u1589bbef4c0792e7@mail.gmail.com%3E it was mentioned that Directory's fileLength behavior is not consistent between Directory implementations if the given file name does not exist. FSDirectory returns a 0 length while RAMDirectory throws FNFE.\n\nThe problem is that the semantics of fileLength() are not defined. As proposed in the thread, we'll define the following semantics:\n\n* Returns the length of the file denoted by <code>name</code> if the file exists. The return value may be anything between 0 and Long.MAX_VALUE.\n* Throws FileNotFoundException if the file does not exist. Note that you can call dir.fileExists(name) if you are not sure whether the file exists or not.\n\nFor backwards we'll create a new method w/ clear semantics. Something like:\n\n{code}\n/**\n * @deprecated the method will become abstract when #fileLength(name) has been removed.\n */\npublic long getFileLength(String name) throws IOException {\n  long len = fileLength(name);\n  if (len == 0 && !fileExists(name)) {\n    throw new FileNotFoundException(name);\n  }\n  return len;\n}\n{code}\n\nThe first line just calls the current impl. If it throws exception for a non-existing file, we're ok. The second line verifies whether a 0 length is for an existing file or not and throws an exception appropriately.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1015",
        "summary": "FieldCache should support longs and doubles",
        "description": "Would be nice if FieldCache supported longs and doubles",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-864",
        "summary": "contrib/benchmark files need eol-style set",
        "description": "The following files in contrib/benchmark don't have eol-style set to native, so when they are checked out, they don't get converted.\n\n./build.xml:                    \n./CHANGES.txt:                                                             \n./conf/sample.alg:                                                                                \n./conf/standard.alg:                                                                           \n./conf/sloppy-phrase.alg:                                                                                 \n./conf/deletes.alg:                                                                                         \n./conf/micro-standard.alg:                                                                   \n./conf/compound-penalty.alg:                                                                          \n",
        "label": "NUG",
        "classified": "TASK",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-721",
        "summary": "Code coverage reports",
        "description": "Hi all,\n\nWe should be able to measure the code coverage of our unit testcases. I believe it would be very helpful for the committers, if they could verify before committing a patch if it does not reduce the coverage. \n\nFurthermore people could take a look in the code coverage reports to figure out where work needs to be done, i. e. where additional testcases are neccessary. It would be nice if we could add a page to the Lucene website showing the report, generated by the nightly build. Maybe you could add that to your preview page (LUCENE-707), Grant?\n\nI attach a patch here that uses the tool EMMA to generate the code coverage reports. EMMA is a very nice open-source tool released under the CPL (same license as junit). The patch adds three targets to common-build.xml: \n- emma-check: verifys if both emma.jar and emma_ant.jar are in the ant classpath \n- emma-instrument: instruments the compiled code \n- generate-emma-report: generates an html code coverage report \n\nThe following steps are neccessary in order to generate a code coverage report:\n- add emma.jar and emma_ant.jar to your ant classpath (download emma from http://emma.sourceforge.net/)\n- execute ant target 'emma-instrument' (depends on compile-test, so it will compile all core and test classes)\n- execute ant target 'test' to run the unit tests\n- execute ant target 'generate-emma-report'\n\nTo view the emma report open build/test/emma/index.html",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2048",
        "summary": "Omit positions but keep termFreq",
        "description": "it would be useful to have an option to discard positional information but still keep the term frequency - currently setOmitTermFreqAndPositions discards both. Even though position-dependent queries wouldn't work in such case, still any other queries would work fine and we would get the right scoring.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2667",
        "summary": "Fix FuzzyQuery's defaults, so its fast.",
        "description": "We worked a lot on FuzzyQuery, but you need to be a rocket scientist to ensure good results.\n\nThe main problem is that the default distance is 0.5f, which doesn't take into account the length of the string.\nTo add insult to injury, the default number of expansions is 1024 (traditionally from BooleanQuery maxClauseCount)\n\nI propose:\n* The syntax of FuzzyQuery is enhanced, so that you can specify raw edits too: such as foobar~2 (all terms within 2 levenshtein edits of foobar). Previously if you specified any amount >=1, you got IllegalArgumentException, so this won't break anyone. You can still use foobar~0.5, and it works just as before\n* The default for minimumSimilarity then becomes LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE, which is 2. This way if you just do foobar~, its always fast.\n* The size of the priority queue is reduced by default from 1024 to a much more reasonable value: 50. This is what FuzzyLikeThis uses.\n\nI think its best to just change the defaults for this query, since it was so aweful before. We can add notes in migrate.txt that if you care about using the old values, then you should provide them explicitly, and you will get the same results!\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2720",
        "summary": "IndexWriter should throw IndexFormatTooOldExc on open, not later during optimize/getReader/close",
        "description": "Spinoff of LUCENE-2618 and also related to the original issue LUCENE-2523...\n\nIf you open IW on a too-old index, you don't find out until much later that the index is too old.\n\nThis is because IW does not go and open segment readers on all segments.  It only does so when it's time to apply deletes, do merges, open an NRT reader, etc.\n\nThis is a serious bug because you can in fact succeed in committing with the new major version of Lucene against your too-old index, which is catastrophic because suddenly the old Lucene version will no longer open the index, and so your index becomes unusable.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2270",
        "summary": "queries with zero boosts don't work",
        "description": "Queries consisting of only zero boosts result in incorrect results.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1648",
        "summary": "when you clone or reopen an IndexReader with pending changes, the new reader doesn't commit the changes",
        "description": "While working on LUCENE-1647, I came across this issue... we are failing to carry over hasChanges, norms/deletionsDirty, etc, when cloning the new reader.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3176",
        "summary": "TestNRTThreads test failure",
        "description": "hit a fail in TestNRTThreads running tests over and over:\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3137",
        "summary": "Benchmark's ExtractReuters creates its temp dir wrongly if provided out-dir param ends by slash",
        "description": "See LUCENE-929 for context.\nAs result, it might fail to create the temp dir at all.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2507",
        "summary": "automaton spellchecker",
        "description": "The current spellchecker makes an n-gram index of your terms, and queries this for spellchecking.\nThe terms that come back from the n-gram query are then re-ranked by an algorithm such as Levenshtein.\n\nAlternatively, we could just do a levenshtein query directly against the index, then we wouldn't need\na separate index to rebuild.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3743",
        "summary": "LuceneTestCase's uncaught exceptions handler should check for AssumptionViolatedExceptions and then not trigger test failure",
        "description": "As in single-threaded tests, {{LuceneTestCase}} should not trigger test failures for {{AssumptionViolatedException}}'s when they occur in multi-threaded tests.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2328",
        "summary": "IndexWriter.synced  field accumulates data leading to a Memory Leak",
        "description": "I am running into a strange OutOfMemoryError. My small test application does\nindex and delete some few files. This is repeated for 60k times. Optimization\nis run from every 2k times a file is indexed. Index size is 50KB. I did analyze\nthe HeapDumpFile and realized that IndexWriter.synced field occupied more than\nhalf of the heap. That field is a private HashSet without a getter. Its task is\nto hold files which have been synced already.\n\nThere are two calls to addAll and one call to add on synced but no remove or\nclear throughout the lifecycle of the IndexWriter instance.\n\nAccording to the Eclipse Memory Analyzer synced contains 32618 entries which\nlook like file names \"_e065_1.del\" or \"_e067.cfs\"\n\nThe index directory contains 10 files only.\n\nI guess synced is holding obsolete data ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-228",
        "summary": "encoding of GermanAnalyzer.java and GermanStemmer.java isn't utf-8",
        "description": "For PyLucene, the gcj/swig - based python integration of java lucene, it would\nbe good if java source files didn't use encodings other than utf-8.\nOn Windows - and systems without iconv support in general - compiling code  \nwith gcj where the java source text is in another encoding than utf-8 is    \ndifficult if not impossible.\n\nTo change the encoding on these files:\n\n iconv -f iso-8859-1 -t utf-8 GermanAnalyzer.java > GermanAnalyzer.java.utf-8\n iconv -f iso-8859-1 -t utf-8 GermanStemmer.java > GermanStemmer.java.utf-8",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1217",
        "summary": "use isBinary cached variable instead of instanceof in Field",
        "description": "Field class can hold three types of values, \nSee: AbstractField.java  protected Object fieldsData = null; \n\ncurrently, mainly RTTI (instanceof) is used to determine the type of the value stored in particular instance of the Field, but for binary value we have mixed RTTI and cached variable \"boolean isBinary\" \n\nThis patch makes consistent use of cached variable isBinary.\n\nBenefit: consistent usage of method to determine run-time type for binary case  (reduces chance to get out of sync on cached variable). It should be slightly faster as well.\n\nThinking aloud: \nWould it not make sense to maintain type with some integer/byte\"poor man's enum\" (Interface with a couple of constants)\ncode:java{\npublic static final interface Type{\npublic static final byte BOOLEAN = 0;\npublic static final byte STRING = 1;\npublic static final byte READER = 2;\n....\n}\n}\n\nand use that instead of isBinary + instanceof? ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-901",
        "summary": "DefaultSimilarity.queryNorm() should never return Infinity",
        "description": "Currently DefaultSimilarity.queryNorm() returns Infinity if sumOfSquaredWeights=0.\nThis can result in a score of NaN (e. g. in TermScorer) if boost=0.0f.\n\nA simple fix would be to return 1.0f in case zero is passed in.\n\nSee LUCENE-698 for discussions about this.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-489",
        "summary": "Allow QP subclasses to support Wildcard Queries with leading \"*\"",
        "description": "It would be usefull for some users if the logic that prevents QueryParser from creating WIldcardQueries with leading wildcard characters (\"?\" or \"*\") be moved from the grammer into the base implimentation of getWildcardQuery so that it may be overridden in subclasses without needing to modifiy the grammer directly.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-252",
        "summary": "[PATCH] Problem with Sort logic on tokenized fields",
        "description": "When you set s SortField to a Text field which gets tokenized\nFieldCacheImpl uses the term to do the sort, but then sorting is off \nespecially with more then one word in the field. I think it is much \nmore logical to sort by field's string value if the sort field is Tokenized and\nstored. This way you'll get the CORRECT sort order",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2860",
        "summary": "SegmentInfo.sizeInBytes ignore includeDocStore when caching",
        "description": "I noticed that SegmentInfo's sizeInBytes cache is potentially buggy -- it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' (sizeInBytes won't include the store files) and then with 'true' (or vice versa), you won't get the right sizeInBytes (it won't re-compute, with the store files).\n\nI'll fix and add a test case demonstrating the bug.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1089",
        "summary": "Add insertWithOverflow to PriorityQueue",
        "description": "This feature proposes to add an insertWithOverflow to PriorityQueue so that callers can reuse the objects that are being dropped off the queue. Also, it changes heap to protected for easier extensibility of PQ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1395",
        "summary": "Javadoc mistake in SegmentMerger",
        "description": "",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1172",
        "summary": "Small speedups to DocumentsWriter",
        "description": "Some small fixes that I found while profiling indexing Wikipedia,\nmainly using our own quickSort instead of Arrays.sort.\n\nTesting first 200K docs of Wikipedia shows a speedup from 274.6\nseconds to 270.2 seconds.\n\nI'll commit in a day or two.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-846",
        "summary": "IOExeception can cause loss of data due to premature segment deletion",
        "description": "If you hit an IOException, e.g., disk full, while making a cfs from its constituent parts, you may not be able to rollback to the before-merge process. This happens via addIndexes.\n\nI don't have a nice easy test for this; generating IOEs ain't so easy. But it does happen in the patch for the factored merge policy with the existing tests because the pseudo-randomly generated IOEs fall in a different place.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1955",
        "summary": "Fix Hits deprecation notice",
        "description": "Just needs to be committed to 2.9 branch since hits is now removed.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1531",
        "summary": "contrib/xml-query-parser, BoostingTermQuery support",
        "description": "I'm not 100% on this patch. \n\nBooleanTermQuery is a part of the spans family, but I generally use that class as a replacement for TermQuery.  Thus in the DTD I have stated that it can be a part of the root queries as well as a part of a span. \n\nHowever, SpanFooQueries xml elements are named <SpanFoo/> rather than <SpanFooQuery/>, I have however chosen to call it <BoostingTermQuery/>. It would be possible to set it up so it would be parsed as <SpanBoostingTerm/> when inside of a <SpanSomething>, but I just find that confusing.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1414",
        "summary": "increase maxmemory for unit tests",
        "description": "We have some unit tests that require a fair amount of RAM.  But, sometimes the JRE does not give itself a very large max heap size, by default.  EG on a Mac Pro with 6 GB physical RAM, I see JRE 1.6.0 defaulting to max 80 GB and it always then hits this exception during testing:\n\n    [junit] Testcase: testHugeFile(org.apache.lucene.store.TestHugeRamFile):\tCaused an ERROR\n    [junit] Java heap space\n    [junit] java.lang.OutOfMemoryError: Java heap space\n    [junit] \tat java.util.Arrays.copyOf(Arrays.java:2760)\n    [junit] \tat java.util.Arrays.copyOf(Arrays.java:2734)\n    [junit] \tat java.util.ArrayList.ensureCapacity(ArrayList.java:167)\n    [junit] \tat java.util.ArrayList.add(ArrayList.java:351)\n    [junit] \tat org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:69)\n    [junit] \tat org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:129)\n    [junit] \tat org.apache.lucene.store.RAMOutputStream.writeBytes(RAMOutputStream.java:115)\n    [junit] \tat org.apache.lucene.store.TestHugeRamFile.testHugeFile(TestHugeRamFile.java:68)\n\nThe fix is simple: add maxmemory=512M into common-build.xml.  I'll commit shortly.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3602",
        "summary": "Add join query to Lucene",
        "description": "Solr has (psuedo) join query for a while now. I think this should also be available in Lucene.  ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3668",
        "summary": "offsets issues with multiword synonyms",
        "description": "as reported on the list, there are some strange offsets with FSTSynonyms, in the case of multiword synonyms.\n\nas a workaround it was suggested to use the older synonym impl, but it has bugs too (just in a different way).\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1741",
        "summary": "Make MMapDirectory.MAX_BBUF user configureable to support chunking the index files in smaller parts",
        "description": "This is a followup for java-user thred: http://www.lucidimagination.com/search/document/9ba9137bb5d8cb78/oom_with_2_9#9bf3b5b8f3b1fb9b\n\nIt is easy to implement, just add a setter method for this parameter to MMapDir.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3469",
        "summary": "move DocumentStoredFieldsVisitor to o.a.l.document",
        "description": "when examining the changes to the field/document API, i noticed this class was in o.a.l.index\n\nI think it should be in o.a.l.document, its more intuitive packaging",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3485",
        "summary": "LuceneTaxonomyReader .decRef() may close the inner IR, renderring the LTR in a limbo.",
        "description": "TaxonomyReader which supports ref-counting, has a decRef() method which delegates to an inner IndexReader and calls its .decRef(). The latter may close the reader (if the ref is zeroes) but the taxonomy would remain 'open' which will fail many of its method calls.\n\nAlso, the LTR's .close() method does not work in the same manner as IndexReader's - which calls decRef(), and leaves the real closing logic to the decRef(). I believe this should be the right approach for the fix.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2314",
        "summary": "Add AttributeSource.copyTo(AttributeSource)",
        "description": "One problem with AttributeSource at the moment is the missing \"insight\" into AttributeSource.State. If you want to create TokenStreams that inspect cpatured states, you have no chance. Making the contents of State public is a bad idea, as it does not help for inspecting (its a linked list, so you have to iterate).\n\nAttributeSource currently contains a cloneAttributes() call, which returns a new AttrubuteSource with all current attributes cloned. This is the (more expensive) captureState. The problem is that you cannot copy back the cloned AS (which is the restoreState). To use this behaviour (by the way, ShingleMatrix can use it), one can alternatively use cloneAttributes and copyTo. You can easily change the cloned attributes and store them in lists and copy them back. The only problem is lower performance of these calls (as State is a very optimized class).\n\nOne use case could be:\n{code}\nAttributeSource state = cloneAttributes();\n// .... do something ...\nstate.getAttribute(TermAttribute.class).setTermBuffer(foobar);\n// ... more work\nstate.copyTo(this);\n{code}",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2601",
        "summary": "Make getAttribute(Class attClass) Generic",
        "description": "org.apache.lucene.util.AttributeSource\n\ncurrent:\npublic Attribute getAttribute(Class attClass) {\n    final Attribute att = (Attribute) this.attributes.get(attClass);\n    if (att == null) {\n      throw new IllegalArgumentException(\"This AttributeSource does not have the attribute '\" + attClass.getName() + \"'.\");\n    }\n    return att;\n}\nsample usage:\nTermAttribute termAtt = (TermAttribute)ts.getAttribute(TermAttribute.class)\n\n\nmy improvment:\n@SuppressWarnings(\"unchecked\")\n\tpublic <T> T getAttribute2(Class<? extends Attribute> attClass) {\n    final T att = (T) this.attributes.get(attClass);\n    if (att == null) {\n      throw new IllegalArgumentException(\"This AttributeSource does not have the attribute '\" + attClass.getName() + \"'.\");\n    }\n    return att;\n }\nsample usage:\nTermAttribute termAtt = ts.getAttribute(TermAttribute.class)",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1488",
        "summary": "multilingual analyzer based on icu",
        "description": "The standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts.  This is because it is unaware of unicode bounds properties.\n\nI actually couldn't figure out how the Thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the Thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you'd basically be reimplementing the bounds properties already stated in the unicode standard. \n\nin general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. While most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). \n\nI've got a partially tested standardanalyzer that uses icu Rule-based BreakIterator instead of jflex. Using this method you can define word boundaries according to the unicode bounds properties. After getting it into some good shape i'd be happy to contribute it for contrib but I wonder if theres a better solution so that out of box lucene will be more friendly to non-ASCII text. Unfortunately it seems jflex does not support use of these properties such as [\\p{Word_Break = Extend}] so this is probably the major barrier.\n\nThanks,\nRobert\n\n\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-974",
        "summary": "Remove Author tags from code",
        "description": "Remove all author tags from the code.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-1343",
        "summary": "A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.",
        "description": "The ISOLatin1AccentFilter takes Unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed.  For example \u00e9 becomes e.  However another equally valid way of representing an accented character in Unicode is to have the unaccented character followed by a non-spacing modifier character (like this:  \u00e9  )    The ISOLatin1AccentFilter doesn't handle the accents in decomposed unicode characters at all.    Additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character  such as  \u0141  but which to make searching easier you want to fold onto the latin1  lookalike  version   L  .   \n\nThe UnicodeNormalizationFilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like ( \u0141  -> L )",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2758",
        "summary": "TestPerFieldCodecSupport intermittent fail",
        "description": "{noformat}\n\n    [junit] Testsuite: org.apache.lucene.index.TestPerFieldCodecSupport\n    [junit] Testcase: testChangeCodecAndMerge(org.apache.lucene.index.TestPerFieldCodecSupport):\tFAILED\n    [junit] expected:<4> but was:<3>\n    [junit] junit.framework.AssertionFailedError: expected:<4> but was:<3>\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:881)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:847)\n    [junit] \tat org.apache.lucene.index.TestPerFieldCodecSupport.assertHybridCodecPerField(TestPerFieldCodecSupport.java:189)\n    [junit] \tat org.apache.lucene.index.TestPerFieldCodecSupport.testChangeCodecAndMerge(TestPerFieldCodecSupport.java:145)\n    [junit] \n    [junit] \n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.416 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestPerFieldCodecSupport -Dtestmethod=testChangeCodecAndMerge -Dtests.seed=1508266713336297966:-102145263724760840\n    [junit] NOTE: test params are: codec=SimpleText, locale=ms, timezone=America/Winnipeg\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestPerFieldCodecSupport]\n    [junit] ------------- ---------------- ---------------\n    [junit] Test org.apache.lucene.index.TestPerFieldCodecSupport FAILED\n{noformat}\n\nI haven't tried to figure it out yet...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1571",
        "summary": "DistanceFilter problem with deleted documents",
        "description": "I know this is the locallucene lib, but wanted to make sure we don't get this bug when it gets into lucene contrib.\n\nI suspect that the issue is that deleted documents are trying to be evaluated by the filter.  I did some debugging and I confirmed that it is bombing on a document that is marked as deleted (using Luke).\n\n\nThanks!\n\nUsing the locallucene library 1.51, I get a NullPointerException at line 123 of DistanceFilter\nThe method is \tpublic BitSet bits(IndexReader reader) \nThe line is double x = NumberUtils.SortableStr2double(sx);\n\nThe stack trace is:\njava.lang.NullPointerException\n\tat org.apache.solr.util.NumberUtils.SortableStr2long(NumberUtils.java:149)\n\tat org.apache.solr.util.NumberUtils.SortableStr2double(NumberUtils.java:104)\n\tat com.pjaol.search.geo.utils.DistanceFilter.bits(DistanceFilter.java:123)\n\tat org.apache.lucene.search.Filter.getDocIdSet(Filter.java:49)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:140)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)\n\tat org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)\n\tat org.apache.lucene.search.Hits.<init>(Hits.java:90)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:72)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1456",
        "summary": "FieldInfo omitTerms bug",
        "description": "Around line 95 you have:\n\n    if (this.omitTf != omitTf) {\n      this.omitTf = true;                // if one require omitTf at least once, it remains off for life\n    }\n\nBoth references of the omitTf booleans in the if statement refer to the same field. I am guessing its meant to be other.omitTf like the norms code above it.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2476",
        "summary": "Constructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtained",
        "description": "Constructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtained.\n\nThe init method in IndexWriter catches IOException only (I got NegativeArraySize by reading up a _corrupt_ index), and now, there is no way to recover, since the writeLock will be kept obtained. Moreover, I don't have IndexWriter instance either, to \"grab\" the lock somehow, since the init() method is called from IndexWriter constructor.\n\nEither broaden the catch to all exceptions, or at least provide some circumvention to clear up. In my case, I'd like to \"fallback\", just delete the corrupted index from disk and recreate it, but it is impossible, since the LOCK_HELD NativeFSLockFactory's entry about obtained WriteLock is _never_ cleaned out and is no (at least apparent) way to clean it out forcibly. I can't create new IndexWriter, since it will always fail with LockObtainFailedException.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2397",
        "summary": "SnapshotDeletionPolicy.snapshot() throws NPE if no commits happened",
        "description": "SDP throws NPE if no commits occurred and snapshot() was called. I will replace it w/ throwing IllegalStateException. I'll also move TestSDP from o.a.l to o.a.l,index. I'll post a patch soon",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-914",
        "summary": "Scorer.skipTo(current) remains on current for some scorers",
        "description": "Background in http://www.nabble.com/scorer.skipTo%28%29-contr-tf3880986.html\n\nIt appears that several scorers do not strictly follow the spec of Scorer.skipTo(n), and skip to current location remain in current location whereas the spec says: \"beyond current\". \n\nWe should (probably) either relax the spec or fix the implementations.",
        "label": "NUG",
        "classified": "SPEC",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2389",
        "summary": "Enforce TokenStream impl / Analyzer finalness by an assertion",
        "description": "As noted in LUCENE-1753 and other issues, TokenStream and Analyzers are based on the decorator pattern. At least all TokenStream and Analyzer implementations in Lucene and Solr should be final.\n\nThe attached patch adds an assertion to the ctors of both classes that does the corresponding checks:\n- Analyzers must be final or private classes or anonymous inner classes\n- TokenStreams must be final or private classes or anonymous inner classes or have a final incrementToken()\n\nI will commit this after robert have fixed solr streams.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2032",
        "summary": "Spatial Filters not Serializable",
        "description": "I am using Lucene in a distributed setup. \n\nThe Filters in the spatial project aren't Serializable even though it inherits it from Filter. Filter is a Serializable class. \n\nDistanceFilter contains the non-Serializable class WeakHashMap.\nCartesianShapeFilter contains the non-Serializable class java.util.logging.Logger\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3071",
        "summary": "PathHierarchyTokenizer adaptation for urls: splits reversed",
        "description": "{{PathHierarchyTokenizer}} should be usable to split urls the a \"reversed\" way (useful for faceted search against urls):\n{{www.site.com}} -> {{www.site.com, site.com, com}}\n\nMoreover, it should be able to skip a given number of first (or last, if reversed) tokens:\n{{/usr/share/doc/somesoftware/INTERESTING/PART}}\nShould give with 4 tokens skipped:\n{{INTERESTING}}\n{{INTERESTING/PART}}",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-482",
        "summary": "JE Directory Implementation",
        "description": "I've created a port of DbDirectory to JE",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3736",
        "summary": "ParallelReader is now atomic, rename to ParallelAtomicReader and also add a ParallelCompositeReader (that requires LogDocMergePolicy to have identical subreader structure)",
        "description": "The plan is:\n- Move all subreaders to ctor (builder-like API. First build reader-set, then call build)\n- Rename ParallelReader to ParallelAtomicReader\n- Add a ParallelCompositeReader with same builder API, but taking any CompositeReader-set and checks them that they are aligned (docStarts identical). The subreaders are ParallelAtomicReaders.",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-214",
        "summary": "[PATCH] Field.toString could be more helpful",
        "description": "org.apache.lucene.document.Field.toString defaults to using Object.toString\nfor some sensible fields. e.g. !isStored && isIndexed && !isTokenized\nfields. This makes debugging slightly more difficult than is really needed.\n\nPlease find pasted below possible alternative:\n\n /** Prints a Field for human consumption. */\n  public final String toString() {\n  \tStringBuffer result = new StringBuffer();\n  \tif (isStored) {\n  \t\tif (isIndexed) {\n  \t\t\tif (isTokenized) {\n  \t\t\t\tresult.append(\"Text\");\n  \t\t\t} else {\n  \t\t\t\tresult.append(\"Keyword\");\n  \t\t\t}\n  \t\t} else {\n\t\t\t// XXX warn on tokenized not indexed?\n  \t\t\tresult.append(\"Unindexed\");\n  \t\t}\n  \t} else {\n  \t\tif (isIndexed) {\n  \t\t\tif (isTokenized) {\n  \t\t\t\tresult.append(\"Unstored\");\n  \t\t\t} else {\n  \t\t\t\tresult.append(\"UnstoredUntokenized\");\n  \t\t\t}\n  \t\t} else {\n\t\t\tresult.append(\"Nonsense_UnstoredUnindexed\");\n  \t\t}\n  \t}\n  \t\n  \tresult.append('<');\n  \tresult.append(name);\n  \tresult.append(':');\n  \tif (readerValue != null) {\n  \t\tresult.append(readerValue.toString());\n  \t} else {\n  \t\tresult.append(stringValue);\n  \t}\n  \tresult.append('>');\n  \treturn result.toString();\n  }\n\n\nNB Im working against CVS HEAD",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2876",
        "summary": "Remove Scorer.getSimilarity()",
        "description": "Originally this was part of the patch for per-field Similarity (LUCENE-2236), but I pulled it \nout here as its own issue as its really mostly unrelated. I also like it as a separate issue \nto apply the deprecation to branch_3x to just make less surprises/migration hassles for 4.0 users.\n\nCurrently Scorer takes a confusing number of ctors, either a Similarity, or a Weight + Similarity.\nAlso, lots of scorers don't use the Similarity at all, and its not really needed in Scorer itself.\nAdditionally, the Weight argument is often null. The Weight makes sense to be here in Scorer, \nits the parent that created the scorer, and used by Scorer itself to support LUCENE-2590's features.\nBut I dont think all queries work with this feature correctly right now, because they pass null.\n\nFinally the situation gets confusing if you start to consider delegators like ScoreCachingWrapperScorer,\nwhich arent really delegating correctly so I'm unsure features like LUCENE-2590 aren't working with this.\n\nSo I think we should remove the getSimilarity, if your scorer uses a Similarity its already coming\nto you via your ctor from your Weight and you can manage this yourself.\n\nAlso, all scorers should pass the Weight (parent) that created them, and this should be Scorer's only ctor.\nI fixed all core/contrib/solr Scorers (even the internal ones) to pass their parent Weight, just for consistency\nof this visitor interface. The only one that passes null is Solr's ValueSourceScorer.\n\nI set fix-for 3.1, not because i want to backport anything, only to mark the getSimilarity deprecated there.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2932",
        "summary": "clean up obselete information on the website",
        "description": "When searching for information on 'lucene indexing speed' I get back some really out of date stuff:\n1. on the features page it proudly proclaims 20MB/minute, on some really old hardware. I think we should\nchange this to 95GB/hour: http://blog.mikemccandless.com/2010/09/lucenes-indexing-is-fast.html\n2. there are ancient benchmarks results from versioned data we link to the website. We list versioned\nwebsites for ancient versions going back to 1.4.3. Also i noticed when just casually googling for\nAPI documentation I tend to get results going to these ancient versions. I think we should remove\nstuff for all versions prior to 2.9",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2970",
        "summary": "SpecialOperations.isFinite can have TERRIBLE TERRIBLE runtime in certain situations",
        "description": "in an application of mine, i experienced some very slow query times with finite automata (all the DFAs are acyclic)\n\nIt turned out, the slowdown is some terrible runtime in SpecialOperations.isFinite <-- this is used to determine if the DFA is acyclic or not.\n\n(in this case I am talking about even up to minutes of cpu).\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3328",
        "summary": "Specialize BooleanQuery if all clauses are TermQueries",
        "description": "During work on LUCENE-3319 I ran into issues with BooleanQuery compared to PhraseQuery in the exact case. If I disable scoring on PhraseQuery and bypass the position matching, essentially doing a conjunction match, ExactPhraseScorer beats plain boolean scorer by 40% which is a sizeable gain. I converted a ConjunctionScorer to use DocsEnum directly but still didn't get all the 40% from PhraseQuery. Yet, it turned out with further optimizations this gets very close to PhraseQuery. The biggest gain here came from converting the hand crafted loop in ConjunctionScorer#doNext to a for loop which seems to be less confusing to hotspot. In this particular case I think code specialization makes lots of sense since BQ with TQ is by far one of the most common queries.\n\nI will upload a patch shortly",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3653",
        "summary": "Lucene Search not scalling",
        "description": "I've noticed that when doing thousands of searches in a single thread the average time is quite low i.e. a few milliseconds. When adding more concurrent searches doing exactly the same search the average time increases drastically. \nI've profiled the search classes and found that the whole of lucene blocks on \n\norg.apache.lucene.index.SegmentCoreReaders.getTermsReader\norg.apache.lucene.util.VirtualMethod\n  public synchronized int getImplementationDistance \norg.apache.lucene.util.AttributeSourcew.getAttributeInterfaces\n\nThese cause search times to increase from a few milliseconds to up to 2 seconds when doing 500 concurrent searches on the same in memory index. Note: That the index is not being updates at all, so not refresh methods are called at any stage.\n\n\nSome questions:\n  Why do we need synchronization here?\n  There must be a non-lockable solution for these, they basically cause lucene to be ok for single thread applications but disastrous for any concurrent implementation.\n\nI'll do some experiments by removing the synchronization from the methods of these classes.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3438",
        "summary": "Move some *TermsEnum.java from oal.search to oal.index",
        "description": "I think FilteredTermsEnum, SingleTermsEnum should move?\n\nI left TermRangeTermsEnum and FuzzyTermsEnum and PrefixTermsEnum since they seemed search specific.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1984",
        "summary": "DisjunctionMaxQuery - Type safety  ",
        "description": "DisjunctionMaxQuery code has containers that are not type-safe . The comments indicate type-safety though. \n\nBetter to express in the API and the internals the explicit type as opposed to type-less containers. \n\nPatch attached. \n\nComments / backward compatibility concerns welcome.  ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3102",
        "summary": "Few issues with CachingCollector",
        "description": "CachingCollector (introduced in LUCENE-1421) has few issues:\n# Since the wrapped Collector may support out-of-order collection, the document IDs cached may be out-of-order (depends on the Query) and thus replay(Collector) will forward document IDs out-of-order to a Collector that may not support it.\n# It does not clear cachedScores + cachedSegs upon exceeding RAM limits\n# I think that instead of comparing curScores to null, in order to determine if scores are requested, we should have a specific boolean - for clarity\n# This check \"if (base + nextLength > maxDocsToCache)\" (line 168) can be relaxed? E.g., what if nextLength is, say, 512K, and I cannot satisfy the maxDocsToCache constraint, but if it was 10K I would? Wouldn't we still want to try and cache them?\n\nAlso:\n* The TODO in line 64 (having Collector specify needsScores()) -- why do we need that if CachingCollector ctor already takes a boolean \"cacheScores\"? I think it's better defined explicitly than implicitly?\n\n* Let's introduce a factory method for creating a specialized version if scoring is requested / not (i.e., impl the TODO in line 189)\n\n* I think it's a useful collector, which stands on its own and not specific to grouping. Can we move it to core?\n\n* How about using OpenBitSet instead of int[] for doc IDs?\n** If the number of hits is big, we'd gain some RAM back, and be able to cache more entries\n** NOTE: OpenBitSet can only be used for in-order collection only. So we can use that if the wrapped Collector does not support out-of-order\n\n* Do you think we can modify this Collector to not necessarily wrap another Collector? We have such Collector which stores (in-memory) all matching doc IDs + scores (if required). Those are later fed into several processes that operate on them (e.g. fetch more info from the index etc.). I am thinking, we can make CachingCollector *optionally* wrap another Collector and then someone can reuse it by setting RAM limit to unlimited (we should have a constant for that) in order to simply collect all matching docs + scores.\n\n* I think a set of dedicated unit tests for this class alone would be good.\n\nThat's it so far. Perhaps, if we do all of the above, more things will pop up.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1296",
        "summary": "Allow use of compact DocIdSet in CachingWrapperFilter",
        "description": "Extends CachingWrapperFilter with a protected method to determine the DocIdSet to be cached.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3461",
        "summary": "Adding same IndexDocValuesField twice trips assert",
        "description": "Doc values fields are single-valued by design, ie a given field name can only occur once in the document.\n\nBut if you accidentally add it more than once, you get an assert error, which is spooky because if you run w/o asserts maybe something eviler happens.\n\nI think we should explicitly check for this and throw clear exc since user could easily do this by accident.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1425",
        "summary": "Add ConstantScore highlighting support to SpanScorer",
        "description": "Its actually easy enough to support the family of constantscore queries with the new SpanScorer. This will also remove the requirement that you rewrite queries against the main index before highlighting (in fact, if you do, the constantscore queries will not highlight).",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2560",
        "summary": "random analyzer tests",
        "description": "we have been finding+fixing lots of bugs by randomizing lucene tests.\nin r966878 I added a variant of random unicode string that gives you a random string within the same unicode block (for other purposes)\n\nI think we should use this to test the analyzers better, for example we should pound tons of random greek strings against the greek analyzer and at least make sure there aren't exceptions.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1861",
        "summary": "Add contrib libs to classpath for javadoc",
        "description": "I don't know Ant well enough to just do this easily, so I've labeled a wish - would be nice to get rid of all the errors/warnings that not finding these classes generates when building javadoc.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": ""
    },
    {
        "key": "LUCENE-2933",
        "summary": "Two-stage state expansion for the FST: distance-from-root and child-count criteria.",
        "description": "In the current implementation FST states are expanded into a binary search on labels (from a vector of transitions) when the child count of a state exceeds a given predefined threshold (NUM_ARCS_FIXED_ARRAY). This threshold affects automaton size and traversal speed (as it turns out when benchmarked). For some degenerate (?) data sets, close-to-the-root nodes could have a small number of children (below the threshold) and yet be traversed on every single seek.\n\nA fix of this is to introduce two control thresholds: \n  EXPAND state if (distance-from-root <= MIN_DISTANCE || children-count >= NUM_ARCS_FIXED_ARRAY)\n\nMy plan is to create a data set that will prove this first and then to implement the workaround above.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3490",
        "summary": "Restructure codec hierarchy",
        "description": "Spinoff of LUCENE-2621. (Hoping we can do some of the renaming etc here in a rote way to make progress).\n\nCurrently Codec.java only represents a portion of the index, but there are other parts of the index \n(stored fields, term vectors, fieldinfos, ...) that we want under codec control. There is also some \ninconsistency about what a Codec is currently, for example Memory and Pulsing are really just \nPostingsFormats, you might just apply them to a specific field. On the other hand, PreFlex actually\nis a Codec: it represents the Lucene 3.x index format (just not all parts yet). I imagine we would\nlike SimpleText to be the same way.\n\nSo, I propose restructuring the classes so that we have something like:\n* CodecProvider <-- dead, replaced by java ServiceProvider mechanism. All indexes are 'readable' if codecs are in classpath.\n* Codec <-- represents the index format (PostingsFormat + FieldsFormat + ...)\n* PostingsFormat: this is what Codec controls today, and Codec will return one of these for a field.\n* FieldsFormat: Stored Fields + Term Vectors + FieldInfos?\n\nI think for PreFlex, it doesnt make sense to expose its PostingsFormat as a 'public' class, because preflex\ncan never be per-field so there is no use in allowing you to configure PreFlex for a specific field.\nSimilarly, I think in the future we should do the same thing for SimpleText. Nobody needs SimpleText for production, it should\njust be a Codec where we try to make as much of the index as plain text and simple as possible for debugging/learning/etc.\nSo we don't need to expose its PostingsFormat. On the other hand, I don't think we need Pulsing or Memory codecs,\nbecause its pretty silly to make your entire index use one of their PostingsFormats. To parallel with analysis:\nPostingsFormat is like Tokenizer and Codec is like Analyzer, and we don't need Analyzers to \"show off\" every Tokenizer.\n\nwe can also move the baked in PerFieldCodecWrapper out (it would basically be PerFieldPostingsFormat). Privately it would\nwrite the ids to the file like it does today. in the future, all 3.x hairy backwards code would move to PreflexCodec. \nSimpleTextCodec would get a plain text fieldinfos impl, etc.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3081",
        "summary": "Document Maven nightly builds, artifact generation, and using Maven to build Lucene/Solr",
        "description": "There should be documentation we can point people to when they ask how to use Maven with Lucene and Solr.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-803",
        "summary": "add svn ignores for eclipse artifacts",
        "description": "Be nice to ignore the files eclipse puts into the project root as we do the .idea file for intellij.\n\nThe two files are\n\n.project\n.classpath\n\nI'm gonna lie and say there's a patch available for this because an svn diff patch with propery changes can't be applied with patch anyway.",
        "label": "NUG",
        "classified": "OTHER",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2885",
        "summary": "Add WaitForMergesTask",
        "description": "When building an index, if you just .close the IW, you may leave merges still needing to be done... so a WaitForMerges task lets algs fix this.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2343",
        "summary": "Add support for benchmarking Collectors",
        "description": "As the title says.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1770",
        "summary": "EnwikiQueryMaker",
        "description": "",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2671",
        "summary": "Add sort missing first/last ability to SortField and ValueComparator",
        "description": "When SortField and ValueComparator use EntryCreators (from LUCENE-2649) they use a special sort value when the field is missing.\n\nThis enables lucene to implement 'sort missing last' or 'sort missing first' for numeric values from the FieldCache.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2739",
        "summary": "Refactor TestIndexWriter",
        "description": "TestIndexWriter is getting a bit unwieldy:\n* 5,315 lines of code\n* 116 test methods\n* runtimes frequently between 1 and 2 minutes.\n\nIt starts to be pretty scary to merge changes because its so massive.\n\nA lot of the tests arguably belong somewhere else (e.g. the addIndex* tests belong in TestAddIndexes)\n\nBut here is a start:\n# Pulls out the *OnDiskFull tests into TestIndexWriterOnDiskFull\n# Pulls out the multithreaded tests into TestIndexWriterWithThreads\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3290",
        "summary": "add FieldInvertState.numUniqueTerms, Terms.sumDocFreq",
        "description": "For scoring systems like lnu.ltc (http://trec.nist.gov/pubs/trec16/papers/ibm-haifa.mq.final.pdf), we need to supply 3 stats:\n* average tf within d\n* # of unique terms within d\n* average number of unique terms across field\n\nIf we add FieldInvertState.numUniqueTerms, you can incorporate the first two into your norms/docvalues (once we cut over),\nthe average tf within d being length / numUniqueTerms.\n\nto compute the average across the field, we can just write the sum of all terms' docfreqs into the terms dictionary header,\nand you can then divide this by maxdoc to get the average.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2574",
        "summary": "Optimize copies between IndexInput and Output",
        "description": "We've created an optimized copy of files from Directory to Directory. We've also optimized copyBytes recently. However, we're missing the opposite side of the copy - from IndexInput to Output. I'd like to mimic the FileChannel API by having copyTo on IndexInput and copyFrom on IndexOutput. That way, both sides can optimize the copy process, depending on the type of the IndexInput/Output that they need to copy to/from.\n\nFSIndexInput/Output can use FileChannel if the two are FS types. RAMInput/OutputStream can copy to/from the buffers directly, w/o going through intermediate ones. Actually, for RAMIn/Out this might be a big win, because it doesn't care about the type of IndexInput/Output given - it just needs to copy to its buffer directly.\n\nIf we do this, I think we can consolidate all Dir.copy() impls down to one (in Directory), and rely on the In/Out ones to do the optimized copy. Plus, it will enable someone to do optimized copies between In/Out outside the scope of Directory.\n\nIf this somehow turns out to be impossible, or won't make sense, then I'd like to optimize RAMDirectory.copy(Dir, src, dest) to not use an intermediate buffer.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1451",
        "summary": "Can't create NIOFSDirectory w/o setting a system property",
        "description": "NIOFSDirectory.getDirectory() returns a FSDirectory object",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3388",
        "summary": "TestTermsEnum.testIntersectRandom fail",
        "description": "{noformat}\n    [junit] Testsuite: org.apache.lucene.index.TestTermsEnum\n    [junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):\tFAILED\n    [junit] (null)\n    [junit] junit.framework.AssertionFailedError\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)\n    [junit] \tat org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.getState(BlockTreeTermsReader.java:894)\n    [junit] \tat org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.seekToStartTerm(BlockTreeTermsReader.java:969)\n    [junit] \tat org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.<init>(BlockTreeTermsReader.java:786)\n    [junit] \tat org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader.intersect(BlockTreeTermsReader.java:483)\n    [junit] \tat org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:293)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)\n    [junit] \n    [junit] \n    [junit] Tests run: 6, Failures: 1, Errors: 0, Time elapsed: 14.762 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestTermsEnum -Dtestmethod=testIntersectRandom -Dtests.seed=320d0949741fc6b1:3cabdb9b04d0d243:-4b7c80572775ed92 -Dtests.multiplier=3\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-644",
        "summary": "Contrib: another highlighter approach",
        "description": "Mark Harwoods highlighter package is a great contribution to Lucene, I've used it a lot! However, when you have *large* documents (fields), highlighting can be quite time consuming if you increase the number of bytes to analyze with setMaxDocBytesToAnalyze(int). The default value of 50k is often too low for indexed PDFs etcetera, which results in empty highlight strings.\n\nThis is an alternative approach using term position vectors only to build fragment info objects. Then a StringReader can read the relevant fragments and skip() between them. This is a lot faster. Also, this method uses the *entire* field for finding the best fragments so you're always guaranteed to get a highlight snippet.\n\nBecause this method only works with fields which have term positions stored one can check if this method works for a particular field using following code (taken from TokenSources.java):\n\n        TermFreqVector tfv = (TermFreqVector) reader.getTermFreqVector(docId, field);\n        if (tfv != null && tfv instanceof TermPositionVector)\n        {\n          // use FulltextHighlighter\n        }\n        else\n        {\n          // use standard Highlighter\n        }\n\nSomeone else might find this useful so I'm posting the code here.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3414",
        "summary": "Bring Hunspell for Lucene into analysis module",
        "description": "Some time ago I along with Robert and Uwe, wrote an Stemmer which uses the Hunspell algorithm.  It has the benefit of supporting dictionaries for a wide array of languages.   \n\nIt seems to still be being used but has fallen out of date.  I think it would benefit from being inside the analysis module where additional features such as decompounding support, could be added.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-583",
        "summary": "ISOLatin1AccentFilter discards position increments of filtered terms",
        "description": "Not sure if this is a bug, but looks like one to me...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1182",
        "summary": "SimilarityDelegator is missing a delegating scorePayload() method",
        "description": "The handy SimilarityDelegator method is missing a scoreDelegator() delegating method.\nThe fix is trivial, add the code below at the end of the class:\n\n  public float scorePayload(String fieldName, byte [] payload, int offset, int length)\n  {\n      return delegee.scorePayload(fieldName, payload, offset, length);\n  }\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1044",
        "summary": "Behavior on hard power shutdown",
        "description": "When indexing a large number of documents, upon a hard power failure  (e.g. pull the power cord), the index seems to get corrupted. We start a Java application as an Windows Service, and feed it documents. In some cases (after an index size of 1.7GB, with 30-40 index segment .cfs files) , the following is observed.\n\nThe 'segments' file contains only zeros. Its size is 265 bytes - all bytes are zeros.\nThe 'deleted' file also contains only zeros. Its size is 85 bytes - all bytes are zeros.\n\nBefore corruption, the segments file and deleted file appear to be correct. After this corruption, the index is corrupted and lost.\n\nThis is a problem observed in Lucene 1.4.3. We are not able to upgrade our customer deployments to 1.9 or later version, but would be happy to back-port a patch, if the patch is small enough and if this problem is already solved.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1142",
        "summary": "Updated Snowball package",
        "description": "Updated Snowball contrib package\n\n * New org.tartarus.snowball java package with patched SnowballProgram to be abstract to avoid using reflection.\n * Introducing Hungarian, Turkish and Romanian stemmers\n * Introducing constructor SnowballFilter(SnowballProgram)\n\nIt is possible there have been some changes made to the some of there stemmer algorithms between this patch and the current SVN trunk of Lucene, an index might thus not be compatible with new stemmers!\n\nThe API is backwards compatibile and the test pass.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-715",
        "summary": "IndexWriter does not release its write lock when trying to open an index which does not yet exist",
        "description": "In version 2.0.0, the private IndexWriter constructor does not properly remove its write lock in the event of an error. This can be seen when one attempts to open (not create) an index in a directory which exists, but in which there is no segments file. Here is the offending code:\n\n    247   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)\n    248     throws IOException {\n    249       this.closeDir = closeDir;\n    250       directory = d;\n    251       analyzer = a;\n    252 \n    253       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);\n    254       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock\n    255         throw new IOException(\"Index locked for write: \" + writeLock);\n    256       this.writeLock = writeLock;                   // save it\n    257 \n    258       synchronized (directory) {        // in- & inter-process sync\n    259         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {\n    260             public Object doBody() throws IOException {\n    261               if (create)\n    262                 segmentInfos.write(directory);\n    263               else\n    264                 segmentInfos.read(directory);\n    265               return null;\n    266             }\n    267           }.run();\n    268       }\n    269   }\n\nOn line 254, a write lock is obtained by the constructor. If an exception is raised inside the doBody() method (on line 260), then that exception is propagated, the constructor will fail, but the lock is not released until the object is garbage collected. This is typically an issue except when using the IndexModifier class.\n\nAs of the filing of this bug, this has not yet been fixed in the trunk (IndexWriter.java#472959):\n\n    251   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)\n    252     throws IOException {\n    253       this.closeDir = closeDir;\n    254       directory = d;\n    255       analyzer = a;\n    256 \n    257       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);\n    258       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock\n    259         throw new IOException(\"Index locked for write: \" + writeLock);\n    260       this.writeLock = writeLock;                   // save it\n    261 \n    262       synchronized (directory) {        // in- & inter-process sync\n    263         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {\n    264             public Object doBody() throws IOException {\n    265               if (create)\n    266                 segmentInfos.write(directory);\n    267               else\n    268                 segmentInfos.read(directory);\n    269               return null;\n    270             }\n    271           }.run();\n    272       }\n    273   }",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-423",
        "summary": "thread pool implementation of parallel queries",
        "description": "This component is a replacement for ParallelMultiQuery that runs a thread pool\nwith queue instead of starting threads for every query execution (so its\nperformance is better).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3606",
        "summary": "Make IndexReader really read-only in Lucene 4.0",
        "description": "As we change API completely in Lucene 4.0 we are also free to remove read-write access and commits from IndexReader. This code is so hairy and buggy (as investigated by Robert and Mike today) when you work on SegmentReader level but forget to flush in the DirectoryReader, so its better to really make IndexReaders readonly.\n\nCurrently with IndexReader you can do things like:\n- delete/undelete Documents -> Can be done by with IndexWriter, too (using deleteByQuery)\n- change norms -> this is a bad idea in general, but when we remove norms at all and replace by DocValues this is obsolete already. Changing DocValues should also be done using IndexWriter in trunk (once it is ready)",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2964",
        "summary": "Allow benchmark tasks from alternative packages",
        "description": "Relax current limitation of all tasks in same package - that of PerfTask.\nAdd a property \"alt.tasks.packages\" - its value are comma separated full package names.\nIf the task class is not found in the default package, an attempt is made to load it from the alternate packages specified in that property.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2010",
        "summary": "Remove segments with all documents deleted in commit/flush/close of IndexWriter instead of waiting until a merge occurs.",
        "description": "I do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed:\n\n{noformat}\n4 of 14: name=_dlo docCount=5\n  compound=true\n  hasProx=true\n  numFiles=2\n  size (MB)=0.059\n  diagnostics = {java.version=1.5.0_21, lucene.version=2.9.0 817268P - 2009-09-21 10:25:09, os=SunOS,\n     os.arch=amd64, java.vendor=Sun Microsystems Inc., os.version=5.10, source=flush}\n  has deletions [delFileName=_dlo_1.del]\n  test: open reader.........OK [5 deleted docs]\n  test: fields..............OK [136 fields]\n  test: field norms.........OK [136 fields]\n  test: terms, freq, prox...OK [1698 terms; 4236 terms/docs pairs; 0 tokens]\n  test: stored fields.......OK [0 total field count; avg ? fields per doc]\n  test: term vectors........OK [0 total vector count; avg ? term/freq vector fields per doc]\n{noformat}\n\nShouldn't such segments not be removed automatically during the next commit/close of IndexWriter?\n\n*Mike McCandless:*\nLucene doesn't actually short-circuit this case, ie, if every single doc in a given segment has been deleted, it will still merge it [away] like normal, rather than simply dropping it immediately from the index, which I agree would be a simple optimization. Can you open a new issue? I would think IW can drop such a segment immediately (ie not wait for a merge or optimize) on flushing new deletes.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3044",
        "summary": "thaiwordfilter uses attributesource.copyTo incorrectly",
        "description": "The bug can be seen by https://builds.apache.org/hudson/job/Lucene-Solr-tests-only-3.x/7367/\n\nIt looks like the issue is this lazy initialization of the cloned token: if the tokenstream is reused\nand the consumer is interested in a different set of attributes, it could be a problem.\n\none probably-probably-not-totally-correct fix would be to add 'clonedToken = null;' to reset(), at \nleast it would solve this case?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-323",
        "summary": "[PATCH] MultiFieldQueryParser and BooleanQuery do not provide adequate support for queries across multiple fields",
        "description": "The attached test case demonstrates this problem and provides a fix:\n  1.  Use a custom similarity to eliminate all tf and idf effects, just to \nisolate what is being tested.\n  2.  Create two documents doc1 and doc2, each with two fields title and \ndescription.  doc1 has \"elephant\" in title and \"elephant\" in description.  \ndoc2 has \"elephant\" in title and \"albino\" in description.\n  3.  Express query for \"albino elephant\" against both fields.\nProblems:\n      a.  MultiFieldQueryParser won't recognize either document as containing \nboth terms, due to the way it expands the query across fields.\n      b.  Expressing query as \"title:albino description:albino title:elephant \ndescription:elephant\" will score both documents equivalently, since each \nmatches two query terms.\n  4.  Comparison to MaxDisjunctionQuery and my method for expanding queries \nacross fields.  Using notation that () represents a BooleanQuery and ( | ) \nrepresents a MaxDisjunctionQuery, \"albino elephant\" expands to:\n        ( (title:albino | description:albino)\n          (title:elephant | description:elephant) )\nThis will recognize that doc2 has both terms matched while doc1 only has 1 \nterm matched, score doc2 over doc1.\n\nRefinement note:  the actual expansion for \"albino query\" that I use is:\n        ( (title:albino | description:albino)~0.1\n          (title:elephant | description:elephant)~0.1 )\nThis causes the score of each MaxDisjunctionQuery to be the score of highest \nscoring MDQ subclause plus 0.1 times the sum of the scores of the other MDQ \nsubclauses.  Thus, doc1 gets some credit for also having \"elephant\" in the \ndescription but only 1/10 as much as doc2 gets for covering another query term \nin its description.  If doc3 has \"elephant\" in title and both \"albino\" \nand \"elephant\" in the description, then with the actual refined expansion, it \ngets the highest score of all (whereas with pure max, without the 0.1, it \nwould get the same score as doc2).\n\nIn real apps, tf's and idf's also come into play of course, but can affect \nthese either way (i.e., mitigate this fundamental problem or exacerbate it).",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3546",
        "summary": "IW#nrtIsCurrent retruns true if changes are in del queue but not in bufferedDeleteStream yet",
        "description": "spinnoff from SOLR-2861 - since the delete queue is not necessarily applied entirely on each request there is a chance that there are changes in the delete queue but not yet in buffered deletes. this can prevent NRT readers from reopen when they should... this shows the problematic code:\n\n{code}\nIndex: java/org/apache/lucene/index/IndexWriter.java\n===================================================================\n--- java/org/apache/lucene/index/IndexWriter.java\t(revision 1195214)\n+++ java/org/apache/lucene/index/IndexWriter.java\t(working copy)\n@@ -4074,7 +4074,7 @@\n   synchronized boolean nrtIsCurrent(SegmentInfos infos) {\n     //System.out.println(\"IW.nrtIsCurrent \" + (infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any()));\n     ensureOpen();\n-    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();\n+    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !docWriter.deleteQueue.anyChanges();\n   }\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1800",
        "summary": "QueryParser should use reusable token streams",
        "description": "Just like indexing, the query parser should use reusable token streams",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3831",
        "summary": "Passing a null fieldname to MemoryFields#terms in MemoryIndex throws a NPE",
        "description": "I found this when querying a MemoryIndex using a RegexpQuery wrapped by a SpanMultiTermQueryWrapper.  If the regexp doesn't match anything in the index, it gets rewritten to an empty SpanOrQuery with a null field value, which then triggers the NPE.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-994",
        "summary": "Change defaults in IndexWriter to maximize \"out of the box\" performance",
        "description": "This is follow-through from LUCENE-845, LUCENE-847 and LUCENE-870;\nI'll commit this once those three are committed.\n\nOut of the box performance of IndexWriter is maximized when flushing\nby RAM instead of a fixed document count (the default today) because\ndocuments can vary greatly in size.\n\nLikewise, merging performance should be faster when merging by net\nsegment size since, to minimize the net IO cost of merging segments\nover time, you want to merge segments of equal byte size.\n\nFinally, ConcurrentMergeScheduler improves indexing speed\nsubstantially (25% in a simple initial test in LUCENE-870) because it\nruns the merges in the backround and doesn't block\nadd/update/deleteDocument calls.  Most machines have concurrency\nbetween CPU and IO and so it makes sense to default to this\nMergeScheduler.\n\nNote that these changes will break users of ParallelReader because the\nparallel indices will no longer have matching docIDs.  Such users need\nto switch IndexWriter back to flushing by doc count, and switch the\nMergePolicy back to LogDocMergePolicy.  It's likely also necessary to\nswitch the MergeScheduler back to SerialMergeScheduler to ensure\ndeterministic docID assignment.\n\nI think the combination of these three default changes, plus other\nperformance improvements for indexing (LUCENE-966, LUCENE-843,\nLUCENE-963, LUCENE-969, LUCENE-871, etc.) should make for some sizable\nperformance gains Lucene 2.3!",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2516",
        "summary": "More clarification, improvements and correct behaviour of backwards tests",
        "description": "Backwards tests are used since 2.9 to assert, that the new Lucene version supports drop-in-replacement over the previous version. For this all tests from the previous version are compiled against the old version but then run against the new JAR file.\n\nAt the beginning the test suite was checking out another branch and doing this, but this was replaced in 3.1 by directly embedding the previous source tree and the previous tests into the backwards/ subdirectory of the SVN source. The whole idea has several problems:\n\n- Tests not only check *public* APIs, they also check internals and sometimes even fields or package-private methods. This is allowed to change in later versions, so we must be able to change the tests, to support this behaviour. This can be done by modifying the backwards tests to pass, but still use the public API unchanged. Sometimes we simply comment out tests, that test internals and not public APIs. For those tests, I would like to propose a Java Annotation for trunk tests like @LuceneInternalTest - so we can tell the tests runner for backwards (when this test is moved as backwards layer, e.g in 4.1, that it runs all tests *but* not this marked one. This can be done easily with Junit3/4 in LuceneTestCase(J4). This is not part of this issue, but a good idea.\n- Sometimes we break backwards compatibility. Currently we do our best to change the tests to reflect this, but this is unneeded and stupi, as it brings two problems. The backwards tests should be compiled against the old version of Lucene. If we change this old Version in the backwards folder, its suddenly becomes nonsense. At least the JAR artifacts of the previous version should stay *unchanged* in all cases! If we break backwards, the correct way to do this, is to simply disable coresponding tests! There is no need to make them work again, as we broke backwards, wy test plugin? The trunk tests already check the functionality, backwards tests only check API. If we fix the break in backwards, we do the contra of what they are for.\n\nSo I propose the following and have implemented in a patch for 3.x branch:\n\n- Only include the *tests* and nothing else into the backwards branch, no source files of previous Lucene Core.\n- Add the latest released JAR artifact of lucene-core.jar into backwards/lib, optimally with checksum (md5/sh1). This enforces that it is not changed and exactly do what they are for: To compile the previous tests against. This is the only reason for this JAR file.\n- If we break backwards, simply *disable* the tests by commenting out, ideally with a short note and the JIRA issue that shows the break.\n- If we change inner behaviour of classes, that are not public, dont fix, disable tests. Its simple: backwards tests are only for API compatibility testsing of public APIs. If a test uses internals it should not be run. For that we should use a new annotation in trunk (see above).\n\nThis has several good things:\n\n- we can package backwards tests in src ZIP. Its not a full distrib, only the core tests and the JAR file. This enables people that doenloaded the src ZIP files to also run backwrads tests\n- Your SVN checkout is not so big and backwards tests run faster!\n\nThere are some problems, with one example in the attached patch:\n\n- If we have mock classes in the tests (e.g. MockRAMDirectory) that extend Lucene classes and have access to their internal APIs, a change in these APIs will make them fail to work unchanged. The above example (MockRAMDir) is used in lots of tests and uses a internal RAMDir field that changed type in 3.1. But we cannot disable all tests using this dir (no tests will survive). As we cannot change the previous versions JAR to reflect this, you have to use some trick in this interbal test class. In this case I removed static linking of this field and replaced by reflection. This enables compilation against old JAR, but supports running in new version. This is really a special case, but works good here.\n\nAny comments?",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1869",
        "summary": "when checking tvx/fdx size mismatch, also include whether the file exists",
        "description": "IndexWriter checks, during flush and during merge, that the size of the index file for stored fields (*.fdx) and term vectors (*.tvx) matches how many bytes it has just written.\n\nThis originally was added for LUCENE-1282, ie, as a safety to catch the nasty \"off by 1\" JRE hotspot bug that would otherwise silently corrupt the index.\n\nHowever, this check also seems to catch a different case, where the size of the file is zero.   The most recent example is LUCENE-1521.  I'd like to improve the message in the exception to include whether or not the file exists, to help understand why users are sometimes hitting this exception.  My best theory at this point is something external is removing the file out from under the IndexWriter.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2599",
        "summary": "Deprecate Spatial Contrib",
        "description": "The spatial contrib is blighted by bugs.  The latest series, found by Grant and discussed [here|http://search.lucidimagination.com/search/document/c32e81783642df47/spatial_rethinking_cartesian_tiers_implementation] shows that we need to re-think the cartesian tier implementation.\n\nGiven the need to create a spatial module containing code taken from both lucene and Solr, it makes sense to deprecate the spatial contrib, and start from scratch in the new module.\n\n\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1954",
        "summary": "InitiatedIndex: CCE on casting NumericField to Field",
        "description": "An unchecked cast to List<Field> throws a ClassCastException when applied to, for example, a NumericField.\nAppearently, this has been fixed trunk, but for a 2.9.1 release, this could be helpful.\nThe patch can be applied against the 2.9.0 tag.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1213",
        "summary": "MultiFieldQueryParser ignores slop parameter",
        "description": "MultiFieldQueryParser.getFieldQuery(String, String, int) calls super.getFieldQuery(String, String), thus obliterating any slop parameter present in the query.\n\nIt should probably be changed to call super.getFieldQuery(String, String, int), except doing only that will result in a recursive loop which is a side-effect of what may be a deeper problem in MultiFieldQueryParser -- getFieldQuery(String, String, int) is documented as delegating to getFieldQuery(String, String), yet what it actually does is the exact opposite.  This also causes problems for subclasses which need to override getFieldQuery(String, String) to provide different behaviour.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3764",
        "summary": "Remove oal.util.MapBackedSet (Java 6 offsers Collections.newSetFromMap())",
        "description": "Easy search and replace job. In 3.x we still need the class, as Java 5 does not have Collections.newSetFromMap().",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2908",
        "summary": "clean up serialization in the codebase",
        "description": "We removed contrib/remote, but forgot to cleanup serialization hell everywhere.\n\nthis is no longer needed, never really worked (e.g. across versions), and slows \ndevelopment (e.g. i wasted a long time debugging stupid serialization of \nSimilarity.idfExplain when trying to make a patch for the scoring system).\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2829",
        "summary": "improve termquery \"pk lookup\" performance",
        "description": "For things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1)\nwe do wasted seeks.\n\nWhile LUCENE-2694 tries to solve some of this issue with TermState, I'm concerned we could every backport that to 3.1 for example.\n\nThis is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve LUCENE-2694,\nbut I don't think we should leave things as they are in 3.x\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3549",
        "summary": "Remove DocumentBuilder interface from facet module",
        "description": "The facet module contains an interface called DocumentBuilder, which contains a single method, build(Document) (it's a builder API). We use it in my company to standardize how different modules populate a Document object. I've included it with the facet contribution so that things will compile with as few code changes as possible.\n\nNow it's time to do some cleanup and I'd like to start with this interface. If people think that this interface is useful to reside in 'core', then I don't mind moving it there. But otherwise, let's remove it from the code. It has only one impl in the facet module: CategoryDocumentBuilder, and we can certainly do without the interface.\n\nMore so, it's under o.a.l package which is inappropriate IMO. If it's moved to 'core', it should be under o.a.l.document.\n\nIf people see any problem with that, please speak up. I will do the changes and post a patch here shortly.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-957",
        "summary": "Lucene RAM Directory doesn't work for Index Size > 8 GB",
        "description": "from user list - http://www.gossamer-threads.com/lists/lucene/java-user/50982\n\nProblem seems to be casting issues in RAMInputStream.\n\nLine 90:\n      bufferStart = BUFFER_SIZE * currentBufferIndex;\nboth rhs are ints while lhs is long.\nso a very large product would first overflow MAX_INT, become negative, and only then (auto) casted to long, but this is too late. \n\nLine 91: \n     bufferLength = (int) (length - bufferStart);\nboth rhs are longs while lhs is int.\nso the (int) cast result may turn negative and the logic that follows would be wrong.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1619",
        "summary": "TermAttribute.termLength() optimization",
        "description": "\n   public int termLength() {\n     initTermBuffer(); // This patch removes this method call \n     return termLength;\n   }\n\nI see no reason to initTermBuffer() in termLength()... all tests pass, but I could be wrong?\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-359",
        "summary": "[PATCH] add term index interval accessors",
        "description": "It should be possible for folks to set the index interval used when writing indexes.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-767",
        "summary": "maxDoc should be explicitly stored in the index, not derived from file length",
        "description": "This is a spinoff of LUCENE-140\n\nIn general we should rely on \"as little as possible\" from the file system.  Right now, maxDoc is derived by checking the file length of the FieldsReader index file (.fdx) which makes me nervous.  I think we should explicitly store it instead.\n\nNote that there are no known cases where this is actually causing a problem. There was some speculation in the discussion of LUCENE-140 that it could be one of the possible, but in digging / discussion there were no specifically relevant JVM bugs found (yet!).  So this would be a defensive fix at this point.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1582",
        "summary": "Make TrieRange completely independent from Document/Field with TokenStream of prefix encoded values",
        "description": "TrieRange has currently the following problem:\n- To add a field, that uses a trie encoding, you can manually add each term to the index or use a helper method from TrieUtils. The helper method has the problem, that it uses a fixed field configuration\n- TrieUtils currently creates per default a helper field containing the lower precision terms to enable sorting (limitation of one term/document for sorting)\n- trieCodeLong/Int() creates unnecessarily String[] and char[] arrays that is heavy for GC, if you index lot of numeric values. Also a lot of char[] to String copying is involved.\n\nThis issue should improve this:\n- trieCodeLong/Int() returns a TokenStream. During encoding, all char[] arrays are reused by Token API, additional String[] arrays for the encoded result are not created, instead the TokenStream enumerates the trie values.\n- Trie fields can be added to Documents during indexing using the standard API: new Field(name,TokenStream,...), so no extra util method needed. By using token filters, one could also add payload and so and customize everything.\n\nThe drawback is: Sorting would not work anymore. To enable sorting, a (sub-)issue can extend the FieldCache to stop iterating the terms, as soon as a lower precision one is enumerated by TermEnum. I will create a \"hack\" patch for TrieUtils-use only, that uses a non-checked Exceptionin the Parser to stop iteration. With LUCENE-831, a more generic API for this type can be used (custom parser/iterator implementation for FieldCache). I will attach the field cache patch (with the temporary solution, until FieldCache is reimplemented) as a separate patch file, or maybe open another issue for it.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-716",
        "summary": "Support unicode escapes in QueryParser",
        "description": "As suggested by Yonik in http://issues.apache.org/jira/browse/LUCENE-573 the QueryParser should be able to handle unicode escapes, i. e. \\uXXXX.\n\nI have already working and tested code. It is based on the patch i submitted for LUCENE-573, so once this is (hopefully ;-)) committed, I will submit another patch here.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1527",
        "summary": "Benchmark deletes.alg fails",
        "description": "Benchmark deletes.alg fails because the index reader defaults to open readonly.  ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-636",
        "summary": "[PATCH] Differently configured Lucene 'instances' in same JVM",
        "description": "Currently Lucene can be configured using system properties. When running multiple 'instances' of Lucene for different purposes in the same JVM, it is not possible to use different settings for each 'instance'.\n\nI made changes to some Lucene classes so you can pass a configuration to that class. The Lucene 'instance' will use the settings from that configuration. The changes do not effect the API and/or the current behavior so are backwards compatible.\n\nIn addition to the changes above I also made the SegmentReader and SegmentTermDocs extensible outside of their package. I would appreciate the inclusion of these changes but don't mind creating a separate issue for them.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2898",
        "summary": "CMS merge throttling is not aggressive enough",
        "description": "I hit this crab while working on the NRT benchmarker (in luceneutil).\n\nCMS today forcefully idles any incoming threads, when there are too many merges pending.\n\nThis is the last line of defense that it has, since it also juggles thread priorities (and forcefully idles the biggest merges) to try to reduce the outstanding merge count.\n\nBut when it cannot keep up it has no choice but to stall those threads responsible for making new segments.\n\nHowever, the logic is in the wrong place now -- the stalling happens after pulling the next merge from IW.  This is poor because it means if you have N indexing threads, you allow max + N outstanding merges.\n\nI have a simple fix, which is to just move the stall logic to before we pull the next merge from IW.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2246",
        "summary": "While indexing Turkish web pages, \"Parse Aborted: Lexical error....\" occurs",
        "description": "When I try to index Turkish page if there is a Turkish specific character in the HTML specific tag HTML parser gives \"Parse Aborted: Lexical error.on ... line\" error.\nFor this case \"<IMG SRC=\"../images/head.jpg\" WIDTH=570 HEIGHT=47 BORDER=0 ALT=\"\u015f\">\" exception address \"\u015f\" character (which has 351 ascii value) as an error. OR \u0131 character in title tag.\n<a title=\"(\u0131\u0131\u0131)\">\n\nTurkish character in the content do not create any problem.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2822",
        "summary": "TimeLimitingCollector starts thread in static {} with no way to stop them",
        "description": "See the comment in LuceneTestCase.\n\nIf you even do Class.forName(\"TimeLimitingCollector\") it starts up a thread in a static method, and there isn't a way to kill it.\n\nThis is broken.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1136",
        "summary": "add ability to not count sub-task doLogic increment to contri/benchmark",
        "description": "Sometimes, you want to run a sub-task like CloseIndex, and include the time it takes to run, but not include the count that it returns when reporting rec/s.\n\nWe could adopt this approach: if a task is preceded by a \"-\" character, then, do not count the value returned by doLogic.\n\nSee discussion leading to this here:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/57081",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3766",
        "summary": "Remove/deprecate Tokenizer's default ctor",
        "description": "I was working on a new Tokenizer... and I accidentally forgot to call super(input) (and super.reset(input) from my reset method)... which then meant my correctOffset() calls were silently a no-op; this is very trappy.\n\nFortunately the awesome BaseTokenStreamTestCase caught this (I hit failures because the offsets were not in fact being corrected).\n\nOne minimal thing we can do (but it sounds like from Robert there may be reasons why we can't) is add {{assert input != null}} in Tokenizer.correctOffset:\n\n{noformat}\nIndex: lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java\n===================================================================\n--- lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java\t(revision 1242316)\n+++ lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java\t(working copy)\n@@ -82,6 +82,7 @@\n    * @see CharStream#correctOffset\n    */\n   protected final int correctOffset(int currentOff) {\n+    assert input != null: \"subclass failed to call super(Reader) or super.reset(Reader)\";\n     return (input instanceof CharStream) ? ((CharStream) input).correctOffset(currentOff) : currentOff;\n   }\n{noformat}\n\nBut best would be to remove the default ctor that leaves input null...",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2414",
        "summary": "add icu-based tokenizer for unicode text segmentation",
        "description": "I pulled out the last part of LUCENE-1488, the tokenizer itself and cleaned it up some.\n\nThe idea is simple:\n* First step is to divide text into writing system boundaries (scripts)\n* You supply an ICUTokenizerConfig (or just use the default) which lets you tailor segmentation on a per-writing system basis.\n* This tailoring can be any BreakIterator, so rule-based or dictionary-based or your own.\n\nThe default implementation (if you do not customize) is just to do UAX#29, but with tailorings for stuff with no clear word division:\n* Thai (uses dictionary-based word breaking)\n* Khmer, Myanmar, Lao (uses custom rules for syllabification)\n\nAdditionally as more of an example i have a tailoring for hebrew that treats the punctuation special. (People have asked before\nfor ways to make standardanalyzer treat dashes differently, etc)\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3393",
        "summary": "Rename EasySimilarity to SimilarityBase",
        "description": "",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-992",
        "summary": "IndexWriter.updateDocument is no longer atomic",
        "description": "Spinoff from LUCENE-847.\n\nNing caught that as of LUCENE-843, we lost the atomicity of the delete\n+ add in IndexWriter.updateDocument.\n\nNing suggested a simple fix: move the buffered deletes into\nDocumentsWriter and let it do the delete + add atomically.  This has a\nnice side effect of also consolidating the \"time to flush\" logic in\nDocumentsWriter.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1951",
        "summary": "wildcardquery rewrite improvements",
        "description": "wildcardquery has logic to rewrite to termquery if there is no wildcard character, but\n* it needs to pass along the boost if it does this\n* if the user asked for a 'constant score' rewriteMethod, it should rewrite to a constant score query for consistency.\n\nadditionally, if the query is really a prefixquery, it would be nice to rewrite to prefix query.\nboth will enumerate the same number of terms, but prefixquery has a simpler comparison function.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-780",
        "summary": "Generalize directory copy operation",
        "description": "The copy operation in RAMDirectory(Directory) constructor can be used more generally to copy one directory to another. Why bound it only to RAMDirectory?. For example, I build index in RAMDirectory but I need it to persist in FSDirectory. I created a patch to solve it.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-295",
        "summary": "[PATCH] MultiSearcher problems with Similarity.docFreq()",
        "description": "When MultiSearcher invokes its subsearchers, it is the subsearchers' docFreq()\nthat is accessed by Similarity.docFreq().  This causes idf's to be computed\nlocal to each index rather than globally, which causes ranking across multiple\nindices to not be equivalent to ranking across the entire global collection.\n\nThe attached files (if I can figure out how to attach them) provide a potential\npartial solution for this.  They properly fix a simple test case, RankingTest,\nthat was provided by Daniel Naber.\n\nThe changes are:\n  1.  Searcher:  Add topmostSearcher() field with getter and setter to record\nthe outermost Searcher.  Default to this.\n  2.  MultiSearcher:  Pass down the topmostSearcher when creating the subsearchers.\n  3.  IndexSearcher:  Call Query.weight() everywhere with the topmostSearcher\ninstead of this.\n  4.  Query:  Provide a default implementation of Query.combine() so that\nMultiSearcher works with all queries.\n\nProblems or possible problems I see:\n  1.  This does not address the same issue with RemoteSearchable. \nRemoteSearchable is not a Searcher, nor can it be due to lack of multiple\ninheritance in Java, but Query.weight() requires a Searcher.  Perhaps\nQuery.weight() should be changed to take a Searchable, but this requires\nchanging many places and I suspect would break apps.\n  2.  There may be other places that topmostSearcher should be used instead of this.\n  3.  The default implementation for Query.combine() is a guess on my part - it\nworks for TermQuery.  It's fragile in that the default implementation will hide\nbugs caused by queries that inadvertently omit a more precise Query.combine()\nmethod.\n  4.  The prior comment on Query.combine() indicates that whoever wrote it was\nfully aware of this problem and so probably had another usage in mind, so the\nwhole issue may just be Daniel's usage in the test case.  It's not apparent to\nme, so I probably don't understand something.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-257",
        "summary": "[PATCH] TermVectorReader and TermVectorWriter",
        "description": "TermVectorReader.close() closes all streams now under any condition. If an\nexcpetion is catched, it is remembered an thrown when all streams are closed.\nUnnecessary variable assignment removed from code. \nFix typo in TermVectorReader and TermVectorWriter.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3600",
        "summary": "BlockJoinQuery advance fails on an assert in case of a single parent with child segment",
        "description": "The BlockJoinQuery will fail on an assert when advance in called on a segment with a single parent with a child. The call to parentBits.prevSetBit(parentTarget - 1) will cause -1 to be returned, and the assert will fail, though its valid. Just removing the assert fixes the problem, since nextDoc will handle it properly.\n\nAlso, I don't understand the \"assert parentTarget != 0;\", with a comment of each parent must have one child. There isn't really a reason to add this constraint, as far as I can tell..., just call nextDoc in this case, no?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1736",
        "summary": "DateTools.java general improvements",
        "description": "Applying the attached patch shows the improvements to DateTools.java that I think should be done. All logic that does anything at all is moved to instance methods of the inner class Resolution. I argue this is more object-oriented.\n\n1. In cases where Resolution is an argument to the method, I can simply invoke the appropriate call on the Resolution object. Formerly there was a big branch if/else.\n2. Instead of \"synchronized\" being used seemingly everywhere, synchronized is used to sync on the object that is not threadsafe, be it a DateFormat or Calendar instance.\n3. Since different DateFormat and Calendar instances are created per-Resolution, there is now less lock contention since threads using different resolutions will not use the same locks.\n4. The old implementation of timeToString rounded the time before formatting it. That's unnecessary since the format only includes the resolution desired.\n5. round() now uses a switch statement that benefits from fall-through (no break).\n\nAnother debatable improvement that could be made is putting the resolution instances into an array indexed by format length. This would mean I could remove the switch in lookupResolutionByLength() and avoid the length constants there. Maybe that would be a bit too over-engineered when the switch is fine.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2129",
        "summary": "standard codec's terms dict seek should only scan if new term is in same index block",
        "description": "TermInfosReader in trunk already optimizes for this case... just need to do the same on flex.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-469",
        "summary": "(Parallel-)MultiSearcher: using Sort object changes the scores",
        "description": "Example: \nHits hits=multiSearcher.search(query);\nreturns different scores for some documents than\nHits hits=multiSearcher.search(query, Sort.RELEVANCE);\n(both for MultiSearcher and ParallelMultiSearcher)\n\nThe documents returned will be the same and in the same order, but the scores in the second case will seem out of order.\n\nInspecting the Explanation objects shows that the scores themselves are ok, but there's a bug in the normalization of the scores.\n\nThe document with the highest score should have score 1.0, so all document scores are divided by the highest score.  (Assuming the highest score was>1.0)\n\nHowever, for MultiSearcher and ParallelMultiSearcher, this normalization factor is applied *per index*, before merging the results together (the merge itself is ok though).\n\nAn example: if you use\nHits hits=multiSearcher.search(query, Sort.RELEVANCE);\nfor a MultiSearcher with two subsearchers, the first document will have score 1.0.\nThe next documents from the same subsearcher will have decreasing scores.\nThe first document from the other subsearcher will however have score 1.0 again !\n\nThe same applies for other Sort objects, but it is less visible.\n\nI will post a TestCase demonstrating the problem and suggested patches to solve it in a moment...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-906",
        "summary": "Elision filter for simple french analyzing",
        "description": "If you don't wont to use stemming, StandardAnalyzer miss some french strangeness like elision.\n\"l'avion\" wich means \"the plane\" must be tokenized as \"avion\" (plane).\nThis filter could be used with other latin language if elision exists.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-223",
        "summary": "[PATCH] remove unused variables",
        "description": "Seems I'm the only person who has the \"unused variable\" warning turned on in \nEclipse :-) This patch removes those unused variables and imports (for now \nonly in the \"search\" package). This doesn't introduce changes in \nfunctionality, but it should be reviewed anyway: there might be cases where \nthe variables *should* be used, but they are not because of a bug.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-513",
        "summary": "Remove superfluous comment in MMapDirectory.java",
        "description": "See title, and I prefer my name to be removed from the source code.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3283",
        "summary": "Move core QueryParsers to queryparser module",
        "description": "Move the contents of lucene/src/java/org/apache/lucene/queryParser to the queryparser module.\n\nTo differentiate these parsers from the others, they are going to be placed a 'classic' package.  We'll rename QueryParser to ClassicQueryParser as well.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-3183",
        "summary": "TestIndexWriter failure: AIOOBE",
        "description": "trunk: r1133486 \n{code}\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter\n    [junit] Testcase: testEmptyFieldName(org.apache.lucene.index.TestIndexWriter):      Caused an ERROR\n    [junit] CheckIndex failed\n    [junit] java.lang.RuntimeException: CheckIndex failed\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)\n    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)\n    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)\n    [junit] \n    [junit] \n    [junit] Tests run: 39, Failures: 0, Errors: 1, Time elapsed: 17.634 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] CheckIndex failed\n    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]\n    [junit]   1 of 1: name=_0 docCount=1\n    [junit]     codec=SegmentCodecs [codecs=[PreFlex], provider=org.apache.lucene.index.codecs.CoreCodecProvider@3f78807]\n    [junit]     compound=false\n    [junit]     hasProx=true\n    [junit]     numFiles=8\n    [junit]     size (MB)=0\n    [junit]     diagnostics = {os.version=2.6.39-gentoo, os=Linux, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=amd64, java.version=1.6.0_25, java.vendor=Sun Microsystems Inc.}\n    [junit]     no deletions\n    [junit]     test: open reader.........OK\n    [junit]     test: fields..............OK [1 fields]\n    [junit]     test: field norms.........OK [1 fields]\n    [junit]     test: terms, freq, prox...ERROR: java.lang.ArrayIndexOutOfBoundsException: -1\n\n    [junit] java.lang.ArrayIndexOutOfBoundsException: -1\n    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.seekEnum(TermInfosReader.java:212)\n    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.seekEnum(TermInfosReader.java:301)\n    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.get(TermInfosReader.java:234)\n    [junit]     at org.apache.lucene.index.codecs.preflex.TermInfosReader.terms(TermInfosReader.java:371)\n    [junit]     at org.apache.lucene.index.codecs.preflex.PreFlexFields$PreTermsEnum.reset(PreFlexFields.java:719)\n    [junit]     at org.apache.lucene.index.codecs.preflex.PreFlexFields$PreTerms.iterator(PreFlexFields.java:249)\n    [junit]     at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader$FieldsIterator.terms(PerFieldCodecWrapper.java:147)\n    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:610)\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:495)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)\n    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)\n    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)\n    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)\n    [junit]     test: stored fields.......OK [0 total field count; avg 0 fields per doc]\n    [junit]     test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    [junit] FAILED\n\n    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:\n    [junit] java.lang.RuntimeException: Term Index test failed\n    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:508)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)\n    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)\n    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:477)\n    [junit]     at org.apache.lucene.index.TestIndexWriter.testEmptyFieldName(TestIndexWriter.java:857)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)\n    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:758)\n    [junit] \n    [junit] WARNING: 1 broken segments (containing 1 documents) detected\n    [junit] \n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testEmptyFieldName -Dtests.seed=-3770357642070518646:-3121175410586002489 -Dtests.multiplier=3\n    [junit] NOTE: test params are: codec=PreFlex, locale=zh, timezone=Indian/Antananarivo\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]\n    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=85972280,total=232521728\n    [junit] ------------- ---------------- ---------------\n    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED\n{code}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1749",
        "summary": "FieldCache introspection API",
        "description": "FieldCache should expose an Expert level API for runtime introspection of the FieldCache to provide info about what is in the FieldCache at any given moment.  We should also provide utility methods for sanity checking that the FieldCache doesn't contain anything \"odd\"...\n   * entries for the same reader/field with different types/parsers\n   * entries for the same field/type/parser in a reader and it's subreader(s)\n   * etc...\n\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-739",
        "summary": "Performance improvement for SegmentMerger.mergeNorms()",
        "description": "This patch makes two improvements to SegmentMerger.mergeNorms():\n\n1) When the SegmentMerger merges the norm values it allocates a new byte array to buffer the values for every field of every segment. The size of such an array equals the size of the corresponding segment, so if large segments are being merged, those arrays become very large, too.\nWe can easily reduce the number of array allocations by reusing a byte array to buffer the norm values that only grows, if a segment is larger than the previous one.\n\n2) Before a norm value is written it is checked if the corresponding document is deleted. If not, the norm is written using IndexOutput.writeByte(byte[]). This patch introduces an optimized case for segments that do not have any deleted docs. In this case the frequent call of IndexReader.isDeleted(int) can be avoided and the more efficient method IndexOutput.writeBytes(byte[], int) can be used.\n\n\nThis patch only changes the method SegmentMerger.mergeNorms(). All unit tests pass.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3876",
        "summary": "TestIndexWriterExceptions fails (reproducible)",
        "description": "{noformat}\nant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testIllegalPositions -Dtests.seed=-228094d3d2f35cf2:-496e33eec9bbd57c:36a1c54f4e1bb32 -Dargs=\"-Dfile.encoding=UTF-8\"\n\n    [junit] junit.framework.AssertionFailedError: position=-2 lastPosition=0\n    [junit]     at org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter.addPosition(Lucene40PostingsWriter.java:215)\n    [junit]     at org.apache.lucene.index.FreqProxTermsWriterPerField.flush(FreqProxTermsWriterPerField.java:519)\n    [junit]     at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:92)\n    [junit]     at org.apache.lucene.index.TermsHash.flush(TermsHash.java:117)\n    [junit]     at org.apache.lucene.index.DocInverter.flush(DocInverter.java:53)\n    [junit]     at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:81)\n    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:475)\n    [junit]     at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:422)\n    [junit]     at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:553)\n    [junit]     at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2640)\n    [junit]     at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2616)\n    [junit]     at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:851)\n    [junit]     at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:810)\n    [junit]     at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:774)\n    [junit]     at org.apache.lucene.index.TestIndexWriterExceptions.testIllegalPositions(TestIndexWriterExceptions.java:1517)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)\n    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)\n    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)\n    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:729)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:645)\n    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:556)\n    [junit]     at org.apache.lucene.util.UncaughtExceptionsRule$1.evaluate(UncaughtExceptionsRule.java:51)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:618)\n    [junit]     at org.junit.rules.RunRules.evaluate(RunRules.java:18)\n    [junit]     at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)\n    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)\n    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)\n    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)\n    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)\n    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)\n    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)\n    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)\n    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)\n    [junit]     at org.apache.lucene.util.UncaughtExceptionsRule$1.evaluate(UncaughtExceptionsRule.java:51)\n    [junit]     at org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)\n    [junit]     at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)\n    [junit]     at org.junit.rules.RunRules.evaluate(RunRules.java:18)\n    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:300)\n    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)\n    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)\n    [junit] \n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-659",
        "summary": "[PATCH] PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap()",
        "description": "The attached patch causes PerFieldAnalyzerWrapper to delegate calls to getPositionIncrementGap() to the analyzer that is appropriate for the field in question.  The current behavior without this patch is to always use the default value from Analyzer, which is a bug because PerFieldAnalyzerWrapper should behave just as if it was the analyzer for the selected field.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2034",
        "summary": "Massive Code Duplication in Contrib Analyzers - unifly the analyzer ctors",
        "description": "Due to the variouse tokenStream APIs we had in lucene analyzer subclasses need to implement at least one of the methodes returning a tokenStream. When you look at the code it appears to be almost identical if both are implemented in the same analyzer.  Each analyzer defnes the same inner class (SavedStreams) which is unnecessary.\nIn contrib almost every analyzer uses stopwords and each of them creates his own way of loading them or defines a large number of ctors to load stopwords from a file, set, arrays etc.. those ctors should be removed / deprecated and eventually removed.\n\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1212",
        "summary": "Basic refactoring of DocumentsWriter",
        "description": "As a starting point for making DocumentsWriter more understandable,\nI've fixed its inner classes to be static, and then broke the classes\nout into separate sources, all in org.apache.lucene.index package.\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1407",
        "summary": "Refactor Searchable to not have RMI Remote dependency",
        "description": "Per http://lucene.markmail.org/message/fu34tuomnqejchfj?q=RemoteSearchable\n\nWe should refactor Searchable slightly so that it doesn't extend the java.rmi.Remote marker interface.  I believe the same could be achieved by just marking the RemoteSearchable and refactoring the RMI implementation out of core and into a contrib.\n\nIf we do this, we should deprecate/denote it for 2.9 and then move it for 3.0",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2641",
        "summary": "BaseTestRangeFilter can be extremely slow",
        "description": "The tests that extend BaseTestRangeFilter can sometimes be very slow:\nTestFieldCacheRangeFilter, TestMultiTermConstantScore, TestTermRangeFilter\n\nfor example, TestFieldCacheRangeFilter just ran for 10 minutes on my computer before I killed it,\nbut i noticed these tests frequently run for over a minute.\n\nI think at the least we should change these to junit4 so the index is built once in @beforeClass",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3292",
        "summary": "IOContext should be part of the SegmentReader cache key ",
        "description": "Once IOContext (LUCENE-2793) is landed the IOContext should be part of the key used to cache that reader in the pool",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3019",
        "summary": "FVH: uncontrollable color tags",
        "description": "The multi-colored tags is a feature of FVH. But it is uncontrollable (or more precisely, unexpected by users) that which color is used for each terms.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1592",
        "summary": "fix or deprecate TermsEnum.skipTo",
        "description": "This method is a trap: it looks legitimate but it has hideously poor performance (simple linear scan implemented in the TermsEnum base class since none of the concrete impls override it with a more efficient implementation).\n\nThe least we should do for 2.9 is deprecate the method with  a strong warning about its performance.\n\nSee here for background: http://www.lucidimagination.com/search/document/77dc4f8e893d3cf3/possible_terminfosreader_speedup\n\nAnd, here for historical context: \n\nhttp://www.lucidimagination.com/search/document/88f1b95b404ebf16/remove_termenum_skipto_term_target",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-824",
        "summary": "IndexWriter#addIndexesNoOptimize has redundent try/catch",
        "description": "With the new transaction code, the try/catch clause at the beginning of IndexWriter#addIndexesNoOptimize is redundant.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2304",
        "summary": "FuzzyLikeThisQuery should set MaxNonCompetitiveBoost for faster speed",
        "description": "FuzzyLikeThisQuery uses FuzzyTermsEnum directly, and maintains \na priority queue for its purposes.\n\nJust like TopTermsRewrite method, it should set the \nMaxNonCompetitiveBoost attribute, so that FuzzyTermsEnum can\nrun faster. Its already tracking the minScore, just not updating\nthe attribute.\n\nThis would be especially nice as it appears to have nice defaults\nalready (pq size of 50)\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3494",
        "summary": "Remove per-document multiply in FilteredQuery",
        "description": "Spinoff of LUCENE-1536.\n\nIn LUCENE-1536, Uwe suggested using FilteredQuery under-the-hood to implement filtered search.\n\nBut this query is inefficient, it does a per-document multiplication (wrapped.score() * boost()).\n\nInstead, it should just pass the boost down in its weight, like BooleanQuery does to avoid this per-document multiply.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3083",
        "summary": "MockRandomMergePolicy optimizes segments not in the Set passed in",
        "description": "The test class MockRandomMergePolicy shuffles the whole SegmentInfos passed to the optimize callback and returns random segments for optimizing. This is fine, but it also returns segments, that are not listed in the Set<SegmentInfo> that is also passed in, containing the subset of segments to optimize.\n\nThis bug was found when writing a testcase for LUCENE-3082: The wrapper MergePolicy (when wrapped around MockRandomMergePolicy) only passes a subset of the segments to the delegate (the ones that are in old index format). But MockRandom created OneMerge in its return MergeSpecification having segments outside this set.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2364",
        "summary": "Add support for terms in BytesRef format to Term, TermQuery, TermRangeQuery & Co.",
        "description": "It would be good to directly allow BytesRefs in TermQuery and TermRangeQuery (as both queries convert the strings to BytesRef internally). For NumericRange support in Solr it will be needed to support numerics as ByteRef in single-term queries.\n\nWhen this will be added, don't forget to change TestNumericRangeQueryXX to use the BytesRef ctor of TRQ.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-274",
        "summary": "[PATCH] to store binary fields with compression",
        "description": "hi all,\n\nas promised here is the enhancement for the binary field patch with optional\ncompression. The attachment includes all necessary diffs based on the latest\nversion from CVS. There is also a small junit test case to test the core\nfunctionality for binary field compression. The base implementation for binary\nfields where this patch relies on, can be found in patch #29370. The existing\nunit tests pass fine.\n\nFor testing binary fields and compression, I'm creating an index from 2700 plain\ntext files (avg. 6kb per file) and store all file content within that index\nwithout using compression. The test was created using the IndexFiles class from\nthe demo distribution. Setting up the index and storing all content without\ncompression took about 60 secs and the final index size was 21 MB. Running the\nsame test, switching compression on, the time to index increase to 75 secs, but\nthe final index size shrinks to 13 MB. This is less than the plain text files\nthem self need in the file system (15 MB)\n\nHopefully this patch helps people dealing with huge index and want to store more\nthan just 300 bytes per document to display a well formed summary.\n\nregards\nBernhard",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2313",
        "summary": "Add VERBOSE to LuceneTestCase and LuceneTestCaseJ4",
        "description": "component-build.xml allows to define tests.verbose as a system property when running tests. Both LuceneTestCase and LuceneTestCaseJ4 don't read that property. It will be useful for overriding tests to access one place for this setting (I believe currently some tests do it on their own). Then (as a separate issue) we can move all tests that don't check the parameter to only print if VERBOSE is true.\n\nI will post a patch soon.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-202",
        "summary": "Document.fields() only returns stored fields",
        "description": "Document.fields() only returns stored fields, not those which are indexed but not \nstored. This is confusing, as there's a isStored() Method which doesn't make much \nsense then. \n \nActually fields() returns all fields only just after Document.add(new Field(...)), even the \nones which are not stored. Sounds confusing? :-) I'll attach a small program that \ndemonstrates this. \n \nThis should either be fixed so that all fields are always returned or it should be \ndocumented.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2959",
        "summary": "[GSoC] Implementing State of the Art Ranking for Lucene",
        "description": "Lucene employs the Vector Space Model (VSM) to rank documents, which compares\nunfavorably to state of the art algorithms, such as BM25. Moreover, the architecture is\ntailored specically to VSM, which makes the addition of new ranking functions a non-\ntrivial task.\n\nThis project aims to bring state of the art ranking methods to Lucene and to implement a\nquery architecture with pluggable ranking functions.\n\nThe wiki page for the project can be found at http://wiki.apache.org/lucene-java/SummerOfCode2011ProjectRanking.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3769",
        "summary": "Simplify NRTManager",
        "description": "NRTManager is hairy now, because the applyDeletes is separately passed\nto ctor, passed to maybeReopen, passed to getSearcherManager, etc.\n\nI think, instead, you should pass it only to the ctor, and if you have\nsome cases needing deletes and others not then you can make two\nNRTManagers.  This should be no less efficient than we have today,\njust simpler.\n\nI think it will also enable NRTManager to subclass ThingyManager\n(LUCENE-3761).\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3448",
        "summary": "Add FixedBitSet.and(other/DISI), andNot(other/DISI)",
        "description": "For the parent issue, and() and andNot() on DISIs and other FixedBitSets are missing. This issue will add those methods.\n\nThe DISI methods (also the already existing or(DISI)) method will check for OpenBitSetIterator and do an inplace operation using the bits as optimization.",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-3605",
        "summary": "revisit segments.gen sleeping",
        "description": "in LUCENE-3601, i worked up a change where we intentionally crash() all un-fsynced files \nin tests to ensure that we are calling sync on files when we should.\n\nI think this would be nice to do always (and with some fixes all tests pass).\n\nBut this is super-slow sometimes because when we corrupt the unsynced segments.gen, it causes\nSIS.read to take 500ms each time (and in checkindex for some reason we do this twice, which seems wrong).\n\nI can workaround this for now for tests (just do a partial crash that avoids corrupting the segments.gen),\nbut I wanted to create this issue for discussion about the sleeping/non-fsyncing of segments.gen, just\nbecause i guess its possible someone could hit this slowness.\n ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2618",
        "summary": "Intermittent failure in TestThreadedOptimize",
        "description": "Failure looks like this:\n\n{noformat}\n    [junit] Testsuite: org.apache.lucene.index.TestThreadedOptimize\n    [junit] Testcase: testThreadedOptimize(org.apache.lucene.index.TestThreadedOptimize):\tFAILED\n    [junit] null\n    [junit] junit.framework.AssertionFailedError: null\n    [junit] \tat org.apache.lucene.index.TestThreadedOptimize.runTest(TestThreadedOptimize.java:125)\n    [junit] \tat org.apache.lucene.index.TestThreadedOptimize.testThreadedOptimize(TestThreadedOptimize.java:149)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:253)\n{noformat}\n\nI just committed some verbosity so next time it strikes we'll have more details.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1214",
        "summary": "Possible hidden exception on SegmentInfos commit",
        "description": "I am not sure if this is that big of a deal, but I just ran into it and thought I might mention it.\n\nSegmentInfos.commit removes the Segments File if it hits an exception. If it cannot remove the Segments file (because its not there or on Windows something has a hold of it), another Exception is thrown about not being able to delete the Segments file. Because of this, you lose the first exception, which might have useful info, including why the segments file might not be there to delete.\n\n- Mark",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3697",
        "summary": "FastVectorHighlighter SimpleBoundaryScanner does not work well when highlighting at the beginning of the text ",
        "description": "The SimpleBoundaryScanner still breaks text not based on characters provided when highlighting text that end up scanning to the beginning of the text to highlight. In this case, just use the start of the text as the offset.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-339",
        "summary": "date encoding limitation removing",
        "description": "currently there is some limitation to date encoding in lucene. I think it's \nbecause dates should preserve lexicografical ordering, i.e. if one date precedes \nanother date then encoded values should keep same ordering.\n\nI know that it can be difficult to integrate it into existing version but there \nis way to remove this limitation.\nDate milliseconds can be encoded as unsigned values with prefix that indicates \npositive or negative value.\n\nIn more details:\nI used hex encoding and prefix &#8216;p&#8217; and &#8216;n&#8217; for positive and negative values. I \ngot following results:\n\nValue -10000 is encoded with nffffffffffffd8f0, \n-100\t- nffffffffffffff9c\n0\t- p0000000000000000\n100\t- p0000000000000064\n10000\t- p0000000000002710\n\nThis preserves ordering between values and theirs encoding.\n\nAlso hex encoding can be replaced with Character.MAX_RADIX encoding.\n\nPart of code that do this work:\n   final static char[] digits = {\n\t'0' , '1' , '2' , '3' , '4' , '5' ,\n\t'6' , '7' , '8' , '9' , 'a' , 'b' ,\n\t'c' , 'd' , 'e' , 'f' , 'g' , 'h' ,\n\t'i' , 'j' , 'k' , 'l' , 'm' , 'n' ,\n\t'o' , 'p' , 'q' , 'r' , 's' , 't' ,\n\t'u' , 'v' , 'w' , 'x' , 'y' , 'z'\n    };\n\n\n    char prefix;\n    if (time >= 0) {\n      prefix = 'p';\n    } else {\n      prefix = 'n';\n    }\n\n    char[] chars = new char[DATE_LEN + 1];\n    int index = DATE_LEN;\n    while (time != 0) {\n      int b = (int) (time & 0x0F);\n      chars[index--] = digits[b];\n      time = time >>> 4;\n    }\n\n    while (index >= 0) {\n      chars[index--] = '0';\n    }\n    chars[0] = prefix;\n\n    return new String(chars);",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2906",
        "summary": "Filter to process output of ICUTokenizer and create overlapping bigrams for CJK ",
        "description": "The ICUTokenizer produces unigrams for CJK. We would like to use the ICUTokenizer but have overlapping bigrams created for CJK as in the CJK Analyzer.  This filter would take the output of the ICUtokenizer, read the ScriptAttribute and for selected scripts (Han, Kana), would produce overlapping bigrams.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1046",
        "summary": "Dead code in SpellChecker.java (branch never executes)",
        "description": "SpellChecker contains the following lines of code:\n\n    final int goalFreq = (morePopular && ir != null) ? ir.docFreq(new Term(field, word)) : 0;\n    // if the word exists in the real index and we don't care for word frequency, return the word itself\n    if (!morePopular && goalFreq > 0) {\n      return new String[] { word };\n    }\n\nThe branch will never execute: the only way for goalFreq to be greater than zero is if morePopular is true, but if morePopular is true, the expression in the if statement evaluates to false.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2422",
        "summary": "don't reuse byte[] in IndexInput/Output for read/writeString",
        "description": "IndexInput now holds a private \"byte[] bytes\", which it re-uses for reading strings.  Likewise, IndexOutput holds a UTF8Result (which holds \"byte[] bytes\"), re-used for writing strings.\n\nThese are both dangerous, since on reading or writing immense strings, we never free this storage.\n\nWe don't use read/writeString in very perf sensitive parts of the code, so, I think we should not reuse the byte[] at all.\n\nI think this is likely the cause of the recent \"IndexWriter and memory usage\" thread, started by Ross Woolf on java-user@.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2183",
        "summary": "Supplementary Character Handling in CharTokenizer",
        "description": "CharTokenizer is an abstract base class for all Tokenizers operating on a character level. Yet, those tokenizers still use char primitives instead of int codepoints. CharTokenizer should operate on codepoints and preserve bw compatibility. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2393",
        "summary": "Utility to output total term frequency and df from a lucene index",
        "description": "This is a pair of command line utilities that provide information on the total number of occurrences of a term in a Lucene index.  The first  takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document).   The second reads the index to determine the top N most frequent terms (by document frequency) and then outputs a list of those terms along with  the document frequency and the total number of occurrences of the term. Both utilities are useful for estimating the size of the term's entry in the *prx files and consequent Disk I/O demands. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3889",
        "summary": "Remove/Uncommit SegmentingTokenizerBase",
        "description": "I added this class in LUCENE-3305 to support analyzers like Kuromoji,\nbut Kuromoji no longer needs it as of LUCENE-3767. So now nothing uses it.\n\nI think we should uncommit before releasing, svn doesn't forget so\nwe can add this back if we want to refactor something like Thai or Smartcn\nto use it.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2855",
        "summary": "Contrib queryparser should not use CharSequence as Map key",
        "description": "Today, contrib query parser uses Map<CharSequence,...> in many different places, which may lead to problems, since CharSequence interface does not enforce the implementation of hashcode and equals methods. Today, it's causing a problem with QueryTreeBuilder.setBuilder(CharSequence,QueryBuilder) method, that does not works as expected.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2344",
        "summary": "PostingsConsumer#merge does not call finishDoc",
        "description": "We discovered that the current merge function in PostingsConsumer is not calling the #finishDoc method. This does not have consequences for the standard codec (since the lastPosition is set to 0 in #startDoc, and its #finishDoc method is empty), but for the SepCodec, this results in position file corruption (the lastPosition is set to 0 in #finishDoc for the SepCodec).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3011",
        "summary": "FST serialization and deserialization from plain DataInput/DataOutput streams.",
        "description": "Currently the automaton can be saved only to a Directory instance (IndexInput/ IndexOutput).",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1240",
        "summary": "TermsFilter: reuse TermDocs",
        "description": "TermsFilter currently calls termDocs(Term) once per term in the TermsFilter.  If we sort the terms it's filtering on, this can be optimised to call termDocs() once and then skip(Term) once per term, which should significantly speed up this filter.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3760",
        "summary": "Cleanup DR.getCurrentVersion/DR.getUserData/DR.getIndexCommit().getUserData()",
        "description": "Spinoff from Ryan's dev thread \"DR.getCommitUserData() vs DR.getIndexCommit().getUserData()\"... these methods are confusing/dups right now.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2935",
        "summary": "Let Codec consume entire document",
        "description": "Currently the codec API is limited to consume Terms & Postings upon a segment flush. To enable stored fields & DocValues to make use of the Codec abstraction codecs should allow to pull a consumer ahead of flush time and consume all values from a document's field though a consumer API. An alternative to consuming the entire document would be extending FieldsConsumer to return a StoredValueConsumer / DocValuesConsumer like it is done in DocValues - Branch right now side by side to the TermsConsumer. Yet, extending this has proven to be very tricky and error prone for several reasons:\n* FieldsConsumer requires SegmentWriteState which might be different upon flush compared to when the document is consumed. SegmentWriteState must therefor be created twice 1. when the first docvalues field is indexed 2. when flushed. \n* FieldsConsumer are current pulled for each indexed field no matter if there are terms to be indexed or not. Yet, if we use something like DocValuesCodec which essentially wraps another codec and creates FieldConsumer on demand the wrapped codecs consumer might not be initialized even if the field is indexed. This causes problems once such a field is opened but missing the required files for that codec. I added some harsh logic to work around this which should be prevented.\n* SegmentCodecs are created for each SegmentWriteState which might yield wrong codec IDs depending on how fields numbers are assigned. We currently depend on the fact that all fields for a segment and therefore their codecs are known when SegmentCodecs are build. To enable consuming perDoc values in codecs we need to do that incrementally\n\nCodecs should instead provide a DocumentConsumer side by side with the FieldsConsumer created prior to flush. This is also a prerequisite for LUCENE-2621",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3519",
        "summary": "BlockJoinCollector only allows retrieving groups for only one BlockJoinQuery",
        "description": "Spinoff from Mark Harwood's email (subject \"BlockJoin concerns\") to\ndev list.\n\nIt's fine to use multiple nested joins in a single query, and\nBlockJoinCollector should let you retrieve the top groups for all of\nthem.\n\nBut currently it always returns null after the first query's groups\nhave been retrieved, because of a silly bug.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-883",
        "summary": "make spell checker test case work again",
        "description": "See attached path which makes the spellchecker test case work again. The problem without the patch is that consecutive calls to indexDictionary() will create a spelling index with duplicate words. Does anybody see a problem with this patch? I see that the spellchecker code is now used in Solr, isn't it? I didn't have time to test this patch inside Solr.\n\nAlso see http://issues.apache.org/jira/browse/LUCENE-632, but the null check is included in this patch so the NPE described there cannot happen anymore.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1335",
        "summary": "Correctly handle concurrent calls to addIndexes, optimize, commit",
        "description": "Spinoff from here:\n\n    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3Cc7b302c50807111018j58b6d08djd56b5889f6b3780d@mail.gmail.com%3E",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2409",
        "summary": "add a tokenfilter for icu transforms",
        "description": "I pulled the ICUTransformFilter out of LUCENE-1488 and create an issue for it here.\n\nThis is a tokenfilter that applies an ICU Transliterator, which is a context-sensitive way\nto transform text. \n\nThese are typically rule-based and you can use ones included with ICU (such as Traditional-Simplified)\nor you can make your own from your own set of rules.\n\nUser's Guide: http://userguide.icu-project.org/transforms/general\nRule Tutorial: http://userguide.icu-project.org/transforms/general/rules\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1301",
        "summary": "Refactor DocumentsWriter",
        "description": "I've been working on refactoring DocumentsWriter to make it more\nmodular, so that adding new indexing functionality (like column-stride\nstored fields, LUCENE-1231) is just a matter of adding a plugin into\nthe indexing chain.\n\nThis is an initial step towards flexible indexing (but there is still\nalot more to do!).\n\nAnd it's very much still a work in progress -- there are intemittant\nthread safety issues, I need to add tests cases and test/iterate on\nperformance, many \"nocommits\", etc.  This is a snapshot of my current\nstate...\n\nThe approach introduces \"consumers\" (abstract classes defining the\ninterface) at different levels during indexing.  EG DocConsumer\nconsumes the whole document.  DocFieldConsumer consumes separate\nfields, one at a time.  InvertedDocConsumer consumes tokens produced\nby running each field through the analyzer.  TermsHashConsumer writes\nits own bytes into in-memory posting lists stored in byte slices,\nindexed by term, etc.\n\nDocumentsWriter*.java is then much simpler: it only interacts with a\nDocConsumer and has no idea what that consumer is doing.  Under that\nDocConsumer there is a whole \"indexing chain\" that does the real work:\n\n  * NormsWriter holds norms in memory and then flushes them to _X.nrm.\n\n  * FreqProxTermsWriter holds postings data in memory and then flushes\n    to _X.frq/prx.\n\n  * StoredFieldsWriter flushes immediately to _X.fdx/fdt\n\n  * TermVectorsTermsWriter flushes immediately to _X.tvx/tvf/tvd\n\nDocumentsWriter still manages things like flushing a segment, closing\ndoc stores, buffering & applying deletes, freeing memory, aborting\nwhen necesary, etc.\n\nIn this first step, everything is package-private, and, the indexing\nchain is hardwired (instantiated in DocumentsWriter) to the chain\ncurrently matching Lucene trunk.  Over time we can open this up.\n\nThere are no changes to the index file format.\n\nFor the most part this is just a [large] refactoring, except for these\ntwo small actual changes:\n\n  * Improved concurrency with mixed large/small docs: previously the\n    thread state would be tied up when docs finished indexing\n    out-of-order.  Now, it's not: instead I use a separate class to\n    hold any pending state to flush to the doc stores, and immediately\n    free up the thread state to index other docs.\n\n  * Buffered norms in memory now remain sparse, until flushed to the\n    _X.nrm file.  Previously we would \"fill holes\" in norms in memory,\n    as we go, which could easily use way too much memory.  Really this\n    isn't a solution to the problem of sparse norms (LUCENE-830); it\n    just delays that issue from causing memory blowup during indexing;\n    memory use will still blowup during searching.\n\nI expect performance (indexing throughput) will be worse with this\nchange.  I'll profile & iterate to minimize this, but I think we can\naccept some loss.  I also plan to measure benefit of manually\nre-cycling RawPostingList instances from our own pool, vs letting GC\nrecycle them.\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2329",
        "summary": "Use parallel arrays instead of PostingList objects",
        "description": "This is Mike's idea that was discussed in LUCENE-2293 and LUCENE-2324.\n\nIn order to avoid having very many long-living PostingList objects in TermsHashPerField we want to switch to parallel arrays.  The termsHash will simply be a int[] which maps each term to dense termIDs.\n\nAll data that the PostingList classes currently hold will then we placed in parallel arrays, where the termID is the index into the arrays.  This will avoid the need for object pooling, will remove the overhead of object initialization and garbage collection.  Especially garbage collection should benefit significantly when the JVM runs out of memory, because in such a situation the gc mark times can get very long if there is a big number of long-living objects in memory.\n\nAnother benefit could be to build more efficient TermVectors.  We could avoid the need of having to store the term string per document in the TermVector.  Instead we could just store the segment-wide termIDs.  This would reduce the size and also make it easier to implement efficient algorithms that use TermVectors, because no term mapping across documents in a segment would be necessary.  Though this improvement we can make with a separate jira issue.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3232",
        "summary": "Move MutableValues to Common Module",
        "description": "Solr makes use of the MutableValue* series of classes to improve performance of grouping by FunctionQuery (I think).  As such they are used in ValueSource implementations.  Consequently we need to move these classes in order to move the ValueSources.\n\nAs Yonik pointed out, these classes have use beyond just FunctionQuerys and might be used by both Solr and other modules.  However I don't think they belong in Lucene core, since they aren't really related to search functionality.  Therefore I think we should put them into a Common module, which can serve as a dependency to Solr and any module.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-1838",
        "summary": "BoostingNearQuery must implement clone - now it clones to a NearSpanQuery - cloning is required for Span container classes",
        "description": "",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3724",
        "summary": "SimpleText sumTotalTermFreq is wrong if only positions are omitted",
        "description": "ant test -Dtestcase=TestOmitPositions -Dtestmethod=testBasic -Dtests.seed=-6c9bd4a6197b9463:-71d0d11bc2db9a15:697690b3dff2369 -Dargs=\"-Dfile.encoding=UTF-8\"\n    [junit] java.lang.AssertionError: sumTotalTermFreq=0,sumDocFreq=400\n    [junit] \tat org.apache.lucene.search.CollectionStatistics.<init>(CollectionStatistics.java:38)\n\nThis assert fails because #of positions for the field is < #of postings, which is impossible.\n\nFrom memory i think SimpleText calculates sumTotalTermFreq \"one the fly\" by reading the positions from its text file...\nIn this case it should write the stat explicitly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3541",
        "summary": "remove IndexInput.copyBuf",
        "description": "this looks really broken/dangerous as an instance variable.\n\nwhat happens on clone() ?! copyBytes can instead make its own array inside the method.\n\nits protected, so ill list in the 3.x backwards breaks section since its technically a backwards break.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1277",
        "summary": "Remove System.out left in SpanHighlighter code",
        "description": "A System.out debug was left in the code when a Query is not supported by the SpanHighlighter. This issue simply removes it.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2378",
        "summary": "Cutover remaining usage of pre-flex APIs",
        "description": "A number of places still use the pre-flex APIs.\n\nThis is actually healthy, since it gives us ongoing testing of the back compat emulation layer.\n\nBut we should at some point cut them all over to flex.  Latest we can do this is 4.0, but I'm not sure we should do them all for 3.1... still marking this as 3.1 to \"remind us\" :)",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1817",
        "summary": "it is impossible to use a custom dictionary for SmartChineseAnalyzer",
        "description": "it is not possible to use a custom dictionary, even though there is a lot of code and javadocs to allow this.\n\nThis is because the custom dictionary is only loaded if it cannot load the built-in one (which is of course, in the jar file and should load)\n{code}\npublic synchronized static WordDictionary getInstance() {\n    if (singleInstance == null) {\n      singleInstance = new WordDictionary(); // load from jar file\n      try {\n        singleInstance.load();\n      } catch (IOException e) { // loading from jar file must fail before it checks the AnalyzerProfile (where this can be configured)\n        String wordDictRoot = AnalyzerProfile.ANALYSIS_DATA_DIR;\n        singleInstance.load(wordDictRoot);\n      } catch (ClassNotFoundException e) {\n        throw new RuntimeException(e);\n      }\n    }\n    return singleInstance;\n  }\n{code}\n\nI think we should either correct this, document this, or disable custom dictionary support...",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3647",
        "summary": "DocValues merging is not associative, leading to different results depending upon how merges execute",
        "description": "recently I cranked up TestDuelingCodecs to actually test docvalues (previously it wasn't testing it at all).\n\nThis test is simple, it indexes the same random content with 2 different indexwriters, it just allows them\nto use different codecs with different indexwriterconfigs.\n\nthen it asserts the indexes are equal.\n\nSometimes, always on BYTES_FIXED_DEREF type, we end out with one reader that has a zero-filled byte[] for a doc,\nbut that same document in the other reader has no docvalues at all.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1680",
        "summary": "Make prefixLength accessible to PrefixTermEnum subclasses",
        "description": "PrefixTermEnum#difference() offers a way to influence scoring based on the difference between the prefix Term and a term in the enumeration. To effectively use this facility the length of the prefix should be accessible to subclasses. Currently the prefix term is private to PrefixTermEnum. I added a getter for the prefix length and made PrefixTermEnum#endEnum(), PrefixTermEnum#termCompare() final for consistency with other TermEnum subclasses.\n\nPatch is attached.\n\nSimon",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3820",
        "summary": "Wrong trailing index calculation in PatternReplaceCharFilter",
        "description": "Reimplementation of PatternReplaceCharFilter to pass randomized tests (used to throw exceptions previously). Simplified code, dropped boundary characters, full input buffered for pattern matching.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2524",
        "summary": "when many query clases are specified in boolean or dismax query, highlighted terms are always \"yellow\" if multi-colored feature is used",
        "description": "The problem is the following snippet:\n\n{code}\nprotected String getPreTag( int num ){\n  return preTags.length > num ? preTags[num] : preTags[0];\n}\n{code}\n\nit should be:\n\n{code}\nprotected String getPreTag( int num ){\n  int n = num % preTags.length;\n  return  preTags[n];\n}\n{code}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2952",
        "summary": "Make license checking/maintenance easier/automated",
        "description": "Instead of waiting until release to check licenses are valid, we should make it a part of our build process to ensure that all dependencies have proper licenses, etc.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3620",
        "summary": "FilterIndexReader does not override all of IndexReader methods",
        "description": "FilterIndexReader does not override all of IndexReader methods. We've hit an error in LUCENE-3573 (and fixed it). So I thought to write a simple test which asserts that FIR overrides all methods of IR (and we can filter our methods that we don't think that it should override). The test is very simple (attached), and it currently fails over these methods:\n{code}\ngetRefCount\nincRef\ntryIncRef\ndecRef\nreopen\nreopen\nreopen\nreopen\nclone\nnumDeletedDocs\ndocument\nsetNorm\nsetNorm\ntermPositions\ndeleteDocument\ndeleteDocuments\nundeleteAll\ngetIndexCommit\ngetUniqueTermCount\ngetTermInfosIndexDivisor\n{code}\n\nI didn't yet fix anything in FIR -- if you spot a method that you think we should not override and delegate, please comment.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1640",
        "summary": "MockRAMDirectory (used only by unit tests) has some synchronization problems",
        "description": "Coming out of a failure that Earwin noted on java-dev this morning, I reworked the synchronization on MockRAMDirectory.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1602",
        "summary": "Rewrite TrieRange to use MultiTermQuery",
        "description": "Issue for discussion here: http://www.lucidimagination.com/search/document/46a548a79ae9c809/move_trierange_to_core_module_and_integration_issues\n\nThis patch is a rewrite of TrieRange using MultiTermQuery like all other core queries. This should make TrieRange identical in functionality to core range queries.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2879",
        "summary": "MultiPhraseQuery sums its own idf instead of Similarity.",
        "description": "MultiPhraseQuery is a generalized version of PhraseQuery, and computes IDF the same way by default (by summing across the terms).\n\nThe problem is it doesn't let the Similarity do this: PhraseQuery calls Similarity.idfExplain(Collection<Term> terms, IndexSearcher searcher),\nbut MultiPhraseQuery just sums itself, calling Similarity.idf(int, int) for each term.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-747",
        "summary": "nightly build failed",
        "description": "javadoc tasked failed due to new project structure in contrib/gdata-server\nadded correct package structure to java/trunk/build.xml\n\njavadoc creation successful.\n\nPatch added as attachment.\n\nregards simon",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2940",
        "summary": "NPE in TestNRTThreads",
        "description": "I hit this when while(1)ing this test... I think it's because the logic on when we ask the SegmentReader to load stored fields is off...\n\n{noformat}\n*** Thread: Lucene Merge Thread #1 ***\norg.apache.lucene.index.MergePolicy$MergeException:\njava.lang.NullPointerException\n       at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)\n       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)\nCaused by: java.lang.NullPointerException\n       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:245)\n       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:242)\n       at org.apache.lucene.util.CloseableThreadLocal.get(CloseableThreadLocal.java:68)\n       at org.apache.lucene.index.SegmentReader.getFieldsReader(SegmentReader.java:749)\n       at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:838)\n       at org.apache.lucene.index.IndexReader.document(IndexReader.java:951)\n       at org.apache.lucene.index.TestNRTThreads$1.warm(TestNRTThreads.java:86)\n       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3311)\n       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2875)\n       at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)\n       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)\n{noformat}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2117",
        "summary": "Fix SnowballAnalyzer casing behavior for Turkish Language",
        "description": "LUCENE-2102 added a new TokenFilter to handle Turkish unique casing behavior correctly. We should fix the casing behavior in SnowballAnalyzer too as it supports a TurkishStemmer.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-393",
        "summary": "Inconsistent scoring with SpanTermQuery in BooleanQuery",
        "description": "When a SpanTermQuery is added to a BooleanQuery, incorrect results are \nreturned.\n\nI am running Lucene 1.9 RC1 on Windows XP.  I have a test case which has \nseveral tests.  It has an index with 4 identical documents in it.\n\nWhen two TermQuerys are used in a BooleanQuery, the score looks like this:\n  4 hits for search: two term queries\n    ID:1 (score:0.54932046)\n    ID:2 (score:0.54932046)\n    ID:3 (score:0.54932046)\n    ID:4 (score:0.54932046)\n\nNotice how it is correctly setting the score to be the same for each document.\n\nWhen two SpanQuerys are used in a BooleanQuery, the score looks like this:\n  2 hits for search: two span queries\n    ID:1 (score:0.3884282)\n    ID:4 (score:0.1942141)\n\nNotice how it only returned two documents instead of four.  And the two it did \nreturn have differing scores.\n\nI believe that there is an error in the scoring algorithm that is making the \nother two documents not show up.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3566",
        "summary": "Parametrizing H1 and H2",
        "description": "The DFR normalizations {{H1}} and {{H2}} are parameter-free. This is in line with the [original article|http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.742], but not with the [thesis|http://theses.gla.ac.uk/1570/], where H2 accepts a {{c}} parameter, nor with [information-based models|http://dl.acm.org/citation.cfm?id=1835490], where H1 also accepts a {{c}} parameter.",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1244",
        "summary": "Get rid of (another) hard coded path",
        "description": "",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-705",
        "summary": "CompoundFileWriter should pre-set its file length",
        "description": "I've read that if you are writing a large file, it's best to pre-set\nthe size of the file in advance before you write all of its contents.\nThis in general minimizes fragmentation and improves IO performance\nagainst the file in the future.\n\nI think this makes sense (intuitively) but I haven't done any real\nperformance testing to verify.\n\nJava has the java.io.File.setLength() method (since 1.2) for this.\n\nWe can easily fix CompoundFileWriter to call setLength() on the file\nit's writing (and add setLength() method to IndexOutput).  The\nCompoundFileWriter knows exactly how large its file will be.\n\nAnother good thing is: if you are going run out of disk space, then,\nthe setLength call should fail up front instead of failing when the\ncompound file is actually written.  This has two benefits: first, you\nfind out sooner that you will run out of disk space, and, second, you\ndon't fill up the disk down to 0 bytes left (always a frustrating\nexperience!).  Instead you leave what space was available\nand throw an IOException.\n\nMy one hesitation here is: what if out there there exists a filesystem\nthat can't handle this call, and it throws an IOException on that\nplatform?  But this is balanced against possible easy-win improvement\nin performance.\n\nDoes anyone have any feedback / thoughts / experience relevant to\nthis?\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3470",
        "summary": "reorder arguments of Field constructor to be more intuitive",
        "description": "I think Field should take (name, value, type) not (name, type, value) ?\n\nThis seems more intuitive and consistent with previous releases\n\nTake this change to some code I had for example:\n{noformat}\n-    d1.add(new Field(\"foo\", \"bar\", Field.Store.YES, Field.Index.ANALYZED));\n+    d1.add(new Field(\"foo\", TextField.TYPE_STORED, \"bar\"));\n{noformat}\n\nI think it would be better if it was\n{noformat}\ndocument.add(new Field(\"foo\", \"bar\", TextField.TYPE_STORED));\n{noformat}",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2709",
        "summary": "If test has methods with @Ignore, we should print out a notice",
        "description": "Currently these silently pass, but there is usually a reason they are @Ignore \n(sometimes good, sometimes really a TODO we should fix)\n\nIn my opinion we should add reasons for all these current @Ignores like Mike did with Test2BTerms.\n\nExample output:\n{noformat}\n[junit] Testsuite: org.apache.lucene.index.Test2BTerms\n[junit] Tests run: 0, Failures: 0, Errors: 0, Time elapsed: 0.184 sec\n[junit]\n[junit] ------------- Standard Error -----------------\n[junit] NOTE: Ignoring test method 'test2BTerms' Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.\n[junit] ------------- ---------------- ---------------\n\n...\n\n[junit] Testsuite: org.apache.solr.handler.dataimport.TestMailEntityProcessor\n[junit] Tests run: 0, Failures: 0, Errors: 0, Time elapsed: 0.043 sec\n[junit]\n[junit] ------------- Standard Error -----------------\n[junit] NOTE: Ignoring test method 'testConnection'\n[junit] NOTE: Ignoring test method 'testRecursion'\n[junit] NOTE: Ignoring test method 'testExclude'\n[junit] NOTE: Ignoring test method 'testInclude'\n[junit] NOTE: Ignoring test method 'testIncludeAndExclude'\n[junit] NOTE: Ignoring test method 'testFetchTimeSince'\n[junit] ------------- ---------------- ---------------\n{noformat}",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2180",
        "summary": "Deprecated API called in o.a.l.store Directories",
        "description": "just ran into NIOFSDirectory and others still call getFile instead of getDirectory",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3901",
        "summary": "Add katakana stem filter to better deal with certain katakana spelling variants",
        "description": "Many Japanese katakana words end in a long sound that is sometimes optional.\n\nFor example, \u30d1\u30fc\u30c6\u30a3\u30fc and \u30d1\u30fc\u30c6\u30a3 are both perfectly valid for \"party\".  Similarly we have \u30bb\u30f3\u30bf\u30fc and \u30bb\u30f3\u30bf that are variants of \"center\" as well as \u30b5\u30fc\u30d0\u30fc and \u30b5\u30fc\u30d0 for \"server\".\n\nI'm proposing that we add a katakana stemmer that removes this long sound if the terms are longer than a configurable length.  It's also possible to add the variant as a synonym, but I think stemming is preferred from a ranking point of view.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2490",
        "summary": "'ant generate-maven-artifacts' should work for lucene+solr 3.x+",
        "description": "The maven build scripts need to be updated so that solr uses the artifacts from lucene.\n\nFor consistency, we should be able to have a different 'maven_version' then the 'version'  That is, we want to build: 3.1-SNAPSHOT with a jar file: 3.1-dev",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1450",
        "summary": "RangeQuery & RangeFilter used with collation seek to lowerTerm using compareTo()",
        "description": "The constructor for RangeTermEnum initializes a TermEnum starting with lowerTermText, but when a collator is defined, all terms in the given field need to be checked, since collation can introduce non-Unicode orderings.  Instead, the RangeTermEnum constructor should test for a non-null collator, and if there is one, point the TermEnum at the first term in the given field.\n\nLUCENE-1424 introduced this bug.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2488",
        "summary": "Fix 2.9 contrib builds to succeed when JDK 1.4 is used (leaving out contribs that require 1.5)",
        "description": "When you build and test Lucene 2.9 with Java 1.4, building and testing of contrib fails. This patch fixes this to repect the current compiler version and disables all contribs that need Java 1.5 by checking their javac.source property.\n\nThis patch can be ported to 3.x or trunk, when 1.6 contribs will appear.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1242",
        "summary": "small speedups to bulk merging",
        "description": "The bulk merging code, for stored fields & term vectors, was calling isDeleted twice for each deleted doc.\n\nPatch also changes DocumentsWriter to use IndexWriter.message for its infoStream messages.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-441",
        "summary": "IntParser and FloatParser unused by FieldCacheImpl",
        "description": "FieldCacheImpl doesn't use IntParser or FloatParser to parse values",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-311",
        "summary": "[PATCH] queryParser.setOperator(int) should be made typesafe",
        "description": "There are AND and DEFAULT_OPERATOR_AND in QueryParser, so calling \nsetOperator(QueryParser.AND) looks okay and compiles, but it's not correct. \nI'll attach a patch that uses a typesafe enum to avoid this problem. As \nthere's also a getOperator method I had to change the name of the new method \nto get/setDefaultOperator. I don't like that, but it seems to be the only way \nto avoid compile errors for people who switch to a new version of Lucene. \n \nOkay to commit?",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-795",
        "summary": "deprecate Directory.renameFile()",
        "description": "Copied from my mailing list post so this issue can be tracked (if necessary). I will commit a patch.\n\nI see that Directory.renameFile() isn't used anymore. I assume it has only \nbeen public for technical reasons, not because we expect this to be used \nfrom outside of Lucene? Should we deprecate this method? Its \nimplementation e.g. in FSDirectory looks a bit scary anyway (the comment \ncorrectly says \"This is not atomic\" while the abstract class says \"This \nreplacement should be atomic\").\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3285",
        "summary": "Move QueryParsers from contrib/queryparser to queryparser module",
        "description": "Each of the QueryParsers will be ported across.\n\nThose which use the flexible parsing framework will be placed under the package flexible.  The StandardQueryParser will be renamed to FlexibleQueryParser and surround.QueryParser will be renamed to SurroundQueryParser.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-2222",
        "summary": "FixedIntBlockIndexInput.Reader does not initialise 'pending' int array",
        "description": "The FixedIntBlockIndexInput.Reader.pending int array is not initialised. As a consequence, the FixedIntBlockIndexInput.Reader#next() method returns always 0.\n\nA call to FixedIntBlockIndexInput.Reader#blockReader.readBlock() during the Reader initialisation may solve the issue (to be tested).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2426",
        "summary": "change sort order to binary order",
        "description": "Since flexible indexing, terms are now represented as byte[], but for backwards compatibility reasons, they are not sorted as byte[], but instead as if they were char[].\n\nI think its time to look at sorting terms as byte[]... this would yield the following improvements:\n* terms are more opaque by default, they are byte[] and sort as byte[]. I think this would make lucene friendlier to customizations.\n* numerics and collation are then free to use their own encoding (full byte) rather than avoiding the use of certain bits to remain compatible with char[] sort order.\n* automaton gets simpler because as in LUCENE-2265, it uses byte[] too, and has special hacks because terms are sorted as char[]\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2094",
        "summary": "Prepare CharArraySet for Unicode 4.0",
        "description": "CharArraySet does lowercaseing if created with the correspondent flag. This causes that  String / char[] with uncode 4 chars which are in the set can not be retrieved in \"ignorecase\" mode.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3583",
        "summary": "benchmark tests always fail on windows because directory cannot be removed",
        "description": "This seems to be a bug recently introduced. I have no idea what's wrong. Attached is a log file, reproduces everytime.\n\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1558",
        "summary": "Make IndexReader/Searcher ctors readOnly=true by default",
        "description": "Another \"change the defaults\" in 3.0.\n\nRight now you get a read/write reader from IndexReader.open and new IndexSearcher(...), and reserving the right to write causes thread contention (on isDeleted).\n\nIn 3.0 let's make readOnly reader the default, but still allow opening a read/write IndexReader.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1663",
        "summary": "Documentation bug.  The 2.4.1 query parser syntax wiki page says it is for 1.9",
        "description": "This page:\n\nhttp://lucene.apache.org/java/2_4_1/queryparsersyntax.html\n\nsays this:\n.bq\nThis page provides the Query Parser syntax in Lucene 1.9. If you are using a different version of Lucene, please consult the copy of docs/queryparsersyntax.html that was distributed with the version you are using. \n\nThis is misleading on a doc page for 2.4.1",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1854",
        "summary": "build.xml's tar task should use longfile=\"gnu\"",
        "description": "The default (used now) is the same, but we get all those nasty false warnings filling the screen.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-429",
        "summary": "Little improvement for SimpleHTMLEncoder",
        "description": "The SimpleHTMLEncoder could be improved slightly: all characters with code >=\n128 should be encoded as character entities. The reason is, that the encoder\ndoes not know the encoding that is used for the response. Therefore it is safer\nto encode all characters beyond ASCII as character entities.\n\nHere is the necessary modification of SimpleHTMLEncoder:\n\n       default:\n         if (c < 128) {\n           result.append(c);\n         } else {\n           result.append(\"&#\").append((int)c).append(\";\");\n         }",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2596",
        "summary": "Impl toString() in MergePolicy and its extensions",
        "description": "These can be important to see for debugging.\n\nWe lost them in the cutover to IWC.\n\nJust opening this issue to remind us to get them back, before releasing...",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3147",
        "summary": "MockDirectoryWrapper should track open file handles of IndexOutput too",
        "description": "MockDirectoryWrapper currently tracks open file handles of IndexInput only. Therefore IO files that are not closed do not fail our tests, which can then lead to test directories fail to delete on Windows. We should make sure all open files are tracked and if they are left open, fail the test. I'll attach a patch shortly.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2398",
        "summary": "Improve tests to work easier from IDEs",
        "description": "As reported by Paolo Castagna on the mailing lists, some tests fail when you run them from eclipse.\n\nSome of the failures he reports are actually code problems such as base test classes not being \nabstract when they should be... we should fix things like that.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2463",
        "summary": "Improve Greek analysis",
        "description": "* Changed tokenstreams to CharTermAttribute\n* Moved stopwords out of private String[] to a txt file (for use by Solr, etc)\n* Removed TODO / fixed unicode conformance of GreekLowerCaseFilter\n* Reformatted touched files to normal indentation\n* Added inflectional stemmer (Ntais algorithm)\n\nall the changes are backwards compatible with Version.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1962",
        "summary": "Persian Arabic Analyzer cleanup",
        "description": "While browsing through the code I found some places for minor improvements in the new Arabic / Persian Analyzer code. \n\n- prevent default stopwords from being loaded each time a default constructor is called\n- replace if blocks with a single switch\n- marking private members final where needed\n- changed protected visibility to final in final class.\n\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-869",
        "summary": "Make FSIndexInput and FSIndexOutput inner classes of FSDirectory",
        "description": "I would like make FSIndexInput and FSIndexOutput protected, static, inner classes of FSDirectory. Currently these classes are located in the same source file as FSDirectory, which means that classes outside the store package can not extend them.\n\nI don't see any performance impacts or other side effects of this trivial patch. All unit tests pass.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3534",
        "summary": "Backport FilteredQuery/IndexSearcher changes to 3.x: Remove filter logic from IndexSearcher and delegate to FilteredQuery",
        "description": "Spinoff from LUCENE-1536: We simplified the code in IndexSearcher to no longer do the filtering there, instead wrap all Query with FilteredQuery, if a non-null filter is given. The conjunction code would then only exist in FilteredQuery which makes it easier to maintain. Currently both implementations differ in 3.x, in trunk we used the more optimized IndexSearcher variant with addition of a simplified in-order conjunction code.\n\nThis issue will backport those changes (without random access bits).",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1681",
        "summary": "DocValues infinite loop caused by - a call to getMinValue | getMaxValue | getAverageValue",
        "description": "org.apache.lucene.search.function.DocValues offers 3 public (optional) methods to access value statistics like min, max and average values of the internal values. A call to one of the methods will result in an infinite loop. The internal counter is not incremented. \nI added a testcase, javadoc and a slightly different implementation to it. I guess this is not breaking any back compat. as a call to those methodes would have caused an infinite loop anyway.\nI changed the return value of all of those methods to Float.NaN if the DocValues implementation does not contain any values.\n\nIt might be considerable to fix this in 2.4.2 and 2.3.3\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1688",
        "summary": "Deprecating StopAnalyzer ENGLISH_STOP_WORDS - General replacement with an immutable Set",
        "description": "StopAnalyzer and StandartAnalyzer are using the static final array ENGLISH_STOP_WORDS by default in various places. Internally this array is converted into a mutable set which looks kind of weird to me. \nI think the way to go is to deprecate all use of the static final array and replace it with an immutable implementation of CharArraySet. Inside an analyzer it does not make sense to have a mutable set anyway and we could prevent set creation each time an analyzer is created. In the case of an immutable set we won't have multithreading issues either. \nin essence we get rid of a fair bit of \"converting string array to set\" code, do not have a PUBLIC static reference to an array (which is mutable) and reduce the overhead of analyzer creation.\n\nlet me know what you think and I create a patch for it.\n\nsimon",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1139",
        "summary": "Various small improvements to contrib/benchmark",
        "description": "I've worked out a few small improvements to contrib/benchmark:\n\n  * Refactored the common code in Open/CreateIndexTask that sets the\n    configuration for the IndexWriter.  This also fixes a bug in\n    OpenIndexTasks that prevented you from disabling flushing by RAM.\n\n  * Added a new config property for LineDocMaker:\n\n      doc.reuse.fields=true|false\n\n    which turns on/off reusing of Field/Document by LineDocMaker.\n    This lets us measure performance impact of sharing Field/Document\n    vs not, and also turn it off when necessary (eg if you have your\n    own consumer that uses private threads).\n\n  * Added merge.scheduler & merge.policy config options.\n\n  * Added param for OptimizeTask, which expects an int and calls\n    optimize(maxNumSegments) with that param.\n\n  * Added param for CloseIndex(true|false) -- if you pass false that\n    means close the index, aborting any running merges\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3439",
        "summary": "add checks/asserts if you search across a closed reader",
        "description": "if you try to search across a closed reader (and/or searcher too),\nthere are no checks, not even assertions statements.\n\nthis results in crazy scary stacktraces deep inside places like FSTs/various term dictionary implementations etc.\n\nIn some situations, depending on codec, you wont even get an error (i'm sure its fun when you try to retrieve the stored fields!)\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-982",
        "summary": "Create new method optimize(int maxNumSegments) in IndexWriter",
        "description": "Spinning this out from the discussion in LUCENE-847.\n\nI think having a way to \"slightly optimize\" your index would be useful\nfor many applications.\n\nThe current optimize() call is very expensive for large indices\nbecause it always optimizes fully down to 1 segment.  If we add a new\nmethod which instead is allowed to stop optimizing once it has <=\nmaxNumSegments segments in the index, this would allow applications to\neg optimize down to say <= 10 segments after doing a bunch of updates.\nThis should be a nice compromise of gaining good speedups of searching\nwhile not spending the full (and typically very high) cost of\noptimizing down to a single segment.\n\nSince LUCENE-847 is now formalizing an API for decoupling merge policy\nfrom IndexWriter, if we want to add this new optimize method we need\nto take it into account in LUCENE-847.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2603",
        "summary": "FastVectorHighlighter: add a method to set an arbitrary char that is used when concatenating multiValued data",
        "description": "If the following multiValued names are in authors field:\n\n* Michael McCandless\n* Erik Hatcher\n* Otis Gospodneti\u0107\n\nSince FragmentsBuilder concatenates multiValued data with a space in BaseFragmentsBuilder.getFragmentSource():\n\n{code}\nwhile( buffer.length() < endOffset && index[0] < values.length ){\n  if( index[0] > 0 && values[index[0]].isTokenized() && values[index[0]].stringValue().length() > 0 )\n    buffer.append( ' ' );\n  buffer.append( values[index[0]++].stringValue() );\n}\n{code}\n\nan entire field snippet (using LUCENE-2464) will be \"Michael McCandless Erik Hatcher Otis Gospodneti\u0107\". There is a requirement an arbitrary char (e.g. '/') can be set so that client can separate the snippet easily. i.e. \"Michael McCandless/Erik Hatcher/Otis Gospodneti\u0107\"",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-607",
        "summary": "ParallelTermEnum is BROKEN",
        "description": "ParallelTermEnum.next() fails to advance properly to new fields.  This is a serious bug. \n\nChristian Kohlschuetter diagnosed this as the root problem underlying LUCENE-398 and posted a first patch there.\n\nI've addressed a couple issues in the patch (close skipped field TermEnum's, generate field iterator only once, integrated Christian's test case as a Lucene test) and packaged in all the revised patch here.\n\nAll Lucene tests pass, and I've further tested in this in my app, which makes extensive use of ParallelReader.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-711",
        "summary": "BooleanWeight should size the weights Vector correctly",
        "description": "The weights field on BooleanWeight uses a Vector that will always be sized exactly the same as the outer class' clauses Vector, therefore can be sized correctly in the constructor. This is a trivial memory saving enhancement.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2370",
        "summary": "Reintegrate flex branch into trunk",
        "description": "This issue is for reintegrating the flex branch into current trunk. I will post the patch here for review and commit, when all contributors to flex have reviewed the patch.\n\nBefore committing, I will tag both trunk and flex.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2459",
        "summary": "FilterIndexReader doesn't work correctly with post-flex SegmentMerger",
        "description": "IndexWriter.addIndexes(IndexReader...) internally uses SegmentMerger to add data from input index readers. However, SegmentMerger uses the new post-flex API to do this, which bypasses the pre-flex TermEnum/TermPositions API that FilterIndexReader implements. As a result, filtering is not applied.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2380",
        "summary": "Add FieldCache.getTermBytes, to load term data as byte[]",
        "description": "With flex, a term is now an opaque byte[] (typically, utf8 encoded unicode string, but not necessarily), so we need to push this up the search stack.\n\nFieldCache now has getStrings and getStringIndex; we need corresponding methods to load terms as native byte[], since in general they may not be representable as String.  This should be quite a bit more RAM efficient too, for US ascii content since each character would then use 1 byte not 2.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2196",
        "summary": "Spellchecker should implement java.io.Closable",
        "description": "As the most of the lucene classes implement Closable (IndexWriter) Spellchecker should do too. ",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-978",
        "summary": "GC resources in TermInfosReader when exception occurs in its constructor",
        "description": "I replaced IndexModifier with IndexWriter in test case TestStressIndexing and noticed the test failed from time to time because some .tis file is still open when MockRAMDirectory.close() is called. It turns out it is because .tis file is not closed if an exception occurs in TermInfosReader's constructor.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-395",
        "summary": "CoordConstrainedBooleanQuery + QueryParser support",
        "description": "Attached 2 new classes:\n\n1) CoordConstrainedBooleanQuery\nA boolean query that only matches if a specified number of the contained clauses\nmatch. An example use might be a query that returns a list of books where ANY 2\npeople from a list of people were co-authors, eg:\n\"Lucene In Action\" would match (\"Erik Hatcher\" \"Otis Gospodneti&#263;\" \"Mark Harwood\"\n\"Doug Cutting\") with a minRequiredOverlap of 2 because Otis and Erik wrote that.\nThe book \"Java Development with Ant\" would not match because only 1 element in\nthe list (Erik) was selected.\n\n2) CustomQueryParserExample\nA customised QueryParser that allows definition of\nCoordConstrainedBooleanQueries. The solution (mis)uses fieldnames to pass\nparameters to the custom query.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1590",
        "summary": "Stored-only fields automatically enable norms and tf when added to document",
        "description": "During updating my internal components to the new TrieAPI, I have seen the following:\n\nI index a lot of numeric fields with trie encoding omitting norms and term frequency. This works great. Luke shows that both is omitted.\n\nAs I sometimes also want to have the components of the field stored and want to use the same field name for it. So I add additionally the field again to the document, but stored only (as the Field c'tor using a TokenStream cannot additionally store the field). As it is stored only, I thought, that I can left out explicit setting of omitNorms and omitTermFreqAndPositions. After adding the stored-only-without-omits field, Luke shows all fields with norms enabled. I am not sure, if the norms/tf were really added to the index, but Luke shows a value for the norms and FieldInfo has it enabled.\n\nIn my opinion, this is not intuitive, o.a.l.document.Field  should switch both omit* options on when storing fields only (and also disable other indexing-only options). Alternatively the internal FieldInfo.update(boolean isIndexed, boolean storeTermVector, boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTermFreqAndPositions) should only change the omit* and other options, if the isIndexed parameter (not this.isIndexed) is also true, elsewhere leave it as it is.\n\nIn principle, when adding a stored-only field, any indexing-specific options should not be changed in FieldInfo. If the field was indexed with norms before, norms should stay enabled (but this would be the default as it is).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3595",
        "summary": "Refactor FieldCacheRangeFilter.FieldCacheDocIdSet to be separate class and fix the dangerous matchDoc() throws AIOOBE requirement",
        "description": "Followup from LUCENE-3593:\nThe FieldCacheRangeFilter.FieldCacheDocIdSet class has a strange requirement on the abstract matchDoc(): It should throw AIOOBE if the docId is > maxDoc. This check should be done by caller as especially on trunk, e.g. FieldCacheTermsFilter does not seem to always throw this exception correctly (getOrd() is a method and no array in TermsIndex cache).\n\nAlso in 3.x the Filter does not correctly respect deletions when a FieldCache based on a reopened reader is used.\n\nThis issue will refactor this and fix the bugs and moves the docId check up to the iterator.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2326",
        "summary": "Remove SVN.exe and revision numbers from build.xml by svn-copy the backwards branch and linking snowball tests by svn:externals",
        "description": "As we often need to update backwards tests together with trunk and always have to update the branch first, record rev no, and update build xml, I would simply like to do a svn copy/move of the backwards branch.\n\nAfter a release, this is simply also done:\n{code}\nsvn rm backwards\nsvn cp releasebranch backwards\n{code}\n\nBy this we can simply commit in one pass, create patches in one pass.\n\nThe snowball tests are currently downloaded by svn.exe, too. These need a fixed version for checkout. I would like to change this to use svn:externals. Will provide patch, soon.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-491",
        "summary": "DateTools needs to use UTC for correct collation,",
        "description": "If your local timezone is Europe/London then the times Sun, 30 Oct 2005 00:00:00 +0000 and exactly one hour later are both converted to 200530010000 by DateTools.dateToString() with minute resolution.   The Linux date command is useful in seeing why:\n\n    $ date --date \"Sun, 30 Oct 2005 00:00:00 +0000\"\n    Sun Oct 30 01:00:00 BST 2005\n\n    $ date --date \"Sun, 30 Oct 2005 01:00:00 +0000\"\n    Sun Oct 30 01:00:00 GMT 2005\n\nBoth times are 1am in the morning, but one is when DST is in force, the other isn't.   Of course, these are actually different times!\n\nOf course, if dates are stored in the index with implicit timezone information then not only do we get problems when the clocks go back at the end of summer, but we also have problems crossing timezones.   If a database is created in California and used in Paris then the times are going to be badly skewed (there's a nine hour time difference most of the year).\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3231",
        "summary": "Add fixed size DocValues int variants & expose Arrays where possible",
        "description": "currently we only have variable bit packed ints implementation. for flexible scoring or loading field caches it is desirable to have fixed int implementations for 8, 16, 32 and 64 bit. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1599",
        "summary": "SpanRegexQuery and SpanNearQuery is not working with MultiSearcher",
        "description": "MultiSearcher is using:\nqueries[i] = searchables[i].rewrite(original);\nto rewrite query and then use combine to combine them.\n\nBut SpanRegexQuery's rewrite is different from others.\nAfter you call it on the same query, it always return the same rewritten queries.\n\nAs a result, only search on the first IndexSearcher work. All others are using the first IndexSearcher's rewrite queries.\nSo many terms are missing and return unexpected result.\n\nBillow",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1520",
        "summary": "OOM erros with CheckIndex with indexes containg a lot of fields with norms",
        "description": "All index readers have a cache of the last used norms (SegmentReader, MultiReader, MultiSegmentReader,...). This cache is never cleaned up, so if you access norms of a field, the norm's byte[maxdoc()] array is not freed until you close/reopen the index.\n\nYou can see this problem, if you create an index with many fields with norms (I tested with about 4,000 fields) and many documents (half a million). If you then call CheckIndex, that calls norms() for each (!) field in the Segment and each of this calls creates a new cache entry, you get OutOfMemoryExceptions after short time (I tested with the above index: I was not able to do a CheckIndex even with \"-Xmx 16GB\" on 64bit Java).\n\nCheckIndex opens and then tests each segment of a index with a separate SegmentReader. The big index with the OutOfMemory problem was optimized, so consisting of one segment with about half a million docs and about 4,000 fields. Each byte[] array takes about a half MiB for this index. The CheckIndex funtion created the norm for 4000 fields and the SegmentReader cached them, which is about 2 GiB RAM. So OOMs are not unusal.\n\nIn my opinion, the best would be to use a Weak- or better a SoftReference so norms.bytes gets java.lang.ref.SoftReference<byte[]> and used for caching. With proper synchronization (which is done on the norms cache in SegmentReader) you can do the best with SoftReference, as this reference is garbage collected only when an OOM may happen. If the byte[] array is freed (but it is only freed if no other references exist), a lter call to getNorms() creates a new array. When code is hard referencing the norms array, it will not be freed, so no problem. The same could be done for the other IndexReaders.\n\nFields without norm() do not have this problem, as all these fields share a one-time allocated dummy norm array. So the same index without norms enabled for most of the fields checked perfectly.\n\nI will prepare a patch tomorrow.\n\nMike proposed another quick fix for CheckIndex:\nbq. we could do something first specifically for CheckIndex (eg it could simply use the 3-arg non-caching bytes method instead) to prevent OOM errors when using it.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1469",
        "summary": "isValid should be invoked after analyze rather than before it so it can validate the output of analyze",
        "description": "The Synonym map has a protected method String analyze(String word) designed for custom stemming.\n\nHowever, before analyze is invoked on a word, boolean isValid(String str) is used to validate the word - which causes the program to discard words that maybe useable by the custom analyze method. \n\nI think that isValid should be invoked after analyze rather than before it so it can validate the output of analyze and allow implemters to decide what is valid for the overridden analyze method. (In fact, if you look at code snippet below, isValid should really go after the empty string check)\n\nThis is a two line change in org.apache.lucene.index.memory.SynonymMap\n\n      /*\n       * Part B: ignore phrases (with spaces and hyphens) and\n       * non-alphabetic words, and let user customize word (e.g. do some\n       * stemming)\n       */\n      if (!isValid(word)) continue; // ignore\n      word = analyze(word);\n      if (word == null || word.length() == 0) continue; // ignore",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-734",
        "summary": "Upload Lucene 2.0 artifacts in the Maven 1 repository",
        "description": "The Lucene 2.0 artifacts can be found in the Maven 2 repository, but not in the Maven 1 repository. There are still projects using Maven 1 who might be interested in upgrading to Lucene 2, so having the artifacts also in the Maven 1 repository would be very helpful.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2068",
        "summary": "fix reverseStringFilter for unicode 4.0",
        "description": "ReverseStringFilter is not aware of supplementary characters: when it reverses it will create unpaired surrogates, which will be replaced by U+FFFD by the indexer (but not at query time).\nThe wrong words will conflate to each other, and the right words won't match, basically the whole thing falls apart.\n\nThis patch implements in-place reverse with the algorithm from apache harmony AbstractStringBuilder.reverse0()\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3744",
        "summary": "Add support for type whitelist in TypeTokenFilter",
        "description": "A usual use case for TypeTokenFilter is allowing only a set of token types. That is, listing allowed types, instead of filtered ones. I'm attaching a patch to add a useWhitelist option for that.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2652",
        "summary": "Rethink LocalizedTestCaseRunner with JUnit 4 - Clover OOM",
        "description": "As a spinn off from this [conversation|http://www.lucidimagination.com/search/document/ae20885bf5baedc5/build_failed_in_hudson_lucene_3_x_116#7ed351341152ee2d] we should rethink the way how we execute testcases with different locals since glover reports appears to throw OOM errors b/c Junit treats each local as a single test case run.\n\nHere are some options:\n* select the local at random only run the test with a single local\n* set the local via system property -Dtest.locale=en.EN\n* run with the default locale only -Dtest.skiplocale=true\n* one from the above but only if instrumented with clover (let common tests run all the locale)\n\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1443",
        "summary": "Performance improvement in OpenBitSetDISI.inPlaceAnd()",
        "description": "",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2128",
        "summary": "Further parallelizaton of ParallelMultiSearcher",
        "description": "When calling {{search(Query, Filter, int)}} on a ParallelMultiSearcher, the {{createWeights}} function of MultiSearcher is called, and sequentially calls {{docFreqs()}} on every sub-searcher. This can take a significant amount of time when there are lots of remote sub-searchers.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-546",
        "summary": "Index corruption when using RAMDirectory( Directory) constructor",
        "description": "LUCENE-475 introduced a bug in creating RAMDirectories for large indexes. It truncates the length of the file to an int, from its original long value. Any files that are larger than an int are truncated. Patch to fix is attached.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2200",
        "summary": "Several final classes have non-overriding protected members",
        "description": "Protected member access in final classes, except where a protected method overrides a superclass's protected method, makes little sense.  The attached patch converts final classes' protected access on fields to private, removes two final classes' unused protected constructors, and converts one final class's protected final method to private.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1595",
        "summary": "Split DocMaker into ContentSource and DocMaker",
        "description": "This issue proposes some refactoring to the benchmark package. Today, DocMaker has two roles: collecting documents from a collection and preparing a Document object. These two should actually be split up to ContentSource and DocMaker, which will use a ContentSource instance.\n\nContentSource will implement all the methods of DocMaker, like getNextDocData, raw size in bytes tracking etc. This can actually fit well w/ 1591, by having a basic ContentSource that offers input stream services, and wraps a file (for example) with a bzip or gzip streams etc.\n\nDocMaker will implement the makeDocument methods, reusing DocState etc.\n\nThe idea is that collecting the Enwiki documents, for example, should be the same whether I create documents using DocState, add payloads or index additional metadata. Same goes for Trec and Reuters collections, as well as LineDocMaker.\nIn fact, if one inspects EnwikiDocMaker and LineDocMaker closely, they are 99% the same and 99% different. Most of their differences lie in the way they read the data, while most of the similarity lies in the way they create documents (using DocState).\nThat led to a somehwat bizzare extension of LineDocMaker by EnwikiDocMaker (just the reuse of DocState). Also, other DocMakers do not use that DocState today, something they could have gotten for free with this refactoring proposed.\n\nSo by having a EnwikiContentSource, ReutersContentSource and others (TREC, Line, Simple), I can write several DocMakers, such as DocStateMaker, ConfigurableDocMaker (one which accpets all kinds of config options) and custom DocMakers (payload, facets, sorting), passing to them a ContentSource instance and reuse the same DocMaking algorithm with many content sources, as well as the same ContentSource algorithm with many DocMaker implementations.\n\nThis will also give us the opportunity to perf test content sources alone (i.e., compare bzip, gzip and regular input streams), w/o the overhead of creating a Document object.\n\nI've already done so in my code environment (I extend the benchmark package for my application's purposes) and I like the flexibility I have. I think this can be a nice contribution to the benchmark package, which can result in some code cleanup as well.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2687",
        "summary": "Remove Priority-Queue size trap in MultiTermQuery.TopTermsBooleanQueryRewrite",
        "description": "These APIs are new in 3.x, so we can do this with no backwards-compatibility issue:\n\nBefore 3.1, FuzzyQuery had its own internal rewrite method.\nWe exposed this in 3.x as TopTermsBooleanQueryRewrite, and then as subclasses for Scoring and Boost-only variants.\n\nThe problem I have is that the PQ has a default (large) size of Integer.MAX_VALUE... of course its later limited by\nthe value of BooleanQuery's maxClauseCount, but I think this is a trap.\n\nInstead its better to simply remove these defaults and force the user to provide a default (reasonable) size.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-750",
        "summary": "don't use finalizers for FSIndexInput clones",
        "description": "finalizers are expensive, and we should avoid using them where possible.\nIt looks like this helped to tickle some kind of bug (looks like a JVM bug?)\nhttp://www.nabble.com/15-minute-hang-in-IndexInput.clone%28%29-involving-finalizers-tf2826906.html#a7891015",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1031",
        "summary": "Fixes a handful of misspellings/mistakes in changes.txt",
        "description": "There are a handful of misspellings/mistakes in changes.txt. This patch fixes them. Avoided the one or two British to English conversions <g>",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3017",
        "summary": "FST should differentiate between final vs non-final stop nodes",
        "description": "I'm breaking out this one improvement from LUCENE-2948...\n\nCurrently, if a node has no outgoing edges (a \"stop node\") the FST\nforcefully marks this as a final node, but it need not do this.  Ie,\nwhether that node is final or not should be orthogonal to whether it\nhas arcs leaving or not.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3409",
        "summary": "NRT reader/writer over RAMDirectory memory leak",
        "description": "with NRT reader/writer, emptying an index using:\nwriter.deleteAll()\nwriter.commit()\ndoesn't release all allocated memory.\n\nfor example the following code will generate a memory leak:\n\n/**\n\t * Reveals a memory leak in NRT reader/writer<br>\n\t * \n\t * The following main() does 10K cycles of:\n\t * <ul>\n\t * <li>Add 10K empty documents to index writer</li>\n\t * <li>commit()</li>\n\t * <li>open NRT reader over the writer, and immediately close it</li>\n\t * <li>delete all documents from the writer</li>\n\t * <li>commit changes to the writer</li>\n\t * </ul>\n\t * \n\t * Running with -Xmx256M results in an OOME after ~2600 cycles\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\t\tRAMDirectory d = new RAMDirectory();\n\t\tIndexWriter w = new IndexWriter(d, new IndexWriterConfig(Version.LUCENE_33, new KeywordAnalyzer()));\n\t\tDocument doc = new Document();\n\t\t\n\t\tfor(int i = 0; i < 10000; i++) {\n\t\t\tfor(int j = 0; j < 10000; ++j) {\n\t\t\t\tw.addDocument(doc);\n\t\t\t}\n\t\t\tw.commit();\n\t\t\tIndexReader.open(w, true).close();\n\n\t\t\tw.deleteAll();\n\t\t\tw.commit();\n\t\t}\n\t\t\n\t\tw.close();\n\t\td.close();\n\t}\t",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1047",
        "summary": "Change MergePolicy & MergeScheduler to be abstract base classes instead of an interfaces",
        "description": "This gives us freedom to add methods with default base implementation over time w/o breaking backwards compatibility.\n\nThanks to Hoss for raising this!",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-316",
        "summary": "[PATCH] Mention RangeFilter in javadoc for maxClauseCount",
        "description": " ",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3800",
        "summary": "Readers wrapping other readers don't prevent usage if any of their subreaders was closed",
        "description": "On recent trunk test we got this problem:\norg.apache.lucene.index.TestReaderClosed.test\nfails because the inner reader is closed but the wrapped outer ones are still open.\n\nI fixed the issue partially for SlowCompositeReaderWrapper and ParallelAtomicReader but it failed again. The cool thing with this test is the following:\n\nThe test opens an DirectoryReader and then creates a searcher, closes the reader and executes a search. This is not an issue, if the reader is closed that the search is running on. This test uses LTC.newSearcher(wrap=true), which randomly wraps the passed Reader with SlowComposite or ParallelReader - or with both!!! If you then close the original inner reader, the close is not detected when excuting search. This can cause SIGSEGV when MMAP is used.\n\nThe problem in (in Slow* and Parallel*) is, that both have their own Fields instances thats are kept alive until the reader itsself is closed. If the child reader is closed, the wrapping reader does not know and still uses its own Fields instance that delegates to the inner readers. On this step no more ensureOpen checks are done, causing the failures.\n\nThe first fix done in Slow and Parallel was to call ensureOpen() on the subReader, too when requesting fields(). This works fine until you wrap two times: ParallelAtomicReader(SlowCompositeReaderWrapper(StandardDirectoryReader(segments_1:3:nrt _0(4.0):C42)))\n\nOne solution would be to make ensureOpen also check all subreaders, but that would do the volatile checks way too often (with n is the total number of subreaders and m is the number of hierarchical levels this is n^m) - we cannot do this. Currently we only have n*m which is fine.\n\nThe proposal how to solve this (closing subreaders under the hood of parent readers is to use the readerClosedListeners. Whenever a composite or slow reader wraps another readers, it registers itself as interested in readerClosed events. When a subreader is then forcefully closed (e.g by a programming error or this crazy test), we automatically close the parents, too.\n\nWe should also fix this in 3.x, if we have similar problems there (needs investigation).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2762",
        "summary": "Don't leak deleted open file handles with pooled readers",
        "description": "If you have CFS enabled today, and pooling is enabled (either directly\nor because you've pulled an NRT reader), IndexWriter will hold open\nSegmentReaders against the non-CFS format of each merged segment.\n\nSo even if you close all NRT readers you've pulled from the writer,\nyou'll still see file handles open against files that have been\ndeleted.\n\nThis count will not grow unbounded, since it's limited by the number\nof segments in the index, but it's still a serious problem since the\napp had turned off CFS in the first place presumably to avoid risk of\ntoo-many-open-files.  It's also bad because it ties up disk space\nsince these files would otherwise be deleted.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-761",
        "summary": "Clone proxStream lazily in SegmentTermPositions",
        "description": "In SegmentTermPositions the proxStream should be cloned lazily, i. e. at the first time nextPosition() is called. Then the initialization costs of TermPositions are not higher anymore compared to TermDocs and thus there is no reason anymore for Scorers to use TermDocs instead of TermPositions. In fact, all Scorers should use TermPositions, because custom subclasses of existing scorers might want to access payloads, which is only possible via TermPositions. We could further merge SegmentTermDocs and SegmentTermPositions into one class and deprecate the interface TermDocs.\n\nI'm going to attach a patch once the payloads feature (LUCENE-755) is committed.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2744",
        "summary": "CheckIndex overstates how many fields have norms enabled",
        "description": "It simply tells you how many unique fields there are... it should instead only say how many have norms.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2113",
        "summary": "singletermsenum",
        "description": "singletermsenum for flex (like the existing singletermenum, it is a filteredtermSenum that only matches one term, to preserve multitermquery semantics)",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-413",
        "summary": "[PATCH] BooleanScorer2 ArrayIndexOutOfBoundsException + alternative NearSpans",
        "description": "From Erik's post at java-dev: \n \n> \u00a0 \u00a0 \u00a0[java] Caused by: java.lang.ArrayIndexOutOfBoundsException: 4 \n> \u00a0 \u00a0 \u00a0[java] \u00a0 \u00a0 at org.apache.lucene.search.BooleanScorer2  \n> $Coordinator.coordFactor(BooleanScorer2.java:54) \n> \u00a0 \u00a0 \u00a0[java] \u00a0 \u00a0 at org.apache.lucene.search.BooleanScorer2.score  \n> (BooleanScorer2.java:292) \n... \n \nand my answer: \n \nProbably nrMatchers is increased too often in score() by calling score() \nmore than once.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1886",
        "summary": "Improve Javadoc",
        "description": "",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-651",
        "summary": "Poor performance race condition in FieldCacheImpl",
        "description": "A race condition exists in FieldCacheImpl that causes a significant performance degradation if multiple threads concurrently request a value that is not yet cached. The degradation is particularly noticable in large indexes and when there a many concurent requests for the cached value.\n\nFor the full discussion see the mailing list thread 'Poor performance \"race condition\" in FieldSortedHitQueue' (http://www.gossamer-threads.com/lists/lucene/java-user/38717).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3348",
        "summary": "IndexWriter applies wrong deletes during concurrent flush-all",
        "description": "Yonik uncovered this with the TestRealTimeGet test: if a flush-all is\nunderway, it is possible for an incoming update to pick a DWPT that is\nstale, ie, not yet pulled/marked for flushing, yet the DW has cutover\nto a new deletes queue.  If this happens, and the deleted term was\nalso updated in one of the non-stale DWPTs, then the wrong document is\ndeleted and the test fails by detecting the wrong value.\n\nThere's a 2nd failure mode that I haven't figured out yet, whereby 2\ndocs are returned when searching by id (there should only ever be 1\ndoc since the test uses updateDocument which is atomic wrt\ncommit/reopen).\n\nYonik verified the test passes pre-DWPT, so my guess is (but I\nhave yet to verify) this test also passes on 3.x.  I'll backport\nthe test to 3.x to be sure.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3023",
        "summary": "Land DWPT on trunk",
        "description": "With LUCENE-2956 we have resolved the last remaining issue for LUCENE-2324 so we can proceed landing the DWPT development on trunk soon. I think one of the bigger issues here is to make sure that all JavaDocs for IW etc. are still correct though. I will start going through that first.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3305",
        "summary": "Kuromoji code donation - a new Japanese morphological analyzer",
        "description": "Atilika Inc. (\u30a2\u30c6\u30a3\u30ea\u30ab\u682a\u5f0f\u4f1a\u793e) would like to donate the Kuromoji Japanese morphological analyzer to the Apache Software Foundation in the hope that it will be useful to Lucene and Solr users in Japan and elsewhere.\n\nThe project was started in 2010 since we couldn't find any high-quality, actively maintained and easy-to-use Java-based Japanese morphological analyzers, and these become many of our design goals for Kuromoji.\n\nKuromoji also has a segmentation mode that is particularly useful for search, which we hope will interest Lucene and Solr users.  Compound-nouns, such as \u95a2\u897f\u56fd\u969b\u7a7a\u6e2f (Kansai International Airport) and \u65e5\u672c\u7d4c\u6e08\u65b0\u805e (Nikkei Newspaper), are segmented as one token with most analyzers.  As a result, a search for \u7a7a\u6e2f (airport) or \u65b0\u805e (newspaper) will not give you a for in these words.  Kuromoji can segment these words into \u95a2\u897f \u56fd\u969b \u7a7a\u6e2f and \u65e5\u672c \u7d4c\u6e08 \u65b0\u805e, which is generally what you would want for search and you'll get a hit.\n\nWe also wanted to make sure the technology has a license that makes it compatible with other Apache Software Foundation software to maximize its usefulness.  Kuromoji has an Apache License 2.0 and all code is currently owned by Atilika Inc.  The software has been developed by my good friend and ex-colleague Masaru Hasegawa and myself.\n\nKuromoji uses the so-called IPADIC for its dictionary/statistical model and its license terms are described in NOTICE.txt.\n\nI'll upload code distributions and their corresponding hashes and I'd very much like to start the code grant process.  I'm also happy to provide patches to integrate Kuromoji into the codebase, if you prefer that.\n\nPlease advise on how you'd like me to proceed with this.  Thank you.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3517",
        "summary": "Fix pulsingcodec to reuse its enums",
        "description": "PulsingCodec currently doesnt always reuse its enums, which could lead to behavior like LUCENE-3515.\n\nThe problem is sometimes it returns the 'wrapped' enum, but other times it returns its 'pulsingenum' depending upon\nwhether terms are pulsed...\n\nwe can use the fact that these enums allow attributes to keep the reuse information for both so it can reuse when stepping through terms.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-258",
        "summary": "[PATCH] HTMLParser doesn't parse hexadecimal character references",
        "description": "I recently inherited a project from an ex-colleague; it uses Lucene and in\nparticular the HTML Parser.  I've found that she had made an amendment to the\nparser to allow it to parse and decode hexadecimal character references, which\nwe depend on, but had not reported a bug.  If she had, someone might have\npointed out that her correction was wrong ...\n\nI don't seem to be able to attach the (fairly trivial) patch to an initial bug\nreport (and in any case I've failed to find the instructions for generating a\ndiff file in the right format, even though I'm sure I've seen it somewhere).",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1607",
        "summary": "String.intern() faster alternative",
        "description": "By using our own interned string pool on top of default, String.intern() can be greatly optimized.\n\nOn my setup (java 6) this alternative runs ~15.8x faster for already interned strings, and ~2.2x faster for 'new String(interned)'\nFor java 5 and 4 speedup is lower, but still considerable.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2888",
        "summary": "Several DocsEnum / DocsAndPositionsEnum return wrong docID when next() / advance(int) return NO_MORE_DOCS",
        "description": "During work on LUCENE-2878 I found some minor problems in PreFlex and Pulsing Codec - they are not returning NO_MORE_DOCS but the last docID instead from DocsEnum#docID() when next() or advance(int) returned NO_MORE_DOCS. The JavaDoc clearly says that it should return NO_MORE_DOCS.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2670",
        "summary": "allow automatontermsenum to work on full byte range",
        "description": "AutomatonTermsEnum is really agnostic to whats in your byte[], only that its in binary order.\nso if you wanted to use this on some non-utf8 terms, thats just fine.\n\nthe patch just does some code cleanup and removes \"utf8\" references, etc.\nadditionally i changed the pkg-private, lucene-internal byte-oriented ctor, to public, lucene.experimental.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1545",
        "summary": "Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E",
        "description": "Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E.\nThe word \"mo\u0364chte\" is incorrectly tokenized into \"mo\" \"chte\", the combining character is lost.\nExpected result is only on token \"mo\u0364chte\".",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1901",
        "summary": "TermAttributeImpl.equals() does not check termLength",
        "description": "If you look at the code from equals(), I think it misses this check :\n\nother.termLength==this.termLength\n\nThis check must be before the comparison of the arrays.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2647",
        "summary": "Move & rename the terms dict, index, abstract postings out of oal.index.codecs.standard",
        "description": "The terms dict components that current live under Standard codec\n(oal.index.codecs.standard.*) are in fact very generic, and in no way\nparticular to the Standard codec.  Already we have many other codecs\n(sep, fixed int block, var int block, pulsing, appending) that re-use\nthe terms dict writer/reader components.\n\nSo I'd like to move these out into oal.index.codecs, and rename them:\n\n  * StandardTermsDictWriter/Reader -> PrefixCodedTermsWriter/Reader\n  * StandardTermsIndexWriter/Reader -> AbstractTermsIndexWriter/Reader\n  * SimpleStandardTermsIndexWriter/Reader -> SimpleTermsIndexWriter/Reader\n  * StandardPostingsWriter/Reader -> AbstractPostingsWriter/Reader\n  * StandardPostingsWriterImpl/ReaderImple -> StandardPostingsWriter/Reader\n\nWith this move we have a nice reusable terms dict impl.  The terms\nindex impl is still well-decoupled so eg we could [in theory] explore\na variable gap terms index.\n\nMany codecs, I expect, don't need/want to implement their own terms\ndict....\n\nThere are no code/index format changes here, besides the renaming &\nfixing all imports/usages of the renamed class.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2207",
        "summary": "CJKTokenizer generates tokens with incorrect offsets",
        "description": "If I index a Japanese *multi-valued* document with CJKTokenizer and highlight a term with FastVectorHighlighter, the output snippets have incorrect highlighted string. I'll attach a program that reproduces the problem soon.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-894",
        "summary": "Custom build.xml for binary distributions",
        "description": "The binary files of a distribution come with the demo sources\nand a build.xml file. However, the build.xml doesn't work for\nthe binary distribution, so it can't be used to build the \ndemos.\n\nThis problem was notices the first time when release 2.1 was\nmade. Before we ship 2.2 we should fix this.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3731",
        "summary": "Create a analysis/uima module for UIMA based tokenizers/analyzers",
        "description": "As discussed in SOLR-3013 the UIMA Tokenizers/Analyzer should be refactored out in a separate module (modules/analysis/uima) as they can be used in plain Lucene. Then the solr/contrib/uima will contain only the related factories.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2592",
        "summary": "Add static readSnapshotsInfo to PersistentSnapshotDeletionPolicy",
        "description": "PSDP persists the snapshots information in a Directory. When you open PSDP, it obtains a write lock on the snapshots dir (by keeping an open IndexWriter), and updates the directory when snapshots are created/released.\n\nThis causes problem in the following scenario -- you have two processes, one updates the 'content' index and keeps PSDP open (because it also takes snapshots). Another process wants to read the existing snapshots information and open a read-only IndexReader on the 'content' index. The other process cannot read the existing snapshots information, because p1 keeps a write lock on the snapshots directory.\n\nThere are two possible solutions:\n# Have PSDP open the IndexWriter over the directory for each snapshot/release. A bit expensive, and unnecessary.\n# Introduce a static readSnapshotsInfo on PSDP which accepts a Directory and returns the snapshots information. IMO it's cleaner, and won't have the performance overhead of opening/closing the IW as before.\n\nI'll post a patch (implementing the 2nd approach) shortly. I'd appreciate any comments.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3095",
        "summary": "TestIndexWriter#testThreadInterruptDeadlock fails with OOM ",
        "description": "Selckin reported a repeatedly failing test that throws OOM Exceptions. According to the heapdump the MockDirectoryWrapper#createdFiles HashSet takes about 400MB heapspace containing 4194304 entries. Seems kind of way too many though :)\n\n{noformat}\n [junit] java.lang.OutOfMemoryError: Java heap space\n    [junit] Dumping heap to /tmp/java_pid25990.hprof ...\n    [junit] Heap dump file created [520807744 bytes in 4.250 secs]\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter\n    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):\tFAILED\n    [junit] \n    [junit] junit.framework.AssertionFailedError: \n    [junit] \tat org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:2249)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)\n    [junit] \n    [junit] \n    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):\tFAILED\n    [junit] Some threads threw uncaught exceptions!\n    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:557)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)\n    [junit] \n    [junit] \n    [junit] Tests run: 67, Failures: 2, Errors: 0, Time elapsed: 3,254.884 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] FAILED; unexpected exception\n    [junit] java.lang.OutOfMemoryError: Java heap space\n    [junit] \tat org.apache.lucene.store.RAMFile.newBuffer(RAMFile.java:85)\n    [junit] \tat org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:58)\n    [junit] \tat org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:132)\n    [junit] \tat org.apache.lucene.store.RAMOutputStream.copyBytes(RAMOutputStream.java:171)\n    [junit] \tat org.apache.lucene.store.MockIndexOutputWrapper.copyBytes(MockIndexOutputWrapper.java:155)\n    [junit] \tat org.apache.lucene.index.CompoundFileWriter.copyFile(CompoundFileWriter.java:223)\n    [junit] \tat org.apache.lucene.index.CompoundFileWriter.close(CompoundFileWriter.java:189)\n    [junit] \tat org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:138)\n    [junit] \tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3344)\n    [junit] \tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2959)\n    [junit] \tat org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)\n    [junit] \tat org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1763)\n    [junit] \tat org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1758)\n    [junit] \tat org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1754)\n    [junit] \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1373)\n    [junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1230)\n    [junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1211)\n    [junit] \tat org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2154)\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160\n    [junit] The following exceptions were thrown by threads:\n    [junit] *** Thread: Thread-379 ***\n    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_3r1n_0.tib=1, _3r1n_0.frq=1, _3r1n_0.pos=1, _3r1m.cfs=1, _3r1n_0.doc=1, _3r1n.tvf=1, _3r1n.tvd=1, _3r1n.tvx=1, _3r1n.fdx=1, _3r1n.fdt=1, _3r1q.cfs=1, _3r1o.cfs=1, _3r1n_0.skp=1, _3r1n_0.pyl=1}\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:448)\n    [junit] \tat org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2217)\n    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.createOutput(MockDirectoryWrapper.java:367)\n    [junit] \tat org.apache.lucene.index.FieldInfos.write(FieldInfos.java:563)\n    [junit] \tat org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:82)\n    [junit] \tat org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:381)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:378)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:505)\n    [junit] \tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2621)\n    [junit] \tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2598)\n    [junit] \tat org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2464)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2537)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2519)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2503)\n    [junit] \tat org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2156)\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockVariableIntBlock(baseBlockSize=105), f6=MockFixedIntBlock(blockSize=1372), f7=Pulsing(freqCutoff=11), f8=MockRandom, f9=MockVariableIntBlock(baseBlockSize=105), f1=MockSep, f0=Pulsing(freqCutoff=11), f3=Pulsing(freqCutoff=11), f2=MockFixedIntBlock(blockSize=1372), f5=MockVariableIntBlock(baseBlockSize=105), f4=MockRandom, f=Standard, c=Pulsing(freqCutoff=11), termVector=MockFixedIntBlock(blockSize=1372), d9=MockVariableIntBlock(baseBlockSize=105), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=11), d7=MockFixedIntBlock(blockSize=1372), d6=MockVariableIntBlock(baseBlockSize=105), d25=SimpleText, d0=Pulsing(freqCutoff=11), c29=SimpleText, d24=MockSep, d1=MockSep, c28=MockVariableIntBlock(baseBlockSize=105), d23=MockRandom, d2=MockVariableIntBlock(baseBlockSize=105), c27=MockRandom, d22=Standard, d3=MockFixedIntBlock(blockSize=1372), d21=MockVariableIntBlock(baseBlockSize=105), d20=MockRandom, c22=SimpleText, c21=MockSep, c20=MockRandom, d29=MockFixedIntBlock(blockSize=1372), c26=MockFixedIntBlock(blockSize=1372), d28=MockVariableIntBlock(baseBlockSize=105), c25=MockVariableIntBlock(baseBlockSize=105), d27=MockSep, c24=MockSep, d26=Pulsing(freqCutoff=11), c23=Pulsing(freqCutoff=11), e9=MockSep, e8=Standard, e7=SimpleText, e6=MockVariableIntBlock(baseBlockSize=105), e5=MockRandom, c17=MockSep, e3=SimpleText, d12=Pulsing(freqCutoff=11), c16=Pulsing(freqCutoff=11), e4=Standard, d11=MockFixedIntBlock(blockSize=1372), c19=MockFixedIntBlock(blockSize=1372), e1=MockRandom, d14=MockVariableIntBlock(baseBlockSize=105), c18=MockVariableIntBlock(baseBlockSize=105), e2=MockVariableIntBlock(baseBlockSize=105), d13=MockRandom, e0=MockFixedIntBlock(blockSize=1372), d10=MockSep, d19=Pulsing(freqCutoff=11), c11=MockVariableIntBlock(baseBlockSize=105), c10=MockRandom, d16=MockRandom, c13=MockRandom, c12=Standard, d15=Standard, d18=SimpleText, c15=SimpleText, d17=MockSep, c14=MockSep, b3=SimpleText, b2=MockSep, b5=Pulsing(freqCutoff=11), b4=MockFixedIntBlock(blockSize=1372), b7=MockFixedIntBlock(blockSize=1372), b6=MockVariableIntBlock(baseBlockSize=105), d50=Pulsing(freqCutoff=11), b9=MockRandom, b8=Standard, d43=MockRandom, d42=Standard, d41=MockFixedIntBlock(blockSize=1372), d40=MockVariableIntBlock(baseBlockSize=105), d47=MockSep, d46=Pulsing(freqCutoff=11), b0=MockFixedIntBlock(blockSize=1372), d45=Standard, b1=Pulsing(freqCutoff=11), d44=SimpleText, d49=Pulsing(freqCutoff=11), d48=MockFixedIntBlock(blockSize=1372), c6=MockRandom, c5=Standard, c4=MockFixedIntBlock(blockSize=1372), c3=MockVariableIntBlock(baseBlockSize=105), c9=Pulsing(freqCutoff=11), c8=Standard, c7=SimpleText, d30=SimpleText, d32=Pulsing(freqCutoff=11), d31=MockFixedIntBlock(blockSize=1372), c1=Standard, d34=MockFixedIntBlock(blockSize=1372), c2=MockRandom, d33=MockVariableIntBlock(baseBlockSize=105), d36=MockRandom, c0=MockFixedIntBlock(blockSize=1372), d35=Standard, d38=Standard, d37=SimpleText, d39=Pulsing(freqCutoff=11), e92=MockFixedIntBlock(blockSize=1372), e93=Pulsing(freqCutoff=11), e90=MockSep, e91=SimpleText, e89=Pulsing(freqCutoff=11), e88=Standard, e87=SimpleText, e86=MockRandom, e85=Standard, e84=MockFixedIntBlock(blockSize=1372), e83=MockVariableIntBlock(baseBlockSize=105), e80=MockVariableIntBlock(baseBlockSize=105), e81=SimpleText, e82=Standard, e77=MockFixedIntBlock(blockSize=1372), e76=MockVariableIntBlock(baseBlockSize=105), e79=MockRandom, e78=Standard, e73=SimpleText, e72=MockSep, e75=Pulsing(freqCutoff=11), e74=MockFixedIntBlock(blockSize=1372), binary=Pulsing(freqCutoff=11), f98=MockSep, f97=Pulsing(freqCutoff=11), f99=MockVariableIntBlock(baseBlockSize=105), f94=MockRandom, f93=Standard, f96=SimpleText, f95=MockSep, e95=MockSep, e94=Pulsing(freqCutoff=11), e97=MockFixedIntBlock(blockSize=1372), e96=MockVariableIntBlock(baseBlockSize=105), e99=MockVariableIntBlock(baseBlockSize=105), e98=MockRandom, id=MockRandom, f34=SimpleText, f33=MockSep, f32=MockRandom, f31=Standard, f30=MockVariableIntBlock(baseBlockSize=105), f39=MockRandom, f38=MockFixedIntBlock(blockSize=1372), f37=MockVariableIntBlock(baseBlockSize=105), f36=MockSep, f35=Pulsing(freqCutoff=11), f43=MockSep, f42=Pulsing(freqCutoff=11), f45=MockFixedIntBlock(blockSize=1372), f44=MockVariableIntBlock(baseBlockSize=105), f41=SimpleText, f40=MockSep, f47=MockVariableIntBlock(baseBlockSize=105), f46=MockRandom, f49=Standard, f48=SimpleText, content=MockFixedIntBlock(blockSize=1372), e19=Standard, e18=SimpleText, e17=MockRandom, f12=Standard, e16=Standard, f11=SimpleText, f10=MockVariableIntBlock(baseBlockSize=105), e15=MockFixedIntBlock(blockSize=1372), e14=MockVariableIntBlock(baseBlockSize=105), f16=Pulsing(freqCutoff=11), e13=Pulsing(freqCutoff=11), f15=MockFixedIntBlock(blockSize=1372), e12=MockFixedIntBlock(blockSize=1372), e11=SimpleText, f14=SimpleText, e10=MockSep, f13=MockSep, f19=Standard, f18=MockFixedIntBlock(blockSize=1372), f17=MockVariableIntBlock(baseBlockSize=105), e29=MockFixedIntBlock(blockSize=1372), e26=Standard, f21=SimpleText, e25=SimpleText, f20=MockSep, e28=MockSep, f23=Pulsing(freqCutoff=11), e27=Pulsing(freqCutoff=11), f22=MockFixedIntBlock(blockSize=1372), f25=MockFixedIntBlock(blockSize=1372), e22=MockFixedIntBlock(blockSize=1372), f24=MockVariableIntBlock(baseBlockSize=105), e21=MockVariableIntBlock(baseBlockSize=105), f27=MockRandom, e24=MockRandom, f26=Standard, e23=Standard, f29=Standard, f28=SimpleText, e20=Pulsing(freqCutoff=11), field=MockRandom, string=Pulsing(freqCutoff=11), e30=MockSep, e31=SimpleText, a98=MockRandom, e34=MockVariableIntBlock(baseBlockSize=105), a99=MockVariableIntBlock(baseBlockSize=105), e35=MockFixedIntBlock(blockSize=1372), f79=MockVariableIntBlock(baseBlockSize=105), e32=Pulsing(freqCutoff=11), e33=MockSep, b97=Pulsing(freqCutoff=11), f77=MockFixedIntBlock(blockSize=1372), e38=SimpleText, b98=MockSep, f78=Pulsing(freqCutoff=11), e39=Standard, b99=MockVariableIntBlock(baseBlockSize=105), f75=MockSep, e36=MockRandom, f76=SimpleText, e37=MockVariableIntBlock(baseBlockSize=105), f73=SimpleText, f74=Standard, f71=MockRandom, f72=MockVariableIntBlock(baseBlockSize=105), f81=MockFixedIntBlock(blockSize=1372), f80=MockVariableIntBlock(baseBlockSize=105), e40=MockSep, e41=MockVariableIntBlock(baseBlockSize=105), e42=MockFixedIntBlock(blockSize=1372), e43=MockRandom, e44=MockVariableIntBlock(baseBlockSize=105), e45=SimpleText, e46=Standard, f86=MockVariableIntBlock(baseBlockSize=105), e47=MockSep, f87=MockFixedIntBlock(blockSize=1372), e48=SimpleText, f88=Standard, e49=MockFixedIntBlock(blockSize=1372), f89=MockRandom, f82=MockSep, f83=SimpleText, f84=MockFixedIntBlock(blockSize=1372), f85=Pulsing(freqCutoff=11), f90=MockVariableIntBlock(baseBlockSize=105), f92=Standard, f91=SimpleText, str=MockFixedIntBlock(blockSize=1372), a76=MockVariableIntBlock(baseBlockSize=105), e56=MockRandom, f59=MockRandom, a77=MockFixedIntBlock(blockSize=1372), e57=MockVariableIntBlock(baseBlockSize=105), a78=Standard, e54=MockFixedIntBlock(blockSize=1372), f57=MockFixedIntBlock(blockSize=1372), a79=MockRandom, e55=Pulsing(freqCutoff=11), f58=Pulsing(freqCutoff=11), e52=Pulsing(freqCutoff=11), e53=MockSep, e50=SimpleText, e51=Standard, f51=Standard, f52=MockRandom, f50=MockFixedIntBlock(blockSize=1372), f55=Pulsing(freqCutoff=11), f56=MockSep, f53=SimpleText, e58=Standard, f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=105), e60=MockRandom, a82=Standard, a81=SimpleText, a84=SimpleText, a83=MockSep, a86=Pulsing(freqCutoff=11), a85=MockFixedIntBlock(blockSize=1372), a89=Pulsing(freqCutoff=11), f68=Standard, e65=Standard, f69=MockRandom, e66=MockRandom, a87=SimpleText, e67=MockSep, a88=Standard, e68=SimpleText, e61=MockFixedIntBlock(blockSize=1372), e62=Pulsing(freqCutoff=11), e63=MockRandom, e64=MockVariableIntBlock(baseBlockSize=105), f60=SimpleText, f61=Standard, f62=Pulsing(freqCutoff=11), f63=MockSep, e69=Pulsing(freqCutoff=11), f64=MockFixedIntBlock(blockSize=1372), f65=Pulsing(freqCutoff=11), f66=MockRandom, f67=MockVariableIntBlock(baseBlockSize=105), f70=MockRandom, a93=Pulsing(freqCutoff=11), a92=MockFixedIntBlock(blockSize=1372), a91=SimpleText, e71=MockSep, a90=MockSep, e70=Pulsing(freqCutoff=11), a97=MockRandom, a96=Standard, a95=MockFixedIntBlock(blockSize=1372), a94=MockVariableIntBlock(baseBlockSize=105), c58=MockFixedIntBlock(blockSize=1372), a63=Pulsing(freqCutoff=11), a64=MockSep, c59=Pulsing(freqCutoff=11), c56=MockSep, d59=MockSep, a61=SimpleText, c57=SimpleText, a62=Standard, c54=SimpleText, c55=Standard, a60=MockRandom, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=105), d53=MockVariableIntBlock(baseBlockSize=105), d54=MockFixedIntBlock(blockSize=1372), d51=Pulsing(freqCutoff=11), d52=MockSep, d57=SimpleText, b62=Standard, d58=Standard, b63=MockRandom, d55=MockRandom, b60=MockVariableIntBlock(baseBlockSize=105), d56=MockVariableIntBlock(baseBlockSize=105), b61=MockFixedIntBlock(blockSize=1372), b56=MockSep, b55=Pulsing(freqCutoff=11), b54=Standard, b53=SimpleText, d61=SimpleText, b59=MockRandom, d60=MockSep, b58=Pulsing(freqCutoff=11), b57=MockFixedIntBlock(blockSize=1372), c62=MockFixedIntBlock(blockSize=1372), c61=MockVariableIntBlock(baseBlockSize=105), a59=MockRandom, c60=MockSep, a58=Standard, a57=MockVariableIntBlock(baseBlockSize=105), a56=MockRandom, a55=Pulsing(freqCutoff=11), a54=MockFixedIntBlock(blockSize=1372), a72=MockFixedIntBlock(blockSize=1372), c67=MockVariableIntBlock(baseBlockSize=105), a73=Pulsing(freqCutoff=11), c68=MockFixedIntBlock(blockSize=1372), a74=MockRandom, c69=Standard, a75=MockVariableIntBlock(baseBlockSize=105), c63=MockSep, c64=SimpleText, a70=Pulsing(freqCutoff=11), c65=MockFixedIntBlock(blockSize=1372), a71=MockSep, c66=Pulsing(freqCutoff=11), d62=MockRandom, d63=MockVariableIntBlock(baseBlockSize=105), d64=SimpleText, b70=MockRandom, d65=Standard, b71=SimpleText, d66=MockSep, b72=Standard, d67=SimpleText, b73=Pulsing(freqCutoff=11), d68=MockFixedIntBlock(blockSize=1372), b74=MockSep, d69=Pulsing(freqCutoff=11), b65=Pulsing(freqCutoff=11), b64=MockFixedIntBlock(blockSize=1372), b67=MockVariableIntBlock(baseBlockSize=105), b66=MockRandom, d70=MockSep, b69=MockRandom, b68=Standard, d72=MockFixedIntBlock(blockSize=1372), d71=MockVariableIntBlock(baseBlockSize=105), c71=MockVariableIntBlock(baseBlockSize=105), c70=MockRandom, a69=Pulsing(freqCutoff=11), c73=Standard, c72=SimpleText, a66=MockRandom, a65=Standard, a68=SimpleText, a67=MockSep, c32=Standard, c33=MockRandom, c30=MockVariableIntBlock(baseBlockSize=105), c31=MockFixedIntBlock(blockSize=1372), c36=Pulsing(freqCutoff=11), a41=MockSep, c37=MockSep, a42=SimpleText, a0=MockSep, c34=SimpleText, c35=Standard, a40=MockRandom, b84=SimpleText, d79=MockSep, b85=Standard, b82=MockRandom, d77=Standard, c38=MockFixedIntBlock(blockSize=1372), b83=MockVariableIntBlock(baseBlockSize=105), d78=MockRandom, c39=Pulsing(freqCutoff=11), b80=MockVariableIntBlock(baseBlockSize=105), d75=MockRandom, b81=MockFixedIntBlock(blockSize=1372), d76=MockVariableIntBlock(baseBlockSize=105), d73=MockFixedIntBlock(blockSize=1372), d74=Pulsing(freqCutoff=11), d83=MockSep, a9=Standard, d82=Pulsing(freqCutoff=11), d81=Standard, d80=SimpleText, b79=MockVariableIntBlock(baseBlockSize=105), b78=Pulsing(freqCutoff=11), b77=MockFixedIntBlock(blockSize=1372), b76=SimpleText, b75=MockSep, a1=SimpleText, a35=MockFixedIntBlock(blockSize=1372), a2=Standard, a34=MockVariableIntBlock(baseBlockSize=105), a3=Pulsing(freqCutoff=11), a33=MockSep, a4=MockSep, a32=Pulsing(freqCutoff=11), a5=MockFixedIntBlock(blockSize=1372), a39=Standard, c40=Pulsing(freqCutoff=11), a6=Pulsing(freqCutoff=11), a38=SimpleText, a7=MockRandom, a37=MockVariableIntBlock(baseBlockSize=105), a8=MockVariableIntBlock(baseBlockSize=105), a36=MockRandom, c41=SimpleText, c42=Standard, c43=Pulsing(freqCutoff=11), c44=MockSep, c45=MockFixedIntBlock(blockSize=1372), a50=Pulsing(freqCutoff=11), c46=Pulsing(freqCutoff=11), a51=MockSep, c47=MockRandom, a52=MockVariableIntBlock(baseBlockSize=105), c48=MockVariableIntBlock(baseBlockSize=105), a53=MockFixedIntBlock(blockSize=1372), b93=MockSep, d88=Pulsing(freqCutoff=11), c49=Standard, b94=SimpleText, d89=MockSep, b95=MockFixedIntBlock(blockSize=1372), b96=Pulsing(freqCutoff=11), d84=Standard, b90=MockVariableIntBlock(baseBlockSize=105), d85=MockRandom, b91=SimpleText, d86=MockSep, b92=Standard, d87=SimpleText, d92=Pulsing(freqCutoff=11), d91=MockFixedIntBlock(blockSize=1372), d94=MockVariableIntBlock(baseBlockSize=105), d93=MockRandom, b87=MockFixedIntBlock(blockSize=1372), b86=MockVariableIntBlock(baseBlockSize=105), d90=MockSep, b89=MockRandom, b88=Standard, a44=MockVariableIntBlock(baseBlockSize=105), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=MockRandom, a49=MockFixedIntBlock(blockSize=1372), c50=Standard, d98=MockRandom, d97=Standard, d96=MockFixedIntBlock(blockSize=1372), d95=MockVariableIntBlock(baseBlockSize=105), d99=SimpleText, a20=Standard, c99=MockSep, c98=Pulsing(freqCutoff=11), c97=Standard, c96=SimpleText, b19=Standard, a16=Standard, a17=MockRandom, b17=MockVariableIntBlock(baseBlockSize=105), a14=MockVariableIntBlock(baseBlockSize=105), b18=MockFixedIntBlock(blockSize=1372), a15=MockFixedIntBlock(blockSize=1372), a12=MockFixedIntBlock(blockSize=1372), a13=Pulsing(freqCutoff=11), a10=MockSep, a11=SimpleText, b11=SimpleText, b12=Standard, b10=MockVariableIntBlock(baseBlockSize=105), b15=MockFixedIntBlock(blockSize=1372), b16=Pulsing(freqCutoff=11), a18=SimpleText, b13=MockSep, a19=Standard, b14=SimpleText, b30=Standard, a31=Pulsing(freqCutoff=11), a30=MockFixedIntBlock(blockSize=1372), b28=SimpleText, a25=SimpleText, b29=Standard, a26=Standard, a27=Pulsing(freqCutoff=11), a28=MockSep, a21=MockVariableIntBlock(baseBlockSize=105), a22=MockFixedIntBlock(blockSize=1372), a23=Standard, a24=MockRandom, b20=MockSep, b21=SimpleText, b22=MockFixedIntBlock(blockSize=1372), b23=Pulsing(freqCutoff=11), a29=MockFixedIntBlock(blockSize=1372), b24=MockVariableIntBlock(baseBlockSize=105), b25=MockFixedIntBlock(blockSize=1372), b26=Standard, b27=MockRandom, b41=MockVariableIntBlock(baseBlockSize=105), b40=MockRandom, c77=SimpleText, c76=MockSep, c75=MockRandom, c74=Standard, c79=MockSep, c78=Pulsing(freqCutoff=11), c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=105), c81=MockFixedIntBlock(blockSize=1372), b39=MockRandom, c82=Pulsing(freqCutoff=11), b37=MockVariableIntBlock(baseBlockSize=105), b38=MockFixedIntBlock(blockSize=1372), b35=Pulsing(freqCutoff=11), b36=MockSep, b33=MockSep, b34=SimpleText, b31=Standard, b32=MockRandom, str2=Standard, b50=MockRandom, b52=SimpleText, str3=Pulsing(freqCutoff=11), b51=MockSep, c86=MockSep, tvtest=Pulsing(freqCutoff=11), c85=Pulsing(freqCutoff=11), c88=MockFixedIntBlock(blockSize=1372), c87=MockVariableIntBlock(baseBlockSize=105), c89=MockRandom, c90=MockRandom, c91=MockVariableIntBlock(baseBlockSize=105), c92=Standard, c93=MockRandom, c94=MockSep, c95=SimpleText, content1=SimpleText, b46=MockRandom, b47=MockVariableIntBlock(baseBlockSize=105), content3=MockRandom, b48=SimpleText, content4=Standard, b49=Standard, content5=MockVariableIntBlock(baseBlockSize=105), b42=Pulsing(freqCutoff=11), b43=MockSep, b44=MockVariableIntBlock(baseBlockSize=105), b45=MockFixedIntBlock(blockSize=1372)}, locale=it_CH, timezone=Europe/Chisinau\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestMergeSchedulerExternal, TestToken, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]\n    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=275548088,total=309395456\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-796",
        "summary": "Change Visibility of fields[] in MultiFieldQueryParser",
        "description": "In MultiFieldQueryParser the two methods \n\n  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException\n  protected Query getWildcardQuery(String field, String termStr) throws ParseException\n\nare intended to be overwritten if one would like to avoid fuzzy and wildcard queries. However, the String[] fields attribute of this class is private and hence it is not accessible in subclasses of MFQParser. If you just change it to protected this issue should be solved.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3446",
        "summary": "NullPointerException in BooleanFilter ",
        "description": "BooleanFilter getDISI() method used with QueryWrapperFilter occur NullPointerException,\nif any QueryWrapperFilter not match terms in IndexReader.\n\n---------------------------------------------------\njava.lang.NullPointerException\n\tat org.apache.lucene.util.OpenBitSetDISI.inPlaceAnd(OpenBitSetDISI.java:66)\n\tat org.apache.lucene.search.BooleanFilter.getDocIdSet(BooleanFilter.java:102)\n\tat org.apache.lucene.search.IndexSearcher.searchWithFilter(IndexSearcher.java:551)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:532)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:463)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:433)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:356)\n\tat test.BooleanFilterTest.main(BooleanFilterTest.java:50)\n---------------------------------------------------\n\nnull-check below lines.\n---------------------------------------------------\nres = new OpenBitSetDISI(getDISI(shouldFilters, i, reader), reader.maxDoc());\nres.inPlaceOr(getDISI(shouldFilters, i, reader));\nres = new OpenBitSetDISI(getDISI(notFilters, i, reader), reader.maxDoc());\nres.inPlaceNot(getDISI(notFilters, i, reader));\nres = new OpenBitSetDISI(getDISI(mustFilters, i, reader), reader.maxDoc());\nres.inPlaceAnd(getDISI(mustFilters, i, reader));\n---------------------------------------------------",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1505",
        "summary": "Change contrib/spatial to use trie's NumericUtils, and remove NumberUtils",
        "description": "Currently spatial contrib includes a copy of NumberUtils from solr (otherwise it would depend on solr)\n\nOnce LUCENE-1496 is sorted out, this copy should be removed.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3690",
        "summary": "JFlex-based HTMLStripCharFilter replacement",
        "description": "A JFlex-based HTMLStripCharFilter replacement would be more performant and easier to understand and maintain.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1428",
        "summary": "Highlighter dist jar includes memory binary class files",
        "description": "Mark Harwood sent me a note about this issue noticed by a colleague. Previous releases have the memory class files in the Highlighter distribution jar. The Highlighter uses the same contrib dependency method that the xml query parser does - the problem doesn't manifest there because of the alphabetical order of build though. Fix is to not inheritAll when launching the ant task to build memory contrib.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3635",
        "summary": "Allow setting arbitrary objects on PerfRunData",
        "description": "PerfRunData is used as the intermediary objects between PerfRunTasks. Just like we can set IndexReader/Writer on it, it will be good if it allows setting other arbitrary objects that are e.g. created by one task and used by another.\n\nA recent example is the enhancement to the benchmark package following the addition of the facet module. We had to add TaxoReader/Writer.\n\nThe proposal is to add a HashMap<String, Object> that custom PerfTasks can set()/get(). I do not propose to move IR/IW/TR/TW etc. into that map. If however people think that we should, I can do that as well.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2024",
        "summary": "\"ant dist\" no longer generates md5's for the top-level artifacts",
        "description": "Mark hit this for 2.9.0, and I just hit it again for 2.9.1.  It used to work...",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-449",
        "summary": "NullPointerException when temporary directory not readable",
        "description": "We have customers reporting errors such as:\n\nCaused by: java.lang.NullPointerException\n\tat org.apache.lucene.store.FSDirectory.create(FSDirectory.java:200)\n\tat org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:144)\n\tat org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)\n\tat org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:205)\n\tat com.atlassian.jira.util.LuceneUtils.getIndexWriter(LuceneUtils.java:46)\n\tat com.atlassian.jira.issue.index.DefaultIndexManager.getIndexWriter(DefaultIndexManager.java:568)\n\tat com.atlassian.jira.issue.index.DefaultIndexManager.indexIssuesAndComments(DefaultIndexManager.java:287)\n\t... 59 more\n\n\nThis occurs when the lock directory is unreadable (eg. because Tomcat sets java.io.tmpdir to temp/ and the permissions here are broken). Attached is ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2768",
        "summary": "add infrastructure for longer running nightly test cases",
        "description": "I'm spinning this out of LUCENE-2762...\n\nThe patch there adds initial infrastructure for tests to pull documents from a line file, and adds a longish running test case using that line file to test NRT.\n\nI'd like to see some tests run on more substantial indices based on real data... so this is just a start.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1334",
        "summary": "Term improvement",
        "description": "Term is designed for reuse of the supplied filter, to minimize intern().\n\nOne of the common use patterns is to create a Term with the txt field being an empty string.\n\nTo simplify this pattern and to document it's usefulness, I suggest adding a constructor:\npublic Term(String fld)\nwith the obvious implementation\nand use it throughout core and contrib as a replacement.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2325",
        "summary": "investigate solr test failures using flex",
        "description": "We have a branch of Solr located here: https://svn.apache.org/repos/asf/lucene/solr/branches/solr\n\nCurrently all the tests pass with lucene trunk jars.\n\nI plopped in the flex jars and they do not, so I thought these might be interesting to look at.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2077",
        "summary": "changes-to-html: better handling of bulleted lists in CHANGES.txt",
        "description": "- bulleted lists\n- should be rendered\n- as such\n- in output HTML",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2710",
        "summary": "\"reproduce with\" on test failure isn't right if you manually overrided anything",
        "description": "If you run a test with eg -Dtests.codec=SimpleText...\n\nIf it fails, the \"reproduce with\" fails to include that manual override (-Dtests.codec=SimpleText), ie it only includes the seed / test class / test method.  So it won't actually reproduce the fail, in general.\n\nWe just need to fix the \"reproduce with\" to add any manual overrides....",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3612",
        "summary": "remove _X.fnx",
        "description": "Currently we store a global (not per-segment) field number->name mapping in _X.fnx\n\nHowever, it doesn't actually save us any performance e.g on IndexWriter's init because\nsince LUCENE-2984 we are to loading the fieldinfos anyway to compute files() for IFD, etc, \nas thats where hasProx/hasVectors is.\n\nAdditionally in the past global files like shared doc stores have caused us problems,\n(recently we just fixed a bug related to this file in LUCENE-3601).\n\nFinally this is trouble for backwards compatibility as its difficult to handle a global\nfile with the codecs mechanism.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-481",
        "summary": "IndexReader.getCurrentVersion() and isCurrent should use commit lock.",
        "description": "There is a race condition if one machine is checking the current version of an index while another wants to update the segments file in IndexWriter.close().\n\njava.io.IOException: Cannot delete segments\n\tat org.apache.lucene.store.FSDirectory.renameFile(FSDirectory.java:213)\n\tat org.apache.lucene.index.SegmentInfos.write(SegmentInfos.java:90)\n\tat org.apache.lucene.index.IndexWriter$3.doBody(IndexWriter.java:503)\n\tat org.apache.lucene.store.Lock$With.run(Lock.java:109)\n\tat org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:501)\n\tat org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:440)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:242)\n\nOn the windows platform reading the contents of a file disallows deleting the file.\n\nI use Lucene to maintain an index of +-700.000 documents, one server adds documents, while other servers handle the searches.\nThe search servers poll the index version regularly to check if they have to reopen their IndexSearcher.\nOnce in a while (about once every two days on average), IndexWriter.close() fails because it cannot delete the previous segments file, even though it hold the commit lock.\nThe reason is probably that search servers are reading the segments file to check the version without using the commit lock.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2645",
        "summary": "False assertion of >0 position delta in StandardPostingsWriterImpl",
        "description": "StandardPostingsWriterImpl line 159 is:\n{code:java}\n    assert delta > 0 || position == 0 || position == -1: \"position=\" + position + \" lastPosition=\" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)\n{code}\n\nI enable assertions when I run my unit tests and I've found this assertion to fail when delta is 0 which occurs when the same position value is sent in twice in arrow.  Once I added RemoveDuplicatesTokenFilter, this problem went away.  Should I really be forced to add this filter?  I think delta >= 0 would be a better assertion.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-873",
        "summary": "nightly builds depend on clover",
        "description": "as reported by Michael Pelz Sherman on java-dev@lucene and solr-user@lucene the nightly builds coming out of hudson current depend on clover...\n\n  [root@crm.test.bbhmedia.net tmp]# strings lucene-core-nightly.jar | grep -i clover|more\norg/apache/lucene/LucenePackage$__CLOVER_0_0.class\norg/apache/lucene/analysis/Analyzer$__CLOVER_1_0.class\n...\n\nthe old nightly.sh dealt with this by running ant nightly twice, first without clover to get the jars and then with clover to get the report.  it loks like maybe this logic never made it into the hudson setup.\n\nsomeone with hudson admin access/knowledge will need to look into this.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1634",
        "summary": "LogMergePolicy should use the number of deleted docs when deciding which segments to merge",
        "description": "I found that IndexWriter.optimize(int) method does not pick up large segments with a lot of deletes even when most of the docs are deleted. And the existence of such segments affected the query performance significantly.\n\nI created an index with 1 million docs, then went over all docs and updated a few thousand at a time.  I ran optimize(20) occasionally. What saw were large segments with most of docs deleted. Although these segments did not have valid docs they remained in the directory for a very long time until more segments with comparable or bigger sizes were created.\n\nThis is because LogMergePolicy.findMergeForOptimize uses the size of segments but does not take the number of deleted documents into consideration when it decides which segments to merge. So, a simple fix is to use the delete count to calibrate the segment size. I can create a patch for this.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-319",
        "summary": "[PATCH] Loosing first matching document in BooleanQuery",
        "description": "This patch fixes loosing of first matching document when BooleanQuery\nwith BooleanClause.Occur.SHOULD is added to another BooleanQuery.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-867",
        "summary": "Incomplete lucene-core-1.9.1 in Maven2 repository",
        "description": "I'm new to Lucene and am setting up a project using v1.9.1 to use Maven2 instead of ANT.\nThe project would not build with Maven2 due to lacking lucene classes.\nI tracked the problem down to that the lucene-core-1.9.1 jar file that Maven2 downloaded from the repository was smaller (2.3KB) than the one I got from the local ANT repository (408KB).\nCan you please update the v1.9.1 file on the Maven2 [1], [2] repositories so other developers don't get frustrated by the incomplete jar?\n\n\n[1] http://repo1.maven.org/maven2/org/apache/lucene/lucene-core/1.9.1/\n[2] http://mirrors.ibiblio.org/pub/mirrors/maven2/org/apache/lucene/lucene-core/1.9.1/\n\nThis issue is a copy of a mail sent to the java-dev@lucene.apache.org list april 4. 2007.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1288",
        "summary": "Add getVersion method to IndexCommit",
        "description": "Returns the equivalent of IndexReader.getVersion for IndexCommit\n\n{code}\npublic abstract long getVersion();\n{code}",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-663",
        "summary": "New feature rich higlighter for Lucene.",
        "description": "Well, I refactored (took) some code from two previous highlighters.\nThis highlighter:\n+ use TermPositionVector where available\n+ use Analyzer if no TermPositionVector found or is forced to use it.\n+ support for all lucene queries (Term, Phrase with slops, Prefix, Wildcard, Range) except Fuzzy Query (can be implemented easly)\n\n- has no support for scoring (yet)\n- use same prefix,postfix for accepted terms (yet)\n\n? It's written in Java5\n\nIn next release I'd like to add support for Fuzzy, \"coloring\" f.e. diffrent color for terms btw. phrase terms (slops), scoring of fragments\n\nIt's apache licensed - I hope so :-) I put licene statement in every file\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2606",
        "summary": "optimize contrib/regex for flex",
        "description": "* changes RegexCapabilities match(String) to match(BytesRef)\n* the jakarta and jdk impls uses CharacterIterator/CharSequence matching against the utf16result instead.\n* i also reuse the matcher for jdk, i don't see why we didnt do this before but it makes sense esp since we reuse the CSQ\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3070",
        "summary": "Enable DocValues by default for every Codec",
        "description": "Currently DocValues are enable with a wrapper Codec so each codec which needs DocValues must be wrapped by DocValuesCodec. The DocValues writer and reader should be moved to Codec to be enabled by default.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2390",
        "summary": "contrib/remote tests fail randomly",
        "description": "The contrib/remote tests will fail randomly.\n\nThis is because they use this _TestUtil.getRandomSocketPort() which\nsimply generates a random number, but if this is already in use, it will fail.\n\nAdditionally there is duplicate RMI logic across all 3 test classes.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1891",
        "summary": "Spatial uses java util logging that causes needless minor work (multiple string concat, a method call) due to not checking log level",
        "description": "Not sure there should be logging here - just used in two spots and looks more for debug - but if its going to be there, should check for isFineEnabled.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2057",
        "summary": "TopDocsCollector should have bounded generic <T extends ScoreDoc>",
        "description": "TopDocsCollector was changed to be TopDocsCollector<T>. However it has methods which specifically assume the PQ stores ScoreDoc. Therefore, if someone extends it and defines a type which is not ScoreDoc, things will break.\n\nWe shouldn't put <T> on TopDocsCollector at all, but rather change its ctor to *protected TopDocsCollector(PriorityQueue<? extends ScoreDoc> pq)*. TopDocsCollector should handle ScoreDoc types. If we do this, we'll need to change FieldValueHitQueue's Entry to extend ScoreDoc as well.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-743",
        "summary": "IndexReader.reopen()",
        "description": "This is Robert Engels' implementation of IndexReader.reopen() functionality, as a set of 3 new classes (this was easier for him to implement, but should probably be folded into the core, if this looks good).\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1204",
        "summary": "IndexWriter.deleteDocuments bug",
        "description": "IndexWriter.deleteDocuments() fails random testing",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3709",
        "summary": "norms reading fails with FileNotFound in exceptional case",
        "description": "If we can't get to the bottom of this, we can always add the fileExists check back...\n\n{noformat}\n    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions\n    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):\tCaused an ERROR\n    [junit] No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])\n    [junit] java.io.FileNotFoundException: No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])\n    [junit] \tat org.apache.lucene.store.CompoundFileDirectory.createSlicer(CompoundFileDirectory.java:313)\n    [junit] \tat org.apache.lucene.store.CompoundFileDirectory.<init>(CompoundFileDirectory.java:65)\n    [junit] \tat org.apache.lucene.codecs.lucene40.Lucene40DocValuesProducer.<init>(Lucene40DocValuesProducer.java:48)\n    [junit] \tat org.apache.lucene.codecs.lucene40.Lucene40NormsFormat$Lucene40NormsDocValuesProducer.<init>(Lucene40NormsFormat.java:70)\n    [junit] \tat org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:49)\n    [junit] \tat org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:62)\n    [junit] \tat org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:122)\n    [junit] \tat org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:54)\n    [junit] \tat org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:65)\n    [junit] \tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:660)\n    [junit] \tat org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:55)\n    [junit] \tat org.apache.lucene.index.IndexReader.open(IndexReader.java:242)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:304)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:530)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)\n    [junit] \tat org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)\n    [junit] \n    [junit] \n    [junit] Tests run: 22, Failures: 0, Errors: 1, Time elapsed: 3.439 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-4ea45cb40d17460b:-459bfb455a2351b9:1abd8f0f3a0611b9 -Dargs=\"-Dfile.encoding=UTF-8\"\n    [junit] NOTE: test params are: codec=Lucene40: {field=MockVariableIntBlock(baseBlockSize=31), id=PostingsFormat(name=NestedPulsing), content=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), contents=MockVariableIntBlock(baseBlockSize=31), content1=MockVariableIntBlock(baseBlockSize=31), content2=PostingsFormat(name=MockSep), content4=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), content5=MockFixedIntBlock(blockSize=964), content6=PostingsFormat(name=Memory), content7=PostingsFormat(name=MockRandom), crash=PostingsFormat(name=NestedPulsing), subid=PostingsFormat(name=NestedPulsing)}, sim=RandomSimilarityProvider(queryNorm=false,coord=true): {other=DFR GB3(800.0), contents=IB SPL-L3(800.0), content=DFR GL3(800.0), id=DFR I(F)L1, field=IB LL-DZ(0.3), content1=DFR I(ne)BZ(0.3), content2=DFR I(n)3(800.0), content3=DFR GZ(0.3), content4=DFR I(ne)B2, content5=IB LL-L3(800.0), content6=IB SPL-D2, crash=DFR I(F)3(800.0), content7=DFR I(F)B3(800.0), subid=IB LL-L1}, locale=de_CH, timezone=Canada/Saskatchewan\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestAssertions, TestNumericTokenStream, TestSimpleAttributeImpl, TestImpersonation, TestPulsingReuse, TestDocument, TestAddIndexes, TestAtomicUpdate, TestByteSlices, TestCheckIndex, TestConcurrentMergeScheduler, TestConsistentFieldNumbers, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentWriter, TestFlex, TestForceMergeForever, TestIndexInput, TestIndexReader, TestIndexWriterConfig, TestIndexWriterExceptions]\n    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=186661872,total=245104640\n{noformat}\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2444",
        "summary": "move contrib/analyzers to modules/analysis",
        "description": "This is a patch to move contrib/analyzers under modules/analyzers\n\nWe can then continue consolidating (LUCENE-2413)... in truth this will sorta be \nan ongoing thing anyway, as we try to distance indexing from analysis, etc\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3706",
        "summary": "add offsets into lucene40 postings",
        "description": "LUCENE-3684 added support for IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS, but\nonly SimpleText implements it.\n\nI think we should implement it in the other 4.0 codecs (starting with Lucene40PostingsFormat).",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-863",
        "summary": "Deprecate StandardBenchmarker and \"old\" benchmarker code in favor of the Task based approach",
        "description": "We should deprecate the StandardBechmarker code that was the start of the benchmark contribution in favor of the much easier to use/extend byTask benchmark code",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1643",
        "summary": "use reusable collation keys in ICUCollationFilter",
        "description": "ICUCollationFilter need not create a new CollationKey object for each token.\nIn ICU there is a mechanism to use a reusable key.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1144",
        "summary": "NPE crash in case of out of memory",
        "description": "The attached class makes Lucene crash with an NPE when starting it with -Xmx10M, although there's probably an OutOfMemory problem. The stacktrace:\n\nException in thread \"main\" java.lang.NullPointerException\n\tat java.util.Arrays.fill(Unknown Source)\n\tat org.apache.lucene.index.DocumentsWriter$ByteBlockPool.reset(DocumentsWriter.java:2873)\n\tat org.apache.lucene.index.DocumentsWriter$ThreadState.resetPostings(DocumentsWriter.java:637)\n\tat org.apache.lucene.index.DocumentsWriter.resetPostingsData(DocumentsWriter.java:458)\n\tat org.apache.lucene.index.DocumentsWriter.abort(DocumentsWriter.java:423)\n\tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2433)\n\tat org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2397)\n\tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1445)\n\tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1424)\n\tat LuceneCrash.myrun(LuceneCrash.java:32)\n\tat LuceneCrash.main(LuceneCrash.java:19)\n\nThe documents are quite big (some hundred KB each), I cannot attach them but I can send them via private mail if needed. The crash happens the first time reset() is called, after indexing 10 documents. I assume the bug is just that the error is misleading, there maybe should be an OOM error.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1057",
        "summary": "indexing doesn't reset token state",
        "description": "IndexWriter (DocumentsWriter) forgets to reset the token state resulting in incorrect positionIncrements, payloads, and token types.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1446",
        "summary": "Run 'test-tag' in nightly build",
        "description": "Changes in this trivial patch:\n- ant target 'nightly' now also depends on 'test-tag'\n- adds property 'compatibility.tag' to common-build.xml that should always point to the last tagged release; its unit tests will be downloaded unless -Dtag=\"\" is used to override\n- 'download-tag' does not fail if the svn checkout wasn't successful; instead 'test-tag' checks if the specified tag is checked-out and available, if not it fails ",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-247",
        "summary": "Confusing code line",
        "description": "Line 81 of TermScorer:\n\n      if (!(target > docs[pointer])) {\n\nCould be replaced with the more readable:\n\n      if (docs[pointer] >= target) {\n\nSorry for nit picking!",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-825",
        "summary": "NullPointerException from SegmentInfos.FindSegmentsFile.run() if FSDirectory.list() returns NULL ",
        "description": "Found this bug while running unit tests to verify an upgrade of our system from 1.4.3 to 2.1.0.  This bug did *not* occur during 1.4.3, it is new to 2.x (I'm pretty sure it's 2.1-only)\n\nIf the index directory gets deleted out from under Lucene after the FSDirectory has been created, then attempts to open an IndexWriter or IndexReader will result in an NPE.  Lucene should be throwing an IOException in this case.\n\nRepro:\n    1) Create an FSDirectory pointing somewhere in the filesystem (e.g. /foo/index/1)\n    2) rm -rf the parent dir (rm -rf /foo/index)\n    3) Try to open an IndexReader\n\nResult: NullPointerException on line \"for(int i=0;i<files.length;i++) { \" -- 'files' is NULL.\n \nExpect: IOException\n\n\n....  \n\nThis is happening because of a missing NULL check in SegmentInfos$FindSegmentsFile.run():\n\n        if (0 == method) {\n          if (directory != null) {\n            files = directory.list();\n          } else {\n            files = fileDirectory.list();\n          }\n\n          gen = getCurrentSegmentGeneration(files);\n\n          if (gen == -1) {\n            String s = \"\";\n            for(int i=0;i<files.length;i++) { \n              s += \" \" + files[i];\n            }\n            throw new FileNotFoundException(\"no segments* file found: files:\" + s);\n          }\n        }\n\n\nThe FSDirectory constructor will make sure the index dir exists, but if it is for some reason deleted out from underneath Lucene after the FSDirectory is instantiated, then java.io.File.list() will return NULL.  Probably better to fix FSDirectory.list() to just check for null and return a 0-length array:\n\n(in org/apache/lucene/store/FSDirectory.java)\n314c314,317\n<         return directory.list(IndexFileNameFilter.getFilter());\n---\n>     String[] toRet = directory.list(IndexFileNameFilter.getFilter());\n>     if (toRet == null)\n>         return new String[]{};\n>     return toRet;\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1449",
        "summary": "IndexDeletionPolicy.delete behaves incorrectly when deleting latest generation ",
        "description": "I have been looking to provide the ability to rollback committed transactions and encountered some issues.\nI appreciate IndexDeletionPolicy's main motivation is to handle cleaning away OLD commit points but it does not explicitly state that it can or cannot be used to clean NEW commit points.\n\nIf this is not supported then the documentation should ideally state this. If the intention is to support this behaviour then read on .......\n\nThere seem to be 2 issues so far:\n1) The first attempt to call IndexCommit.delete on the latest commit point fails to remove any contents. The subsequent call succeeds however\n2) Deleting the latest commit point fails to update the segments.gen file to point to segments_N-1. New IndexReaders that are opened are then misdirected to open segments_N which has been deleted\n\nJunit test to follow...\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2156",
        "summary": "use AtomicInteger/Boolean to track IR.refCount and IW.closed",
        "description": "Less costly than synchronized methods we have now...",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3412",
        "summary": "SloppyPhraseScorer returns non-deterministic results for queries with many repeats",
        "description": "Proximity queries with many repeats (four or more, based on my testing) return non-deterministic results. I run the same query multiple times with the same data set and get different results.\n\nSo far I've reproduced this with Solr 1.4.1, 3.1, 3.2, 3.3, and latest 4.0 trunk.\n\nSteps to reproduce (using the Solr example):\n1) In solrconfig.xml, set queryResultCache size to 0.\n2) Add some documents with text \"dog dog dog\" and \"dog dog dog dog\". http://localhost:8983/solr/update?stream.body=%3Cadd%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E1%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%3C/field%3E%3C/doc%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E2%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%20dog%3C/field%3E%3C/doc%3E%3C/add%3E&commit=true\n3) Do a \"dog dog dog dog\"~1 query. http://localhost:8983/solr/select?q=%22dog%20dog%20dog%20dog%22~1\n4) Repeat step 3 many times.\n\nExpected results: The document with id 2 should be returned.\n\nActual results: The document with id 2 is always returned. The document with id 1 is sometimes returned.\n\nDifferent proximity values show the same bug - \"dog dog dog dog\"~5, \"dog dog dog dog\"~100, etc show the same behavior.\n\nSo far I've traced it down to the \"repeats\" array in SloppyPhraseScorer.initPhrasePositions() - depending on the order of the elements in this array, the document may or may not match. I think the HashSet may be to blame, but I'm not sure - that at least seems to be where the non-determinism is coming from.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1618",
        "summary": "Allow setting the IndexWriter docstore to be a different directory",
        "description": "Add an IndexWriter.setDocStoreDirectory method that allows doc\nstores to be placed in a different directory than the IW default\ndir.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1615",
        "summary": "deprecated method used in fieldsReader / setOmitTf()",
        "description": "setOmitTf(boolean) is deprecated and should not be used by core classes. One place where it appears is FieldsReader , this patch fixes it. It was necessary to change Fieldable to AbstractField at two places, only local variables.   ",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2659",
        "summary": "lucenetestcase ease of use improvements",
        "description": "I started working on this in LUCENE-2658, here is the finished patch.\n\nThere are some problems with LuceneTestCase:\n* a tests beforeClass, or the test itself (its @befores and its method), might have some\n  random behavior, but only the latter can be reproduced with -Dtests.seed\n* if you want to do things in beforeClass, you have to use a different API: newDirectory(random)\n  instead of newDirectory, etc.\n* for a new user, the current output can be verbose, confusing and overwhelming.\n\nSo, I refactored this class to address these problems. \nA class still needs 2 seeds internally, as the beforeClass will only run once, \nbut the methods or setUp() might run many times, especially when increasing iterations.\n\nbut lucenetestcase deals with this, and the \"seed\" is 128-bit (UUID): \nthe MSB is initialized in beforeClass, the LSB varied for each method run.\nif you provide a seed with a -D, they are both fixed to the UUID you provided.\n\nI fixed the API to be consistent, so you should be able to migrate a test from \nsetUp() to beforeClass() [junit3 to junit4] without changing parameters.\n\nThe codec, locale, timezone is only printed once at the end if any tests fail, \nas its per-class anyway (setup in beforeClass)\n\nfinally, when a test fails, you get a single \"reproduce with\" command line you can copy and paste to reproduce.\nthis way you dont have to spend time trying to figure out what the command line should be.\n\n{noformat}\n    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 0.197 sec\n    [junit]\n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodA \n              -Dtests.seed=a51e707b-6550-7800-9f8c-72622d14bf5f\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodB \n              -Dtests.seed=a51e707b-6550-7800-f7eb-2efca3820738\n    [junit] NOTE: test params are: codec=PreFlex, locale=ar_LY, timezone=Etc/UCT\n    [junit] ------------- ---------------- ---------------\n    [junit] Test org.apache.lucene.util.TestExample FAILED\n{noformat}\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2989",
        "summary": "TestCollectionUtil fails on IBM JRE",
        "description": "    [junit] Testcase: testEmptyArraySort(org.apache.lucene.util.TestCollectionUtil):    Caused an ERROR\n    [junit] CollectionUtil can only sort random access lists in-place.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1606",
        "summary": "Automaton Query/Filter (scalable regex)",
        "description": "Attached is a patch for an AutomatonQuery/Filter (name can change if its not suitable).\n\nWhereas the out-of-box contrib RegexQuery is nice, I have some very large indexes (100M+ unique tokens) where queries are quite slow, 2 minutes, etc. Additionally all of the existing RegexQuery implementations in Lucene are really slow if there is no constant prefix. This implementation does not depend upon constant prefix, and runs the same query in 640ms.\n\nSome use cases I envision:\n 1. lexicography/etc on large text corpora\n 2. looking for things such as urls where the prefix is not constant (http:// or ftp://)\n\nThe Filter uses the BRICS package (http://www.brics.dk/automaton/) to convert regular expressions into a DFA. Then, the filter \"enumerates\" terms in a special way, by using the underlying state machine. Here is my short description from the comments:\n\n     The algorithm here is pretty basic. Enumerate terms but instead of a binary accept/reject do:\n      \n     1. Look at the portion that is OK (did not enter a reject state in the DFA)\n     2. Generate the next possible String and seek to that.\n\nthe Query simply wraps the filter with ConstantScoreQuery.\n\nI did not include the automaton.jar inside the patch but it can be downloaded from http://www.brics.dk/automaton/ and is BSD-licensed.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1727",
        "summary": "Order of stored Fields not maintained",
        "description": "As noted in these threads...\n\nhttp://www.nabble.com/Order-of-fields-returned-by-Document.getFields%28%29-to21034652.html\nhttp://www.nabble.com/Order-of-fields-within-a-Document-in-Lucene-2.4%2B-to24210597.html\n\nsomewhere prior to Lucene 2.4.1 a change was introduced that prevents the Stored fields of a Document from being returned in same order that they were originally added in.  This can cause serious performance problems for people attempting to use LoadFirstFieldSelector or a custom FieldSelector with the LOAD_AND_BREAK, or the SIZE_AND_BREAK options (since the fields don't come back in the order they expect)\n\nSpeculation in the email threads is that the origin of this bug is code introduced by LUCENE-1301 -- but the purpose of that issue was refactoring, so if it really is the cause of the change this would seem to be a bug, and not a side affect of a conscious implementation change.\n\nSomeone who understands indexing internals should investigate this.  At a minimum, if it's decided that this is not actual a bug, then prior to resolving this bug the wiki docs and some of the FIeldSelector javadocs should be updated to make it clear what order Fields will be returned in.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1169",
        "summary": "Search with Filter does not work!",
        "description": "See attached JUnitTest, self-explanatory\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2104",
        "summary": "IndexWriter.unlock does does nothing if NativeFSLockFactory is used",
        "description": "If NativeFSLockFactory is used, IndexWriter.unlock will return, silently doing nothing. The reason is that NativeFSLockFactory's makeLock always creates a new NativeFSLock. NativeFSLock's release first checks if its lock is not null. However, only if obtain() is called, that lock is not null. So release actually does nothing, and so IndexWriter.unlock does not delete the lock, or fail w/ exception.\nThis is only a problem in NativeFSLock, and not in other Lock implementations, at least as I was able to see.\n\nNeed to think first how to reproduce in a test, and then fix it. I'll work on it.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2584",
        "summary": "Concurrency issues in SegmentInfo.files() could lead to ConcurrentModificationException",
        "description": "The multi-threaded call of the files() in SegmentInfo could lead to the ConcurrentModificationException if one thread is not finished additions to the ArrayList (files) yet while the other thread already obtained it as cached (see below). This is a rare exception, but it would be nice to fix. I see the code is no longer problematic in the trunk (and others ported from flex_1458), looks it was fixed while implementing post 3.x features. The fix to 3.x and 2.9.x branches could be the same - create the files set first and populate it, and then assign to the member variable at the end of the method. This will resolve the issue. I could prepare the patch for 2.9.4 and 3.x, if needed.\n\n--\n\nINFO: [19] webapp= path=/replication params={command=fetchindex&wt=javabin} status=0 QTime=1\nJul 30, 2010 9:13:05 AM org.apache.solr.core.SolrCore execute\nINFO: [19] webapp= path=/replication params={command=details&wt=javabin} status=0 QTime=24\nJul 30, 2010 9:13:05 AM org.apache.solr.handler.ReplicationHandler doFetch\nSEVERE: SnapPull failed\njava.util.ConcurrentModificationException\n        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)\n        at java.util.AbstractList$Itr.next(AbstractList.java:343)\n        at java.util.AbstractCollection.addAll(AbstractCollection.java:305)\n        at org.apache.lucene.index.SegmentInfos.files(SegmentInfos.java:826)\n        at org.apache.lucene.index.DirectoryReader$ReaderCommit.<init>(DirectoryReader.java:916)\n        at org.apache.lucene.index.DirectoryReader.getIndexCommit(DirectoryReader.java:856)\n        at org.apache.solr.search.SolrIndexReader.getIndexCommit(SolrIndexReader.java:454)\n        at org.apache.solr.handler.SnapPuller.fetchLatestIndex(SnapPuller.java:261)\n        at org.apache.solr.handler.ReplicationHandler.doFetch(ReplicationHandler.java:264)\n        at org.apache.solr.handler.ReplicationHandler$1.run(ReplicationHandler.java:146)\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-371",
        "summary": "RangeQuery - add equals and hashCode methods",
        "description": "I'm attaching a patch with an equals() and hashCode() implementation for \nRangeQuery.java, and a new unit test for TestRangeQuery.java, as per a recent \nmessage on lucene-user mailing list (subject \"RangeQuery doesn't override \nequals() or hashCode() - intentional?\")\n\npatches to follow",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1459",
        "summary": "CachingWrapperFilter crashes if you call both bits() and getDocIdSet()",
        "description": "CachingWrapperFilter uses only a single cache, so calling bits() after calling getDocIdSet() will result in a type error. Additionally, more code than is necessary is wrapped in the @synchronized blocks.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1299",
        "summary": "Spell Checker suggestSimilar throws NPE when IndexReader is not null and field is null",
        "description": "The SpellChecker.suggestSimilar(String word, int numSug, IndexReader ir,   String field, boolean morePopular) throws a NullPointerException when the IndexReader is not null, but the Field is.  The Javadocs say that it is fine to have the field be null, but doesn't comment on the fact that the IndexReader also needs to be null in that case.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2617",
        "summary": "coord should still apply to missing terms/clauses",
        "description": "Missing terms in a boolean query \"disappear\" (i.e. they don't even affect the coord factor).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2624",
        "summary": "add new snowball languages",
        "description": "Snowball added new languages. This patch adds support for them.\n\nhttp://snowball.tartarus.org/algorithms/armenian/stemmer.html\nhttp://snowball.tartarus.org/algorithms/catalan/stemmer.html\nhttp://snowball.tartarus.org/algorithms/basque/stemmer.html\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1295",
        "summary": "Make retrieveTerms(int docNum) public in MoreLikeThis",
        "description": "It would be useful if \n{code}\nprivate PriorityQueue retrieveTerms(int docNum) throws IOException {\n{code}\n\nwere public, since it is similar in use to \n{code}\npublic PriorityQueue retrieveTerms(Reader r) throws IOException {\n{code}\n\nIt also seems useful to add \n{code}\npublic String [] retrieveInterestingTerms(int docNum) throws IOException{\n{code}\nto mirror the one that works on Reader.\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1686",
        "summary": "Remove Unnecessary NULL check in FindSegmentsFile - cleanup",
        "description": "FindSegmentsFile accesses the member \"directory\" in line 579 while performing a null check in 592. The null check is unnecessary as if directory is null line 579 would throw a NPE.\nI removed the null check and made the member \"directory\" final. In addition I added a null check in the constructor as If the value is null we should catch it asap. \n\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3103",
        "summary": "create a simple test that indexes and searches byte[] terms",
        "description": "Currently, the only good test that does this is Test2BTerms (disabled by default)\n\nI think we should test this capability, and also have a simpler example for how to do this.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1928",
        "summary": "PayloadTermQuery refers to a deprecated documentation for redirection ",
        "description": "When PayloadTermQuery refers to the function for scoring Similarity - it refers to override the deprecated method - \n\nSimilarity#scorePayload(String, byte[],int,int) . \n\nThat method has been deprecated by  Similarity#scorePayload(int, String, int, int, byte[],int,int) . \n\n\nThis javadoc patch addresses the class level javadoc for the class to provide the right signature in Similarity to be overridden. ",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2215",
        "summary": "paging collector",
        "description": "http://issues.apache.org/jira/browse/LUCENE-2127?focusedCommentId=12796898&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12796898\n\nSomebody assign this to Aaron McCurry and we'll see if we can get enough votes on this issue to convince him to upload his patch.  :)",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-710",
        "summary": "Implement \"point in time\" searching without relying on filesystem semantics",
        "description": "This was touched on in recent discussion on dev list:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/41700#41700\n\nand then more recently on the user list:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/42088\n\nLucene's \"point in time\" searching currently relies on how the\nunderlying storage handles deletion files that are held open for\nreading.\n\nThis is highly variable across filesystems.  For example, UNIX-like\nfilesystems usually do \"close on last delete\", and Windows filesystem\ntypically refuses to delete a file open for reading (so Lucene retries\nlater).  But NFS just removes the file out from under the reader, and\nfor that reason \"point in time\" searching doesn't work on NFS\n(see LUCENE-673 ).\n\nWith the lockless commits changes (LUCENE-701 ), it's quite simple to\nre-implement \"point in time searching\" so as to not rely on filesystem\nsemantics: we can just keep more than the last segments_N file (as\nwell as all files they reference).\n\nThis is also in keeping with the design goal of \"rely on as little as\npossible from the filesystem\".  EG with lockless we no longer re-use\nfilenames (don't rely on filesystem cache being coherent) and we no\nlonger use file renaming (because on Windows it can fails).  This\nwould be another step of not relying on semantics of \"deleting open\nfiles\".  The less we require from filesystem the more portable Lucene\nwill be!\n\nWhere it gets interesting is what \"policy\" we would then use for\nremoving segments_N files.  The policy now is \"remove all but the last\none\".  I think we would keep this policy as the default.  Then you\ncould imagine other policies:\n\n  * Keep past N day's worth\n\n  * Keep the last N\n\n  * Keep only those in active use by a reader somewhere (note: tricky\n    how to reliably figure this out when readers have crashed, etc.)\n\n  * Keep those \"marked\" as rollback points by some transaction, or\n    marked explicitly as a \"snaphshot\".\n\n  * Or, roll your own: the \"policy\" would be an interface or abstract\n    class and you could make your own implementation.\n\nI think for this issue we could just create the framework\n(interface/abstract class for \"policy\" and invoke it from\nIndexFileDeleter) and then implement the current policy (delete all\nbut most recent segments_N) as the default policy.\n\nIn separate issue(s) we could then create the above more interesting\npolicies.\n\nI think there are some important advantages to doing this:\n\n  * \"Point in time\" searching would work on NFS (it doesn't now\n    because NFS doesn't do \"delete on last close\"; see LUCENE-673 )\n    and any other Directory implementations that don't work\n    currently.\n\n  * Transactional semantics become a possibility: you can set a\n    snapshot, do a bunch of stuff to your index, and then rollback to\n    the snapshot at a later time.\n\n  * If a reader crashes or machine gets rebooted, etc, it could choose\n    to re-open the snapshot it had previously been using, whereas now\n    the reader must always switch to the last commit point.\n\n  * Searchers could search the same snapshot for follow-on actions.\n    Meaning, user does search, then next page, drill down (Solr),\n    drill up, etc.  These are each separate trips to the server and if\n    searcher has been re-opened, user can get inconsistent results (=\n    lost trust).  But with, one series of search interactions could\n    explicitly stay on the snapshot it had started with.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-439",
        "summary": "Filters need hashCode() and equals()",
        "description": "Filters need to implement hashCode() and equals(), esp since certain query types can contain a filter (FilteredQuery, ConstantScoreQuery)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3208",
        "summary": "Move Query.weight() to IndexSearcher as protected method",
        "description": "We had this issue several times, latest in LUCENE-3207.\n\nThe method Query.weight() was left in Query for backwards reasons in Lucene 2.9 when we changed Weight class. This method is only to be called on top-level queries - and this is done by IndexSearcher. This method is just a utility method, that has nothing to do with the query itsself (it just combines the createWeight method and calls the normalization afterwards). \n\nThe problem we have is that any query that wraps other queries (like CustomScore, ConstantScore, Boolean) calls Query.weight() instead of Query.createWeight(), it will do normalization two times, leading to strange bugs.\n\nFor 3.3 I will make Query.weight() simply delegate to IndexSearcher's replacement method with a big deprecation warning, so user sees this. In IndexSearcher itsself the method will be protected to only be called by itsself or subclasses of IndexSearcher. Delegation for backwards is no problem, as protected is accessible by classes in same package.\n\nI would suggest the method name to be IndexSearcher.createNormalizedWeight(Query q)",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2098",
        "summary": "make BaseCharFilter more efficient in performance",
        "description": "Performance degradation in Solr 1.4 was reported. See:\n\nhttp://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4\n\nThe inefficiency has been pointed out in BaseCharFilter javadoc by Mike:\n\n{panel}\nNOTE: This class is not particularly efficient. For example, a new class instance is created for every call to addOffCorrectMap(int, int), which is then appended to a private list. \n{panel}\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2650",
        "summary": "improve windows defaults in FSDirectory",
        "description": "Currently windows defaults to SimpleFSDirectory, but this is a problem due to the synchronization.\n\nI have been benchmarking queries *sequentially* and was pretty surprised at how much faster\nMMapDirectory is, for example for cases that do many seeks.\n\nI think we should change the defaults for windows as such:\n\nif (WINDOWS and UNMAP_SUPPORTED and 64-bit)\n  use MMapDirectory\nelse\n  use SimpleFSDirectory \n\nI think we should just consider doing this for 4.0 only and see how it goes.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2924",
        "summary": "fix getting started / demo docs",
        "description": "Opening a new issue for this since there are a number of problems...:\n\n  * We should get the versions right, eg when we explain how to do a src checkout it should point to the path for that release\n\n  * Source checkout / build JARs instructions must be updated for the merger\n\n  * Analyzers JAR must be on the classpath too\n\n  * Demo sources are no longer shipped in a binary release\n\n  * Fixup from LUCENE-2923 (remove web app, new command-line ops for IndexFiles, etc.)",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2523",
        "summary": "if index is too old you should hit an exception saying so",
        "description": "If you create an index in 2.3.x (I used demo's IndexFiles) and then try to read it in 4.0.x (I used CheckIndex), you hit a confusing exception like this:\n{noformat}\njava.io.IOException: read past EOF\n        at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)\n        at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)\n        at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)\n        at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)\n        at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:171)\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)\n        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:269)\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:484)\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:265)\n        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:308)\n        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:287)\n        at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:930)\n{noformat}\n\nI think instead we should throw an IndexTooOldException or something like that?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-213",
        "summary": "[PATCH] Ordered spanquery with slop can fail",
        "description": "In CVS of 7 April 2004. \nAn ordered SpanQuery with slop 1 querying: w1 w2 w3 \nin document: w1 w3 w2 w3 \nfails. It should match as: w1 . w2 w3",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2804",
        "summary": "check all tests that use FSDirectory.open",
        "description": "In LUCENE-2471 we were discussing the copyBytes issue, and Shai and I had a discussion about how we could prevent such bugs in the future.\n\nOne thing that lead to the bug existing in our code for so long, was that it only happened on windows (e.g. never failed in hudson!)\nThis was because the bug only happened if you were copying from SimpleFSDirectory, and the test used FSDirectory.open\n\nToday the situation is improving: most tests use newDirectory() which is random by default and never use FSDir.open,\nit always uses SimpleFS or NIOFS so that the same random seed will reproduce across both windows and unix.\n\nSo I think we need to review all uses of FSDirectory.open in our tests, and minimize these.\nIn general tests should use newDirectory().\nIf the test comes with say a zip-file and wants to explicitly open stuff from disk, I think it should open the contents with say SimpleFSDir,\nand then call newDirectory(Directory) to copy into a new \"random\" implementation for actual testing. This method already exists:\n{noformat}\n  /**\n   * Returns a new Dictionary instance, with contents copied from the\n   * provided directory. See {@link #newDirectory()} for more\n   * information.\n   */\n  public static MockDirectoryWrapper newDirectory(Directory d) throws IOException {\n{noformat}\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-1243",
        "summary": "A few new benchmark tasks",
        "description": "Some tasks that would be helpful to see added. Might want some expansion, but here are some basic ones I have been using:\n\nCommitIndexTask\nReopenReaderTask\nSearchWithSortTask\n\nI do the sort in a similar way that the highlighting was done, but another method may be better. Just would be great to have sorting.\nAlso, since there is no great field for sorting (reuters date always appears to be the same) I changed the id field from doc+id to just id. Again maybe not the best solution, but here I am to get the ball rolling :)",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3883",
        "summary": "Analysis for Irish",
        "description": "Adds analysis for Irish.\n\nThe stemmer is generated from a snowball stemmer. I've sent it to Martin Porter, who says it will be added during the week.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1411",
        "summary": "Enable IndexWriter to open an arbitrary commit point",
        "description": "With a 2-phase commit involving multiple resources, each resource\nfirst does its prepareCommit and then if all are successful they each\ncommit.  If an exception or timeout/power loss is hit in any of the\nresources during prepareCommit or commit, all of the resources must\nthen rollback.\n\nBut, because IndexWriter always opens the most recent commit, getting\nLucene to rollback after commit() has been called is not easy, unless\nyou make Lucene the last resource to commit.  A simple workaround is\nto simply remove the segments_N files of the newer commits but that's\nsort of a hassle.\n\nTo fix this, we just need to add a ctor to IndexWriter that takes an\nIndexCommit.  We recently added this for IndexReader (LUCENE-1311) as\nwell.  This ctor is definitely an \"expert\" method, and only makes\nsense if you have a custom DeletionPolicy that preserves more than\njust the most recent commit.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2994",
        "summary": "When 3.1 is released, update backwards tests in 3.x branch",
        "description": "When we have released the official artifacts of Lucene 3.1 (the final ones!!!), we need to do the following:\n\n- svn rm backwards/src/test\n- svn cp https://svn.apache.org/repos/asf/lucene/dev/branches/lucene_solr_3_1/lucene/src/test backwards/src/test\n- Copy the lucene-core-3.1.0.jar from the last release tarball to lucene/backwards/lib and delete old one.\n- Check that everything is correct: The backwards folder should contain a src/ folder that now contains \"test\". The files should be the ones from the branch.\n- Run \"ant test-backwards\"\n\nUwe will take care of this!",
        "label": "NUG",
        "classified": "TEST",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3378",
        "summary": "Some contribs depend on core tests to be compiled and fail when ant clean was done before",
        "description": "If you do \"ant clean\" on the root level of Lucene and then go to e.g. contrib/queryparser (3.x only) or contrib/misc (3.x and trunk) and call \"ant test\", the build of tests fails:\n- contrib/queryparser's ExtendedableQPTests extend a core TestQueryParser (3.x only, in module this works, of course)\n- contrib/misc/TestIndexSplitter uses a core class to build its index\n\nTo find the root cause: We should first remove the core tests from contrib classpath, so the issue gets visible even without \"ant clean\" before. Then we can fix this.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3535",
        "summary": "move PerFieldCodecWrapper into codecs package",
        "description": "PerFieldCodecWrapper is a codec, but its 'hardwired' as lucene's only codec currently (except for PreFlex/3.x case)\n\nit lets you choose a format for the postings lists per-field.\n\nI think we should move this to the codecs package as a start... just a rote refactor.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1845",
        "summary": "if the build fails to download JARs for contrib/db, just skip its tests",
        "description": "Every so often our nightly build fails because contrib/db is unable to download the necessary BDB JARs from http://downloads.osafoundation.org.  I think in such cases we should simply skip contrib/db's tests, if it's the nightly build that's running, since it's a false positive failure.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1274",
        "summary": "Expose explicit 2-phase commit in IndexWriter",
        "description": "Currently when IndexWriter commits, it does so with a two-phase\ncommit, internally: first it prepares all the new index files, syncs\nthem; then it writes a new segments_N file and syncs that, and only if\nthat is successful does it remove any now un-referenced index files.\n\nHowever, these two phases are done privately, internal to the commit()\nmethod.\n\nBut when Lucene is involved in a transaction with external resources\n(eg a database), it's very useful to explicitly break out the prepare\nphase from the commit phase.\n\nSpinoff from this thread:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200804.mbox/%3C16627610.post@talk.nabble.com%3E\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3521",
        "summary": "upgrade icu jar to 4.8.1.1 / remove lucenetestcase hack",
        "description": "This bug fix release fixes problems with icu under java7: http://bugs.icu-project.org/trac/ticket/8734",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1512",
        "summary": "Incorporate GeoHash in contrib/spatial",
        "description": "Based on comments from Yonik and Ryan in SOLR-773 \nGeoHash provides the ability to store latitude / longitude values in a single field consistent hash field.\nWhich elements the need to maintain 2 field caches for latitude / longitude fields, reducing the size of an index\nand the amount of memory needed for a spatial search.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-881",
        "summary": "QueryParser escaping/parsin issue with strings starting/ending with ||",
        "description": "There is a problem with query parser when search string starts/ends with ||.  When string contains || in the middle like 'something || something' everything runs without a problem.\n\nPart of code: \n  searchText = QueryParser.escape(searchText);\n  QueryParser parser = null;\n  parser = new QueryParser(fieldName, new CustomAnalyser());\n  parser.parse(searchText);\n\nCustomAnalyser class extends Analyser. Here is the only redefined method: \n\n    @Override\n    public TokenStream tokenStream(String fieldName, Reader reader) {\n      return new PorterStemFilter( (new StopAnalyzer()).tokenStream(fieldName, reader));\n    }\n\nI have tested this on Lucene 2.1 and latest source I have checked-out from SVN (Revision 538867) and in both cases parsing exception was thrown.\n\nPart of Stack Trace (Lucene - SVN checkout - Revision 538867):\nCannot parse 'someting ||': Encountered \"<EOF>\" at line 1, column 11.\nWas expecting one of:\n    <NOT> ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"*\" ...\n    <QUOTED> ...\n    <TERM> ...\n    <PREFIXTERM> ...\n    <WILDTERM> ...\n    \"[\" ...\n    \"{\" ...\n    <NUMBER> ...\n    \n org.apache.lucene.queryParser.ParseException: Cannot parse 'someting ||': Encountered \"<EOF>\" at line 1, column 11.\nWas expecting one of:\n    <NOT> ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"*\" ...\n    <QUOTED> ...\n    <TERM> ...\n    <PREFIXTERM> ...\n    <WILDTERM> ...\n    \"[\" ...\n    \"{\" ...\n    <NUMBER> ...\n    \n        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:150)\n\n\nPart of Stack Trace (Lucene 2.1):\nCannot parse 'something ||': Encountered \"<EOF>\" at line 1, column 12.\nWas expecting one of:\n    <NOT> ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"*\" ...\n    <QUOTED> ...\n    <TERM> ...\n    <PREFIXTERM> ...\n    <WILDTERM> ...\n    \"[\" ...\n    \"{\" ...\n    <NUMBER> ...\n    \n org.apache.lucene.queryParser.ParseException: Cannot parse 'something ||': Encountered \"<EOF>\" at line 1, column 12.\nWas expecting one of:\n    <NOT> ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"*\" ...\n    <QUOTED> ...\n    <TERM> ...\n    <PREFIXTERM> ...\n    <WILDTERM> ...\n    \"[\" ...\n    \"{\" ...\n    <NUMBER> ...\n    \n        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:149)\n\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2136",
        "summary": "MultiReader should not use PQ for its Term/sEnum if it has only 1 reader",
        "description": "Related to LUCENE-2130....\n\nEven though we've switched to segment-based searching, there are still times when the Term/sEnum is used against the top-level reader.  I think Solr does this, and from LUCENE-2130, certain rewrite modes of MTQ will do this as well.\n\nCurrently, on an optimized index, MTQ is still using a PQ to present the terms, which is silly because this just adds a sizable amount of overhead.  In such cases we should simply delecate to the single segment.\n\nNote that the single segment can have deletions, and we should still delegate.  Ie, the index need not be optimized, just have a single segment.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2648",
        "summary": "Allow PackedInts.ReaderIterator to advance more than one value",
        "description": "The iterator-like API in LUCENE-2186 makes effective use of PackedInts.ReaderIterator but frequently skips multiple values. ReaderIterator currently requires to loop over ReaderInterator#next() to advance to a certain value. We should allow ReaderIterator to expose a #advance(ord) method to make use-cases like that more efficient. \n\nThis issue is somewhat part of my efforts to make LUCENE-2186 smaller while breaking it up in little issues for parts which can be generally useful.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-264",
        "summary": "[PATCH] Improved javadoc for maxClauseCount",
        "description": "As discussed on lucene-dev before, queries with lots of terms can use \nup a lot of unused buffer space for their TermDocs, because most terms \nhave few documents.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3168",
        "summary": "Enable Throttling only during nightly builds",
        "description": "Some of my tests take forever even on a big :) machine. In order to speed up our tests we should default the IO throttling to NEVER and only run in during nightly.\n\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1857",
        "summary": "Change NumericRangeQuery to generics (for the numeric type)",
        "description": "NumericRangeQuery/Filter can use generics for more type-safety: NumericRangeQuery<T extends Number>",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1656",
        "summary": "When sorting by field, IndexSearcher should not compute scores by default",
        "description": "In 2.9 we've added the ability to turn off scoring (maxScore &\ntrackScores, separately) when sorting by field.\n\nI expect most apps don't use the scores when sorting by field, and\nthere's a sizable performance gain when scoring is off, so I think for\n2.9 we should not score by default, and add show in CHANGES how to\nenable scoring if you rely on it.\n\nIf there are no objections, I'll commit that change in a day or two\n(it's trivial).\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1267",
        "summary": "add numDocs() and maxDoc() methods to IndexWriter; deprecate docCount()",
        "description": "Spinoff from here:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c405706.11550.qm@web65411.mail.ac4.yahoo.com%3e\n\nI think we should add maxDoc() and numDocs() methods to IndexWriter,\nand deprecate docCount() in favor of maxDoc().  To do this I think we\nshould cache the deletion count of each segment in the segments file.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3646",
        "summary": "throw exception for fieldcache on a non-atomic reader",
        "description": "In Lucene 4.0, we go through a lot of effort to prevent slow uses of non-atomic readers:\n\nDirectoryReader/MultiReader etc throw exception if you don't try to access postings or docvalues apis per-segment, etc.\n\nBut the biggest trap of all is still too easy to fall into, we don't do the same for FieldCache.\n\nI think we should throw exception, forcing the user to either change their code or use a SlowMultiReaderWrapper.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3026",
        "summary": "smartcn analyzer throw NullPointer exception when the length of analysed text over 32767",
        "description": "That's all because of org.apache.lucene.analysis.cn.smart.hhmm.SegGraph's makeIndex() method:\n  public List<SegToken> makeIndex() {\n    List<SegToken> result = new ArrayList<SegToken>();\n    int s = -1, count = 0, size = tokenListTable.size();\n    List<SegToken> tokenList;\n    short index = 0;\n    while (count < size) {\n      if (isStartExist(s)) {\n        tokenList = tokenListTable.get(s);\n        for (SegToken st : tokenList) {\n          st.index = index;\n          result.add(st);\n          index++;\n        }\n        count++;\n      }\n      s++;\n    }\n    return result;\n  }\n\nhere 'short index = 0;' should be 'int index = 0;'. And that's reported here http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=2 and http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=11, the author XiaoPingGao have already fixed this bug:http://code.google.com/p/imdict-chinese-analyzer/source/browse/trunk/src/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2737",
        "summary": "Codec is not consistently passed in internal API",
        "description": "While working on SOLR-1942 I ran into a couple of glitches with codec which is not consistently passed to SegmentsInfo and friends. Codecs should really be consistently passed though. I have fixed the pieces which lead to errors in Solr but I  guess there might be others too. Patch is coming up... ",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3508",
        "summary": "Decompounders based on CompoundWordTokenFilterBase cannot be used with custom attributes",
        "description": "The CompoundWordTokenFilterBase.setToken method will call clearAttributes() and then will reset only the default Token attributes (term, position, flags, etc) resulting in any custom attributes losing their value. Commenting out clearAttributes() seems to do the trick, but will fail the TestCompoundWordTokenFilter tests..",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3357",
        "summary": "Unit and integration test cases for the new Similarities",
        "description": "Write test cases to test the new Similarities added in [LUCENE-3220|https://issues.apache.org/jira/browse/LUCENE-3220]. Two types of test cases will be created:\n * unit tests, in which mock statistics are provided to the Similarities and the score is validated against hand calculations;\n * integration tests, in which a small collection is indexed and then searched using the Similarities.\n\nPerformance tests will be performed in a separate issue.",
        "label": "NUG",
        "classified": "TEST",
        "type": ""
    },
    {
        "key": "LUCENE-2875",
        "summary": "NumericTokenStream.NumericTermAttribute does not support cloning -> Solr analysis request handlers fail",
        "description": "During converting Solr's AnalysisRequestHandlers (LUCENE-2374) I noticed, that the current implementation of NumericTokenStream fails on cloneAttributes(), which is needed to buffer the tokens for structured display.\n\nThis issue should fix this by refactoring the inner class.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-505",
        "summary": "MultiReader.norm() takes up too much memory: norms byte[] should be made into an Object",
        "description": "MultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every segment.  This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.  Although this is cached, it's still a baseline of memory that is unnecessary.\n\nThe problem is that the Normalization Factors are passed around as a byte[].  If it were instead replaced with an Object, you could perform a whole host of optimizations\na.  When reading, you wouldn't have to construct a \"fakeNorms\" array of all 1.0fs.  You could instead return a singleton object that would just return 1.0f.\nb.  MultiReader could use an object that could delegate to NormFactors of the subreaders\nc.  You could write an implementation that could use mmap to access the norm factors.  Or if the index isn't long lived, you could use an implementation that reads directly from the disk.\n\nThe patch provided here replaces the use of byte[] with a new abstract class called NormFactors.  \nNormFactors has two methods on it\n    public abstract byte getByte(int doc) throws IOException;  // Returns the byte[doc]\n    public float getFactor(int doc) throws IOException;            // Calls Similarity.decodeNorm(getByte(doc))\n\nThere are four implementations of this abstract class\n1.  NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0\n2.  NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors.\n3.  MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array.\n4.  SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access.\n\nIn addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[].  I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated.  I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don't know how to test that.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1255",
        "summary": "CheckIndex should allow term position = -1",
        "description": "\nSpinoff from this discussion:\n\n    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3CPine.LNX.4.62.0803292323350.16762@radix.cryptio.net%3E\n\nRight now CheckIndex claims the index is corrupt if you index a Token with -1 position, which happens if your first token has positionIncrementGap set to 0.\n\nBut, as far as I can tell, Lucene doesn't \"mind\" when this happens.\n\nSo I plan to fix CheckIndex to allow this case.  I'll backport to 2.3.2 as well.\n\nLUCENE-1253 is one example where Lucene's core analyzers could do this.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2625",
        "summary": "IndexReader.termDocs() retrieves no documents",
        "description": "TermDocs object returned by indexReader.termDocs() retrieves no documents, howerver, the documents are retrieved correctly when using indexReader.termDocs(Term), indexReader.termDocs(null) and indexSearcher.search(Query).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-577",
        "summary": "SweetSpotSimiliarity",
        "description": "This is a new Similarity implimention for the contrib/miscellaneous/ package, it provides a Similiarty designed for people who know the \"sweetspot\" of their data.  three major pieces of functionality are included:\n\n1) a lengthNorm which creates a \"platuea\" of values.\n2) a baseline tf that provides a fixed value for tf's up to a minimum, at which point it becomes a sqrt curve (this is used by the tf(int) function.\n3) a hyperbolic tf function which is best explained by graphing the equation.  this isn't used by default, but is available for subclasses to call from their own tf functions.\n\nAll constants used in all functions are configurable.  In the case of lengthNorm, the constants are configurable per field, as well as allowing for defaults for unspecified fields.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2116",
        "summary": "Add link to irc channel #lucene on the website",
        "description": "We should add a link to #lucene IRC channel on chat.freenode.org. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-536",
        "summary": "JEDirectory delete issue",
        "description": "JEDirectory is not deleting files properly.  Blocks are left behind due to an error in cursor operations.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2717",
        "summary": "BasicOperations.concatenate creates invariants",
        "description": "I started writing a test for LUCENE-2716, and i found a problem with BasicOperations.concatenate(Automaton, Automaton):\nit creates automata with invariant representation (which should never happen, unless you manipulate states/transitions manually).\n\nstrangely enough the BasicOperations.concatenate(List<Automaton>) does not have this problem.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1102",
        "summary": "EnwikiDocMaker id field",
        "description": "The EnwikiDocMaker is fairly usable outside of the benchmarking class, but it would benefit from indexing the ID field of the docs.\n\nPatch to follow that adds an ID field.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1624",
        "summary": "Don't commit an empty segments_N when IW is opened with create=true",
        "description": "If IW is opened with create=true, it forcefully commits an empty\nsegments_N.  But really it should not: if autoCommit is false, nothing\nshould be committed until commit or close is explicitly called.\n\nSpinoff from http://www.nabble.com/no-segments*-file-found:-files:-Error-on-opening-index-td23219520.html\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2170",
        "summary": "Thread starvation problems in some tests",
        "description": "In some of the tests, a time limit is set and the tests have a \"while (inTime)\" loop. If creation of thread under heavy load is too slow, the tasks are not done. Most tests are only useful, if the task is at least done once (most would even fail).\n\nThis thread changes the loops to be do...while, so the task is run at least one time.",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1535",
        "summary": "Make tests using java.util.Random reproducible on failure",
        "description": "This is a patch for LuceneTestCase to support logging of the Random seed used in randomized tests. The patch also includes an example implementation in TestTrieRangeQuery.\n\nIt overrides the protected method runTest() and inserts a try-catch around the super.runTest() call. Two new methods newRandom() and newRandom(long) are available for the test case. As each test case is run in an own TestCase object instance (so 5 test methods in a class instantiate 5 instances each method working in separate), the random seed is saved on newRandom() and when the test fails with any Throwable, a message with the seed (if not null) is printed out. If newRandom was never called no message will be printed.\n\nThis patch has only one problem: If a single test method calls newRandom() more than once, only the last seed is saved and printed out. But each test method in a Testcase should call newRandom() exactly once for usage during the execution of this test method. And it is not thread save (no sync, no volatile), but for tests it's unimportant.\n\nI forgot to mention: If a test fails, the message using the seed is printed to stdout. The developer can then change the test temporarily:\n\n{code}LuceneTestCase.newRandom() -> LuceneTestCase.newRandom(long){code}\n\nusing the seed from the failed test printout.\n\n*Reference:*\n{quote}\n: By allowing Random to randomly seed itself, we effectively test a much\n: much larger space, ie every time we all run the test, it's different.  We can\n: potentially cast a much larger net than a fixed seed.\n\ni guess i'm just in favor of less randomness and more iterations.\n\n: Fixing the bug is the \"easy\" part; discovering a bug is present is where\n: we need all the help we can get ;)\n\nyes, but knowing a bug is there w/o having any idea what it is or how to \ntrigger it can be very frustrating.\n\nit would be enough for tests to pick a random number, log it, and then use \nit as the seed ... that way if you get a failure you at least know what \nseed was used and you can then hardcode it temporarily to reproduce/debug\n\n-Hoss\n{quote}",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-898",
        "summary": "contrib/javascript is not packaged into releases",
        "description": "the contrib/javascript directory is (apparently) a collection of javascript utilities for lucene .. but it has not build files or any mechanism to package it, so it is excluded form releases.\n\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2551",
        "summary": "change jdk & icu collation to use byte[]",
        "description": "Now that term is byte[], we should switch collation to use byte[] instead of 'indexablebinarystring'.\n\nThis is faster and results in much smaller sort keys.\n\nI figure we can work it out here, and fix termrangequery to use byte in parallel, but we can already test sorting etc now.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2937",
        "summary": "small float underflow detection bug",
        "description": "Underflow detection in small floats has a bug, and can incorrectly result in a byte value of 0 for a non-zero float.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-415",
        "summary": "Merge error during add to index (IndexOutOfBoundsException)",
        "description": "I've been batch-building indexes, and I've build a couple hundred indexes with \na total of around 150 million records.  This only happened once, so it's \nprobably impossible to reproduce, but anyway... I was building an index with \naround 9.6 million records, and towards the end I got this:\n\njava.lang.IndexOutOfBoundsException: Index: 54, Size: 24\n        at java.util.ArrayList.RangeCheck(ArrayList.java:547)\n        at java.util.ArrayList.get(ArrayList.java:322)\n        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)\n        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)\n        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java\n:149)\n        at org.apache.lucene.index.SegmentTermEnum.next\n(SegmentTermEnum.java:115)\n        at org.apache.lucene.index.SegmentMergeInfo.next\n(SegmentMergeInfo.java:52)\n        at org.apache.lucene.index.SegmentMerger.mergeTermInfos\n(SegmentMerger.java:294)\n        at org.apache.lucene.index.SegmentMerger.mergeTerms\n(SegmentMerger.java:254)\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)\n        at org.apache.lucene.index.IndexWriter.mergeSegments\n(IndexWriter.java:487)\n        at org.apache.lucene.index.IndexWriter.maybeMergeSegments\n(IndexWriter.java:458)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2193",
        "summary": "Get rid of backwards tags",
        "description": "This is a followup on: [http://www.lucidimagination.com/search/document/bb6c23b6e87c0b63/back_compat_folders_in_tags_when_i_svn_update#3000a2232c678031]\n\nCurrently we use tags for specifying the revision number in the backwards branch that matches the current development branch revision (in common-build.xml). The idea is to just specify the corresponding revision no of the backwards branch in common-build.xml and the backwards-test target automatically handles up/down/co:\n\n- We just give the rev number in common-build in common-build.xml as a property backwards-rev=\"XXXX\". This property is used also in building the command line which is also a property backwards-svn-args=\"-r $backwards-rev\". By that you can use \"ant -Dbackwards-svn-args=''\" to force test-backwards to checkout/update to head of branch (recommened for developers).\n\n- we should rename target to \"test-backwards\" and keep a \"test-tag\" with dependency to that for compatibility\n\n- The checkout on backwards creates a directory \"backwards/${backwards-branch}\" and uses \"svn co ${backwards-svn-args} 'http://svn.../${backwards-branch}' 'backwards/${backwards-branch}'\". The cool thing, the dir is checked out if non existent, but if the checkout already exists, svn co implicitely does an svn up to the given revision (it will also downgrade and merge if newer). So the test-backwards target always updates the checkout to the correct revision. I had not tried with local changes, but this should simply merge as an svn up.\n\nThe workflow for committing fixes to bw would be:\n\n- First use \"svn up\" to upgrade the backwards working copy to HEAD.\n- Commit the changes\n- Copy and paste the message \"Committed revision XXXX\" to common-build.xml\n- Commit the changes in trunk\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1705",
        "summary": "Add deleteAllDocuments() method to IndexWriter",
        "description": "Ideally, there would be a deleteAllDocuments() or clear() method on the IndexWriter\n\nThis method should have the same performance and characteristics as:\n* currentWriter.close()\n* currentWriter = new IndexWriter(..., create=true,...)\n\nThis would greatly optimize a delete all documents case. Using deleteDocuments(new MatchAllDocsQuery()) could be expensive given a large existing index.\n\nIndexWriter.deleteAllDocuments() should have the same semantics as a commit(), as far as index visibility goes (new IndexReader opening would get the empty index)\n\nI see this was previously asked for in LUCENE-932, however it would be nice to finally see this added such that the IndexWriter would not need to be closed to perform the \"clear\" as this seems to be the general recommendation for working with an IndexWriter now\n\ndeleteAllDocuments() method should:\n* abort any background merges (they are pointless once a deleteAll has been received)\n* write new segments file referencing no segments\n\nThis method would remove one of the final reasons i would ever need to close an IndexWriter and reopen a new one \n",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-1472",
        "summary": "DateTools.stringToDate() can cause lock contention under load",
        "description": "Load testing our application (the JIRA Issue Tracker) has shown that threads spend a lot of time blocked in DateTools.stringToDate().\n\nThe stringToDate() method uses a singleton SimpleDateFormat object to parse the dates.\nEach call to SimpleDateFormat.parse() is *synchronized* because SimpleDateFormat is not thread safe.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-940",
        "summary": "SimpleDateFormat used in a non thread safe manner",
        "description": "As Mike pointed out in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html SimpleDateFormat is not thread safe and hence DocMakers need to maintain it in a ThreadLocal.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1448",
        "summary": "add getFinalOffset() to TokenStream",
        "description": "If you add multiple Fieldable instances for the same field name to a document, and you then index those fields with TermVectors storing offsets, it's very likely the offsets for all but the first field instance will be wrong.\n\nThis is because IndexWriter under the hood adds a cumulative base to the offsets of each field instance, where that base is 1 + the endOffset of the last token it saw when analyzing that field.\n\nBut this logic is overly simplistic.  For example, if the WhitespaceAnalyzer is being used, and the text being analyzed ended in 3 whitespace characters, then that information is lost and then next field's offsets are then all 3 too small.  Similarly, if a StopFilter appears in the chain, and the last N tokens were stop words, then the base will be 1 + the endOffset of the last non-stopword token.\n\nTo fix this, I'd like to add a new getFinalOffset() to TokenStream.  I'm thinking by default it returns -1, which means \"I don't know so you figure it out\", meaning we fallback to the faulty logic we have today.\n\nThis has come up several times on the user's list.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-929",
        "summary": "contrib/benchmark build doesn't handle checking if content is properly extracted",
        "description": "The contrib/benchmark build does not properly handle checking to see if the content (such as Reuters coll.) is properly extracted.  It only checks to see if the directory exists.  Thus, it is possible that the directory gets created and the extraction fails.  Then, the next time it is run, it skips the extraction part and tries to continue on running the benchmark.\n\nThe workaround is to manually delete the extraction directory.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3330",
        "summary": "revise Scorer visitor API",
        "description": "Currently there is an (expert) API in scorer to allow you to take a scorer, and visit its children etc (e.g. booleanquery).\n\nI think we should improve this, so its general to all queries.\n\nThe current issues are:\n* the current api is hardcoded for booleanquery's Occurs, but we should support other types of children (e.g. disjunctionmax)\n* it can be difficult to use the API, because of the amount of generics and the visitor callback API. \n* the current API enforces a DFS traversal when you might prefer BFS instead.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3278",
        "summary": "Rename contrib/queryparser project to queryparser-contrib",
        "description": "Much like with contrib/queries, we should differentiate the contrib/queryparser from the queryparser module.  No directory structure changes will be made, just ant and maven.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-2268",
        "summary": "Add test to check maven artifacts and their poms",
        "description": "As release manager it is hard to find out if the maven artifacts work correct. It would be good to have an ant task that executes maven with a .pom file that requires all contrib/core artifacts (or one for each contrib) that \"downloads\" the artifacts from the local dist/maven folder and builds that test project. This would require maven to execute the build script. Also it should pass the ${version} ANT property to this pom.xml",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3640",
        "summary": "remove IndexSearcher.close",
        "description": "Now that IS is never \"heavy\" (since you have to pass in your own IR), IS.close is truly a no-op... I think we should remove it.\n\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2106",
        "summary": "Benchmark does not close its Reader when OpenReader/CloseReader are not used",
        "description": "Only the Searcher is closed, but because the reader is passed to the Searcher, the Searcher does not close the Reader, causing a resource leak.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-669",
        "summary": "finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close already closed file",
        "description": "Hi all,\n\nI found a small problem in FSDirectory: The finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close the underlying file. This is not a problem unless the file has been closed before by calling the close() method. If it has been closed before, the finalize method throws an IOException saying that the file is already closed. Usually this IOException would go unnoticed, because the GarbageCollector, which calls finalize(), just eats it. However, if I use the Eclipse debugger the execution of my code will always be suspended when this exception is thrown.\n\nEven though this exception probably won't cause problems during normal execution of Lucene, the code becomes cleaner if we apply this small patch. Might this IOException also have a performance impact, if it is thrown very frequently?\n\nI attached the patch which applies cleanly on the current svn HEAD. All testcases pass and I verfied with the Eclipse debugger that the IOException is not longer thrown.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3254",
        "summary": "BitVector.isSparse is sometimes wrong",
        "description": "In working on LUCENE-3246, I found a few problems with\nBitVector.isSparse:\n\n  * Its math can overflow int, such that if there are enough deleted\n    docs and maxDoc() is largish, isSparse may incorrectly return true\n\n  * It over-estimates the size of the sparse file, since when\n    estimating number of bytes for the vInt dgaps it uses bits.length\n    instead of bits.length divided by number of set bits (ie, the\n    \"average\" gap between set bits)\n\nThis is relatively harmless (just affects performance / size of .del\nfile on disk, not correctness).\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3580",
        "summary": "Most 4.0 PostingsReaders don't obey DISI contract",
        "description": "While trying to do some refactoring, I noticed funky things going on with some codecs.\n\nOne problem is that DocIdSetIterator says the following:\n{noformat}\nReturns the following:\n   * <ul>\n   * <li>-1 or {@link #NO_MORE_DOCS} if {@link #nextDoc()} or\n   * {@link #advance(int)} were not called yet.\n{noformat}\n\nBut most 4.0 Docs/DocsAndPositionsEnums don't actually do this (e.g. return 0). instead we \nare relying on Scorers to 'cover' for them, which is inconsistent. Some scorers actually rely\nupon this behavior, for example look at ReqExclScorer.toNonExcluded(), it calls docId() on the\nexcluded part before it calls nextDoc()\n\nSo we need to either fix these enums, change these enums to not extend DocIdSetIterator (and redefine\nwhat the actual contract should be for these enums), change DocIdSetIterator, or something else.\n\nFixing the enums to return -1 here when they are uninitialized kinda sucks for the ones summing up\ndocument deltas...\n",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3201",
        "summary": "improved compound file handling",
        "description": "Currently CompoundFileReader could use some improvements, i see the following problems\n* its CSIndexInput extends bufferedindexinput, which is stupid for directories like mmap.\n* it seeks on every readInternal\n* its not possible for a directory to override or improve the handling of compound files.\n\nfor example: it seems if you were impl'ing this thing from scratch, you would just wrap the II directly (not extend BufferedIndexInput,\nand add compound file offset X to seek() calls, and override length(). But of course, then you couldnt throw read past EOF always when you should,\nas a user could read into the next file and be left unaware.\n\nhowever, some directories could handle this better. for example MMapDirectory could return an indexinput that simply mmaps the 'slice' of the CFS file.\nits underlying bytebuffer etc naturally does bounds checks already etc, so it wouldnt need to be buffered, not even needing to add any offsets to seek(),\nas its position would just work.\n\nSo I think we should try to refactor this so that a Directory can customize how compound files are handled, the simplest \ncase for the least code change would be to add this to Directory.java:\n\n{code}\n  public Directory openCompoundInput(String filename) {\n    return new CompoundFileReader(this, filename);\n  }\n{code}\n\nBecause most code depends upon the fact compound files are implemented as a Directory and transparent. at least then a subclass could override...\nbut the 'recursion' is a little ugly... we could still label it expert+internal+experimental or whatever.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2016",
        "summary": "replace invalid U+FFFF character during indexing",
        "description": "If the invalid U+FFFF character is embedded in a token, it actually causes indexing to silently corrupt the index by writing duplicate terms into the terms dict.  CheckIndex will catch the error, and merging will hit exceptions (I think).\n\nWe already replace invalid surrogate pairs with the replacement character U+FFFD, so I'll just do the same with U+FFFF.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3057",
        "summary": "LuceneTestCase#newFSDirectoryImpl misses to set LockFactory if ctor call throws exception",
        "description": "selckin reported on IRC that if you run ant test -Dtestcase=TestLockFactory -Dtestmethod=testNativeFSLockFactoryPrefix -Dtests.directory=FSDirectory the test fails. Since FSDirectory is an abstract class it can not be instantiated so our code falls back to FSDirector.open. yet we miss to set the given lockFactory though.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-561",
        "summary": "ParallelReader fails on deletes and on seeks of previously unused fields",
        "description": "In using ParallelReader I've hit two bugs:\n\n1.  ParallelReader.doDelete() and doUndeleteAll() call doDelete() and doUndeleteAll() on the subreaders, but these methods do not set hasChanges.  Thus the changes are lost when the readers are closed.  The fix is to call deleteDocument() and undeleteAll() on the subreaders instead.\n\n2.  ParallelReader discovers the fields in each subindex by using IndexReader.getFieldNames() which only finds fields that have occurred on at least one document.  In general a parallel index is designed with assignments of fields to sub-indexes and term seeks (including searches) may be done on any of those fields, even if no documents in a particular state of the index have yet had an assigned field.  Seeks/searches on fields that have not yet been indexed generated an NPE in ParallelReader's various inner class seek() and next() methods because fieldToReader.get() returns null on the unseen field.  The fix is to extend the add() methods to supply the correct list of fields for each subindex.\n\nPatch that corrects both of these issues attached.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3474",
        "summary": "pass liveDocs Bits down in scorercontext, instead of Weights pulling from the reader ",
        "description": "Spinoff from LUCENE-1536, this would allow filters to work in a more flexible way (besides just cleaning up)",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1433",
        "summary": "Changes.html generation improvements",
        "description": "Bug fixes for and improvements to changes2html.pl, which generates Changes.html from CHANGES.txt:\n\n# When the current location has a fragment identifier, expand parent sections, so that the linked-to section is visible.\n# Properly handle beginning-of-release comments that don't fall under a section heading (previously: some content in release \"1.9 final\" was invisible).\n# Auto-linkify SOLR-XXX and INFRA-XXX JIRA issues (previously: only LUCENE-XXX issues).\n# Auto-linkify Bugzilla bugs prefaced with \"Issue\" (previously: only \"Bug\" and \"Patch\").\n# Auto-linkify Bugzilla bugs in the form \"bugs XXXXX and YYYYY\".\n# Auto-linkify issues that follow attributions.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1510",
        "summary": "InstantiatedIndexReader throws NullPointerException in norms() when used with a MultiReader",
        "description": "\nWhen using InstantiatedIndexReader under a MultiReader where the other Reader contains documents, a NullPointerException is thrown here;\n\n public void norms(String field, byte[] bytes, int offset) throws IOException {\n    byte[] norms = getIndex().getNormsByFieldNameAndDocumentNumber().get(field);\n    System.arraycopy(norms, 0, bytes, offset, norms.length);\n  }\n\nthe 'norms' variable is null. Performing the copy only when norms is not null does work, though I'm sure it's not the right fix.\n\njava.lang.NullPointerException\n\tat org.apache.lucene.store.instantiated.InstantiatedIndexReader.norms(InstantiatedIndexReader.java:297)\n\tat org.apache.lucene.index.MultiReader.norms(MultiReader.java:273)\n\tat org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:70)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:131)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:136)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:146)\n\tat org.apache.lucene.store.instantiated.TestWithMultiReader.test(TestWithMultiReader.java:41)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:164)\n\tat junit.framework.TestCase.runBare(TestCase.java:130)\n\tat junit.framework.TestResult$1.protect(TestResult.java:106)\n\tat junit.framework.TestResult.runProtected(TestResult.java:124)\n\tat junit.framework.TestResult.run(TestResult.java:109)\n\tat junit.framework.TestCase.run(TestCase.java:120)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:230)\n\tat junit.framework.TestSuite.run(TestSuite.java:225)\n\tat org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)\n\tat org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1171",
        "summary": "Make DocumentsWriter more robust on hitting OOM",
        "description": "I've been stress testing DocumentsWriter by indexing wikipedia, but not\ngiving enough memory to the JVM, in varying heap sizes to tickle the\ndifferent interesting cases.  Sometimes DocumentsWriter can deadlock;\nother times it will hit a subsequent NPE or AIOOBE or assertion\nfailure.\n\nI've fixed all the cases I've found, and added some more asserts.  Now\nit just produces plain OOM exceptions.  All changes are contained to\nDocumentsWriter.java.\n\nAll tests pass.  I plan to commit in a day or two!",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-523",
        "summary": "FSDirectory.openFile(String) causes ClassCastException",
        "description": "When you call FSDirectory.openFile(String) you get a ClassCastException since FSIndexInput is not an org.apache.lucene.store.InputStream\n\nThe workaround is to reimplement using openInput(String). I personally don't need this to be fixed but wanted to document it here in case anyone else runs into this for any reason.\n\nThe reason I'm calling this is that I have a requirement on my project to create read only indexes and name the index segments consistently from one build to the next. So, after creating and optimizing the index, I rename the files and rewrite the segments file. It would be nice if I had an API that would allow me to say \"I only want one segment and I want its name to be 'foo'\". For instance IndexWriter.optimize(String segmentName)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3617",
        "summary": "Graduate appendingcodec from contrib/misc",
        "description": "* All tests pass with this codec (at least once, maybe we don't test that two-phase commit stuff very well!)\n* It doesn't require special client side configuration anymore to work (just set it on indexwriter and go)\n* it now works with the compound file format.\n\nI don't think it needs to live in contrib anymore.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TASK"
    },
    {
        "key": "LUCENE-490",
        "summary": "JavaCC 4.0 fails to generate QueryParser.java",
        "description": "When generating the Java source for QueryParser via the ant task 'javacc-QueryParser' against Subversion trunk (updated Jan. 25, 2006), JavaCC 4.0 gives the following error:\n\njavacc-QueryParser:\n   [javacc] Java Compiler Compiler Version 4.0 (Parser Generator)\n   [javacc] (type \"javacc\" with no arguments for help)\n   [javacc] Reading from file [...]/src/java/org/apache/lucene/queryParser/QueryParser.jj . . .\n   [javacc] org.javacc.parser.ParseException: Encountered \"<<\" at line 754, column 3.\n   [javacc] Was expecting one of:\n   [javacc]     <STRING_LITERAL> ...\n   [javacc]     \"<\" ...\n   [javacc]     \n   [javacc] Detected 1 errors and 0 warnings.\n\nBUILD FAILED\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1863",
        "summary": "SnowballAnalyzer has a link to net.sf (a package that is empty and needs to be removed).",
        "description": "need to remove net.sf and points to org.tartarus.snowball.ext. Doesn't work as a link though, so I'll also remove the @link to lose the javadoc error and broken link.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2181",
        "summary": "benchmark for collation",
        "description": "Steven Rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under LUCENE-2084, along with some instructions to run it... \n\nI think it would be a nice if we could turn this into a committable patch and add it to benchmark.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1324",
        "summary": "TokenFilter should implement reset()",
        "description": "TokenFilter maintains a private member of TokenStream.\nIt should implement reset() and call its member TokenStream's reset() method. Otherwise, that TokenStream never gets reset.\nPatch applied.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1341",
        "summary": "BoostingNearQuery class (prototype)",
        "description": "This patch implements term boosting for SpanNearQuery. Refer to: http://www.gossamer-threads.com/lists/lucene/java-user/62779\n\nThis patch works but probably needs more work. I don't like the use of 'instanceof', but I didn't want to touch Spans or TermSpans. Also, the payload code is mostly a copy of what's in BoostingTermQuery and could be common-sourced somewhere. Feel free to throw darts at it :)\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2772",
        "summary": "Make SlowMultiReaderWrapper wrap always so close() is safe",
        "description": "The overhead when wrapping an atomic reader using SlowMultiReaderWrapper is very low, the work done in the static wrap method is much higher (instantiate ArrayList, recusively went through all subreaders), just to check the number of readers than simply always wrapping.\n\nMultiFields already is optimized when called by one-segment or atomic readers, so there is no overhead at all. So this patch removes the static wrap method and you simply wrap like a TokenFilter with ctor: new SlowMultiReaderWrapper(reader)\n\nWhen this is done, there is also no risk to close a SegmentReader (which you should not do), when wrap() returns a single SegmentReader. This help in parent issue with cleaning up the case in close().\n\nThe patch a\u00f6lso removes the now useless mainReader/reader variables and simply closes the wrapper.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": ""
    },
    {
        "key": "LUCENE-799",
        "summary": "Garbage data when reading a compressed, text field, lazily",
        "description": "lazy compressed text fields is a case that was neglected during lazy field implementation.  TestCase and patch provided.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1924",
        "summary": "BalancedSegmentMergePolicy, contributed from the Zoie project for realtime indexing",
        "description": "Written by Yasuhiro Matsuda for Zoie realtime indexing system used to handle high update rates to avoid large segment merges.\nDetailed write-up is at:\n\nhttp://code.google.com/p/zoie/wiki/ZoieMergePolicy\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1067",
        "summary": "TestStressIndexing has intermittent failures",
        "description": "See http://www.gossamer-threads.com/lists/lucene/java-dev/55092 copied below:\n\n OK, I have seen this twice in the last two days:\nTestsuite: org.apache.lucene.index.TestStressIndexing\n[junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 18.58\nsec\n[junit]\n[junit] ------------- Standard Output ---------------\n[junit] java.lang.NullPointerException\n[junit] at\norg.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:67)\n[junit] at\norg.apache.lucene.store.IndexInput.readInt(IndexInput.java:66)\n[junit] at org.apache.lucene.index.SegmentInfos\n$FindSegmentsFile.run(SegmentInfos.java:544)\n[junit] at\norg\n.apache\n.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)\n[junit] at\norg.apache.lucene.index.IndexReader.open(IndexReader.java:209)\n[junit] at\norg.apache.lucene.index.IndexReader.open(IndexReader.java:192)\n[junit] at\norg.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:56)\n[junit] at org.apache.lucene.index.TestStressIndexing\n$SearcherThread.doWork(TestStressIndexing.java:111)\n[junit] at org.apache.lucene.index.TestStressIndexing\n$TimedThread.run(TestStressIndexing.java:55)\n[junit] ------------- ---------------- ---------------\n[junit] Testcase:\ntestStressIndexAndSearching\n(org.apache.lucene.index.TestStressIndexing): FAILED\n[junit] hit unexpected exception in search1\n[junit] junit.framework.AssertionFailedError: hit unexpected\nexception in search1\n[junit] at\norg\n.apache\n.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:\n159)\n[junit] at\norg\n.apache\n.lucene\n.index\n.TestStressIndexing\n.testStressIndexAndSearching(TestStressIndexing.java:187)\n[junit]\n[junit]\n[junit] Test org.apache.lucene.index.TestStressIndexing FAILED\n\nSubsequent runs have, however passed. Has anyone else hit this on\ntrunk?\n\nI am running using \"ant clean test\"\n\nI'm on a Mac Pro 4 core, 4GB machine, if that helps at all. Not sure\nhow to reproduce at this point, but strikes me as a threading issue.\nOh joy!\n\nI'll try to investigate more tomorrow to see if I can dream up a test\ncase.\n\n-Grant \n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2757",
        "summary": "Refactor RewriteMethods out of MultiTermQuery",
        "description": "Policeman work :-) - as usual",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3152",
        "summary": "MockDirectoryWrapper should wrap the lockfactory",
        "description": "After applying the patch from LUCENE-3147, I added a line to make the test fail if it cannot remove its temporary directory.\n\nI ran 'ant test' on linux 50 times, and it passed all 50 times.\nBut on windows, it failed often because of write.lock... this is because of unclosed writers in the test.\n\nMockDirectoryWrapper is currently unaware of this write.lock, I think it should wrap the lockfactory so that .close() will fail if there are any outstanding locks.\nThen hopefully these tests would fail on linux too.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1803",
        "summary": "Wrong javadoc on LowerCaseTokenizer.normalize",
        "description": "The javadoc on LowerCaseTokenizer.normalize seems to be copy/paste from LetterTokenizer.isTokenChar.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2226",
        "summary": "move contrib/snowball to contrib/analyzers",
        "description": "to fix bugs in some duplicate, handcoded impls of these stemmers (nl, fr, ru, etc) we should simply merge snowball and analyzers, and replace the buggy impls with the proper snowball stemfilters.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2733",
        "summary": "Add private ctors to static utility classes",
        "description": "During development in 3.x and trunk we added some new classes like IOUtils and CodecUtils that are only providing static methods, but have no ctor at all. This adds the default empty public ctor, which is wrong, the classes should never be instantiated.\n\nWe should add private dummy ctors to prevent creating instances.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2103",
        "summary": "NoLockFactory should have a private constructor",
        "description": "NoLockFactory documents in its javadocs that one should use the static getNoLockFactory() method. However the class does not declare a private empty constructor, which breaks its Singleton purpose. We cannot add the empty private constructor because that'd break break-compat (even though I think it's very low chance someone actually instantiates the class), therefore we'll add a @deprecated warning to the class about this, and add the method in 4.0. I personally prefer to add an empty constructor w/ the @deprecated method, but am fine either way.\n\nDon't know if a patch is needed, as this is a trivial change. ",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3108",
        "summary": "Land DocValues on trunk",
        "description": "Its time to move another feature from branch to trunk. I want to start this process now while still a couple of issues remain on the branch. Currently I am down to a single nocommit (javadocs on DocValues.java) and a couple of testing TODOs (explicit multithreaded tests and unoptimized with deletions) but I think those are not worth separate issues so we can resolve them as we go. \nThe already created issues (LUCENE-3075 and LUCENE-3074) should not block this process here IMO, we can fix them once we are on trunk. \n\nHere is a quick feature overview of what has been implemented:\n * DocValues implementations for Ints (based on PackedInts), Float 32 / 64, Bytes (fixed / variable size each in sorted, straight and deref variations)\n * Integration into Flex-API, Codec provides a PerDocConsumer->DocValuesConsumer (write) / PerDocValues->DocValues (read) \n * By-Default enabled in all codecs except of PreFlex\n * Follows other flex-API patterns like non-segment reader throw UOE forcing MultiPerDocValues if on DirReader etc.\n * Integration into IndexWriter, FieldInfos etc.\n * Random-testing enabled via RandomIW - injecting random DocValues into documents\n * Basic checks in CheckIndex (which runs after each test)\n * FieldComparator for int and float variants (Sorting, currently directly integrated into SortField, this might go into a separate DocValuesSortField eventually)\n * Extended TestSort for DocValues\n * RAM-Resident random access API plus on-disk DocValuesEnum (currently only sequential access) -> Source.java / DocValuesEnum.java\n * Extensible Cache implementation for RAM-Resident DocValues (by-default loaded into RAM only once and freed once IR is closed) -> SourceCache.java\n \nPS: Currently the RAM resident API is named Source (Source.java) which seems too generic. I think we should rename it into RamDocValues or something like that, suggestion welcome!   \n\n\nAny comments, questions (rants :)) are very much appreciated.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3028",
        "summary": "IW.getReader() returns inconsistent reader on RT Branch",
        "description": "I extended the testcase TestRollingUpdates#testUpdateSameDoc to pull a NRT reader after each update and asserted that is always sees only one document. Yet, this fails with current branch since there is a problem in how we flush in the getReader() case. What happens here is that we flush all threads and then release the lock (letting other flushes which came in after we entered the flushAllThread context, continue) so that we could concurrently get a new segment that transports global deletes without the corresponding add. They sneak in while we continue to open the NRT reader which in turn sees inconsistent results.\n\nI will upload a patch soon",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2001",
        "summary": "wordnet parsing bug",
        "description": "A user reported that wordnet parses the prolog file incorrectly.\n\nAlso need to check the wordnet parser in the memory contrib for this problem.\n\nIf this is a false alarm, i'm not worried, because the test will be the first unit test wordnet package ever had.\n\n{noformat}\nFor example, looking up the synsets for the\nword \"king\", we get:\n\njava SynLookup wnindex king\nbaron\nmagnate\nmogul\npower\nqueen\nrex\nscrofula\nstruma\ntycoon\n\nHere, \"scrofula\" and \"struma\" are extraneous. This happens because, the line\nparser code in Syns2Index.java interpretes the two consecutive single quotes\nin entry s(114144247,3,'king''s evil',n,1,1) in  wn_s.pl file, as\ntermination\nof the string and separates into \"king\". This entry concerns\nsynset of words \"scrofula\" and \"struma\", and thus they get inserted in the\nsynset of \"king\". *There 1382 such entries, in wn_s.pl* and more in other\nWordNet\nProlog data-base files, where such use of two consecutive single quotes\nappears.\n\nWe have resolved this by adding a statement in the line parsing portion of\nSyns2Index.java, as follows:\n\n           // parse line\n           line = line.substring(2);\n          * line = line.replaceAll(\"\\'\\'\", \"`\"); // added statement*\n           int comma = line.indexOf(',');\n           String num = line.substring(0, comma);  ... ... etc.\nIn short we replace \"''\" by \"`\" (a back-quote). Then on recreating the\nindex, we get:\n\njava SynLookup zwnindex king\nbaron\nmagnate\nmogul\npower\nqueen\nrex\ntycoon\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-981",
        "summary": "NewAnalyzerTask",
        "description": "NewAnalyzerTask (patch to follow) allows a contrib/benchmark algorithm to change Analyzers during a run.  This is useful when comparing Analyzers\n\n{\"NewAnalyzer\" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) >\n\nis a sample declaration in an algorithm file.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2478",
        "summary": "CachingWrapperFilter throws NPE when Filter.getDocIdSet() returns null",
        "description": "Followup for [http://www.lucidimagination.com/search/document/1014ea92f15677bd/filter_getdocidset_returning_null_and_what_this_means_for_cachingwrapperfilter]:\n\nDaniel Noll is seeing an exception like this:\n\n{noformat}\njava.lang.NullPointerException\n    at org.apache.lucene.search.CachingWrapperFilter.docIdSetToCache(CachingWrapperFilter.java:84)\n    at org.apache.lucene.search.CachingWrapperFilter.getDocIdSet(CachingWrapperFilter.java:112)\n    at com.nuix.storage.search.LazyConstantScoreQuery$LazyFilterWrapper.getDocIdSet(SourceFile:91)\n    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)\n    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)\n    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)\n    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)\n    at org.apache.lucene.search.QueryWrapperFilter$2.iterator(QueryWrapperFilter.java:75)\n{noformat}\n\nThe class of our own is just an intermediary which delays creating the Filter object...\n\n{code}\n@Override\npublic DocIdSet getDocIdSet(IndexReader reader) throws IOException {\n            if (delegate == null) {\n                delegate = factory.createFilter();\n            }\n            return delegate.getDocIdSet(reader);\n}\n{code}\n\nTracing through the code in CachingWrapperFilter, I can see that this NPE would occur if getDocIdSet() were to return null.\n\nThe Javadoc on Filter says that null will be returned if no documents will be accepted by the filter, but it doesn't seem that Lucene itself is handling null return values correctly, so which is correct?  The code or the Javadoc?  Supposing that null really is OK, does this cause any problems with how CachingWrapperFilter is implementing the caching?  I notice it's calling get() and then comparing against null so it wouldn't appear that it can distinguish \"the entry isn't in the cache\" from \"the entry is in the cache but it's null\".",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3527",
        "summary": "Implement getDistance() on DirectSpellChecker.INTERNAL_LEVENSHTEIN",
        "description": "DirectSpellChecker.INTERNAL_LEVENSHTEIN is currently not a full-fledged implementation of StringDistance.  But an full implementation is needed for Solr's SpellCheckComponent.finishStage(), and also would be helpful for those trying to take the advice given in LIA 2nd ed section sect8.5.3.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1114",
        "summary": "contrib/Highlighter javadoc example needs to be updated",
        "description": "The Javadoc package.html example code is outdated, as it still uses QueryParser.parse.  \n\nhttp://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/contrib-highlighter/index.html",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2392",
        "summary": "Enable flexible scoring",
        "description": "This is a first step (nowhere near committable!), implementing the\ndesign iterated to in the recent \"Baby steps towards making Lucene's\nscoring more flexible\" java-dev thread.\n\nThe idea is (if you turn it on for your Field; it's off by default) to\nstore full stats in the index, into a new _X.sts file, per doc (X\nfield) in the index.\n\nAnd then have FieldSimilarityProvider impls that compute doc's boost\nbytes (norms) from these stats.\n\nThe patch is able to index the stats, merge them when segments are\nmerged, and provides an iterator-only API.  It also has starting point\nfor per-field Sims that use the stats iterator API to compute boost\nbytes.  But it's not at all tied into actual searching!  There's still\ntons left to do, eg, how does one configure via Field/FieldType which\nstats one wants indexed.\n\nAll tests pass, and I added one new TestStats unit test.\n\nThe stats I record now are:\n\n  - field's boost\n\n  - field's unique term count (a b c a a b --> 3)\n\n  - field's total term count (a b c a a b --> 6)\n\n  - total term count per-term (sum of total term count for all docs\n    that have this term)\n\nStill need at least the total term count for each field.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2182",
        "summary": "DEFAULT_ATTRIBUTE_FACTORY faills to load implementation class when iterface comes from different classloader",
        "description": "This is a followup for [http://www.lucidimagination.com/search/document/1724fcb3712bafba/using_the_new_tokenizer_api_from_a_jar_file]:\n\nThe DEFAULT_ATTRIBUTE_FACTORY should load the implementation class for a given attribute interface from the same classloader like the attribute interface. The current code loads it from the classloader of the lucene-core.jar file. In solr this fails when the interface is in a JAR file coming from the plugins folder. \n\nThe interface is loaded correctly, because the addAttribute(FooAttribute.class) loads the FooAttribute.class from the plugin code and this with success. But as addAttribute tries to load the class from its local lucene-core.jar classloader it will not find the attribute.\n\nThe fix is to tell Class.forName to use the classloader of the corresponding interface, which is the correct way to handle it, as the impl and the attribute should always be in the same classloader and file.\n\nI hope I can somehow add a test for that.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1059",
        "summary": "bad java practices which affect performance (result of code inspection)",
        "description": "IntelliJ IDEA found the following issues in the Lucense source code and tests:\n\n1) explicit for loops where calls to System.arraycopy() should have been\n2) calls to Boolean constructor (in stead of the appropriate static method/field)\n3) instantiation of unnecessary Integer instances for toString, instead of calling the static one\n4) String concatenation using + inside a call to StringBuffer.append(), in stead of chaining the append calls\n\nall minor issues. patch is forthcoming.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3078",
        "summary": "Add generics to DocumentsWriterDeleteQueue.Node",
        "description": "DocumentsWriterDeleteQueue.Note should be generic as the subclasses hold different types of items. This generification is a little bit tricks, but the generics policeman can't wait to fix this *g*.",
        "label": "NUG",
        "classified": "RFE",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1814",
        "summary": "Some Lucene tests try and use a Junit Assert in new threads",
        "description": "There are a few cases in Lucene tests where JUnit Asserts are used inside a new threads run method - this won't work because Junit throws an exception when a call to Assert fails - that will kill the thread, but the exception will not propagate to JUnit - so unless a failure is caused later from the thread termination, the Asserts are invalid.\n\nTestThreadSafe\nTestStressIndexing2\nTestStringIntern",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-582",
        "summary": "Don't throw TooManyClauses exception",
        "description": "I wonder if it would make sense to fall back to a ConstantScoreQuery instead of throwing a TooManyClauses exception?",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-986",
        "summary": "Refactor segmentInfos from IndexReader into its subclasses",
        "description": "References to segmentInfos in IndexReader cause different kinds of problems\nfor subclasses of IndexReader, like e. g. MultiReader.\n\nOnly subclasses of IndexReader that own the index directory, namely \nSegmentReader and MultiSegmentReader, should have a SegmentInfos object\nand be able to access it.\n\nFurther information:\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/51808\nhttp://www.gossamer-threads.com/lists/lucene/java-user/52460\n\nA part of the refactoring work was already done in LUCENE-781",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1254",
        "summary": "addIndexesNoOptimize should not enforce maxMergeDocs/maxMergeSize limit",
        "description": "If you pass an index that has a segment > maxMergeDocs or maxMergeSize\nto addIndexesNoOptimize, it throws an IllegalArgumentException.\n\nBut this check isn't reasonable because segment merging can easily\nproduce segments over these sizes since those limits apply to each\nsegment being merged, not to the final size of the segment produced.\n\nSo if you set maxMergeDocs to X, build up and index, then try to add\nthat index to another index that also has maxMergeDocs X, you can\neasily hit the exception.\n\nI think it's being too pedantic; I plan to just remove the checks for\nsizes.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2984",
        "summary": "Move hasVectors() & hasProx() responsibility out of SegmentInfo to FieldInfos ",
        "description": "Spin-off from LUCENE-2881 which had this change already but due to some random failures related to this change I remove this part of the patch to make it more isolated and easier to test. ",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2934",
        "summary": "Alternative depth-based DOT layout ordering in FST's Utils",
        "description": "Utils.toDot() dumps GraphViz's DOT file, but it can be quite difficult to read. This patch provides an alternative layout that is probably a little bit easier on the eyes (well, as far as larger FSTs can be ;)",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-277",
        "summary": "Sorting produces duplicates",
        "description": "If you run the code below the exception will be thrown. I believe that it isn't \ncorrect behaviour (the duplicities, of course), index id of hits should be \nunique as it is without sort.\n\nLucene versions:\n1.4-final\n1.4.1\nCVS 1.5-rc1-dev\n\n\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.queryParser.ParseException;\nimport org.apache.lucene.queryParser.QueryParser;\nimport org.apache.lucene.search.Hits;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.Searcher;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.RAMDirectory;\n\nimport java.io.IOException;\nimport java.util.HashSet;\nimport java.util.LinkedList;\nimport java.util.ListIterator;\nimport java.util.Set;\n\n/**\n * Run this test with Lucene 1.4 final or 1.4.1\n */\npublic class DuplicityTest\n{\n    public static void main(String[] args) throws IOException, ParseException\n    {\n        Directory directory = create_index();\n\n        search_index(directory);\n    }\n\n    private static void search_index(Directory directory) throws IOException, \nParseException\n    {\n        IndexReader reader = IndexReader.open(directory);\n        Searcher searcher = new IndexSearcher(reader);\n\n        Sort sort = new Sort(new SortField(\"co\", SortField.INT, false));\n\n        Query q = QueryParser.parse(\"sword\", \"text\", new StandardAnalyzer());\n\n        find_duplicity(searcher.search(q), \"no sort\");\n\n        find_duplicity(searcher.search(q, sort), \"using sort\");\n\n        searcher.close();\n        reader.close();\n    }\n\n    private static void find_duplicity(Hits hits, String message) throws \nIOException\n    {\n        System.out.println(message + \" hits size: \" + hits.length());\n\n        Set set = new HashSet();\n        for (int i = 0; i < hits.length(); i++) {\n//            System.out.println(hits.id(i) + \": \" + hits.doc(i).toString());\n            Integer id = new Integer(hits.id(i));\n            if (!set.contains(id))\n                set.add(id);\n            else\n                throw new RuntimeException(\"duplicity found, index id: \" + id);\n        }\n        System.out.println(\"no duplicity found\");\n    }\n\n    private static LinkedList words;\n\n    static {\n        words = new LinkedList();\n\n        words.add(\"word\");\n        words.add(\"sword\");\n        words.add(\"dwarf\");\n        words.add(\"whale\");\n        words.add(\"male\");\n    }\n\n    private static Directory create_index() throws IOException\n    {\n        Directory directory = new RAMDirectory();\n\n        ListIterator e_words1 = words.listIterator();\n        ListIterator e_words2 = words.listIterator(words.size());\n\n        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), \ntrue);\n\n        int co = 1;\n\n        for (int i = 0; i < 300; i++) {\n\n            if (!e_words1.hasNext()) {\n                e_words1 = words.listIterator();\n                e_words1.hasNext();\n            }\n            String word1 = (String)e_words1.next();\n            if (!e_words2.hasPrevious()) {\n                e_words2 = words.listIterator(words.size());\n                e_words2.hasPrevious();\n            }\n            String word2 = (String)e_words2.previous();\n\n            Document doc = new Document();\n\n            doc.add(Field.Keyword(\"co\", String.valueOf(co)));\n            doc.add(Field.Text(\"text\", word1 + \" \" + word2));\n            writer.addDocument(doc);\n\n            if (i % 20 == 0)\n                co++;\n        }\n        writer.optimize();\n        System.err.println(\"index size: \" + writer.docCount());\n        writer.close();\n\n        return directory;\n    }\n}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3501",
        "summary": "random sampler is not random (and so facet SamplingWrapperTest occasionally fails)",
        "description": "RandomSample is not random at all:\nIt does not even import java.util.Random, and its behavior is deterministic.\n\nin addition, the test testCountUsingSamping() never retries as it was supposed to (for taking care of the hoped-for randomness).",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-243",
        "summary": "[PATCH] setIndexInterval() in IndexWriter",
        "description": "Following a discussion with Doug (see\nhttp://article.gmane.org/gmane.comp.jakarta.lucene.devel/5804) here is a patch\nthat add a setIndexInterval() in IndexWriter.\n\nThis patch adds also a getDirectory method to IndexWriter and modifies \nSegmentMerger, IndexWriter and TermInfosWriter.\n\nThis patch passes all tests.\n\nAny comments/criticisms welcome.\n\nJulien",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2657",
        "summary": "Replace Maven POM templates with full POMs, and change documentation accordingly",
        "description": "The current Maven POM templates only contain dependency information, the bare bones necessary for uploading artifacts to the Maven repository.\n\nThe full Maven POMs in the attached patch include the information necessary to run a multi-module Maven build, in addition to serving the same purpose as the current POM templates.\n\nSeveral dependencies are not available through public maven repositories.  A profile in the top-level POM can be activated to install these dependencies from the various {{lib/}} directories into your local repository.  From the top-level directory:\n\n{code}\nmvn -N -Pbootstrap install\n{code}\n\nOnce these non-Maven dependencies have been installed, to run all Lucene/Solr tests via Maven's surefire plugin, and populate your local repository with all artifacts, from the top level directory, run:\n\n{code}\nmvn install\n{code}\n\nWhen one Lucene/Solr module depends on another, the dependency is declared on the *artifact(s)* produced by the other module and deposited in your local repository, rather than on the other module's un-jarred compiler output in the {{build/}} directory, so you must run {{mvn install}} on the other module before its changes are visible to the module that depends on it.\n\nTo create all the artifacts without running tests:\n\n{code}\nmvn -DskipTests install\n{code}\n\nI almost always include the {{clean}} phase when I do a build, e.g.:\n\n{code}\nmvn -DskipTests clean install\n{code}\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2699",
        "summary": "Update StandardTokenizer and UAX29Tokenizer to Unicode 6.0.0",
        "description": "Newly released Unicode 6.0.0 contains some character property changes from the previous release (5.2.0) that affect word segmentation (UAX#29), and JFlex 1.5.0-SNAPSHOT now supports Unicode 6.0.0, so Lucene's UAX#29-based tokenizers should be updated accordingly.\n\nNote that the UAX#29 word break rules themselves did not change between Unicode versions 5.2.0 and 6.0.0.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1568",
        "summary": "Fix for NPE's in Spatial Lucene for searching bounding box only",
        "description": "NPE occurs when using DistanceQueryBuilder for minimal bounding box search without the distance filter.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2333",
        "summary": "Failures during contrib builds, when classes in core were changed without ant clean",
        "description": "From java-dev by Shai Erera:\n\n{quote}\nI've noticed that sometimes, after I run test-core and test-contrib, and then change core code, test-contrib fail on NoSuchMethodError and stuff like that. I've noticed that core.jar exists under build, and I assumed it's used by test-contrib, and probably is not recreated after core code has changed.\n\nI verified it when looking in contrib-build.xml, which defines a property lucene.jar.present which is set to true if the jar is ... well, present. Which I believe is the reason for these failures. I've been thinking how to resolve that, and I can think of two ways:\n\n(1) have test-core always delete that file, but that has two issues:\n(1.1) It's redundant if the code hasn't changed.\n(1.2) It forces you to either jar-core or test-core before you test-contrib, if you want to make sure you run w/ the latest jar.\n\nor\n\n(2) have test-contrib always call jar-core, which will first delete the file and then re-create it by compiling first. Compiling should not do anything if the code hasn't changed. So the only waste would be to create the .jar, but I think that's quite fast?\n\nDoes anyone, with more Ant skills than me, know of a better way to detect from test-contrib that core code has changed and only then rebuild the jar?\n{quote}",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-610",
        "summary": "BooleanScorer2 does not compile with ecj",
        "description": "BooleanScorer2, derived from scorer, has two inner classes both derived, ultimately, from Scorer.\nAs such they all define doc() or inherit it.\necj produces an error when doc() is called from score in the inner classes in the methods\n        countingDisjunctionSumScorer\n    and\n        countingConjunctionSumScorer\n\nThe error message is:\n    The method doc is defined in an inherited type and in an enclosing scope.\n\nThe affected lines are: 160, 161, 178, and 179\n\n\nI have run the junit test TestBoolean2 (as well as all the others) with\n        doc()\n    changed to\n        BooleanScorer2.this.doc()\n    and also to:\n        this.doc();\nThe result was that the tests passed for both.\n\nI added debug statements to all the doc methods and the score methods in the affected classes, but I could not determine what it should be.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3623",
        "summary": "SegmentReader.getFieldNames ignores FieldOption.DOC_VALUES",
        "description": "we use this getFieldNames api in segmentmerger if we merge something that isn't a SegmentReader (e.g. FilterIndexReader)\n\nit looks to me that if you use a FilterIndexReader, call addIndexes(Reader...) the docvalues will be simply dropped.\n\nI dont think its enough to just note that the field has docvalues either right? We need to also set the type \ncorrectly in the merged field infos? This would imply that instead of FieldOption.DOCVALUES, we need to have a \nFieldOption for each ValueType so that we correctly update the type.\n\nBut looking at FI.update/setDocValues, it doesn't look like we 'type-promote' here anyway?\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1575",
        "summary": "Refactoring Lucene collectors (HitCollector and extensions)",
        "description": "This issue is a result of a recent discussion we've had on the mailing list. You can read the thread [here|http://www.nabble.com/Is-TopDocCollector%27s-collect()-implementation-correct--td22557419.html].\n\nWe have agreed to do the following refactoring:\n* Rename MultiReaderHitCollector to Collector, with the purpose that it will be the base class for all Collector implementations.\n* Deprecate HitCollector in favor of the new Collector.\n* Introduce new methods in IndexSearcher that accept Collector, and deprecate those that accept HitCollector.\n** Create a final class HitCollectorWrapper, and use it in the deprecated methods in IndexSearcher, wrapping the given HitCollector.\n** HitCollectorWrapper will be marked deprecated, so we can remove it in 3.0, when we remove HitCollector.\n** It will remove any instanceof checks that currently exist in IndexSearcher code.\n* Create a new (abstract) TopDocsCollector, which will:\n** Leave collect and setNextReader unimplemented.\n** Introduce protected members PriorityQueue and totalHits.\n** Introduce a single protected constructor which accepts a PriorityQueue.\n** Implement topDocs() and getTotalHits() using the PQ and totalHits members. These can be used as-are by extending classes, as well as be overridden.\n** Introduce a new topDocs(start, howMany) method which will be used a convenience method when implementing a search application which allows paging through search results. It will also attempt to improve the memory allocation, by allocating a ScoreDoc[] of the requested size only.\n* Change TopScoreDocCollector to extend TopDocsCollector, use the topDocs() and getTotalHits() implementations as they are from TopDocsCollector. The class will also be made final.\n* Change TopFieldCollector to extend TopDocsCollector, and make the class final. Also implement topDocs(start, howMany).\n* Change TopFieldDocCollector (deprecated) to extend TopDocsCollector, instead of TopScoreDocCollector. Implement topDocs(start, howMany)\n* Review other places where HitCollector is used, such as in Scorer, deprecate those places and use Collector instead.\n\nAdditionally, the following proposal was made w.r.t. decoupling score from collect():\n* Change collect to accecpt only a doc Id (unbased).\n* Introduce a setScorer(Scorer) method.\n* If during collect the implementation needs the score, it can call scorer.score().\nIf we do this, then we need to review all places in the code where collect(doc, score) is called, and assert whether Scorer can be passed. Also this raises few questions:\n* What if during collect() Scorer is null? (i.e., not set) - is it even possible?\n* I noticed that many (if not all) of the collect() implementations discard the document if its score is not greater than 0. Doesn't it mean that score is needed in collect() always?\n\nOpen issues:\n* The name for Collector\n* TopDocsCollector was mentioned on the thread as TopResultsCollector, but that was when we thought to call Colletor ResultsColletor. Since we decided (so far) on Collector, I think TopDocsCollector makes sense, because of its TopDocs output.\n* Decoupling score from collect().\n\nI will post a patch a bit later, as this is expected to be a very large patch. I will split it into 2: (1) code patch (2) test cases (moving to use Collector instead of HitCollector, as well as testing the new topDocs(start, howMany) method.\nThere might be even a 3rd patch which handles the setScorer thing in Collector (maybe even a different issue?)",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3880",
        "summary": "UAX29URLEmailTokenizer fails to recognize emails as such when the mailto: scheme is prepended",
        "description": "As [reported by Kai G\u00fclzau on solr-user|http://markmail.org/message/n32kji3okqm2c5qn]:\n\nUAX29URLEmailTokenizer seems to split at the wrong place:\n\n{noformat}mailto:test@example.org{noformat} ->\n{noformat}mailto:test{noformat}\n{noformat}example.org{noformat}\n\nAs a workaround I use\n\n{code:xml}\n<charFilter class=\"solr.PatternReplaceCharFilterFactory\" pattern=\"mailto:\" replacement=\"mailto: \"/>\n{code}",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2177",
        "summary": "The Field ctors that take byte[] shouldn't take Store, since it must be YES",
        "description": "API silliness.  Makes you think you can set Store.NO for binary fields.  This used to be meaningful when we also accepted COMPRESS, but now it's an orphan.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-854",
        "summary": "Create merge policy that doesn't periodically inadvertently optimize",
        "description": "The current merge policy, at every maxBufferedDocs *\npower-of-mergeFactor docs added, will do a fully cascaded merge, which\nis the same as an optimize.\n\nI think this is not good because at that \"optimization poin\", the\nparticular addDocument call is [surprisingly] very expensive.  While,\namortized over all addDocument calls, the cost is low, the cost is\npaid \"up front\" and in a very \"bunched up\" manner.\n\nI think of this as \"pay it forward\": you are paying the full cost of\nan optimize right now on the expectation / hope that you will be\nadding a great many more docs.  But, if you don't add that many more\ndocs, then, the amortized cost for your index is in fact far higher\nthan it should have been.  Better to \"pay as you go\" instead.\n\nSo we could make a small change to the policy by only merging the\nfirst mergeFactor segments once we hit 2X the merge factor.  With\nmergeFactor=10, when we have created the 20th level 0 (just flushed)\nsegment, we merge the first 10 into a level 1 segment.  Then on\ncreating another 10 level 0 segments, we merge the second set of 10\nlevel 0 segments into a level 1 segment, etc.\n\nWith this new merge policy, an index that's a bit bigger than a\ncurrent \"optimization point\" would then have a lower amortized cost\nper document.  Plus the merge cost is less \"bunched up\" and less \"pay\nit forward\": instead you pay for what you are actually using.\n\nWe can start by creating this merge policy (probably, combined with\nwith the \"by size not by doc count\" segment level computation from\nLUCENE-845) and then later decide whether we should make it the\ndefault merge policy.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1805",
        "summary": "CloseableThreadLocal should allow null Objects",
        "description": "CloseableThreadLocal does not allow null Objects in its get() method, but does nothing to prevent them in set(Object). The comment in get() before assert v != null is irrelevant - the application might have passed null.\n\nNull is an important value for Analyzers. Since tokenStreams (a ThreadLocal private member in Analyzer) is not accessible by extending classes, the only way for an Analyzer to reset the tokenStreams is by calling setPreviousTokenStream(null).\n\nI will post a patch w/ a test",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1929",
        "summary": "Highlighter doesn't support NumericRangeQuery or deprecated RangeQuery",
        "description": "Sucks. Will throw a NullPointer exception. \n\nOnly NumericRangeQuery will throw the exception.\nRangeQuery just won't highlight.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3860",
        "summary": "3.x indexes have the wrong normType set in fieldinfos",
        "description": "3.x codec claims the single byte norms are BYTES_VAR_STRAIGHT in FieldInfos,\nbut the norms implementation itself then has the type as FIXED_INTS_8.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1262",
        "summary": "IndexOutOfBoundsException from FieldsReader after problem reading the index",
        "description": "There is a situation where there is an IOException reading from Hits, and then the next time you get a NullPointerException instead of an IOException.\n\nExample stack traces:\n\njava.io.IOException: The specified network name is no longer available\n\tat java.io.RandomAccessFile.readBytes(Native Method)\n\tat java.io.RandomAccessFile.read(RandomAccessFile.java:322)\n\tat org.apache.lucene.store.FSIndexInput.readInternal(FSDirectory.java:536)\n\tat org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:74)\n\tat org.apache.lucene.index.CompoundFileReader$CSIndexInput.readInternal(CompoundFileReader.java:220)\n\tat org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:93)\n\tat org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:34)\n\tat org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:57)\n\tat org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:88)\n\tat org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)\n\tat org.apache.lucene.index.IndexReader.document(IndexReader.java:368)\n\tat org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)\n\tat org.apache.lucene.search.Hits.doc(Hits.java:104)\n\nThat error is fine.  The problem is the next call to doc generates:\n\njava.lang.NullPointerException\n\tat org.apache.lucene.index.FieldsReader.getIndexType(FieldsReader.java:280)\n\tat org.apache.lucene.index.FieldsReader.addField(FieldsReader.java:216)\n\tat org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:101)\n\tat org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)\n\tat org.apache.lucene.index.IndexReader.document(IndexReader.java:368)\n\tat org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)\n\tat org.apache.lucene.search.Hits.doc(Hits.java:104)\n\nPresumably FieldsReader is caching partially-initialised data somewhere.  I would normally expect the exact same IOException to be thrown for subsequent calls to the method.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1436",
        "summary": "Make ReqExclScorer package private, and use DocIdSetIterator for excluded part.",
        "description": "",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2087",
        "summary": "Remove recursion in NumericRangeTermEnum",
        "description": "The current FilteredTermEnum in NRQ uses setEnum() which itsself calls next(). This may lead to a recursion that can overflow stack, if the index is empty and a large range with low precStep is used. With 64 bit numbers and precStep == 1 there may be 127 recursions, as each sub-range would hit no term on empty index and the setEnum call would then call next() which itsself calls setEnum again. This leads to recursion depth of 256.\n\nAttached is a patch that converts to iterative approach. setEnum is now unused and throws UOE (like endEnum()).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3720",
        "summary": "OOM in TestBeiderMorseFilter.testRandom",
        "description": "This has been OOM'ing a lot... we should see why, its likely a real bug.\n\nant test -Dtestcase=TestBeiderMorseFilter -Dtestmethod=testRandom -Dtests.seed=2e18f456e714be89:310bba5e8404100d:-3bd11277c22f4591 -Dtests.multiplier=3 -Dargs=\"-Dfile.encoding=ISO8859-1\"",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3296",
        "summary": "Enable passing a config into PKIndexSplitter",
        "description": "I need to be able to pass the IndexWriterConfig into the IW used by PKIndexSplitter.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1276",
        "summary": "Build file for Highlighter contrib works when run in isolation, but not when core dist is run",
        "description": "Build.xml for Highlighter does not work when compilation is triggered by clean core dist call.\n\nPatch has changes to fix this by updating build.xml to follow xml-query-parser build.xml",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1006",
        "summary": "QueryParser doesn't accept empty string",
        "description": "foo:\"\" currently throws a parse exception\nfoo: bar is also parsed as foo:bar (not serious since it's arguably illegal syntax)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2466",
        "summary": "fix some more locale problems in lucene/solr",
        "description": "set ANT_ARGS=\"-Dargs=-Duser.language=tr -Duser.country=TR\"\nant clean test\n\nWe should make sure this works across all of lucene/solr",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2391",
        "summary": "Spellchecker uses default IW mergefactor/ramMB settings of 300/10",
        "description": "These settings seem odd - I'd like to investigate what makes most sense here.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1202",
        "summary": "Clover setup currently has some problems",
        "description": "(tracking as a bug before it get lost in email...\n  http://www.nabble.com/Clover-reports-missing-from-hudson--to15510616.html#a15510616\n)\n\nThe clover setup for Lucene currently has some problems, 3 i think...\n\n1) instrumentation fails on contrib/db/ because it contains java packages the ASF Clover lscence doesn't allow instrumentation of.  i have a patch for this.\n\n2) running instrumented contrib tests for other contribs produce strange errors...\n\n{{monospaced}}\n    [junit] Testsuite: org.apache.lucene.analysis.el.GreekAnalyzerTest\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.126 sec\n    [junit]\n    [junit] ------------- Standard Error -----------------\n    [junit] [CLOVER] FATAL ERROR: Clover could not be initialised. Are you sure you have Clover\nin the runtime classpath? (class\njava.lang.NoClassDefFoundError:com_cenqua_clover/CloverVersionInfo)\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testAnalyzer(org.apache.lucene.analysis.el.GreekAnalyzerTest):    Caused\nan ERROR\n    [junit] com_cenqua_clover/g\n    [junit] java.lang.NoClassDefFoundError: com_cenqua_clover/g\n    [junit]     at org.apache.lucene.analysis.el.GreekAnalyzer.<init>(GreekAnalyzer.java:157)\n    [junit]     at\norg.apache.lucene.analysis.el.GreekAnalyzerTest.testAnalyzer(GreekAnalyzerTest.java:60)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.analysis.el.GreekAnalyzerTest FAILED\n{{monospaced}}\n\n...i'm not sure what's going on here.  the error seems to happen both when\ntrying to run clover on just a single contrib, or when doing the full\nbuild ... i suspect there is an issue with the way the batchtests fork\noff, but I can't see why it would only happen to contribs (the regular\ntests fork as well)\n\n3) according to Grant...\n\n{{quote}}\n...There is also a bit of a change on Hudson during the migration to the new servers that needs to be ironed  out. \n{{quote}}\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-367",
        "summary": "Can not subscribe",
        "description": "Hello, \nI have sent email to lucene-dev-subscribe@jakarta.apache.org and it always \nreturns failed: \n \n<lucene-dev-subscribe@jakarta.apache.org>: \nSorry, no mailbox here by that name. (#5.1.1) \n \nPlease help me subscribe.",
        "label": "NUG",
        "classified": "OTHER",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1889",
        "summary": "FastVectorHighlighter: support for additional queries",
        "description": "I am using fastvectorhighlighter for some strange languages and it is working well! \n\nOne thing i noticed immediately is that many query types are not highlighted (multitermquery, multiphrasequery, etc)\nHere is one thing Michael M posted in the original ticket:\n\n{quote}\nI think a nice [eventual] model would be if we could simply re-run the\nscorer on the single document (using InstantiatedIndex maybe, or\nsimply some sort of wrapper on the term vectors which are already a\nmini-inverted-index for a single doc), but extend the scorer API to\ntell us the exact term occurrences that participated in a match (which\nI don't think is exposed today).\n{quote}\n\nDue to strange requirements I am using something similar to this (but specialized to our case).\nI am doing strange things like forcing multitermqueries to rewrite into boolean queries so they will be highlighted,\nand flattening multiphrasequeries into boolean or'ed phrasequeries.\nI do not think these things would be 'fast', but i had a few ideas that might help:\n\n* looking at contrib/highlighter, you can support FilteredQuery in flatten() by calling getQuery() right?\n* maybe as a last resort, try Query.extractTerms() ?\n",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-1593",
        "summary": "Optimizations to TopScoreDocCollector and TopFieldCollector",
        "description": "This is a spin-off of LUCENE-1575 and proposes to optimize TSDC and TFC code to remove unnecessary checks. The plan is:\n# Ensure that IndexSearcher returns segements in increasing doc Id order, instead of numDocs().\n# Change TSDC and TFC's code to not use the doc id as a tie breaker. New docs will always have larger ids and therefore cannot compete.\n# Pre-populate HitQueue with sentinel values in TSDC (score = Float.NEG_INF) and remove the check if reusableSD == null.\n# Also move to use \"changing top\" and then call adjustTop(), in case we update the queue.\n# some methods in Sort explicitly add SortField.FIELD_DOC as a \"tie breaker\" for the last SortField. But, doing so should not be necessary (since we already break ties by docID), and is in fact less efficient (once the above optimization is in).\n# Investigate PQ - can we deprecate insert() and have only insertWithOverflow()? Add a addDummyObjects method which will populate the queue without \"arranging\" it, just store the objects in the array (this can be used to pre-populate sentinel values)?\n\nI will post a patch as well as some perf measurements as soon as I have them.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-683",
        "summary": "Lazy Field Loading has edge case bug causing read past EOF",
        "description": "While trying to run some benchmarking of Lazy filed loading, i discovered there seems to be an edge case when accessing the last field of the last doc of an index.\n\nthe problem seems to only happen when the doc has been accessed after at least one other doc.\n\ni have not tried to dig into the code to find the root cause, testcase to follow...",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3233",
        "summary": "HuperDuperSynonymsFilter\u2122",
        "description": "The current synonymsfilter uses a lot of ram and cpu, especially at build time.\n\nI think yesterday I heard about \"huge synonyms files\" three times.\n\nSo, I think we should use an FST-based structure, sharing the inputs and outputs.\nAnd we should be more efficient with the tokenStream api, e.g. using save/restoreState instead of cloneAttributes()\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2477",
        "summary": "remove MoreLikeThis's default analyzer",
        "description": "MoreLikeThis has the following:\n\n{code}\npublic static final Analyzer DEFAULT_ANALYZER = new StandardAnalyzer(Version.LUCENE_CURRENT);\n{code}",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3658",
        "summary": "NRTCachingDir has invalid asserts (if same file name is written twice)",
        "description": "Normally Lucene is write-once (except for segments.gen file, which NRTCachingDir never caches), but in some tests (TestDoc, TestCrash) we can write the same file more than once.\n\nI don't think NRTCachingDir should have these asserts, and I think on createOutput it should remove any old file if present.\n\nI also found & fixed a possible concurrency issue (if more than one thread syncs at the same time; IndexWriter doesn't ever do this today but it has in the past).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2595",
        "summary": "most tests should use MockRAMDirectory not RAMDirectory",
        "description": "",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-885",
        "summary": "clean up build files so contrib tests are run more easily",
        "description": "Per mailing list discussion...\n\nhttp://www.nabble.com/Tests%2C-Contribs%2C-and-Releases-tf3768924.html#a10655448\n\nTests for contribs should be run when \"ant test\" is used,  existing \"test\" target renamed to \"test-core\"\n",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3638",
        "summary": "IndexReader.document always return a doc with all the stored fields loaded. And this can be slow for the indexed document contain huge fields",
        "description": "when generating digest for some documents with huge fields, it should be unnecessary to load the field but just interesting part of the field with the offset information. but indexreader always return the whole field content. afterward, the customized storedfieldsreader will got a repeated loading",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1165",
        "summary": "Reduce exposure of nightly build documentation",
        "description": "From LUCENE-1157  -\n\n ..the nightly build documentation is too prominent. A search for \"indexwriter api\" on Google or Yahoo! returns nightly documentation before released documentation.\n\n(https://issues.apache.org/jira/browse/LUCENE-1157?focusedCommentId=12565820#action_12565820)\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3633",
        "summary": "Remove code duplication in MultiReader/DirectoryReader, make everything inside final",
        "description": "After making IndexReader readOnly (LUCENE-3606) there is no need to have completely different DirectoryReader and MultiReader, the current code is heavy code duplication and violations against finalness patterns. There are only few differences in reopen and things like isCurrent/getDirectory/...\n\nThis issue will clean this up by introducing a hidden package-private base class for both and only handling reopen and incRef/decRef different. DirectoryReader is now final and all fields in BaseMultiReader, MultiReader and DirectoryReader are final now. DirectoryReader has now only static factories, no public ctor anymore.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1818",
        "summary": "contrib-spatial java.lang.UnsupportedOperationException on QueryWrapperFilter.getDocIdSet",
        "description": "We use in our Project (which is in the devel phase) the latest Snapshot release of lucene. After i updated to the latest Snapshot a few days ago one of our JUnit tests fails and throws the following error:\n\njava.lang.UnsupportedOperationException\n\tat org.apache.lucene.search.Query.createWeight(Query.java:91)\n\tat org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:72)\n\tat org.apache.lucene.misc.ChainedFilter.getDISI(ChainedFilter.java:150)\n\tat org.apache.lucene.misc.ChainedFilter.initialResult(ChainedFilter.java:173)\n\tat org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:211)\n\tat org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:141)\n\tat org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)\n\tat org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:244)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:172)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:183)\n\tat org.hibernate.search.query.QueryHits.updateTopDocs(QueryHits.java:100)\n\tat org.hibernate.search.query.QueryHits.<init>(QueryHits.java:61)\n\tat org.hibernate.search.query.QueryHits.<init>(QueryHits.java:51)\n\tat org.hibernate.search.query.FullTextQueryImpl.getQueryHits(FullTextQueryImpl.java:373)\n\tat org.hibernate.search.query.FullTextQueryImpl.list(FullTextQueryImpl.java:293)\n\t...\n\nI think it appeared after the Hudson build 917... and the following commit of the Query.java http://hudson.zones.apache.org/hudson/job/Lucene-trunk/917/changes#detail4 and is in connection with this JIRA issue: LUCENE-1771\nI hope i'm at the right place and that you can fix it. Thanks!",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1160",
        "summary": "MergeException from CMS threads should record the Directory",
        "description": "When you hit an unhandled exception in ConcurrentMergeScheduler, it\nthrows a MergePolicy.MergeException, but there's no easy way to figure\nout which index caused this (if you have more than one).\n\nI plan to add the Directory to the MergeException.  I also made a few\nother small changes to ConcurrentMergeScheduler:\n\n  * Added handleMergeException method, which is called on exception,\n    so that you can subclass ConcurrentMergeScheduler to do something\n    when an exception occurs.\n\n  * Added getMergeThread() method so you can override how the threads\n    are created (eg, if you want to make them in a different thread\n    group, use a pool, change priorities, etc.).\n\n  * Added doMerge(...) to actually do this merge, so you can do\n    something before starting and after finishing a merge.\n\n  * Changed private -> protected on a few attrs\n\nI plan to commit in a day or two.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2005",
        "summary": "Add LuSql project to \"Apache Lucene - Contributions\" wiki page",
        "description": "Add [LuSql|http://lab.cisti-icist.nrc-cnrc.gc.ca/cistilabswiki/index.php/LuSql] to the Apache Lucene - Contributions page [http://lucene.apache.org/java/2_9_0/contributions.html]\nI am the author of LuSql. I can supply any text needed. \n\nPerhaps a new heading is needed to capture Database/JDBC oriented Lucene tools (there are others out there)?",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1683",
        "summary": "RegexQuery matches terms the input regex doesn't actually match",
        "description": "I was writing some unit tests for our own wrapper around the Lucene regex classes, and got tripped up by something interesting.\n\nThe regex \"cat.\" will match \"cats\" but also anything with \"cat\" and 1+ following letters (e.g. \"cathy\", \"catcher\", ...)  It is as if there is an implicit .* always added to the end of the regex.\n\nHere's a unit test for the behaviour I would expect myself:\n\n    @Test\n    public void testNecessity() throws Exception {\n        File dir = new File(new File(System.getProperty(\"java.io.tmpdir\")), \"index\");\n        IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true);\n        try {\n            Document doc = new Document();\n            doc.add(new Field(\"field\", \"cat cats cathy\", Field.Store.YES, Field.Index.TOKENIZED));\n            writer.addDocument(doc);\n        } finally {\n            writer.close();\n        }\n\n        IndexReader reader = IndexReader.open(dir);\n        try {\n            TermEnum terms = new RegexQuery(new Term(\"field\", \"cat.\")).getEnum(reader);\n            assertEquals(\"Wrong term\", \"cats\", terms.term());\n            assertFalse(\"Should have only been one term\", terms.next());\n        } finally {\n            reader.close();\n        }\n    }\n\nThis test fails on the term check with terms.term() equal to \"cathy\".\n\nOur workaround is to mangle the query like this:\n\n    String fixed = String.format(\"(?:%s)$\", original);\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-514",
        "summary": "MultiPhraseQuery should allow access to terms",
        "description": "",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3391",
        "summary": "Make EasySimilarityProvider a full-fledged class ",
        "description": "The {{EasySimilarityProvider}} in {{TestEasySimilarity}} would be a good candidate for a full-fledged class. Both {{DefaultSimilarity}} and {{BM25Similarity}} have their own providers, which are effectively the same,so I don't see why we couldn't add one generic provider for convenience.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-2541",
        "summary": "NumericRangeQuery errors with endpoints near long min and max values",
        "description": "This problem first reported in Solr:\n\nhttp://lucene.472066.n3.nabble.com/range-query-on-TrieLongField-strange-result-tt970974.html#a970974",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3197",
        "summary": "Optimize runs forever if you keep deleting docs at the same time",
        "description": "Because we \"cascade\" merges for an optimize... if you also delete documents while the merges are running, then the merge policy will see the resulting single segment as still not optimized (since it has pending deletes) and do a single-segment merge, and will repeat indefinitely (as long as your app keeps deleting docs).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1978",
        "summary": "Remove HitCollector",
        "description": "Remove the rest of HitCollectors",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-476",
        "summary": "BooleanQuery add public method that returns number of clauses this query",
        "description": "BooleanQuery add public method getClausesCount() that returns number of clauses this query.\n\ncurrent ways of getting clauses count are:\n1).\n int clausesCount  = booleanQuery.getClauses().length;\n\nor \n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3857",
        "summary": "exceptions from other threads in beforeclass/etc do not fail the test",
        "description": "Lots of tests create indexes in beforeClass methods, but if an exception is thrown from another thread\nit won't fail the test... e.g. this test passes:\n{code}\npublic class TestExc extends LuceneTestCase {\n  @BeforeClass\n  public static void beforeClass() {\n    new Thread() {\n      public void run() {\n        throw new RuntimeException(\"boo!\");\n      }  \n    }.start();\n  }\n  \n  public void test() { }\n}\n{code}\n\nthis is because the uncaught exception handler is in setup/teardown",
        "label": "NUG",
        "classified": "TEST",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3703",
        "summary": "DirectoryTaxonomyReader.refresh misbehaves with ref counts",
        "description": "DirectoryTaxonomyReader uses the internal IndexReader in order to track its own reference counting. However, when you call refresh(), it reopens the internal IndexReader, and from that point, all previous reference counting gets lost (since the new IndexReader's refCount is 1).\n\nThe solution is to track reference counting in DTR itself. I wrote a simple unit test which exposes the bug (will be attached with the patch shortly).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3694",
        "summary": "DocValuesField should not overload setInt/setFloat etc",
        "description": "See my description on LUCENE-3687. In general we should avoid this for primitive types and give them each unique names.\n\nSo I think instead of setInt(byte), setInt(short), setInt(int), setInt(long), setFloat(float) and setFloat(double),\nwe should have setByte(byte), setShort(short), setInt(int), setLong(long), setFloat(float) and setDouble(double).",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "BUG"
    },
    {
        "key": "LUCENE-241",
        "summary": "NullPointerException in CompoundFileReader",
        "description": "Hello,\n\nwe have got a NullPointerException in the Lucene-class CompoundFileReader:\n\njava.lang.NullPointerException\n        at\norg.apache.lucene.index.CompoundFileReader.<init>(CompoundFileReader.java:94)\n        at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:97)\n        at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:466)\n        at\norg.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:426)\n        at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:236)\n\nLucene has been working fine for some days, until this NullPointerException\nhas occured which has corrupted the complete index.\n\nThe reason for this NullPointerException is the following Code \nin Lucenes source file CompoundFileReader.java:\n\n    public CompoundFileReader(Directory dir, String name)\n    throws IOException\n    {\n        boolean success = false;\n        ...\n\n        try {\n            stream = dir.openFile(name);\n\n            // read the directory and init files\n            ...\n\n            success = true;\n\n        } finally {\n            if (! success) {\n                try {\n                    stream.close();\n                } catch (IOException e) { }\n            }\n        }\n    }\n\nIf the IO-method-call \"dir.openFile()\" throws an IOExeption,\nthen the variable \"stream\" remains its null value.\nThe statement \"stream.close()\" in the finally clause will then cause a\nNullPointerException.\n\nI would suggest that you change the code from:\n    stream.close();\nto:\n    if ( stream != null ) {\n        stream.close();\n    }\n\nThere are a lot of reasons why an IO-operation like \"dir.openFile()\"\ncould throw an IOException.\nI cannot guarantee that such an IO exception will never occur again.\nTherefore it is better to handle such an IO exception correctly.\n\nThis issue is similar to bug# 29774, except that I recommand an easy way\nto solve this problem.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1908",
        "summary": "Similarity javadocs for scoring function to relate more tightly to scoring models in effect",
        "description": "See discussion in the related issue.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1946",
        "summary": "Remove deprecated TokenStream API",
        "description": "I looked into clover analysis: It seems to be no longer used since I removed the tests yesterday - I am happy!",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3608",
        "summary": "MultiFields.getUniqueFieldCount is broken",
        "description": "this returns terms.size(), but terms is lazy-initted. So it wrongly returns 0.\n\nSimplest fix would be to return -1.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2158",
        "summary": "NRT can temporarily lose deletions at high indexing rates",
        "description": "OK, I found a sneaky case where NRT can temporarily lose deletions.\nThe deletions aren't permanently lost - they are seen on the next\nopened NRT reader.\n\nIt happens like this (in IW.getReader):\n\n  * First flush() is called, to flush added docs & materialize the\n    deletes.\n\n  * The very next statement enters a sync'd block to open the reader,\n    but, if indexing rate is very high, and threads get scheduled\n    \"appropriately\", a \"natural\" flush (due to RAM buffer being full\n    or flush doc count being reached) could be hit before the sync\n    block is entered, in which case that 2nd flush won't materialize\n    the deletes associated with it, and the returned NRT reader will\n    only see its adds until it's next reopened.\n\nThe fix is simple -- we should materialize deletes inside the sync\nblock, not during the flush.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1716",
        "summary": "Adding norms, properties indexing and writer.infoStream support to benchmark",
        "description": "I would like to add the following support in benchmark:\n# Ability to specify whether norms should be stored in the index.\n# Ability to specify whether norms should be stored for the body field (assuming norms are usually stored for that field in real life applications, make it explicit)\n# Ability to specify an infoStream for IndexWriter\n# Ability to specify whether to index the properties returned on DocData (for content sources like TREC, these may include arbitrary <meta> tags, which we may not want to index).\n\nPatch to come shortly.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-388",
        "summary": "[PATCH] IndexWriter.maybeMergeSegments() takes lots of CPU resources",
        "description": "Note: I believe this to be the same situation with 1.4.3 as with SVN HEAD.\n\nAnalysis using hprof utility shows that during index creation with many\ndocuments highlights that the CPU spends a large portion of it's time in\nIndexWriter.maybeMergeSegments(), which seems to be a 'waste' compared with\nother valuable CPU intensive operations such as tokenization etc.\n\nUsing the following test snippet to retrieve some rows from the db and create an\nindex:\n\n        Analyzer a = new StandardAnalyzer();\n        writer = new IndexWriter(indexDir, a, true);\n        writer.setMergeFactor(1000);\n        writer.setMaxBufferedDocs(10000);\n        writer.setUseCompoundFile(false);\n        connection = DriverManager.getConnection(\n                \"jdbc:inetdae7:tower.aconex.com?database=<somedb>\", \"secret\",\n                \"squirrel\");\n        String sql = \"select userid, userfirstname, userlastname, email from userx\";\n        LOG.info(\"sql=\" + sql);\n        Statement statement = connection.createStatement();\n        statement.setFetchSize(5000);\n        LOG.info(\"Executing sql\");\n        ResultSet rs = statement.executeQuery(sql);\n        LOG.info(\"ResultSet retrieved\");\n        int row = 0;\n\n        LOG.info(\"Indexing users\");\n        long begin = System.currentTimeMillis();\n        while (rs.next()) {\n            int userid = rs.getInt(1);\n            String firstname = rs.getString(2);\n            String lastname = rs.getString(3);\n            String email = rs.getString(4);\n            String fullName = firstname + \" \" + lastname;\n            Document doc = new Document();\n            doc.add(Field.Keyword(\"userid\", userid+\"\"));\n            doc.add(Field.Keyword(\"firstname\", firstname.toLowerCase()));\n            doc.add(Field.Keyword(\"lastname\", lastname.toLowerCase()));\n            doc.add(Field.Text(\"name\", fullName.toLowerCase()));\n            doc.add(Field.Keyword(\"email\", email.toLowerCase()));\n            writer.addDocument(doc);\n            row++;\n            if((row % 100)==0){\n                LOG.info(row + \" indexed\");\n            }\n        }\n        double end = System.currentTimeMillis();\n        double diff = (end-begin)/1000;\n        double rate = row/diff;\n        LOG.info(\"rate:\" +rate);\n\nOn my 1.5GHz PowerBook with 1.5Gb RAM and a 5400 RPM drive, my CPU is maxed out,\nand I end up getting a rate of indexing between 490-515 documents/second run\nover 10 times in succession.  \n\nBy applying a simple patch to IndexWriter (see attached shortly), which defers\nthe calling of maybeMergeSegments() so that it is only called every 2000\ntimes(an arbitrary figure), I appear to get a new rate of between 945-970\ndocuments/second.  Using Luke to look inside each index created between these 2\nthere does not appear to be any difference.  Same number of Documents, same\nnumber of Terms.\n\nI'm not suggesting one should apply this patch, I'm just highlighting the\ndifference in performance that this sort of change gives you.  \n\nWe are about to use Lucene to index 4 million construction document records, and\nso speeding up the indexing process is in our best interest! :)  If one\nconsiders the amount of CPU time spent in maybeMergeSegments over the initial\nindex creation of 4 million documents, I think one could see how it would be\nideal to try to speed this area up (at least move the bottleneck to IO). \n\nI woul appreciate anyone taking a moment to comment on this.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1499",
        "summary": "Minor refactoring to IndexFileNameFilter",
        "description": "IndexFileNameFilter looks like it's designed to be a singleton, however its constructor is public and its singleton member is package visible. The proposed patch changes the constructor and member to private. Since it already has a static getFilter() method, and no code in Lucene references those two, I don't think it creates any problems from an API perspective.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2846",
        "summary": "omitTF is viral, but omitNorms is anti-viral.",
        "description": "omitTF is viral. if you add document 1 with field \"foo\" as omitTF, then document 2 has field \"foo\" without omitTF, they are both treated as omitTF.\n\nbut omitNorms is the opposite. if you have a million documents with field \"foo\" with omitNorms, then you add just one document without omitting norms, \nnow you suddenly have a million 'real norms'.\n\nI think it would be good for omitNorms to be viral too, just for consistency, and also to prevent huge byte[]'s.\nbut another option is to make omitTF anti-viral, which is more \"schemaless\" i guess.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2185",
        "summary": "add @Deprecated annotations",
        "description": "as discussed on LUCENE-2084, I think we should be consistent about use of @Deprecated annotations if we are to use it.\n\nThis patch adds the missing annotations... unfortunately i cannot commit this for some time, because my internet connection does not support heavy committing (it is difficult to even upload a large patch).\n\nSo if someone wants to take it, have fun, otherwise in a week or so I will commit it if nobody objects.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-3012",
        "summary": "if you use setNorm, lucene writes a headerless separate norms file",
        "description": "In this case SR.reWrite just writes the bytes with no header...\nwe should write it always.\n\nwe can detect in these cases (segment written <= 3.1) with a \nsketchy length == maxDoc check.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3222",
        "summary": "Buffered deletes under count RAM",
        "description": "I found this while working on LUCENE-2548: when we freeze the deletes (create FrozenBufferedDeletes), when we set the bytesUsed we are failing to account for RAM required for the term bytes (and now term field).",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-377",
        "summary": "GCJ build fails with JDK 1.5",
        "description": "The build.xml doesn't specify a target VM version. Using JDK 1.5, this means the compiled .class files \nare automatically made for 1.5, with java.lang.StringBuilder insidiously used for string concatenation. \nGCJ doesn't seem to include this class yet, so when it gets to the gcj build it dies trying to read the class \nfiles.\n\nSteps to reproduce:\n1. Install Sun JDK 1.5 for a Java compiler\n2. Check out Lucene from svn\n3. 'ant gcj'\n\nExpected behavior:\nShould build Lucene to .class files and .jar with the JDK compiler and then compile an .a with GCJ.\n\nActual behavior:\nThe GCJ build fails, complaining of being unable to find java.lang.StringBuilder.\n\nSuggested fix:\nAdding source=\"1.3\" target=\"1.3\" to the <javac> tasks seems to take care of this. Patch to be attached.\n\nAdditional notes:\nUsing Lucene from SVN and GCJ pulled from GCC CVS circa 2005-04-19. Ant 1.6.2.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1152",
        "summary": "SpellChecker does not work properly on calling indexDictionary after clearIndex",
        "description": "We have to call clearIndex and indexDictionary to rebuild dictionary from fresh. The call to SpellChecker clearIndex() function does not reset the searcher. Hence, when we call indexDictionary after that, many entries that are already in the stale searcher will not be indexed.\n\nAlso, I see that IndexReader reader is used for the sole purpose of obtaining the docFreq of a given term in exist() function. This functionality can also be obtained by using just the searcher by calling searcher.docFreq. Thus, can we get away completely with reader and code associated with it like\n      if (IndexReader.isLocked(spellIndex)){\n\tIndexReader.unlock(spellIndex);\n      }\nand the reader related code in finalize?\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3911",
        "summary": "improve BaseTokenStreamTestCase random string generation",
        "description": "Most analysis tests use mocktokenizer (which splits on whitespace), but\nits rare that we generate a string with 'many tokens'. So I think we should\ntry to generate more realistic test strings.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2382",
        "summary": "Merging implemented by codecs must catch aborted merges",
        "description": "This is a regression (we lost functionality on landing flex).\n\nWhen you close IW with \"false\" (meaning abort all running merges), IW asks the merge threads to abort.  The threads are supposed to periodically check if they are aborted and throw an exception if so.\n\nBut on the cutover to flex, where the codec can override how merging is done (but a default impl is in the base enum classes), we lost this.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3712",
        "summary": "Remove unused (and untested) methods from ReaderUtil that are also veeeeery ineffective",
        "description": "ReaderUtil contains two methods that are nowhere used and not even tested. Additionally those are implemented with useless List->array copying; ineffective docStart calculation for a binary search later instead directly returning the reader while scanning -- and I am not sure if they really work as expected. As ReaderUtil is @lucene.internal we should remove them in 3.x and trunk, alternatively the useless array copy / docStarts handling should be removed and tests added:\n\n{code:java}\npublic static IndexReader subReader(int doc, IndexReader reader)\npublic static IndexReader subReader(IndexReader reader, int subIndex)\n{code}\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-866",
        "summary": "Multi-level skipping on posting lists",
        "description": "To accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. \nThe default skip interval is set to 16. If we want to skip e. g. 100 documents, \nthen it is not necessary to read 100 entries from the posting list, but only \n100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This \nspeeds up conjunction (AND) and phrase queries significantly.\n\nHowever, the skip interval is always a compromise. If you have a very big index \nwith huge posting lists and you want to skip over lets say 100k documents, then \nit is still necessary to read 100k/16 = 6250 entries from the skip list. For big \nindexes the skip interval could be set to a higher value, but then after a big \nskip a long scan to the target doc might be necessary.\n\nA solution for this compromise is to have multi-level skip lists that guarantee a \nlogarithmic amount of skips to any target in the posting list. This patch \nimplements such an approach in the following way:\n\n  Example for skipInterval = 3:\n                                                      c            (skip level 2)\n                  c                 c                 c            (skip level 1) \n      x     x     x     x     x     x     x     x     x     x      (skip level 0)\n  d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)\n      3     6     9     12    15    18    21    24    27    30     (df)\n \n  d - document\n  x - skip data\n  c - skip data with child pointer\n \nSkip level i contains every skipInterval-th entry from skip level i-1. Therefore the \nnumber of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).\n \nEach skip entry on a level i>0 contains a pointer to the corresponding skip entry in \nlist i-1. This guarantees a logarithmic amount of skips to find the target document.\n\n\nImplementations details:\n\n   * I factored the skipping code out of SegmentMerger and SegmentTermDocs to \n     simplify those classes. The two new classes AbstractSkipListReader and \n\t AbstractSkipListWriter implement the skipping functionality.\n   * While AbstractSkipListReader and Writer take care of writing and reading the \n     multiple skip levels, they do not implement an actual skip data format. The two \n\t new subclasses DefaultSkipListReader and Writer implement the skip data format \n\t that is currently used in Lucene (with two file pointers for the freq and prox \n\t file and with payload length information). I added this extra layer to be \n\t prepared for flexible indexing and different posting list formats. \n      \n   \nFile format changes: \n\n   * I added the new parameter 'maxSkipLevels' to the term dictionary and increased the\n     version of this file. If maxSkipLevels is set to one, then the format of the freq \n\t file does not change at all, because we only have one skip level as before. For \n\t backwards compatibility maxSkipLevels is set to one automatically if an index \n\t without the new parameter is read. \n   * In case maxSkipLevels > 1, then the frq file changes as follows:\n     FreqFile (.frq) --> <TermFreqs, SkipData>^TermCount\n\t SkipData        --> <<SkipLevelLength, SkipLevel>^(Min(maxSkipLevels, \n\t                       floor(log(DocFreq/log(skipInterval))) - 1)>, SkipLevel>\n\t SkipLevel       --> <SkipDatum>^DocFreq/(SkipInterval^(Level + 1))\n\n\t Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not \n\t needed, and 2) the format of this file does not change for maxSkipLevels=1 then.\n\t \n\t \nAll unit tests pass with this patch.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1021",
        "summary": "Unit tests do not fail if a ConcurrentMergeScheduler thread hits an exception",
        "description": "Now that CMS is the default, it's important to fail any unit test that\nhits an exception in a CMS thread.  But they do not fail now.  The\npreferred solution (thanks to Erik Hatcher) is to fix all Lucene unit\ntests to subclass from a new LuceneTestCase (in o.a.l.util) base that\nasserts that there were no such exceptions during the test.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2114",
        "summary": "Improve org.apache.lucene.search.Filter Documentation and Tests to reflect per segment readers",
        "description": "Filter Javadoc does not mention that the Reader passed to getDocIDSet(Reader) could be on a per-segment basis.\nThis caused confusion on the users-list -- see http://lucene.markmail.org/message/6knz2mkqbpxjz5po?q=date:200912+list:org.apache.lucene.java-user&page=1\nWe should improve the javadoc and also add a testcase that reflects filtering on a per-segment basis.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2639",
        "summary": "remove random juggling in tests, add -Dtests.seed",
        "description": "Since we added newIndexWriterConfig/newDirectory, etc, a lot of tests are juggling randoms around.\n\nInstead this patch:\n* changes it so LuceneTestCase[J4] manage the random.\n* allow you to set -Dtests.seed=23432432432 to reproduce a test, rather than editing the code\n* removes random arguments from newIndexWriterConfig, newDirectory.\n\nI want to do this before looking at doing things like newField so we can vary term vectors, etc.\n\nI also fixed the solr contrib builds so they arent hiding the exceptions i noted in SOLR-2002.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3220",
        "summary": "Implement various ranking models as Similarities",
        "description": "With [LUCENE-3174|https://issues.apache.org/jira/browse/LUCENE-3174] done, we can finally work on implementing the standard ranking models. Currently DFR, BM25 and LM are on the menu.\n\nDone:\n * {{EasyStats}}: contains all statistics that might be relevant for a ranking algorithm\n * {{EasySimilarity}}: the ancestor of all the other similarities. Hides the DocScorers and as much implementation detail as possible\n * _BM25_: the current \"mock\" implementation might be OK\n * _LM_\n * _DFR_\n * The so-called _Information-Based Models_\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-2642",
        "summary": "merge LuceneTestCase and LuceneTestCaseJ4",
        "description": "We added Junit4 support, but as a separate test class.\n\nSo unfortunately, we have two separate base classes to maintain: LuceneTestCase and LuceneTestCaseJ4.\nThis creates a mess and is difficult to manage.\n\nInstead, I propose a single base test class that works both junit3 and junit4 style.\n\nI modified our LuceneTestCaseJ4 in the following way:\n* the methods to run are not limited to the ones annotated with @Test, but also any void no-arg methods that start with \"test\", like junit3. this means you dont have to sprinkle @Test everywhere.\n* of course, @Ignore works as expected everywhere.\n* LuceneTestCaseJ4 extends TestCase so you dont have to import static Assert.* to get all the asserts.\n\nfor most tests, no changes are required. but a few very minor things had to be changed:\n* setUp() and tearDown() must be public, not protected.\n* useless ctors must be removed, such as TestFoo(String name) { super(name); }\n* LocalizedTestCase is gone, instead of\n{code}\npublic class TestQueryParser extends LocalizedTestCase {\n{code}\nit is now\n{code}\n@RunWith(LuceneTestCase.LocalizedTestCaseRunner.class)\npublic class TestQueryParser extends LuceneTestCase {\n{code}\n* Same with MultiCodecTestCase: (LuceneTestCase.MultiCodecTestCaseRunner.class}\n\nI only did the core tests in the patch as a start, and i just made an empty LuceneTestCase extends LuceneTestCaseJ4.\n\nI'd like to do contrib and solr and rename this LuceneTestCaseJ4 to only a single class: LuceneTestCase.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-684",
        "summary": "Refrase javadoc 1st sentence for IndexReader.deleteDocuments",
        "description": "",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3628",
        "summary": "Cut Norms over to DocValues",
        "description": "since IR is now fully R/O and norms are inside codecs we can cut over to use a IDV impl for writing norms. LUCENE-3606 has some [ideas|https://issues.apache.org/jira/browse/LUCENE-3606?focusedCommentId=13160559&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13160559] about how this could be implemented",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1060",
        "summary": "Correct 2 minor javadoc mistakes in core, javadoc.access=private",
        "description": "Patches Token.java and TermVectorsReader.java",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-545",
        "summary": "Field Selection and Lazy Field Loading",
        "description": "The patch to come shortly implements a Field Selection and Lazy Loading mechanism for Document loading on the IndexReader.\n\nIt introduces a FieldSelector interface that defines the accept method:\nFieldSelectorResult accept(String fieldName);\n\n(Perhaps we want to expand this to take in other parameters such as the field metadata (term vector, etc.))\n\nAnyone can implement a FieldSelector to define how they want to load fields for a Document.  \nThe FieldSelectorResult can be one of four values: LOAD, LAZY_LOAD, NO_LOAD, LOAD_AND_BREAK.  \nThe FieldsReader, as it is looping over the FieldsInfo, applies the FieldSelector to determine what should be done with the current field.\n\nI modeled this after the java.io.FileFilter mechanism.  There are two implementations to date: SetBasedFieldSelector and LoadFirstFieldSelector.  The former takes in two sets of field names, one to load immed. and one to load lazily.  The latter returns LOAD_AND_BREAK on the first field encountered.  See TestFieldsReader for examples.\n\nIt should support UTF-8 (I borrowed code from Issue 509, thanks!).  See TestFieldsReader for examples\n\nI added an expert method on IndexInput  named skipChars that takes in the number of characters to skip.  This is a compromise on changing the file format of the fields to better support seeking.  It does some of the work of readChars, but not all of it.  It doesn't require buffer storage and it doesn't do the bitwise operations.  It just reads in the appropriate number of bytes and promptly ignores them.  This is useful for skipping non-binary, non-compressed stored fields.\n\nThe biggest change is by far the introduction of the Fieldable interface (as per Doug's suggestion from a mailing list email on Lazy Field loading from a while ago).  Field is now a Fieldable.  All uses of Field have been changed to use Fieldable.  FieldsReader.LazyField also implements Fieldable.\n\nLazy Field loading is now implemented.  It has a major caveat (that is Documented) in that it clones the underlying IndexInput upon lazy access to the Field value.  IT IS UNDEFINED whether a Lazy Field can be loaded after the IndexInput parent has been closed (although, from what I saw, it does work).  I thought about adding a reattach method, but it seems just as easy to reload the document.  See the TestFieldsReader and DocHelper for examples.\n\nI updated a couple of other tests to reflect the new fields that are on the DocHelper document.\n\nAll tests pass.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3140",
        "summary": "Backport FSTs to 3.x",
        "description": "",
        "label": "NUG",
        "classified": "BACKPORT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2210",
        "summary": "trectopicsreader doesn't properly read descriptions or narratives",
        "description": "TrecTopicsReader does not read these fields correctly, as demonstrated by the test case.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2622",
        "summary": "Random Test Failure org.apache.lucene.TestExternalCodecs.testPerFieldCodec (from TestExternalCodecs)",
        "description": "Error Message\n\nstate.ord=54 startOrd=0 ir.isIndexTerm=true state.docFreq=1\nStacktrace\n\njunit.framework.AssertionFailedError: state.ord=54 startOrd=0 ir.isIndexTerm=true state.docFreq=1\n\tat org.apache.lucene.index.codecs.standard.StandardTermsDictReader$FieldReader$SegmentTermsEnum.seek(StandardTermsDictReader.java:395)\n\tat org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1099)\n\tat org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1028)\n\tat org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4213)\n\tat org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3381)\n\tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3221)\n\tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3211)\n\tat org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2345)\n\tat org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2323)\n\tat org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2293)\n\tat org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:645)\n\tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:381)\n\tat org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:373)\nStandard Output\n\nNOTE: random codec of testcase 'testPerFieldCodec' was: MockFixedIntBlock(blockSize=1327)\nNOTE: random locale of testcase 'testPerFieldCodec' was: lt_LT\nNOTE: random timezone of testcase 'testPerFieldCodec' was: Africa/Lusaka\nNOTE: random seed of testcase 'testPerFieldCodec' was: 812019387131615618",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3020",
        "summary": "better payload testing with mockanalyzer",
        "description": "MockAnalyzer currently always indexes some fixed-length payloads.\n\nInstead it should decide for each field randomly (and remember it for that field):\n* if the field should index no payloads at all\n* field should index fixed length payloads\n* field should index variable length payloads.\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-2247",
        "summary": "Add CharArrayMap to lucene and make CharAraySet an proxy on the keySet() of it",
        "description": "This patch adds a CharArrayMap<V> to Lucene's analysis package as compagnon of CharArraySet. It supports fast retrieval of char[] keys like CharArraySet does. This is important for some stemmers and other places in Lucene.\n\nStemers generally use CharArrayMap<String>, which has then get(char[]) returning String. Strings are compact and can be easily copied into termBuffer. A Map<String,String> would be slow as the termBuffer would be first converted to String, then looked up. The return value as String is perfectly legal, as it can be copied easily into termBuffer.\n\nThis class borrows lots of code from Solr's pendant, but has additional features and more consistent API according to CharArraySet. The key is always <?>, because as of CharArraySet, anything that has a toString() representation can be used as key (of course with overhead). It also defines a unmodifiable map and correct iterators (returning the native char[]).\n\nCharArraySet was made consistent and now returns for matchVersion>=3.1 also an iterator on char[]. CharArraySet's code was almost completely copied to CharArrayMap and removed in the Set. CharArraySet is now a simple proxy on the keySet().\n\nIn future we can think of making CharArraySet/CharArrayMap/CharArrayCollection an interface so the whole API would be more consistent to the Java collections API. But this would be a backwards break. But it would be possible to use better impl instead of hashing (like prefix trees).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1692",
        "summary": "Contrib analyzers need tests",
        "description": "The analyzers in contrib need tests, preferably ones that test the behavior of all the Token 'attributes' involved (offsets, type, etc) and not just what they do with token text.\n\nThis way, they can be converted to the new api without breakage.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-801",
        "summary": "build.xml in cnotrib/benchmark should auto build core java and demo if required",
        "description": "Currently one needs to build core jar and demo jar before building/running benchmark.\nThis is not very convenient. \nChange it to \n- use core classes and demo classes (instead of jars).\n- build core and demo by dependency if required.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2756",
        "summary": "MultiSearcher.rewrite() incorrectly rewrites queries",
        "description": "This was reported on the userlist, in the context of range queries.\n\nIts also easy to make our existing tests fail with my patch on LUCENE-2751:\n{noformat}\nant test-core -Dtestcase=TestBoolean2 -Dtestmethod=testRandomQueries -Dtests.seed=7679849347282878725:-903778383189134045\n{noformat}\n\nThe fundamental problem is that MultiSearcher first rewrites against individual subs, \nthen uses Query.combine() which simply OR's these sub-clauses.\n\nThis is incorrect for expanded MUST_NOT queries (e.g. from wildcard), as it violates demorgan's law.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2809",
        "summary": "IndexWriter.numDocs doesn't take into account applied but not flushed deletes",
        "description": "The javadoc states that buffered deletes are not taken into account and so you must call commit first.\n\nBut, if you do that, and you're using CMS, and you're unlucky enough to have a background merge commit just after you call commit but before you call .numDocs, you can still get a wrong count back.\n\nThe fix is trivial -- numDocs should also consult any pooled readers for their current del count.\n\nThis is causing an intermittent failure in the new TestNRTThreads.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-848",
        "summary": "Add supported for Wikipedia English as a corpus in the benchmarker stuff",
        "description": "Add support for using Wikipedia for benchmarking.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1137",
        "summary": "Token type as BitSet: typeBits()",
        "description": "It is sometimes useful to have a more compact, easy to parse, type representation for Token than the current type() String.  This patch adds a BitSet onto Token, defaulting to null, with accessors for setting bit flags on a Token.  This is useful for communicating information about a token to TokenFilters further down the chain.  \n\nFor example, in the WikipediaTokenizer, the possibility exists that a token could be both a category and bold (or many other variations), yet it is difficult to communicate this without adding in a lot of different Strings for type.  Unlike using the payload information (which could serve this purpose), the BitSet does not get added to the index (although one could easily convert it to a payload.)",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-404",
        "summary": "Crash when querying an index using multiple term positions.",
        "description": "file: MultipleTermPositions.java, line: 201, function: skipTo(int).\n\nThis refers to the source that can currently be downloaded from the lucene site,\nLucene v. 1.4.3.\n\nThe function peek() returns null (because top() also retruned null). There is no\ncheck for this, as far as I can understand. The function doc() is called on a\nnull-object, which results in a NullPointerException.\n\nI switched the specified line to this one:\n\nwhile(_termPositionsQueue.peek() != null && target >\n_termPositionsQueue.peek().doc())\n\nThis got rid of the crash for me.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3382",
        "summary": "fix compound-file/NoSuchDirectoryException bugs in NRTCachingDir",
        "description": "found some bugs over on LUCENE-3374, but we should fix these separately from whether or not we move it to core,\nand the bugs apply to 3.x too.\n\nhere we can just add explicit tests for the problems.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-380",
        "summary": "A new Greek Analyzer for Lucene",
        "description": "I would like to contribute a greek analyzer for lucene. It is based on the\nexisting Russian analyzer and features:\n\n- most common greek character sets, such as Unicode, ISO-8859-7 and Windows-1253\n- a collection of common greek stop words\n- conversion of characters with diacritics (accent, diaeresis) in the lower case\nfilter, as well as handling of special characters, such as small final sigma\n\nFor the character sets I used RFC 1947 (Greek Character Encoding for Electronic\nMail Messages) as a reference. I have incorporated this analyzer in Luke as well\nas used it successfully in a recent project of my company (EBS Ltd.).\n\nI hope you will find it a useful addition to the project.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1967",
        "summary": "make it easier to access default stopwords for language analyzers",
        "description": "DM Smith made the following comment: (sometimes it is hard to dig out the stop set from the analyzers)\n\nLooking around, some of these analyzers have very different ways of storing the default list.\nOne idea is to consider generalizing something like what Simon did with LUCENE-1965, LUCENE-1962,\nand having all stopwords lists stored as .txt files in resources folder.\n\n{code}\n  /**\n   * Returns an unmodifiable instance of the default stop-words set.\n   * @return an unmodifiable instance of the default stop-words set.\n   */\n  public static Set<String> getDefaultStopSet()\n{code}\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3574",
        "summary": "Add some more constants for newer Java versions to Constants.class, remove outdated ones.",
        "description": "Preparation for LUCENE-3235:\nThis adds constants to quickly detect Java6 and Java7 to Constants.java. It also deprecated and removes the outdated historical Java versions.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3092",
        "summary": "NRTCachingDirectory, to buffer small segments in a RAMDir",
        "description": "I created this simply Directory impl, whose goal is reduce IO\ncontention in a frequent reopen NRT use case.\n\nThe idea is, when reopening quickly, but not indexing that much\ncontent, you wind up with many small files created with time, that can\npossibly stress the IO system eg if merges, searching are also\nfighting for IO.\n\nSo, NRTCachingDirectory puts these newly created files into a RAMDir,\nand only when they are merged into a too-large segment, does it then\nwrite-through to the real (delegate) directory.\n\nThis lets you spend some RAM to reduce I0.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2919",
        "summary": "IndexSplitter that divides by primary key term",
        "description": "Index splitter that divides by primary key term.  The contrib MultiPassIndexSplitter we have divides by docid, however to guarantee external constraints it's sometimes necessary to split by a primary key term id.  I think this implementation is a fairly trivial change.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2690",
        "summary": "Do MultiTermQuery boolean rewrites per segment",
        "description": "MultiTermQuery currently rewrites FuzzyQuery (using TopTermsBooleanQueryRewrite), the auto constant rewrite method and the ScoringBQ rewrite methods using a MultiFields wrapper on the top-level reader. This is inefficient.\n\nThis patch changes the rewrite modes to do the rewrites per segment and uses some additional datastructures (hashed sets/maps) to exclude duplicate terms. All tests currently pass, but FuzzyQuery's tests should not, because it depends for the minimum score handling, that the terms are collected in order..\n\nRobert will fix FuzzyQuery in this issue, too. This patch is just a start.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1117",
        "summary": "Intermittent thread safety issue with EnwikiDocMaker",
        "description": "Intermittent thread safety issue with EnwikiDocMaker\n\nWhen I run the conf/wikipediaOneRound.alg, sometimes it gets started\nOK, other times (about 1/3rd the time) I see this:\n\n     Exception in thread \"Thread-0\" java.lang.RuntimeException: java.io.IOException: Bad file descriptor\n     \tat org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:76)\n     \tat java.lang.Thread.run(Thread.java:595)\n     Caused by: java.io.IOException: Bad file descriptor\n     \tat java.io.FileInputStream.readBytes(Native Method)\n     \tat java.io.FileInputStream.read(FileInputStream.java:194)\n     \tat org.apache.xerces.impl.XMLEntityManager$RewindableInputStream.read(Unknown Source)\n     \tat org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)\n     \tat org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)\n     \tat org.apache.xerces.impl.XMLEntityScanner.scanQName(Unknown Source)\n     \tat org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)\n     \tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)\n     \tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\n     \tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n     \tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n     \tat org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\n     \tat org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\n     \tat org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:60)\n     \t... 1 more\n\nThe problem is that the thread that pulls the XML docs is started as\nsoon as EnwikiDocMaker class is instantiated.  When it's started, it\nuses the fileIS (FileInputStream) to feed the XML Parser.  But,\nopenFile is actually called twice on starting the alg, if you use any\ntask deriving from ResetInputsTask, which closes the original fileIS\nthat the XML parser may be using.\n\nI changed the thread to instead start on-demand the first time next()\nis called.  I also removed a redundant resetInputs() call (which was\nopening the file more frequently than needed).  Finally, I added logic\nin the thread to detect that the input stream was closed (because\nLineDocMaker.resetInputs() was called, eg, if we are not running the\ndoc maker to exhaustion).\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-456",
        "summary": "Duplicate hits and missing hits in sorted search",
        "description": "If using a searcher that subclasses from IndexSearcher I get different result sets (besides the ordering of course). The problem only occurrs if the searcher is wrapped by (Parallel)MultiSearcher and the index is not too small. The number of hits returned by un unsorted and a sorted search are identical but the hits are referencing different documents. A closer look at the result sets revealed that the sorted search returns duplicate hits.\n\nI created test cases for Lucene 1.4.3 as well as for the head release. The problem showed up for both, the number of duplicates beeing bigger for the head realease. The test cases are written for package org.apache.lucene.search. There are messages describing the problem written to the console. In order to see all those hints the asserts are commented out. So dont't be confused if junit reports no errors. (Sorry, beeing a novice user of the bug tracker I don't see any means to attach the test cases on this screen. Let's see.)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2002",
        "summary": "Add oal.util.Version ctor to QueryParser",
        "description": "This is a followup of LUCENE-1987:\n\nIf somebody uses StandardAnalyzer with Version.LUCENE_CURRENT and then uses QueryParser, phrase queries will not work, because the StopFilter enables position Increments for stop words, but QueryParser ignores them per default. The user has to explicitely enable them.\n\nThis issue would add a ctor taking the Version constant and automatically enable this setting. The same applies to the contrib queryparser. Eventually also StopAnalyzer should add this version ctor.\n\nTo be able to remove the default ctor for 3.0 (to remove a possible trap for users of QueryParser), it must be deprecated and the new one also added to 2.9.1.",
        "label": "NUG",
        "classified": "RFE",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1303",
        "summary": "BoostingTermQuery's explanation should be marked as Match even if the payload part negated or zero'ed it",
        "description": "Since BTQ multiplies the payload on the score it might return a negative score.\nThe explanation should be marked as \"Match\" otherwise it is not added to container explanations,\nSee also in LUCENE-1302.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-475",
        "summary": " RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.",
        "description": "recently I found that  RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.\nfiles from source index are read entirely intro memory as single byte array which is after all is thrown away. And if I want to load my 200M optimized, compound format index to memory for faster search I should give JVM at least 400Mb memory limit. For larger indexes this can be an issue.\n\nI've attached patch how to solve this problem.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-499",
        "summary": "Documentation improvements for 1.9 release",
        "description": "I've poked arround the 1.9-rc1 builds and noticed a few simple documentation things that could be cleaned up, a patch will follow that...\n\n1) Adds some additional info to the README.txt\n2) Updates the version info in queryparsersyntax.xml and fileformats.html, and advises people \n     with older versions how to find the correct documentation for their version\n3) Builds javadocs for all of the contrib modules (the list was incomplete)",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-730",
        "summary": "Restore top level disjunction performance",
        "description": "This patch restores the performance of top level disjunctions. \nThe introduction of BooleanScorer2 had impacted this as reported\non java-user on 21 Nov 2006 by Stanislav Jordanov.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3240",
        "summary": "Move FunctionQuery, ValueSources and DocValues to Queries module",
        "description": "Having resolved the FunctionQuery sorting issue and moved the MutableValue classes, we can now move FunctionQuery, ValueSources and DocValues to a Queries module.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": ""
    },
    {
        "key": "LUCENE-3811",
        "summary": "remove unused benchmark dependencies",
        "description": "Benchmark has a huge number of jar files in its lib/ (some of which even have different versions than the same libs used in e.g. solr)\n\nBut the worst thing is, most of these it doesn't even use.\n* commons-collection: unused\n* commons-beanutils: unused\n* commons-logging: unused\n* commons-digetser: unused\n\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-823",
        "summary": "Lucene fails to close file handles under certain situations",
        "description": "As a followon to LUCENE-820, I've added a further check in\nMockRAMDirectory to assert that there are no open files when the\ndirectory is closed.\n\nThat check caused a few unit tests to fail, and in digging into the\nreason I uncovered these cases where Lucene fails to close file\nhandles:\n\n  * TermInfosReader.close() was setting its ThreadLocal enumerators to\n    null without first closing the SegmentTermEnum in there.  It looks\n    like this was part of the fix for LUCENE-436.  I just added the\n    call to close.\n\n    This is somewhat severe since we could leak many file handles for\n    use cases that burn through threads and/or indexes.  Though,\n    FSIndexInput does have a finalize() to close itself.\n\n  * Flushing of deletes in IndexWriter opens SegmentReader to do the\n    flushing, and it correctly calls close() to close the reader.  But\n    if an exception is hit during commit and before actually closing,\n    it will leave open those handles.  I fixed this first calling\n    doCommit() and then doClose() in a finally.  The \"disk full\" tests\n    we now have were hitting this.\n\n  * IndexWriter's addIndexes(IndexReader[]) method was opening a\n    reader but not closing it with a try/finally.  I just put a\n    try/finally in.\n\nI've also changed some unit tests to use MockRAMDirectory instead of\nRAMDirectory to increase testing coverage of \"leaking open file\nhandles\".\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2462",
        "summary": "remove DocsAndPositionsEnum.getPayloadLength",
        "description": "This was an accidental leftover; now that getPayload returns a BytesRef, this method is not needed.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3698",
        "summary": "FastVectorHighlighter adds a multi value separator (space) to the end of the highlighted text",
        "description": "The FVH adds an additional ' ' (the multi value separator) to the end of the highlighted text.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3149",
        "summary": "upgrade icu to 4.8",
        "description": "we should upgrade from 4.6 to 4.8.\n\nsome internal methods became public, also a package-private reflection hack can be removed.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2922",
        "summary": "Optimize BlockTermsReader.seek",
        "description": "When we seek, we first consult the terms index to find the right block\nof 32 (default) terms that may hold the target term.  Then, we scan\nthat block looking for an exact match.\n\nThe scanning just uses next() and then compares the full term, but\nthis is actually rather wasteful.  First off, since all terms in the\nblock share a common prefix, we should compare the target against that\ncommon prefix once, and then only compare the new suffix of each\nterm.  Second, since the term suffixes have already been read up front\ninto a byte[], we should do a no-copy comparison (vs today, where we\nfirst read a copy into the local BytesRef and then compare).\n\nWith this opto, I removed the ability for BlockTermsWriter/Reader to\nsupport arbitrary term sort order -- it's now hardwired to\nBytesRef.utf8SortedAsUnicode.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1494",
        "summary": "masking field of span for cross searching across multiple fields (many-to-one style)",
        "description": "This issue is to cover the changes required to do a search across multiple fields with the same name in a fashion similar to a many-to-one database. Below is my post on java-dev on the topic, which details the changes we need:\n\n---\n\nWe have an interesting situation where we are effectively indexing two 'entities' in our system, which share a one-to-many relationship (imagine 'User' and 'Delivery Address' for demonstration purposes). At the moment, we index one Lucene Document per 'many' end, duplicating the 'one' end data, like so:\n\n    userid: 1\n    userfirstname: fred\n    addresscountry: au\n    addressphone: 1234\n\n    userid: 1\n    userfirstname: fred\n    addresscountry: nz\n    addressphone: 5678\n\n    userid: 2\n    userfirstname: mary\n    addresscountry: au\n    addressphone: 5678\n\n(note: 2 Documents indexed for user 1). This is somewhat annoying for us, because when we search in Lucene the results we want back (conceptually) are at the 'user' level, so we have to collapse the results by distinct user id, etc. etc (let alone that it blows out the size of our index enormously). So why do we do it? It would make more sense to use multiple fields:\n    userid: 1\n    userfirstname: fred\n    addresscountry: au\n    addressphone: 1234\n    addresscountry: nz\n    addressphone: 5678\n\n    userid: 2\n    userfirstname: mary\n    addresscountry: au\n    addressphone: 5678\n\nBut imagine the search \"+addresscountry:au +addressphone:5678\". We'd like this to match ONLY Mary, but of course it matches Fred also because he matches both those terms (just for different addresses).\n\nThere are two aspects to the approach we've (more or less) got working but I'd like to run them past the group and see if they're worth trying to get them into Lucene proper (if so, I'll create a JIRA issue for them)\n\n1) Use a modified SpanNearQuery. If we assume that country + phone will always be one token, we can rely on the fact that the positions of 'au' and '5678' in Fred's document will be different.\n\n   SpanQuery q1 = new SpanTermQuery(new Term(\"addresscountry\", \"au\"));\n   SpanQuery q2 = new SpanTermQuery(new Term(\"addressphone\", \"5678\"));\n   SpanQuery snq = new SpanNearQuery(new SpanQuery[]{q1, q2}, 0, false);\n\nthe slop of 0 means that we'll only return those where the two terms are in the same position in their respective fields. This works brilliantly, BUT requires a change to SpanNearQuery's constructor (which checks that all the clauses are against the same field). Are people amenable to perhaps adding another constructor to SNQ which doesn't do the check, or subclassing it to do the same (give it a protected non-checking constructor for the subclass to call)?\n\n2) (snipped ... see LUCENE-1626 for second idea)",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-3852",
        "summary": "Rename BaseMultiReader class to BaseCompositeReader and make public",
        "description": "Currently the abstract DirectoryReader and MultiReader and ParallelCompositeReader extend a package private class. Users that want to implement a composite reader, should be able to subclass this pkg-private class, as it implements lots of abstract methods, useful for own implementations. In fact MultiReader is a shallow subclass only implementing correct closing&refCounting.\n\nBy making it public after the rename, the generics problems (type parameter R is not correctly displayed) in the JavaDocs are solved, too.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2691",
        "summary": "Consolidate Near Real Time and Reopen API semantics",
        "description": "We should consolidate the IndexWriter.getReader and the IndexReader.reopen semantics, since most people are already using the IR.reopen() method, we should simply add::\n{code}\nIR.reopen(IndexWriter)\n{code}\n\nInitially, it could just call the IW.getReader(), but it probably should switch to just using package private methods for sharing the internals",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2294",
        "summary": "Create IndexWriterConfiguration and store all of IW configuration there",
        "description": "I would like to factor out of all IW configuration parameters into a single configuration class, which I propose to name IndexWriterConfiguration (or IndexWriterConfig). I want to store there almost everything besides the Directory, and to reduce all the ctors down to one: IndexWriter(Directory, IndexWriterConfiguration). What I was thinking of storing there are the following parameters:\n* All of ctors parameters, except for Directory.\n* The different setters where it makes sense. For example I still think infoStream should be set on IW directly.\n\nI'm thinking that IWC should expose everything in a setter/getter methods, and defaults to whatever IW defaults today. Except for Analyzer which will need to be defined in the ctor of IWC and won't have a setter.\n\nI am not sure why MaxFieldLength is required in all IW ctors, yet IW declares a DEFAULT (which is an int and not MaxFieldLength). Do we still think that 10000 should be the default? Why not default to UNLIMITED and otherwise let the application decide what LIMITED means for it? I would like to make MFL optional on IWC and default to something, and I hope that default will be UNLIMITED. We can document that on IWC, so that if anyone chooses to move to the new API, he should be aware of that ...\n\nI plan to deprecate all the ctors and getters/setters and replace them by:\n* One ctor as described above\n* getIndexWriterConfiguration, or simply getConfig, which can then be queried for the setting of interest.\n* About the setters, I think maybe we can just introduce a setConfig method which will override everything that is overridable today, except for Analyzer. So someone could do iw.getConfig().setSomething(); iw.setConfig(newConfig);\n** The setters on IWC can return an IWC to allow chaining set calls ... so the above will turn into iw.setConfig(iw.getConfig().setSomething1().setSomething2()); \n\nBTW, this is needed for Parallel Indexing (see LUCENE-1879), but I think it will greatly simplify IW's API.\n\nI'll start to work on a patch.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-903",
        "summary": "FilteredQuery explanation inaccuracy with boost",
        "description": "The value of explanation is different than the product of its part if boost > 1.\nThis is exposed after tightening the explanation check (part of LUCENE-446).\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3306",
        "summary": "disable positions for spellchecker ngram fields",
        "description": "In LUCENE-2391 we optimized spellchecker (re)build time/ram usage by omitting frequencies/positions/norms for single-valued fields,\namong other things.\n\nNow that we can disable positions but keep freqs, we should disable them for the n-gram fields, because the spellchecker does\nnot use positional queries.\n",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3396",
        "summary": "Make TokenStream Reuse Mandatory for Analyzers",
        "description": "In LUCENE-2309 it became clear that we'd benefit a lot from Analyzer having to return reusable TokenStreams.  This is a big chunk of work, but its time to bite the bullet.\n\nI plan to attack this in the following way:\n\n- Collapse the logic of ReusableAnalyzerBase into Analyzer\n- Add a ReuseStrategy abstraction to Analyzer which controls whether the TokenStreamComponents are reused globally (as they are today) or per-field.\n- Convert all Analyzers over to using TokenStreamComponents.  I've already seen that some of the TokenStreams created in tests need some work to be reusable (even if they aren't reused).\n- Remove Analyzer.reusableTokenStream and convert everything over to using .tokenStream (which will now be returning reusable TokenStreams).",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2482",
        "summary": "Index sorter",
        "description": "A tool to sort index according to a float document weight. Documents with high weight are given low document numbers, which means that they will be first evaluated. When using a strategy of \"early termination\" of queries (see TimeLimitedCollector) such sorting significantly improves the quality of partial results.\n\n(Originally this tool was created by Doug Cutting in Nutch, and used norms as document weights - thus the ordering was limited by the limited resolution of norms. This is a pure Lucene version of the tool, and it uses arbitrary floats from a specified stored field).",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-350",
        "summary": "counter field in segments file is not documented in fileformats.xml",
        "description": "The counter field in the current segments file format is not documented.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-478",
        "summary": "CJK char list",
        "description": "Seems the character list in the CJK section of the StandardTokenizer.jj is not quite complete. Following is a more complete list:\n\n< CJK:                                          // non-alphabets\n      [\n\t   \"\\u1100\"-\"\\u11ff\",\n       \"\\u3040\"-\"\\u30ff\",\n       \"\\u3130\"-\"\\u318f\",\n       \"\\u31f0\"-\"\\u31ff\",\n       \"\\u3300\"-\"\\u337f\",\n       \"\\u3400\"-\"\\u4dbf\",\n       \"\\u4e00\"-\"\\u9fff\",\n       \"\\uac00\"-\"\\ud7a3\",\n       \"\\uf900\"-\"\\ufaff\",\n       \"\\uff65\"-\"\\uffdc\"       \n      ]\n  >\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-890",
        "summary": "Fix for small syntax omission in TermQuery documentation",
        "description": "A coding example, which could be cut'n'paste by a user, has unbalanced parenthesis.\n\nThis fix corrects the documentation, making no changes to functionality, only readability.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2437",
        "summary": "Indonesian Analyzer",
        "description": "This is an implementation of http://www.illc.uva.nl/Publications/ResearchReports/MoL-2003-02.text.pdf\n\nThe only change is that I added an option to disable derivational stemming, \nin case you want to just remove inflectional particles and possessive pronouns.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1153",
        "summary": "Lucene needs to ship the JUnit jar for testing",
        "description": "In order for Hudson builds, etc. to work properly, Lucene needs to ship the JUnit jar and have it made available in the testing classpath.  Our system reqs say 3.8.1, but I have 3.8.2 laying around, so I will update the system requirements, too.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2705",
        "summary": "TestThreadSafety.testLazyLoadThreadSafety test failure",
        "description": "TestThreadSafety.testLazyLoadThreadSafety failed with this error:\n\nunable to create new native thread\n\nMaybe because of SimpleText\n\nHere is the stacktrace:\n{noformat}\n [junit] Testsuite: org.apache.lucene.search.TestThreadSafe\n    [junit] Testcase: testLazyLoadThreadSafety(org.apache.lucene.search.TestThreadSafe):\tCaused an ERROR\n    [junit] unable to create new native thread\n    [junit] java.lang.OutOfMemoryError: unable to create new native thread\n    [junit] \tat java.lang.Thread.start0(Native Method)\n    [junit] \tat java.lang.Thread.start(Thread.java:614)\n    [junit] \tat org.apache.lucene.search.TestThreadSafe.doTest(TestThreadSafe.java:129)\n    [junit] \tat org.apache.lucene.search.TestThreadSafe.testLazyLoadThreadSafety(TestThreadSafe.java:148)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit] \n    [junit] \n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 6.051 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestThreadSafe -Dtestmethod=testLazyLoadThreadSafety -Dtests.seed=-277698010445513699:-89599297372877779\n    [junit] NOTE: test params are: codec=SimpleText, locale=zh_SG, timezone=Pacific/Tongatapu\n    [junit] ------------- ---------------- ---------------\n    [junit] TEST org.apache.lucene.search.TestThreadSafe FAILED\n{noformat}",
        "label": "NUG",
        "classified": "TEST",
        "type": "BUG"
    },
    {
        "key": "LUCENE-455",
        "summary": "FieldsReader does not regard offset and position flags",
        "description": "When creating a Field the FieldsReader looks at the storeTermVector flag of the FieldInfo. If true Field.TermVector.YES is used as parameter. But it should be checked if storeOffsetWithTermVector and storePositionWithTermVector are set and Field.TermVector.WITH_OFFSETS, ...WITH_POSITIONS, or ...WITH_POSITIONS_OFFSETS should be used as appropriate.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-884",
        "summary": "Query Syntax page does not make it clear that wildcard searches are not allowed in Phrase Queries",
        "description": "The queryparsersyntax page which is where I expect most novices (such as myself) start with lucene seems to indicate that wildcards can be used in phrase terms\n\nQuoting:\n'Terms: A query is broken up into terms and operators. There are two types of terms: Single Terms and Phrases.\nA Single Term is a single word such as \"test\" or \"hello\".\nA Phrase is a group of words surrounded by double quotes such as \"hello dolly\".\n\n....\n\nWildcard Searches\nLucene supports single and multiple character wildcard searches.\nTo perform a multiple character wildcard search use the \"*\" symbol.\nMultiple character wildcard searches looks for 0 or more characters. For example, to search for test, tests or tester, you can use the search:\n\ntest*\nYou can also use the wildcard searches in the middle of a term.\n\n'\nthere is nothing to indicate in the section on Wildcard Searches that it can be performed only on Single word terms not Phrase terms.\n\nChris  argues 'that there is nothing in the description of a Phrase to indicate that it can be anything other then what it says \"a group of words surrounded by double quotes\" .. at no point does it\nsuggest that other types of queries or syntax can be used inside the quotes.  likewise the discussion of Wildcards makes no mention of phrases to suggest that wildcard characters can be used in a phrase.'\nbut I don't accept this because there is nothing in the description of a Single Term either to indicate it can use wildcards either. Wildcards are only mentioned in the Wildcard section and there it says thay can be used in a term, it does not restrict the type of term\n\n\nI Propose a simple solution modify:\n\nLucene supports single and multiple character wildcard searches.\n\nto \n\nLucene supports single and multiple character wildcard searches within single terms.\n\n(Chris asked for a patch, but Im not sure how to do this, but the change is simple enough)\n\n\n\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-631",
        "summary": "GData Server - Milestone 3 Patch, Bugfixes, Documentation",
        "description": "For Milestone 3 added Features:\n\n- Update Delete Concurrency\n- Version control\n- Second storage impl. based on Db4o. (Distributed Storage)\n- moved all configuration in one single config file.\n- removed dependencies in testcases.\n- added schema validation for and all  xml files in the project (Configuration etc.)\n- added JavaDoc\n- much better Performance after reusing some resources\n- added recovering component to lucene based storage to recover entries after a server crash or OOM Error (very simple)\n\n- solved test case fail on hyperthread / multi core machines (@ hossman: give it a go)\n\n@Yonik && Doug could you get that stuff in the svn please\n\nregards simon\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2489",
        "summary": "promote TestExternalCodecs.PerFieldCodecWrapper to core",
        "description": "PerFieldCodecWrapper lets you set the Codec for each field; I'll promote to core & mark experimental.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3816",
        "summary": "FilteredDocIdSet does not handle a case where the inner set iterator is null",
        "description": "DocIdSet#iterator is allowed to return null, when used in FilteredDocIdSet, if null is returned from the inner set, the FilteredDocIdSetIterator fails since it does not allow for nulls to be passed to it.\n\nThe fix is simple, return null in FilteredDocIdSet in the iterator method is the iterator is null.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2415",
        "summary": "Remove JakarteRegExCapabilities shim to access package protected field",
        "description": "To access the prefix in Jakarta RegExes we use a shim class in the same package as jakarta. I will remove this and replace by reflection like Robert does in his ICUTokenizer rule compiler.\n\nShim classes have the problem wth signed artifacts, as you cannot insert a new class into a foreign package if you sign regex classes.\n\nThis shim-removal also allows users to use later jakarta regex versions, if they are in classpath and cannot be removed (even if they have bugs). Performance is no problem, as the prefix is only get once per TermEnum.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2062",
        "summary": "Bulgarian Analyzer",
        "description": "someone asked about bulgarian analysis on solr-user today... http://www.lucidimagination.com/search/document/e1e7a5636edb1db2/non_english_languages\nI was surprised we did not have anything.\n\nThis analyzer implements the algorithm specified here, http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf\n\nIn the measurements there, this improves MAP approx 34%\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-918",
        "summary": "Interface TermFreqVector has incomplete Javadocs",
        "description": "We should improve the Javadocs of org.apache.lucene.index.TermFreqVector",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-236",
        "summary": "typos on FAQ",
        "description": "I found out the following typos on the FAQ (http://lucene.sourceforge.net/cgi-\nbin/faq/faqmanager.cgi) of lucene:\nin 8. Will Lucene work with my Java application ?\n- felxible\n- applciations",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2814",
        "summary": "stop writing shared doc stores across segments",
        "description": "Shared doc stores enables the files for stored fields and term vectors to be shared across multiple segments.  We've had this optimization since 2.1 I think.\n\nIt works best against a new index, where you open an IW, add lots of docs, and then close it.  In that case all of the written segments will reference slices a single shared doc store segment.\n\nThis was a good optimization because it means we never need to merge these files.  But, when you open another IW on that index, it writes a new set of doc stores, and then whenever merges take place across doc stores, they must now be merged.\n\nHowever, since we switched to shared doc stores, there have been two optimizations for merging the stores.  First, we now bulk-copy the bytes in these files if the field name/number assignment is \"congruent\".  Second, we now force congruent field name/number mapping in IndexWriter.  This means this optimization is much less potent than it used to be.\n\nFurthermore, the optimization adds *a lot* of hair to IndexWriter/DocumentsWriter; this has been the source of sneaky bugs over time, and causes odd behavior like a merge possibly forcing a flush when it starts.  Finally, with DWPT (LUCENE-2324), which gets us truly concurrent flushing, we can no longer share doc stores.\n\nSo, I think we should turn off the write-side of shared doc stores to pave the path for DWPT to land on trunk and simplify IW/DW.  We still must support reading them (until 5.0), but the read side is far less hairy.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1798",
        "summary": "FieldCacheSanityChecker called directly by FieldCache.get*",
        "description": "As suggested by McCandless in LUCENE-1749, we can make FieldCacheImpl a client of the FieldCacheSanityChecker and have it sanity check itself each time it creates a new cache entry, and log a warning if it thinks there is a problem.  (although we'd probably only want to do this if the caller has set some sort of infoStream/warningStream type property on the FieldCache object.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2039",
        "summary": "Regex support and beyond in JavaCC QueryParser",
        "description": "Since the early days the standard query parser was limited to the queries living in core, adding other queries or extending the parser in any way always forced people to change the grammar file and regenerate. Even if you change the grammar you have to be extremely careful how you modify the parser so that other parts of the standard parser are affected by customisation changes. Eventually you had to live with all the limitation the current parser has like tokenizing on whitespaces before a tokenizer / analyzer has the chance to look at the tokens. \nI was thinking about how to overcome the limitation and add regex support to the query parser without introducing any dependency to core. I added a new special character that basically prevents the parser from interpreting any of the characters enclosed in the new special characters. I choose the forward slash  '/' as the delimiter so that everything in between two forward slashes is basically escaped and ignored by the parser. All chars embedded within forward slashes are treated as one token even if it contains other special chars like * []?{} or whitespaces. This token is subsequently passed to a pluggable \"parser extension\" with builds a query from the embedded string. I do not interpret the embedded string in any way but leave all the subsequent work to the parser extension. Such an extension could be another full featured query parser itself or simply a ctor call for regex query. The interface remains quiet simple but makes the parser extendible in an easy way compared to modifying the javaCC sources.\n\nThe downsides of this patch is clearly that I introduce a new special char into the syntax but I guess that would not be that much of a deal as it is reflected in the escape method though. It would truly be nice to have more than once extension an have this even more flexible so treat this patch as a kickoff though.\n\nAnother way of solving the problem with RegexQuery would be to move the JDK version of regex into the core and simply have another method like:\n{code}\nprotected Query newRegexQuery(Term t) {\n  ... \n}\n{code}\n\nwhich I would like better as it would be more consistent with the idea of the query parser to be a very strict and defined parser.\n\nI will upload a patch in a second which implements the extension based approach I guess I will add a second patch with regex in core soon too.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1103",
        "summary": "WikipediaTokenizer",
        "description": "I have extended StandardTokenizer to recognize Wikipedia syntax and mark tokens with certain attributes.  It isn't necessarily complete, but it does a good enough job for it to be consumed and improved by others.\n\nIt sets the Token.type() value depending on the Wikipedia syntax (links, internal links, bold, italics, etc.) based on my pass at http://en.wikipedia.org/wiki/Wikipedia:Tutorial\n\nI have only tested it with the benchmarking EnwikiDocMaker wikipedia stuff and it seems to do a decent job.\n\nCaveats:  I am not sure how to best handle testing, since the content is licensed under GNU Free Doc License, I believe I can't copy and paste a whole document into the unit test.  I have hand coded one doc and have another one that just generally runs over the benchmark Wikipedia download.\n\nOne more question is where to put it.  It could go in analysis, but the tests at least will have a dependency on Benchmark.  I am thinking of adding a new contrib/wikipedia where this could grow to have other wikipedia things (perhaps we would move EnwikiDocMaker there????) and reverse the dependency on Benchmark.\n\nI will post a patch over the next few days.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-787",
        "summary": "ant test won't run in 'out of the box' installation",
        "description": "one possible solution would be to remove 'lib' from the junit.classpath",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3112",
        "summary": "Add IW.add/updateDocuments to support nested documents",
        "description": "I think nested documents (LUCENE-2454) is a very compelling addition\nto Lucene.  It's also a popular (many votes) issue.\n\nBeyond supporting nested document querying, which is already an\nincredible addition since it preserves the relational model on\nindexing normalized content (eg, DB tables, XML docs), LUCENE-2454\nshould also enable speedups in grouping implementation when you group\nby a nested field.\n\nFor the same reason, it can also enable very fast post-group facet\ncounting impl (LUCENE-3097) when you what to\ncount(distinct(nestedField)), instead of unique documents, as your\n\"identifier\".  I expect many apps that use faceting need this ability\n(to count(distinct(nestedField)) not distinct(docID)).\n\nTo support these use cases, I believe the only core change needed is\nthe ability to atomically add or update multiple documents, which you\ncannot do today since in between add/updateDocument calls a flush (eg\ndue to commit or getReader()) could occur.\n\nThis new API (addDocuments(Iterable<Document>), updateDocuments(Term\ndelTerm, Iterable<Document>) would also further guarantee that the\ndocuments are assigned sequential docIDs in the order the iterator\nprovided them, and that the docIDs all reside in one segment.\n\nSegment merging never splits segments apart, so this invariant would\nhold even as merges/optimizes take place.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1670",
        "summary": "Cosmetic JavaDoc updates",
        "description": "I've taken the liberty of making a few cosmetic updates to various JavaDocs:\n\n* MergePolicy (minor cosmetic change)\n* LogMergePolicy (minor cosmetic change)\n* IndexWriter (major cleanup in class description, changed anchors to JavaDoc links [now works in Eclipse], no content change)\n\nAttached diff from SVN r780545.\n\nI would appreciate if whomever goes over this can let me know if my issue parameter choices were correct (yeah, blame my OCD), and if there's a more practical/convenient way to send these in, please let me know :-)\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1514",
        "summary": "ShingleMatrixFilter eaily throws StackOverFlow as the complexity of a matrix grows",
        "description": "ShingleMatrixFilter#next makes a recursive function invocation when the current permutation iterator is exhausted or if the current state of the permutation iterator already has produced an identical shingle. In a not too complex matrix this will require a gigabyte sized stack per thread.\n\nMy solution is to avoid the recursive invocation by refactoring like this:\n\n{code:java}\npublic Token next(final Token reusableToken) throws IOException {\n    assert reusableToken != null;\n    if (matrix == null) {\n      matrix = new Matrix();\n      // fill matrix with maximumShingleSize columns\n      while (matrix.columns.size() < maximumShingleSize && readColumn()) {\n        // this loop looks ugly\n      }\n    }\n\n    // this loop exists in order to avoid recursive calls to the next method\n    // as the complexity of a large matrix\n    // then would require a multi gigabyte sized stack.\n    Token token;\n    do {\n      token = produceNextToken(reusableToken);\n    } while (token == request_next_token);\n    return token;\n  }\n\n  \n  private static final Token request_next_token = new Token();\n\n  /**\n   * This method exists in order to avoid reursive calls to the method\n   * as the complexity of a fairlt small matrix then easily would require\n   * a gigabyte sized stack per thread.\n   *\n   * @param reusableToken\n   * @return null if exhausted, instance request_next_token if one more call is required for an answer, or instance parameter resuableToken.\n   * @throws IOException\n   */\n  private Token produceNextToken(final Token reusableToken) throws IOException {\n\n{code}\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-269",
        "summary": "[PATCH] demo HTML parser corrupts foreign characters",
        "description": "We are using HTML parser for parsing English and other NL documents in \nEclipse.  Post Lucene 1.2 there has been a regression in the parser.  \nCharacters coming from Reader (obtained from getReader() ) are corrupted.  \nOnly the characters that can be encoded using the default machine encoding go \nthrough correctly.  For example, parsing Chinese document on an English \nmachine results with all characters, except the few English words, corrupted.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1795",
        "summary": "New QueryParser should not allow leading wildcard by default",
        "description": "The current QueryParser disallows leading wildcard characters by default.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1483",
        "summary": "Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector",
        "description": "This issue changes how an IndexSearcher searches over multiple segments. The current method of searching multiple segments is to use a MultiSegmentReader and treat all of the segments as one. This causes filters and FieldCaches to be keyed to the MultiReader and makes reopen expensive. If only a few segments change, the FieldCache is still loaded for all of them.\n\nThis patch changes things by searching each individual segment one at a time, but sharing the HitCollector used across each segment. This allows FieldCaches and Filters to be keyed on individual SegmentReaders, making reopen much cheaper. FieldCache loading over multiple segments can be much faster as well - with the old method, all unique terms for every segment is enumerated against each segment - because of the likely logarithmic change in terms per segment, this can be very wasteful. Searching individual segments avoids this cost. The term/document statistics from the multireader are used to score results for each segment.\n\nWhen sorting, its more difficult to use a single HitCollector for each sub searcher. Ordinals are not comparable across segments. To account for this, a new field sort enabled HitCollector is introduced that is able to collect and sort across segments (because of its ability to compare ordinals across segments). This TopFieldCollector class will collect the values/ordinals for a given segment, and upon moving to the next segment, translate any ordinals/values so that they can be compared against the values for the new segment. This is done lazily.\n\nAll and all, the switch seems to provide numerous performance benefits, in both sorted and non sorted search. We were seeing a good loss on indices with lots of segments (1000?) and certain queue sizes / queries, but the latest results seem to show thats been mostly taken care of (you shouldnt be using such a large queue on such a segmented index anyway).\n\n* Introduces\n** MultiReaderHitCollector - a HitCollector that can collect across multiple IndexReaders. Old HitCollectors are wrapped to support multiple IndexReaders.\n** TopFieldCollector - a HitCollector that can compare values/ordinals across IndexReaders and sort on fields.\n** FieldValueHitQueue - a Priority queue that is part of the TopFieldCollector implementation.\n** FieldComparator - a new Comparator class that works across IndexReaders. Part of the TopFieldCollector implementation.\n** FieldComparatorSource - new class to allow for custom Comparators.\n* Alters\n** IndexSearcher uses a single HitCollector to collect hits against each individual SegmentReader. All the other changes stem from this ;)\n* Deprecates\n** TopFieldDocCollector\n** FieldSortedHitQueue\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2451",
        "summary": "remove dead code from oal.util.cache",
        "description": "We have dead cache impls in oal.util.cache*; we only use DBLRUCache.\n\nThese are internal APIs; I'd like to remove all but DBLRUcache.",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1965",
        "summary": "Lazy Atomic Loading Stopwords in SmartCN ",
        "description": "The default constructor in SmartChineseAnalyzer loads the default (jar embedded) stopwords each time the constructor is invoked. \nThis should be atomically loaded only once in an unmodifiable set.\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1629",
        "summary": "contrib intelligent Analyzer for Chinese",
        "description": "I wrote a Analyzer for apache lucene for analyzing sentences in Chinese language. it's called \"imdict-chinese-analyzer\", the project on google code is here: http://code.google.com/p/imdict-chinese-analyzer/\n\nIn Chinese, \"\u6211\u662f\u4e2d\u56fd\u4eba\"(I am Chinese), should be tokenized as \"\u6211\"(I)   \"\u662f\"(am)   \"\u4e2d\u56fd\u4eba\"(Chinese), not \"\u6211\" \"\u662f\u4e2d\" \"\u56fd\u4eba\". So the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by Lucene, and the accuracy of the search engine will be affected seriously!\n\nAlthough there are two analyzer packages in apache repository which can handle Chinese: ChineseAnalyzer and CJKAnalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly.\n\nThe algorithm of imdict-chinese-analyzer is based on Hidden Markov Model (HMM), so it can tokenize chinese sentence in a really intelligent way. Tokenizaion accuracy of this model is above 90% according to the paper \"HHMM-based Chinese Lexical analyzer ICTCLAL\" while other analyzer's is about 60%.\n\nAs imdict-chinese-analyzer is a really fast and intelligent. I want to contribute it to the apache lucene repository.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1883",
        "summary": "Fix typos in CHANGES.txt and contrib/CHANGES.txt prior to 2.9 release",
        "description": "I noticed a few typos in CHANGES.txt and contrib/CHANGES.txt.  (Once they make it past a release, they're set in stone...)\n\nWill attach a patch shortly.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2808",
        "summary": "Intermitted failure on DocValues branch",
        "description": "I lately ran into two random failures on the CSF branch that seem not to be related to docValues but I can't reproduce them neither on docvalues branch nor on trunk.\n\n{code}\njError Message\n\nIndexFileDeleter doesn't know about file _1e.tvx\nStacktrace\n\njunit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx\n\tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)\n\tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)\n\tat org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)\n\tat org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)\n\tat org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)\n\tat org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)\n\tat org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)\n\tat org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)\n\tat org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)\nStandard Output\n\nNOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-6528669668419768890:4860241142852689334 -Dtests.codec=randomPerField -Dtests.multiplier=3\nNOTE: test params are: codec=PreFlex, locale=sv, timezone=Atlantic/South_Georgia\nStandard Error\n\nNOTE: all tests run in this JVM:\n[TestDemo, TestToken, TestBinaryDocument, TestCodecs, TestDirectoryReader, TestIndexInput, TestIndexWriterExceptions]\n{code}\n\nand\n\n{code}\n\n[junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen\n    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):\tCaused an ERROR\n    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}\n    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:387)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:859)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)\n    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:342)\n    [junit] \tat org.apache.lucene.store.Directory.openInput(Directory.java:122)\n    [junit] \tat org.apache.lucene.index.codecs.standard.StandardPostingsReader.<init>(StandardPostingsReader.java:49)\n    [junit] \tat org.apache.lucene.index.codecs.standard.StandardCodec.fieldsProducer(StandardCodec.java:87)\n    [junit] \tat org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:119)\n    [junit] \tat org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:211)\n    [junit] \tat org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:137)\n    [junit] \tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:532)\n    [junit] \tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:509)\n    [junit] \tat org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:238)\n    [junit] \tat org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:500)\n    [junit] \tat org.apache.lucene.index.DirectoryReader.access$000(DirectoryReader.java:48)\n    [junit] \tat org.apache.lucene.index.DirectoryReader$2.doBody(DirectoryReader.java:493)\n    [junit] \tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:623)\n    [junit] \tat org.apache.lucene.index.DirectoryReader.doReopenNoWriter(DirectoryReader.java:488)\n    [junit] \tat org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:446)\n    [junit] \tat org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:406)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:770)\n    [junit] \tat org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:897)\n    [junit] \n    [junit] \n    [junit] Tests run: 17, Failures: 0, Errors: 1, Time elapsed: 13.766 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexReaderReopen -Dtestmethod=testThreadSafety -Dtests.seed=-5455993123574190959:-1935535300313439968 -Dtests.codec=randomPerField -Dtests.multiplier=3\n    [junit] NOTE: test params are: codec=RandomCodecProvider: {field5=MockVariableIntBlock(baseBlockSize=29), id=Standard, fielda=Standard, field4=MockFixedIntBlock(blockSize=924), field3=Standard, field2=SimpleText, id2=Standard, field6=MockSep, field1=Pulsing(freqCutoff=8)}, locale=zh_CN, timezone=Asia/Hovd\n{code}\n\nI haven't seen those before - let me know if you have!",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2767",
        "summary": "Missing sync in IndexWriter.addIndexes(IndexReader[])",
        "description": "The 3.x build just hit this:\n\n{noformat}\n    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes\n    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):\tFAILED\n    [junit] expected:<3160> but was:<2701>\n    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<2701>\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:779)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:745)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:708)\n    [junit] \n    [junit] \n    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 9.28 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] java.lang.AssertionError: RefCount is 0 pre-decrement for file \"_8a.tvf\"\n    [junit] \tat org.apache.lucene.index.IndexFileDeleter$RefCount.DecRef(IndexFileDeleter.java:608)\n    [junit] \tat org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:505)\n    [junit] \tat org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:496)\n    [junit] \tat org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2972)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes.doBody(TestAddIndexes.java:681)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:624)\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=-6912763261803132408:-5575674032550262483 -Dtests.multiplier=3\n    [junit] NOTE: test params are: locale=en_AU, timezone=America/Atka\n{noformat}\n\nIt looks like it's caused by a long-standing missing sync (since at least 2.9.x).  I think likely we hit it just now thanks to adding random Thread.yield()'s in MockDirWrapper!",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1045",
        "summary": "SortField.AUTO doesn't work with long",
        "description": "This is actually the same as LUCENE-463 but I cannot find a way to re-open that issue. I'm attaching a test case by dragon-fly999 at hotmail com that shows the problem and a patch that seems to fix it.\n\nThe problem is that a long (as used for dates) cannot be parsed as an integer, and the next step is then to parse it as a float, which works but which is not correct. With the patch the following parsers are used in this order: int, long, float.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1020",
        "summary": "Basic tool for checking & repairing an index",
        "description": "This has been requested a number of times on the mailing lists.  Most\nrecently here:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/53474\n\nI think we should provide a basic tool out of the box.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1092",
        "summary": "KeywordTokenizer/Analyzer cannot be re-used",
        "description": "\nThe new reusableTokenStream API in KeywordAnalyzer fails to reset the tokenizer when it re-uses it.\n\nThis issue came from this thread:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/55929\n\nThanks to Hideaki Takahashi for finding this!",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-573",
        "summary": "Escaped quotes inside a phrase cause a ParseException",
        "description": "QueryParser cannot handle escaped quotes when inside a phrase. Escaped quotes not in a phrase are not a problem. This can be added to TestQueryParser.testEscaped() to demonstrate the issue - the second assert throws an exception:\n\nassertQueryEquals(\"a \\\\\\\"b c\\\\\\\" d\", a, \"a \\\"b c\\\" d\");\nassertQueryEquals(\"\\\"a \\\\\\\"b c\\\\\\\" d\\\"\", a, \"\\\"a \\\"b c\\\" d\\\"\");\n\nSee also this thread:\nhttp://www.nabble.com/ParseException-with-escaped-quotes-in-a-phrase-t1647115.html\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3064",
        "summary": "add checks to MockTokenizer to enforce proper consumption",
        "description": "we can enforce things like consumer properly iterates through tokenstream lifeycle\nvia MockTokenizer. this could catch bugs in consumers that don't call reset(), etc.",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3403",
        "summary": "Term vectors missing after addIndexes + optimize",
        "description": "I encountered a problem with addIndexes where term vectors disappeared following optimize(). I wrote a simple test case which demonstrates the problem. The bug appears with both addIndexes() versions, but does not appear if addDocument is called twice, committing changes in between.\n\nI think I tracked the problem down to IndexWriter.mergeMiddle() -- it sets term vectors before merger.merge() was called. In the addDocs case, merger.fieldInfos is already populated, while in the addIndexes case it is empty, hence fieldInfos.hasVectors returns false.\n\nwill post a patch shortly.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1440",
        "summary": "Add ability to run backwards-compatibility tests automatically",
        "description": "This is an idea Doug mentioned on LUCENE-1422.\n\nThis patch adds new targets to build.xml to automatically download the junit tests from a previous Lucene release and run them against the current core.\nExecute tests like this:\nant -Dtag=lucene_2_4_0 test-tag\n\nIt will create a new directory tags/lucene_2_4_0 and fetch the tests from the svn repository and run them.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-1005",
        "summary": "GData TestDateFormater (sic) fails when the Date returned is: Sun, 23 Sep 2007 01:29:06 GMT+00:00",
        "description": "TestDateFormater.testFormatDate fails when the Date returned is Sun, 23 Sep 2007 01:29:06 GMT+00:00\n\nThe issue lies with the +00:00 at the end of the string.  \n\nThe question is, though, is that a valid date for GData?\n\nThis is marked as major b/c it is causing nightly builds to fail.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1748",
        "summary": "getPayloadSpans on org.apache.lucene.search.spans.SpanQuery should be abstract",
        "description": "I just spent a long time tracking down a bug resulting from upgrading to Lucene 2.4.1 on a project that implements some SpanQuerys of its own and was written against 2.3.  Since the project's SpanQuerys didn't implement getPayloadSpans, the call to that method went to SpanQuery.getPayloadSpans which returned null and caused a NullPointerException in the Lucene code, far away from the actual source of the problem.  \n\nIt would be much better for this kind of thing to show up at compile time, I think.\n\nThanks!",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-337",
        "summary": "Combination of BooleanQuery and PhrasePrefixQuery can provoke UnsupportedOperationException",
        "description": "A BooleanQuery including a PhrasePrefixQuery can cause an exception to be thrown\nfrom BooleanScorer#skipTo when the search is executed:  \n\njava.lang.UnsupportedOperationException\n\tat org.apache.lucene.search.BooleanScorer.skipTo(BooleanScorer.java:189)\n\tat org.apache.lucene.search.ConjunctionScorer.doNext(ConjunctionScorer.java:53)\n\tat org.apache.lucene.search.ConjunctionScorer.next(ConjunctionScorer.java:48)\n\tat org.apache.lucene.search.Scorer.score(Scorer.java:37)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)\n\tat org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)\n\tat org.apache.lucene.search.Hits.<init>(Hits.java:43)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:33)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:27)\n        ... (non-lucene code)\n\nThe problem appears to be that PhrasePrefixQuery optimizes itself into a\nBooleanQuery when it contains only one term.  However, it does this in the\ncreateWeight() method of its scorer instead of in the rewrite method of the\nquery itself.  Thus it bypasses the boolean typecheck when BooleanQuery is\ndeciding whether to use ConjunctionScorer or BooleanScorer, eventually resulting\nin the UOE.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1718",
        "summary": "IndexReader.setTermInfosIndexDivisor doesn't carry over to reopened readers",
        "description": "When you reopen a reader, some segments are shared (and thus properly inherit the index divisor) but others are newly opened and use the default index divisor.  You then have no way to change the index divisor of those newly opened ones.  The only workaround is to not use reopen (always open a new reader).\n\nI'd like to make termInfosDivisor an up-front param to IndexReader, anyway, for LUCENE-1609, so likely I'll fix both of these issues there.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1878",
        "summary": "remove deprecated classes from spatial",
        "description": "spatial has not been released, so we can remove the deprecated classes",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1810",
        "summary": "Add a LATENT FieldSelectorResult",
        "description": "I propose adding LATENT FieldSelectorResult\n\nthis would be similar to LAZY_LOAD except that it would NEVER cache the stored value\n\nThis will be useful for very large fields that should always go direct to disk (because they will take so much memory)\nwhen caching Documents returned from a Searcher, the large field may be initially requested as LAZY_LOAD, however once someone reads this field, it will then get locked into memory. if this Document (and others like it) are cached, it can start to use a very large amount of memory for these fields\n\nContract for FieldSelectorResult.LATENT should be that it will always be pulled direct from the IndexInput and never be persisted in memory as part of a Fieldable\n\nI could prepare a patch if desired\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-2076",
        "summary": "Add org.apache.lucene.store.FSDirectory.getDirectory()",
        "description": "On the Apache Lucene.Net side, we have done some clean up with the upcoming 2.9.1 such that we are now depreciating improperly use of parameter type for some public APIs.  When we release 3.0, those depreciated code will be removed.\n\nOne area where we had difficulty with required us to add a new method like so: Lucene.Net.Store.FSDirectory.GetDirectory().  This method does the same thing as Lucene.Net.Store.FSDirectory.GetFile().  This was necessary because we switched over from using System.IO.FileInfo to System.IO.DirectoryInfo.  Why?  In the .NET world, a file and a directory are two different things.\n\nWhy did we have to add Lucene.Net.Store.FSDirectory.GetDirectory()?  Because we can't change the return type of Lucene.Net.Store.FSDirectory.GetFile() and still remain backward compatible (API wise) to be depreciated with the next release.\n\nWhy ask for Java Lucene to add org.apache.lucene.store.FSDirectory.getDirectory()?  To keep the APIs 1-to-1 in par with Java Lucene and Lucene.Net.",
        "label": "NUG",
        "classified": "RFE",
        "type": ""
    },
    {
        "key": "LUCENE-3665",
        "summary": "Make WeightedSpanTermExtractor extensible to handle custom query implemenations",
        "description": "Currently if I have a custom query which subclasses query directly I can't use the QueryScorer for highlighting since it does explicit instanceof checks. In some cases its is possible to rewrite the query before passing it to the highlighter to obtain a primitive query. However I had the usecase where this was not possible ie. the original index was not available on the machine which highlights the results. To still use the highlighter I had to copy a bunch of code due to visibility issues in those classes. I think we can make this extensible with minor effort to allow this usecase without massive code duplication.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-341",
        "summary": "The deprecated constructor of BooleanClause does not set new state",
        "description": "Nick Burch reported this on lucene-user. \n \nPatch will follow. \n \nRegards, \nPaul Elschot",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2310",
        "summary": "Reduce Fieldable, AbstractField and Field complexity",
        "description": "In order to move field type like functionality into its own class, we really need to try to tackle the hierarchy of Fieldable, AbstractField and Field.  Currently AbstractField depends on Field, and does not provide much more functionality that storing fields, most of which are being moved over to FieldType.  Therefore it seems ideal to try to deprecate AbstractField (and possible Fieldable), moving much of the functionality into Field and FieldType.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": ""
    },
    {
        "key": "LUCENE-1404",
        "summary": "NPE in NearSpansUnordered.isPayloadAvailable() ",
        "description": "Using RC1 of lucene 2.4 resulted in null pointer exception with some constructed SpanNearQueries\n\nImplementation of isPayloadAvailable() (results in exception)\n{code}\n public boolean isPayloadAvailable() {\n   SpansCell pointer = min();\n   do {\n     if(pointer.isPayloadAvailable()) {\n       return true;\n     }\n     pointer = pointer.next;\n   } while(pointer.next != null);\n\n   return false;\n  }\n{code}\n\n\"Fixed\" isPayloadAvailable()\n{code}\n public boolean isPayloadAvailable() {\n   SpansCell pointer = min();\n   while (pointer != null) {\n     if(pointer.isPayloadAvailable()) {\n       return true;\n     }\n     pointer = pointer.next;\n   }\n\n   return false;\n  }\n{code}\n\nException produced:\n{code}\n  [junit] java.lang.NullPointerException\n    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered$SpansCell.access$300(NearSpansUnordered.java:65)\n    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered.isPayloadAvailable(NearSpansUnordered.java:235)\n    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.shrinkToAfterShortestMatch(NearSpansOrdered.java:246)\n    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.advanceAfterOrdered(NearSpansOrdered.java:154)\n    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.next(NearSpansOrdered.java:122)\n    [junit]     at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:54)\n    [junit]     at org.apache.lucene.search.Scorer.score(Scorer.java:57)\n    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:137)\n    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:113)\n    [junit]     at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)\n    [junit]     at org.apache.lucene.search.Hits.<init>(Hits.java:80)\n    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:50)\n    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:40)\n    [junit]     at com.attivio.lucene.SpanQueryTest.search(SpanQueryTest.java:79)\n    [junit]     at com.attivio.lucene.SpanQueryTest.assertHitCount(SpanQueryTest.java:75)\n    [junit]     at com.attivio.lucene.SpanQueryTest.test(SpanQueryTest.java:67)\n{code}\n\nwill attach unit test that causes exception (and passes with updated isPayloadAvailable())\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3571",
        "summary": "nuke IndexSearcher(directory)",
        "description": "IndexSearcher is supposed to be a cheap wrapper around a reader,\nbut sometimes it is, sometimes it isn't.\n\nI think its confusing tangling of a heavyweight and lightweight\nobject that it sometimes 'houses' a reader and must close it in that case.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "TASK"
    },
    {
        "key": "LUCENE-262",
        "summary": "SegmentReader.hasSeparateNorms always returns false",
        "description": "The loop in that method looks like this: \n \nfor(int i = 0; i < 0; i++){ \n \nI guess \"i < 0\" should be replaced by \"i < result.length\"?",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-812",
        "summary": "Unable to set LockFactory implementation via ${org.apache.lucene.store.FSDirectoryLockFactoryClass}",
        "description": "While trying to move from Lucene 2.0 to Lucene 2.1 I noticed a problem with the LockFactory instantiation code.\nDuring previous tests we successfully specified the LockFactory implementation by setting the property\n${org.apache.lucene.store.FSDirectoryLockFactoryClass} to \"org.apache.lucene.store.NativeFSLockFactory\".\nThis does no longer work due to a bug in the FSDirectory class. The problem is caused from the fact that this\nclass tries to invoke the default constructor of the specified LockFactory class. However neither NativeFSLockFactory\nnor SimpleFSLockFactory do have a default constructor.\n\nFSDirectory, Line 285:\n          try {\n            lockFactory = (LockFactory) c.newInstance();          \n          } catch (IllegalAccessException e) {\n            throw new IOException(\"IllegalAccessException when instantiating LockClass \" + lockClassName);\n          } catch (InstantiationException e) {\n            throw new IOException(\"InstantiationException when instantiating LockClass \" + lockClassName);\n          } catch (ClassCastException e) {\n            throw new IOException(\"unable to cast LockClass \" + lockClassName + \" instance to a LockFactory\");\n          }\n\nA possible workaround is to not set the property at all and call FSDirectory.setLockFactory(...) instead. ",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1082",
        "summary": "IndexReader.lastModified - throws NPE",
        "description": "IndexReader.lastModified(String dir) or its variants always return NPE on 2.3, perhaps something to do with SegmentInfo.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1831",
        "summary": "TokenWrapperAttributeFactory, CachingWrapperFilterHelper implements equals and so should also implement hashCode",
        "description": "its part of the contract of Object \n\nbq. If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result.",
        "label": "NUG",
        "classified": "DESIGN_DEFECT",
        "type": "BUG"
    },
    {
        "key": "LUCENE-789",
        "summary": "Custom similarity is ignored when using MultiSearcher",
        "description": "Symptoms:\nI am using Searcher.setSimilarity() to provide a custom similarity that turns off tf() factor. However, somewhere along the way the custom similarity is ignored and the DefaultSimilarity is used. I am using MultiSearcher and BooleanQuery.\n\nProblem analysis:\nThe problem seems to be in MultiSearcher.createWeight(Query) method. It creates an instance of CachedDfSource but does not set the similarity. As the result CachedDfSource provides DefaultSimilarity to queries that use it.\n\nPotential solution:\nAdding the following line:\n    cacheSim.setSimilarity(getSimilarity());\nafter creating an instance of CacheDfSource (line 312) seems to fix the problem. However, I don't understand enough of the inner workings of this class to be absolutely sure that this is the right thing to do.\n\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2981",
        "summary": "Review and potentially remove unused/unsupported Contribs",
        "description": "Some of our contribs appear to be lacking for development/support or are missing tests.  We should review whether they are even pertinent these days and potentially deprecate and remove them.\n\nOne of the things we did in Mahout when bringing in Colt code was to mark all code that didn't have tests as @deprecated and then we removed the deprecation once tests were added.  Those that didn't get tests added over about a 6 mos. period of time were removed.\n\nI would suggest taking a hard look at:\nant\ndb\nlucli\nswing\n\n(spatial should be gutted to some extent and moved to modules)",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-727",
        "summary": "MMapDirectory can't create new index on Windows",
        "description": "When I set the system property to request the use of the mmap directory, and start building a large index, the process dies with an IOException trying to delete a file. Apparently, Lucene isn't closing down the memory map before deleting the file.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2678",
        "summary": "TestCachingSpanFilter sometimes fails",
        "description": "if I run \n{noformat} \nant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146 -Dtests.iter=100\n{noformat} \n\nI get two failures on my machine against current trunk\n{noformat} \n\njunit-sequential:\n    [junit] Testsuite: org.apache.lucene.search.TestCachingSpanFilter\n    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):\tFAILED\n    [junit] expected:<2> but was:<3>\n    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit] \tat org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)\n    [junit] \n    [junit] \n    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):\tFAILED\n    [junit] expected:<2> but was:<3>\n    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)\n    [junit] \tat org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)\n    [junit] \n    [junit] \n    [junit] Tests run: 100, Failures: 2, Errors: 0, Time elapsed: 2.297 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146\n    [junit] NOTE: test params are: codec=MockVariableIntBlock(baseBlockSize=43), locale=fr, timezone=Africa/Bangui\n    [junit] ------------- ---------------- ---------------\n    [junit] Test org.apache.lucene.search.TestCachingSpanFilter FAILED\n{noformat}\n\nnot sure what it is but it seems likely to be a WeakRef / GC issue in the cache. ",
        "label": "NUG",
        "classified": "TEST",
        "type": "TEST"
    },
    {
        "key": "LUCENE-3210",
        "summary": "TieredMergePolicy should expose control over how aggressively segments with deletions are targeted ",
        "description": "TMP today always does a linear pro-rating of a merge's score according to what pctg of the documents are deleted; I'd like to 1) put a power factor in (score is multiplicative), and 2) default it to stronger favoring of merging away deletions.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1687",
        "summary": "Remove ExtendedFieldCache by rolling functionality into FieldCache",
        "description": "It is silly that we have ExtendedFieldCache.  It is a workaround to our supposed back compatibility problem.  This patch will merge the ExtendedFieldCache interface into FieldCache, thereby breaking back compatibility, but creating a much simpler API for FieldCache.",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2884",
        "summary": "StandardCodec sometimes supplies skip pointers past EOF",
        "description": "Pretty sure this is 4.0-only:\nI added an assertion, the test to reproduce is:\n\nant test-core -Dtestcase=TestPayloadNearQuery -Dtestmethod=testMinFunction -Dtests.seed=4841190615781133892:3888521539169738727 -Dtests.multiplier=3\n\n{noformat}\n    [junit] Testcase: testMinFunction(org.apache.lucene.search.payloads.TestPayloadNearQuery):  FAILED\n    [junit] invalid skip pointer: 404, length=337\n    [junit] junit.framework.AssertionFailedError: invalid skip pointer: 404, length=337\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1127)\n    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1059)\n    [junit]     at org.apache.lucene.index.codecs.MultiLevelSkipListReader.init(MultiLevelSkipListReader.java:176)\n    [junit]     at org.apache.lucene.index.codecs.standard.DefaultSkipListReader.init(DefaultSkipListReader.java:50)\n    [junit]     at org.apache.lucene.index.codecs.standard.StandardPostingsReader$SegmentDocsAndPositionsAndPayloadsEnum.advance(StandardPostingsReader.java:742)\n    [junit]     at org.apache.lucene.search.spans.TermSpans.skipTo(TermSpans.java:72)\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3559",
        "summary": "remove IndexSearcher.docFreq/maxDoc",
        "description": "As pointed out by Mark on SOLR-1632, these are no longer used by the scoring system.\n\nWe've added new stats to Lucene, so having these methods on indexsearcher makes no sense.\nIts confusing to people upgrading if they subclassed IndexSearcher to provide distributed stats,\nonly to find these are not used (LUCENE-3555 has a correct API for them to do this).\n\nSo I think we should remove these in 4.0.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-786",
        "summary": "Extended javadocs in spellchecker",
        "description": "Added some javadocs that explains why the spellchecker does not work as one might expect it to.\n\nhttp://www.nabble.com/SpellChecker%3A%3AsuggestSimilar%28%29-Question-tf3118660.html#a8640395\n\n> Without having looked at the code for a long time, I think the problem is what the\n> lucene scoring consider to be best. First the grams are searched, resulting in a number\n> of hits. Then the edit-distance is calculated on each hit. \"Genetics\" is appearently the\n> third most similar hit according to Lucene, but the best according to Levenshtein.\n>\n> I.e. Lucene does not use edit-distance as similarity. You need to get a bunch of best hits\n> in order to find the one with the smallest edit-distance.\n\nI took a look at the code, and my assessment seems to be right.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1916",
        "summary": "smartcn HHMM doc translation",
        "description": "My coworker Patricia Peng translated the documentation and code comments for smartcn HHMM package.\n",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2892",
        "summary": "Add QueryParser.newFieldQuery",
        "description": "Note: this patch changes no behavior, just makes QP more subclassable.\n\nCurrently we have Query getFieldQuery(String field, String queryText, boolean quoted)\nThis contains very hairy methods for producing a query from QP's analyzer.\n\nI propose we factor this into newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)\nThen getFieldQuery just calls newFieldQuery(this.analyzer, field, queryText, quoted);\n\nThe reasoning is: it can be quite useful to consider the double quote as more than phrases, but a \"more exact\" search.\nIn the case the user quoted the terms, you might want to analyze the text with an alternate analyzer that:\ndoesn't produce synonyms, doesnt decompose compounds, doesn't use WordDelimiterFilter \n(you would need to be using preserveOriginal=true at index time for the WDF one), etc etc.\n\nThis is similar to the way google's double quote operator works, its not defined as phrase but \"this exact wording or phrase\".\nFor example compare results to a query of tests versus \"tests\".\n\nCurrently you can do this without heavy code duplication, but really only if you make a separate field (which is wasteful),\nand make your custom QP lie about its field... in the examples I listed above you can do this with a single field, yet still\nhave a more exact phrase search.\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1973",
        "summary": "Remove deprecated query components",
        "description": "Remove the rest of the deprecated query components.",
        "label": "NUG",
        "classified": "CLEANUP",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2912",
        "summary": "remove field param from computeNorm, scorePayload ; remove UOE'd lengthNorm, switch SweetSpot to per-field ",
        "description": "In LUCENE-2236 we switched sim to per field (SimilarityProvider returns a per-field similarity).\n\nBut we didn't completely cleanup there... I think we should now do this:\n* SweetSpotSimilarity loses all its hashmaps. Instead, just configure one per field and return it in your SimilarityProvider. this means for example, all its TF factors can now be configured per-field too, not just the length normalization factors.\n* computeNorm and scorePayload lose their field parameter, as its redundant and confusing.\n* the UOE'd obselete lengthNorm is removed. I also updated javadocs that were pointing to it (this is bad!).\n\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1333",
        "summary": "Token implementation needs improvements",
        "description": "This was discussed in the thread (not sure which place is best to reference so here are two):\nhttp://mail-archives.apache.org/mod_mbox/lucene-java-dev/200805.mbox/%3C21F67CC2-EBB4-48A0-894E-FBA4AECC0D50@gmail.com%3E\nor to see it all at once:\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/62851\n\nIssues:\n1. JavaDoc is insufficient, leading one to read the code to figure out how to use the class.\n2. Deprecations are incomplete. The constructors that take String as an argument and the methods that take and/or return String should *all* be deprecated.\n3. The allocation policy is too aggressive. With large tokens the resulting buffer can be over-allocated. A less aggressive algorithm would be better. In the thread, the Python example is good as it is computationally simple.\n4. The parts of the code that currently use Token's deprecated methods can be upgraded now rather than waiting for 3.0. As it stands, filter chains that alternate between char[] and String are sub-optimal. Currently, it is used in core by Query classes. The rest are in contrib, mostly in analyzers.\n5. Some internal optimizations can be done with regard to char[] allocation.\n6. TokenStream has next() and next(Token), next() should be deprecated, so that reuse is maximized and descendant classes should be rewritten to over-ride next(Token)\n7. Tokens are often stored as a String in a Term. It would be good to add constructors that took a Token. This would simplify the use of the two together.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1913",
        "summary": "FastVectorHighlighter: AIOOBE occurs if one PhraseQuery is contained by another PhraseQuery",
        "description": "I'm very sorry but this is another one. If q=\"a b c d\" OR \"b c\", then ArrayIndexOutOfBoundsException occurs in FieldQuery.checkOverlap(). I'm working on this and fix with test case soon to be posted.\nThank you for your patient!\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-905",
        "summary": "left nav of docs/index.html in dist artifacts links to hudson for javadocs",
        "description": "When building the zip or tgz release artifacts, the docs/index.html file contained in that release (the starter point for people to read documentation) links \"API Docs\" to \nhttp://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/ instead of to ./api/index.html (the local copy of the javadocs)\n\nthis relates to the initial migration to hudson for the nightly builds and a plan to copy the javadocs back to lucene.apache.org that wasn't considered urgent since it was just for transient nightly docs, but a side affect is that the release documentation also links to hudson.\n\neven if we don't modify the nightly build process before the 2.2 release, we should update the link in the left nav in the 2.2 release branch before building the final release.",
        "label": "NUG",
        "classified": "BUILD_SYSTEM",
        "type": "BUG"
    },
    {
        "key": "LUCENE-467",
        "summary": "Use Float.floatToRawIntBits over Float.floatToIntBits",
        "description": "Copied From my Email:\n  Float.floatToRawIntBits (in Java1.4) gives the raw float bits without\nnormalization (like *(int*)&floatvar would in C).  Since it doesn't do\nnormalization of NaN values, it's faster (and hopefully optimized to a\nsimple inline machine instruction by the JVM).\n\nOn my Pentium4, using floatToRawIntBits is over 5 times as fast as\nfloatToIntBits.\nThat can really add up in something like Similarity.floatToByte() for\nencoding norms, especially if used as a way to compress an array of\nfloat during query time as suggested by Doug.",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-544",
        "summary": "MultiFieldQueryParser field boost multiplier",
        "description": "Allows specific boosting per field, e.g. +(name:foo^1 description:foo^0.1).\n\nWent from String[] field to MultiFieldQueryParser.FieldSetting[] field in constructor. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1647",
        "summary": "IndexReader.undeleteAll can mess up the deletion count stored in the segments file",
        "description": "Spinoff from LUCENE-1474.  I'll attach a test case showing the issue.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1457",
        "summary": "There are a few binary search implmentations in lucene that suffer from a now well known overflow bug",
        "description": "http://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html\n\nThe places I see it are:\n\nMultiSearcher.subSearcher(int)\nTermInfosReader.getIndexOffset(Term)\nMultiSegmentReader.readerIndex(int, int[], int)\nMergeDocIDRemapper.remap(int)\n\nI havn't taken much time to consider how likely any of these are to overflow. The values being averaged would have to be very large. That would rule out possible problems for at least a couple of these, but how about something like the MergeDocIDRemapper? Is there a document number that could be reached that has a chance of triggering this bug? If not we can close this and have a record of looking into it.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2770",
        "summary": "Optimize SegmentMerger to work on atomic (Segment)Readers where possible",
        "description": "This is a spin-off from LUCENE-2769:\n\nCurrently SegmentMerger has some optimizations when it merges segments that are SegmentReaders (e.g. when doing normal indexing or optimizing). But when you do IndexWriter.addIndexes(IndexReader...) the listed IndexReaders may not really be per-segment. SegmentMerger should track down all passed in reads down to the lowest level (Segment)Reader (or other atomic readers like SlowMultiReaderWrapper) and then merge. We can then remove most MultiFields usage (except term merging itsself) and clean up the code.\n\nThis especially saves lots of memory for merging norms, as no longer the duplicate norms arrays are created when MultiReaders are used!",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1134",
        "summary": "BooleanQuery.rewrite does not work properly for minNumberShouldMatch",
        "description": "BooleanQuery.rewrite does not respect minNumberShouldMatch if the number of clauses is 1. This causes inconsistencies for the queries \"+def\" and \"+abc +def\", while setting the minNumShouldMatch to '1' for both.\nFor the first query, results are returned although there are no SHOULD clauses in the query.\nFor the second query no results are returned.\nThe reason lies in the optimization BooleanQuery.rewrite has for one clauses queries.\nPatch included - optimize the query for a single clause only if the minNumShouldMatch <= 0.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1789",
        "summary": "getDocValues should provide a MultiReader DocValues abstraction",
        "description": "When scoring a ValueSourceQuery, the scoring code calls ValueSource.getValues(reader) on *each* leaf level subreader -- so DocValue instances are backed by the individual FieldCache entries of the subreaders -- but if Client code were to inadvertently  called getValues() on a MultiReader (or DirectoryReader) they would wind up using the \"outer\" FieldCache.\n\nSince getValues(IndexReader) returns DocValues, we have an advantage here that we don't have with FieldCache API (which is required to provide direct array access). getValues(IndexReader) could be implimented so that *IF* some a caller inadvertently passes in a reader with non-null subReaders, getValues could generate a DocValues instance for each of the subReaders, and then wrap them in a composite \"MultiDocValues\".\n\n\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-921",
        "summary": "IndexReader.FieldOption has incomplete Javadocs",
        "description": "IndexReader.FieldOption has no javadocs at all.",
        "label": "NUG",
        "classified": "DOCUMENTATION",
        "type": ""
    },
    {
        "key": "LUCENE-1016",
        "summary": "TermVectorAccessor, transparent vector space access ",
        "description": "This class visits TermVectorMapper and populates it with information transparent by either passing it down to the default terms cache (documents indexed with Field.TermVector) or by resolving the inverted index.",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-2003",
        "summary": "Highlighter has problems when you use StandardAnalyzer with LUCENE_29 or simplier StopFilter with stopWordsPosIncr mode switched on",
        "description": "This is a followup on LUCENE-1987:\n\nIf you set in HighligterTest the constant static final Version TEST_VERSION = Version.LUCENE_24 to LUCENE_29 or LUCENE_CURRENT, the test testSimpleQueryScorerPhraseHighlighting fails. Please note, that currently (before LUCENE-2002 is fixed), you must also set the QueryParser to respect posIncr.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-843",
        "summary": "improve how IndexWriter uses RAM to buffer added documents",
        "description": "I'm working on a new class (MultiDocumentWriter) that writes more than\none document directly into a single Lucene segment, more efficiently\nthan the current approach.\n\nThis only affects the creation of an initial segment from added\ndocuments.  I haven't changed anything after that, eg how segments are\nmerged.\n\nThe basic ideas are:\n\n  * Write stored fields and term vectors directly to disk (don't\n    use up RAM for these).\n\n  * Gather posting lists & term infos in RAM, but periodically do\n    in-RAM merges.  Once RAM is full, flush buffers to disk (and\n    merge them later when it's time to make a real segment).\n\n  * Recycle objects/buffers to reduce time/stress in GC.\n\n  * Other various optimizations.\n\nSome of these changes are similar to how KinoSearch builds a segment.\nBut, I haven't made any changes to Lucene's file format nor added\nrequirements for a global fields schema.\n\nSo far the only externally visible change is a new method\n\"setRAMBufferSize\" in IndexWriter (and setMaxBufferedDocs is\ndeprecated) so that it flushes according to RAM usage and not a fixed\nnumber documents added.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1836",
        "summary": "Flexible QueryParser fails with local different from en_US",
        "description": "I get the following error during the mentioned testcases on my computer, if I use the Locale de_DE (windows 32):\n\n{code}\n    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQPHelper\n    [junit] Tests run: 29, Failures: 1, Errors: 0, Time elapsed: 1,156 sec\n    [junit]\n    [junit] ------------- Standard Output ---------------\n    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQPHelper): FAILED\n    [junit] expected:<1> but was:<0>\n    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>\n    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.assertHits(TestQPHelper.java:1148)\n    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.testLocalDateFormat(TestQPHelper.java:1005)\n    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.queryParser.standard.TestQPHelper FAILED\n    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQueryParserWrapper\n    [junit] Tests run: 27, Failures: 1, Errors: 0, Time elapsed: 1,219 sec\n    [junit]\n    [junit] ------------- Standard Output ---------------\n    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQueryParserWrapper):       FAILED\n    [junit] expected:<1> but was:<0>\n    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>\n    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.assertHits(TestQueryParserWrapper.java:1120)\n    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.testLocalDateFormat(TestQueryParserWrapper.java:985)\n    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.queryParser.standard.TestQueryParserWrapper FAILED\n{code}\n\nWith en_US as locale it works.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-2419",
        "summary": "Improve parallel tests",
        "description": "As mentioned on the dev@ mailing list here: http://www.lucidimagination.com/search/document/93432a677917b9bd/lucenejunitresultformatter_sometimes_fails_to_lock\n\nIt would be useful to not create a lockfactory for each test suite (As they are run sequentially in the same separate JVM).\nAdditionally, we create a lot of JVMs (26) for each batch, because we have to run one for each letter.\nInstead, we use a technique here to divide up the tests with a custom selector: http://blog.code-cop.org/2009/09/parallel-junit.html\n(I emailed the blog author and received permission to use this code)\n\nThis gives a nice boost to the speed of overall tests, especially Solr tests, as many start with an \"S\", but this is no longer a problem.\n\n",
        "label": "NUG",
        "classified": "TEST",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-934",
        "summary": "Uploade Lucene 2.1 to ibiblio",
        "description": "Please uploaded Lucene (specifically lucene-core) 2.1.0 to ibiblio. I see 2.0.0 but not 2.1.0.\n\nThanks!",
        "label": "NUG",
        "classified": "TASK",
        "type": "TASK"
    },
    {
        "key": "LUCENE-1346",
        "summary": "replace Vector with ArrayList in Queries",
        "description": "Replace Vector with ArrayList in Queries.  This can make a difference in heavily concurrent scenarios when Query objects are examined or compared (e.g. used as cache keys).",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-540",
        "summary": "Merging multiple indexes does not maintain document order.",
        "description": "When I merge multiple indexes into a single, empty index, the document addition order is not being maintained.\n\nSelf contained test case coming (as soon as I figure out how to attach it)",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1484",
        "summary": "Remove SegmentReader.document synchronization",
        "description": "This is probably the last synchronization issue in Lucene.  It is the document method in SegmentReader.  It is avoidable by using a threadlocal for FieldsReader.  ",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-1007",
        "summary": "Flexibility to turn on/off any flush triggers",
        "description": "See discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/53186\n\nProvide the flexibility to turn on/off any flush triggers - ramBufferSize, maxBufferedDocs and maxBufferedDeleteTerms. One of ramBufferSize and maxBufferedDocs must be enabled.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-2090",
        "summary": "convert automaton to char[] based processing and TermRef / TermsEnum api",
        "description": "The automaton processing is currently done with String, mostly because TermEnum is based on String.\nit is easy to change the processing to work with char[], since behind the scenes this is used anyway.\n\nin general I think we should make sure char[] based processing is exposed in the automaton pkg anyway, for things like pattern-based tokenizers and such.\n",
        "label": "NUG",
        "classified": "IMPROVEMENT",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-352",
        "summary": "[PATCH] NullPointerException when using nested SpanOrQuery in SpanNotQuery",
        "description": "Overview description: \nI'm using the span query classes in Lucene to generate higher scores for \nsearch results where the search terms are closer together. In certain \nsituations I want to exclude terms from the span. When I attempt to exclude \nmore than one term I get an error. \n \nThe example query I'm using is:  \n \n'brighton AND tourism' -pier -contents \n \nI construct the query objects and the toString() version is: \n \nspanNot(spanNear([contents:brighton contents:tourism], 10, false), \nspanOr([contents:pier, contents:road])) \n  \n \nSteps to reproduce: \n1. Construct a SpanNearQuery (must have at least one term, but at least two \nmakes more sense) \n2. Construct a SpanOrQuery containing two or more terms \n3. Construct a SpanNotQuery to include the first query object and exclude the \nsecond (SpanOrQuery) \n4. Execute the search \n \n \nActual Results: \nA null pointer exception is thrown while generating the scores within the \nsearch. \n \nStack trace:  \njava.lang.NullPointerException   \n        at   \norg.apache.lucene.search.spans.SpanOrQuery$1.doc(SpanOrQuery.java:174)   \n        at   \norg.apache.lucene.search.spans.SpanNotQuery$1.next(SpanNotQuery.java:75)   \n        at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:50)   \n        at org.apache.lucene.search.Scorer.score(Scorer.java:37)   \n        at   \norg.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)   \n        at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)   \n        at org.apache.lucene.search.Hits.<init>(Hits.java:43)   \n        at org.apache.lucene.search.Searcher.search(Searcher.java:33)   \n        at org.apache.lucene.search.Searcher.search(Searcher.java:27)   \n        at   \ncom.runtimecollective.search.LuceneSearch.search(LuceneSearch.java:362)   \n \n \nExpected Resuts: \nIt executes the search and results where the first search terms (near query) \nare close together but without the second terms (or query) appearing.",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3063",
        "summary": "factor CharTokenizer/CharacterUtils into analyzers module",
        "description": "Currently these analysis components are in the lucene core, but should really\nbe .util in the analyzers module.\n\nAlso, with MockTokenizer extending Tokenizer directly, we can add some additional\nchecks in the future to try to ensure our consumers are being good consumers (e.g. calling reset).\n\nThis is mentioned in http://wiki.apache.org/lucene-java/TestIdeas, I didn't implement it here yet,\nthis is just the factoring. I think we should try to do this before LUCENE-3040.\n",
        "label": "NUG",
        "classified": "REFACTORING",
        "type": "TASK"
    },
    {
        "key": "LUCENE-2734",
        "summary": "Use IndexWriterConfig in benchmark",
        "description": "We should use IndexWriterConfig instead of deprecated methods in benchmark. ",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-421",
        "summary": "Numeric range searching with large value sets",
        "description": "I have a set of enhancements that build on the numeric sorting cache introduced\nby Tim Jones and that provide integer and floating point range searches over\nnumeric ranges that are far too large to be implemented via the current term\nrange rewrite mechanism.  I'm new to Apache and trying to find out how to attach\nthe source files for the changes for your consideration.",
        "label": "NUG",
        "classified": "RFE",
        "type": "IMPROVEMENT"
    },
    {
        "key": "LUCENE-3035",
        "summary": "TestIndexWriter.testCommitThreadSafety fails on realtime_search branch",
        "description": "Hudson failed on RT with this error - I wasn't able to reproduce yet....\n\n{noformat}\nNOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3\nNOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3\nThe following exceptions were thrown by threads:\n*** Thread: Thread-331 ***\njava.lang.RuntimeException: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>\n\tat org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2416)\nCaused by: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>\n\tat org.junit.Assert.fail(Assert.java:91)\n\tat org.junit.Assert.failNotEquals(Assert.java:645)\n\tat org.junit.Assert.assertEquals(Assert.java:126)\n\tat org.junit.Assert.assertEquals(Assert.java:470)\n\tat org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2410)\nNOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=MockVariableIntBlock(baseBlockSize=91), f7=MockFixedIntBlock(blockSize=1289), f8=Standard, f9=MockRandom, f1=MockSep, f0=Pulsing(freqCutoff=15), f3=Pulsing(freqCutoff=15), f2=MockFixedIntBlock(blockSize=1289), f5=MockVariableIntBlock(baseBlockSize=91), f4=MockRandom, f=MockSep, c=MockVariableIntBlock(baseBlockSize=91), termVector=SimpleText, d9=SimpleText, d8=MockSep, d5=MockVariableIntBlock(baseBlockSize=91), d4=MockRandom, d7=Standard, d6=SimpleText, d25=Standard, d0=MockVariableIntBlock(baseBlockSize=91), c29=Standard, d24=SimpleText, d1=MockFixedIntBlock(blockSize=1289), c28=MockFixedIntBlock(blockSize=1289), d23=MockVariableIntBlock(baseBlockSize=91), d2=Standard, c27=MockVariableIntBlock(baseBlockSize=91), d22=MockRandom, d3=MockRandom, d21=MockFixedIntBlock(blockSize=1289), d20=MockVariableIntBlock(baseBlockSize=91), c22=MockVariableIntBlock(baseBlockSize=91), c21=MockRandom, c20=Pulsing(freqCutoff=15), d29=MockVariableIntBlock(baseBlockSize=91), c26=SimpleText, d28=MockRandom, c25=MockSep, d27=Pulsing(freqCutoff=15), c24=MockRandom, d26=MockFixedIntBlock(blockSize=1289), c23=Standard, e9=MockRandom, e8=MockFixedIntBlock(blockSize=1289), e7=MockVariableIntBlock(baseBlockSize=91), e6=MockSep, e5=Pulsing(freqCutoff=15), c17=Standard, e3=MockFixedIntBlock(blockSize=1289), d12=SimpleText, c16=SimpleText, e4=Pulsing(freqCutoff=15), d11=MockSep, c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=Pulsing(freqCutoff=15), e2=SimpleText, d13=MockFixedIntBlock(blockSize=1289), e0=Standard, d10=Standard, d19=Pulsing(freqCutoff=15), c11=SimpleText, c10=MockSep, d16=MockRandom, c13=MockSep, c12=Pulsing(freqCutoff=15), d15=Standard, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1289), d17=MockSep, c14=MockVariableIntBlock(baseBlockSize=91), b3=MockRandom, b2=Standard, b5=SimpleText, b4=MockSep, b7=MockSep, b6=Pulsing(freqCutoff=15), d50=MockVariableIntBlock(baseBlockSize=91), b9=MockFixedIntBlock(blockSize=1289), b8=MockVariableIntBlock(baseBlockSize=91), d43=Pulsing(freqCutoff=15), d42=MockFixedIntBlock(blockSize=1289), d41=SimpleText, d40=MockSep, d47=MockRandom, d46=Standard, b0=SimpleText, d45=MockFixedIntBlock(blockSize=1289), b1=Standard, d44=MockVariableIntBlock(baseBlockSize=91), d49=MockSep, d48=Pulsing(freqCutoff=15), c6=MockVariableIntBlock(baseBlockSize=91), c5=MockRandom, c4=Pulsing(freqCutoff=15), c3=MockFixedIntBlock(blockSize=1289), c9=MockSep, c8=MockRandom, c7=Standard, d30=MockFixedIntBlock(blockSize=1289), d32=MockRandom, d31=Standard, c1=MockVariableIntBlock(baseBlockSize=91), d34=Standard, c2=MockFixedIntBlock(blockSize=1289), d33=SimpleText, d36=MockSep, c0=MockSep, d35=Pulsing(freqCutoff=15), d38=MockVariableIntBlock(baseBlockSize=91), d37=MockRandom, d39=SimpleText, e92=MockFixedIntBlock(blockSize=1289), e93=Pulsing(freqCutoff=15), e90=MockSep, e91=SimpleText, e89=MockVariableIntBlock(baseBlockSize=91), e88=MockSep, e87=Pulsing(freqCutoff=15), e86=SimpleText, e85=MockSep, e84=MockRandom, e83=Standard, e80=MockFixedIntBlock(blockSize=1289), e81=Standard, e82=MockRandom, e77=MockVariableIntBlock(baseBlockSize=91), e76=MockRandom, e79=Standard, e78=SimpleText, e73=MockSep, e72=Pulsing(freqCutoff=15), e75=MockFixedIntBlock(blockSize=1289), e74=MockVariableIntBlock(baseBlockSize=91), binary=MockVariableIntBlock(baseBlockSize=91), f98=Pulsing(freqCutoff=15), f97=MockFixedIntBlock(blockSize=1289), f99=MockRandom, f94=Standard, f93=SimpleText, f96=MockSep, f95=Pulsing(freqCutoff=15), e95=SimpleText, e94=MockSep, e97=Pulsing(freqCutoff=15), e96=MockFixedIntBlock(blockSize=1289), e99=MockFixedIntBlock(blockSize=1289), e98=MockVariableIntBlock(baseBlockSize=91), id=MockRandom, f34=Standard, f33=SimpleText, f32=MockVariableIntBlock(baseBlockSize=91), f31=MockRandom, f30=MockFixedIntBlock(blockSize=1289), f39=Standard, f38=MockVariableIntBlock(baseBlockSize=91), f37=MockRandom, f36=Pulsing(freqCutoff=15), f35=MockFixedIntBlock(blockSize=1289), f43=MockSep, f42=Pulsing(freqCutoff=15), f45=MockFixedIntBlock(blockSize=1289), f44=MockVariableIntBlock(baseBlockSize=91), f41=SimpleText, f40=MockSep, f47=Standard, f46=SimpleText, f49=MockSep, f48=Pulsing(freqCutoff=15), content=MockSep, e19=SimpleText, e18=MockSep, e17=Standard, f12=SimpleText, e16=SimpleText, f11=MockSep, f10=MockRandom, e15=MockVariableIntBlock(baseBlockSize=91), e14=MockRandom, f16=MockRandom, e13=MockSep, f15=Standard, e12=Pulsing(freqCutoff=15), e11=Standard, f14=MockFixedIntBlock(blockSize=1289), e10=SimpleText, f13=MockVariableIntBlock(baseBlockSize=91), f19=Pulsing(freqCutoff=15), f18=Standard, f17=SimpleText, e29=MockRandom, e26=MockSep, f21=Pulsing(freqCutoff=15), e25=Pulsing(freqCutoff=15), f20=MockFixedIntBlock(blockSize=1289), e28=MockFixedIntBlock(blockSize=1289), f23=MockVariableIntBlock(baseBlockSize=91), e27=MockVariableIntBlock(baseBlockSize=91), f22=MockRandom, f25=SimpleText, e22=MockFixedIntBlock(blockSize=1289), f24=MockSep, e21=MockVariableIntBlock(baseBlockSize=91), f27=Pulsing(freqCutoff=15), e24=MockRandom, f26=MockFixedIntBlock(blockSize=1289), e23=Standard, f29=MockFixedIntBlock(blockSize=1289), f28=MockVariableIntBlock(baseBlockSize=91), e20=Pulsing(freqCutoff=15), field=MockRandom, string=Standard, e30=MockRandom, e31=MockVariableIntBlock(baseBlockSize=91), a98=Standard, e34=MockSep, a99=MockRandom, e35=SimpleText, f79=MockSep, e32=Standard, e33=MockRandom, b97=MockRandom, f77=MockRandom, e38=Standard, b98=MockVariableIntBlock(baseBlockSize=91), f78=MockVariableIntBlock(baseBlockSize=91), e39=MockRandom, b99=SimpleText, f75=MockFixedIntBlock(blockSize=1289), e36=MockVariableIntBlock(baseBlockSize=91), f76=Pulsing(freqCutoff=15), e37=MockFixedIntBlock(blockSize=1289), f73=Pulsing(freqCutoff=15), f74=MockSep, f71=SimpleText, f72=Standard, f81=MockFixedIntBlock(blockSize=1289), f80=MockVariableIntBlock(baseBlockSize=91), e40=Standard, e41=Pulsing(freqCutoff=15), e42=MockSep, e43=MockFixedIntBlock(blockSize=1289), e44=Pulsing(freqCutoff=15), e45=MockRandom, e46=MockVariableIntBlock(baseBlockSize=91), f86=SimpleText, e47=MockSep, f87=Standard, e48=SimpleText, f88=Pulsing(freqCutoff=15), e49=MockFixedIntBlock(blockSize=1289), f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=91), f83=MockFixedIntBlock(blockSize=1289), f84=Standard, f85=MockRandom, f90=MockRandom, f92=SimpleText, f91=MockSep, str=MockSep, a76=SimpleText, e56=SimpleText, f59=MockVariableIntBlock(baseBlockSize=91), a77=Standard, e57=Standard, a78=Pulsing(freqCutoff=15), e54=MockRandom, f57=Pulsing(freqCutoff=15), a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=91), f58=MockSep, e52=MockVariableIntBlock(baseBlockSize=91), e53=MockFixedIntBlock(blockSize=1289), e50=Pulsing(freqCutoff=15), e51=MockSep, f51=MockFixedIntBlock(blockSize=1289), f52=Pulsing(freqCutoff=15), f50=SimpleText, f55=Standard, f56=MockRandom, f53=MockVariableIntBlock(baseBlockSize=91), e58=MockFixedIntBlock(blockSize=1289), f54=MockFixedIntBlock(blockSize=1289), e59=Pulsing(freqCutoff=15), a80=MockRandom, e60=MockRandom, a82=SimpleText, a81=MockSep, a84=MockSep, a83=Pulsing(freqCutoff=15), a86=MockFixedIntBlock(blockSize=1289), a85=MockVariableIntBlock(baseBlockSize=91), a89=Standard, f68=Standard, e65=Pulsing(freqCutoff=15), f69=MockRandom, e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=91), e67=MockVariableIntBlock(baseBlockSize=91), a88=MockFixedIntBlock(blockSize=1289), e68=MockFixedIntBlock(blockSize=1289), e61=Standard, e62=MockRandom, e63=MockSep, e64=SimpleText, f60=MockRandom, f61=MockVariableIntBlock(baseBlockSize=91), f62=SimpleText, f63=Standard, e69=SimpleText, f64=MockSep, f65=SimpleText, f66=MockFixedIntBlock(blockSize=1289), f67=Pulsing(freqCutoff=15), f70=MockSep, a93=MockVariableIntBlock(baseBlockSize=91), a92=MockRandom, a91=Pulsing(freqCutoff=15), e71=Pulsing(freqCutoff=15), a90=MockFixedIntBlock(blockSize=1289), e70=MockFixedIntBlock(blockSize=1289), a97=SimpleText, a96=MockSep, a95=MockRandom, a94=Standard, c58=MockFixedIntBlock(blockSize=1289), a63=MockVariableIntBlock(baseBlockSize=91), a64=MockFixedIntBlock(blockSize=1289), c59=Pulsing(freqCutoff=15), c56=MockSep, d59=MockRandom, a61=Pulsing(freqCutoff=15), c57=SimpleText, a62=MockSep, c54=SimpleText, c55=Standard, a60=SimpleText, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=91), d53=Standard, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=91), d52=MockFixedIntBlock(blockSize=1289), d57=Pulsing(freqCutoff=15), b62=MockFixedIntBlock(blockSize=1289), d58=MockSep, b63=Pulsing(freqCutoff=15), d55=SimpleText, b60=MockSep, d56=Standard, b61=SimpleText, b56=SimpleText, b55=MockSep, b54=MockRandom, b53=Standard, d61=SimpleText, b59=MockVariableIntBlock(baseBlockSize=91), d60=MockSep, b58=MockSep, b57=Pulsing(freqCutoff=15), c62=MockSep, c61=Pulsing(freqCutoff=15), a59=Pulsing(freqCutoff=15), c60=Standard, a58=MockFixedIntBlock(blockSize=1289), a57=MockSep, a56=Pulsing(freqCutoff=15), a55=Standard, a54=SimpleText, a72=Standard, c67=MockRandom, a73=MockRandom, c68=MockVariableIntBlock(baseBlockSize=91), a74=MockSep, c69=SimpleText, a75=SimpleText, c63=Pulsing(freqCutoff=15), c64=MockSep, a70=MockRandom, c65=MockVariableIntBlock(baseBlockSize=91), a71=MockVariableIntBlock(baseBlockSize=91), c66=MockFixedIntBlock(blockSize=1289), d62=MockSep, d63=SimpleText, d64=MockFixedIntBlock(blockSize=1289), b70=MockFixedIntBlock(blockSize=1289), d65=Pulsing(freqCutoff=15), b71=MockRandom, d66=MockVariableIntBlock(baseBlockSize=91), b72=MockVariableIntBlock(baseBlockSize=91), d67=MockFixedIntBlock(blockSize=1289), b73=SimpleText, d68=Standard, b74=Standard, d69=MockRandom, b65=Pulsing(freqCutoff=15), b64=MockFixedIntBlock(blockSize=1289), b67=MockVariableIntBlock(baseBlockSize=91), b66=MockRandom, d70=Pulsing(freqCutoff=15), b69=MockRandom, b68=Standard, d72=MockVariableIntBlock(baseBlockSize=91), d71=MockRandom, c71=MockFixedIntBlock(blockSize=1289), c70=MockVariableIntBlock(baseBlockSize=91), a69=SimpleText, c73=MockRandom, c72=Standard, a66=MockFixedIntBlock(blockSize=1289), a65=MockVariableIntBlock(baseBlockSize=91), a68=MockRandom, a67=Standard, c32=MockSep, c33=SimpleText, c30=Standard, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=91), a41=MockRandom, c37=MockFixedIntBlock(blockSize=1289), a42=MockVariableIntBlock(baseBlockSize=91), a0=SimpleText, c34=Pulsing(freqCutoff=15), c35=MockSep, a40=Pulsing(freqCutoff=15), b84=Pulsing(freqCutoff=15), d79=MockSep, b85=MockSep, b82=SimpleText, d77=Standard, c38=SimpleText, b83=Standard, d78=MockRandom, c39=Standard, b80=Standard, d75=MockRandom, b81=MockRandom, d76=MockVariableIntBlock(baseBlockSize=91), d73=MockFixedIntBlock(blockSize=1289), d74=Pulsing(freqCutoff=15), d83=Standard, a9=MockSep, d82=SimpleText, d81=MockVariableIntBlock(baseBlockSize=91), d80=MockRandom, b79=MockSep, b78=Standard, b77=SimpleText, b76=MockVariableIntBlock(baseBlockSize=91), b75=MockRandom, a1=SimpleText, a35=Pulsing(freqCutoff=15), a2=Standard, a34=MockFixedIntBlock(blockSize=1289), a3=Pulsing(freqCutoff=15), a33=SimpleText, a4=MockSep, a32=MockSep, a5=MockFixedIntBlock(blockSize=1289), a39=MockRandom, c40=Pulsing(freqCutoff=15), a6=Pulsing(freqCutoff=15), a38=Standard, a7=MockRandom, a37=MockFixedIntBlock(blockSize=1289), a8=MockVariableIntBlock(baseBlockSize=91), a36=MockVariableIntBlock(baseBlockSize=91), c41=MockFixedIntBlock(blockSize=1289), c42=Pulsing(freqCutoff=15), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=91), c45=Standard, a50=SimpleText, c46=MockRandom, a51=Standard, c47=MockSep, a52=Pulsing(freqCutoff=15), c48=SimpleText, a53=MockSep, b93=MockVariableIntBlock(baseBlockSize=91), d88=MockFixedIntBlock(blockSize=1289), c49=MockVariableIntBlock(baseBlockSize=91), b94=MockFixedIntBlock(blockSize=1289), d89=Pulsing(freqCutoff=15), b95=Standard, b96=MockRandom, d84=SimpleText, b90=SimpleText, d85=Standard, b91=MockFixedIntBlock(blockSize=1289), d86=Pulsing(freqCutoff=15), b92=Pulsing(freqCutoff=15), d87=MockSep, d92=MockSep, d91=Pulsing(freqCutoff=15), d94=MockFixedIntBlock(blockSize=1289), d93=MockVariableIntBlock(baseBlockSize=91), b87=MockSep, b86=Pulsing(freqCutoff=15), d90=SimpleText, b89=MockFixedIntBlock(blockSize=1289), b88=MockVariableIntBlock(baseBlockSize=91), a44=MockVariableIntBlock(baseBlockSize=91), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=Standard, a49=MockFixedIntBlock(blockSize=1289), c50=SimpleText, d98=MockFixedIntBlock(blockSize=1289), d97=MockVariableIntBlock(baseBlockSize=91), d96=MockSep, d95=Pulsing(freqCutoff=15), d99=MockRandom, a20=MockRandom, c99=MockVariableIntBlock(baseBlockSize=91), c98=MockRandom, c97=Pulsing(freqCutoff=15), c96=MockFixedIntBlock(blockSize=1289), b19=MockVariableIntBlock(baseBlockSize=91), a16=SimpleText, a17=Standard, b17=Pulsing(freqCutoff=15), a14=MockRandom, b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=91), a12=MockVariableIntBlock(baseBlockSize=91), a13=MockFixedIntBlock(blockSize=1289), a10=Pulsing(freqCutoff=15), a11=MockSep, b11=MockFixedIntBlock(blockSize=1289), b12=Pulsing(freqCutoff=15), b10=SimpleText, b15=Standard, b16=MockRandom, a18=MockFixedIntBlock(blockSize=1289), b13=MockVariableIntBlock(baseBlockSize=91), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1289), b30=MockSep, a31=Pulsing(freqCutoff=15), a30=MockFixedIntBlock(blockSize=1289), b28=Standard, a25=Pulsing(freqCutoff=15), b29=MockRandom, a26=MockSep, a27=MockVariableIntBlock(baseBlockSize=91), a28=MockFixedIntBlock(blockSize=1289), a21=Standard, a22=MockRandom, a23=MockSep, a24=SimpleText, b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=91), b22=SimpleText, b23=Standard, a29=SimpleText, b24=MockSep, b25=SimpleText, b26=MockFixedIntBlock(blockSize=1289), b27=Pulsing(freqCutoff=15), b41=MockFixedIntBlock(blockSize=1289), b40=MockVariableIntBlock(baseBlockSize=91), c77=MockRandom, c76=Standard, c75=MockFixedIntBlock(blockSize=1289), c74=MockVariableIntBlock(baseBlockSize=91), c79=Standard, c78=SimpleText, c80=MockVariableIntBlock(baseBlockSize=91), c83=MockSep, c84=SimpleText, c81=Standard, b39=MockSep, c82=MockRandom, b37=MockRandom, b38=MockVariableIntBlock(baseBlockSize=91), b35=MockFixedIntBlock(blockSize=1289), b36=Pulsing(freqCutoff=15), b33=Pulsing(freqCutoff=15), b34=MockSep, b31=SimpleText, b32=Standard, str2=Pulsing(freqCutoff=15), b50=MockRandom, b52=SimpleText, str3=MockRandom, b51=MockSep, c86=SimpleText, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=15), c87=MockFixedIntBlock(blockSize=1289), c89=MockVariableIntBlock(baseBlockSize=91), c90=Pulsing(freqCutoff=15), c91=MockSep, c92=MockFixedIntBlock(blockSize=1289), c93=Pulsing(freqCutoff=15), c94=MockRandom, c95=MockVariableIntBlock(baseBlockSize=91), content1=MockSep, b46=SimpleText, b47=Standard, content3=Standard, b48=Pulsing(freqCutoff=15), content4=SimpleText, b49=MockSep, content5=MockRandom, b42=MockVariableIntBlock(baseBlockSize=91), b43=MockFixedIntBlock(blockSize=1289), b44=Standard, b45=MockRandom}, locale=lv_LV, timezone=Australia/Lindeman\nNOTE: all tests run in this JVM:\n[TestNumericTokenStream, TestIndexFileDeleter, TestIndexInput, TestIndexReaderCloneNorms, TestIndexReaderReopen, TestIndexWriter]\nNOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=44228576,total=213778432\n{noformat}",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-3575",
        "summary": "Field names can be wrong for stored fields / term vectors after merging",
        "description": "The good news is this bug only exists in trunk... the bad news is it's\nbeen here for some time (created by accident in LUCENE-2881).  But the\ngood news is it should strike fairly rarely.\n\nSegmentMerger sometimes incorrectly thinks it can bulk-copy TVs/stored\nfields when it cannot (because field numbers don't map to the same\nnames across segments).\n\nI think it happens only with addIndexes, or indexes that have\npre-trunk segments, and then SM falsely thinks it can bulk-merge only\nwhen the last field number has the same field name across segments.\n",
        "label": "BUG",
        "classified": "BUG",
        "type": "BUG"
    },
    {
        "key": "LUCENE-1461",
        "summary": "Cached filter for a single term field",
        "description": "These classes implement inexpensive range filtering over a field containing a single term. They do this by building an integer array of term numbers (storing the term->number mapping in a TreeMap) and then implementing a fast integer comparison based DocSetIdIterator.\n\nThis code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. I have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. \n\nThe code here is fairly rough; it works but lacks javadocs and toString() and hashCode() methods etc. I'm posting it here to discover if there is other interest in this feature; I don't mind fixing it up but would hate to go to the effort if it's not going to make it into Lucene.\n\n",
        "label": "NUG",
        "classified": "RFE",
        "type": "RFE"
    },
    {
        "key": "LUCENE-433",
        "summary": "Issue LUCENE-352 was closed, but the patch there is not applied in the current trunk",
        "description": "See here:\nhttp://issues.apache.org/jira/browse/LUCENE-352\n\nAnd thanks for making JIRA easier, I noticed the Lucene Java project\nwas preselected for me.\n\nRegards,\nPaul Elschot",
        "label": "NUG",
        "classified": "OTHER",
        "type": "BUG"
    }
]